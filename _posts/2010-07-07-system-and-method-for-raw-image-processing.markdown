---

title: System and method for RAW image processing
abstract: An automated RAW image processing method and system are disclosed. A RAW image and metadata related to the RAW image are obtained from a digital camera or other source. The RAW image and the related metadata are automatically processed using an Operating System service of a processing device to produce a resulting image in an absolute color space. The resulting image is then made available to an application program executing on the processing device through an application program interface with the Operating System service.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08493473&OS=08493473&RS=08493473
owner: Apple Inc.
number: 08493473
owner_city: Cupertino
owner_country: US
publication_date: 20100707
---
This is a continuation of U.S. application Ser. No. 11 756 906 which is a non provisional of U.S. Provisional Application Ser. No. 60 829 519 filed 13 Oct. 2006 and entitled System and Method for Raw Image Processing which are each incorporated herein by reference in their entireties and to which priority is claimed.

The subject matter of the present disclosure generally relates to processing RAW images into an absolute color space and more particularly to a system and method for pre processing RAW images using an operating system service of a computer or other processing device.

The digital image sensor such as a charged coupled device CCD of a digital camera has a plurality of photo sites arranged in a colored filtered array or pattern such as a RGB Bayer pattern described in U.S. Pat. No. 3 971 065. In the RGB Bayer pattern each photo site is filtered so that it is receptive to either red green or blue or some variation thereof. The original bit for bit digital image file captured by the digital imaging sensor is referred to as a RAW file or RAW image. Depending on a number of variables the RAW image may typically require 8 to 18 MB of memory space. The type of colored filter array and digital imaging sensor varies typically based on the manufacture of the digital camera. For example some color filtered arrays use a pattern of yellow cyan green and magenta.

Typically the digital camera has an image pipeline that performs a demosaicing or de Bayering process on the RAW image and transforms the image with a compressing algorithm to output a JPEG or other type of compressed file suitable for display and viewing. However the RAW image captured by the digital camera can be uploaded to a computer and computer software such as Apple s Aperture 1.0 operating on the computer can allow a user to perform various manual operations on RAW image.

The color information of processed digital images can be characterized by a number of color models. One color model is the RGB color space which uses combinations of red R green G and blue B to produce a plurality of colors. Some RGB color spaces used for digital cameras include Standard RGB sRGB and Adobe RGB. Another color model is the CIE XYZ color space created by the International Commission on Illumination CIE in 1931. Mathematical techniques can be used to transform color information from one color space to another.

Digital cameras have white balance settings such as auto incandescent fluorescent cloudy sunny or sensitivity e.g. ISO 100 ISO 2000 etc . The settings are used to match the white balance of the digital camera to the color temperature of light from the illuminant that is illuminating the subject in the image. Characteristics of various standard illuminants are defined in the art. For example illuminant A is used to represent incandescent lighting and is defined by the profile of a black body radiator at 2856 K. Illuminant series D is used to represent natural daylight. A number is used in the name of the D series illuminant to indicate the correlated color temperature CCT of the source. For example illuminant D50 has a CCT of 5000 K and illuminant D65 has one of 6500 K. Illuminant series F is used to represents various types of fluorescent lighting. For example illuminant F2 represents cool white fluorescent while illuminant F11 represents a narrow band fluorescent. When a digital image is obtained under one illuminant with a digital camera it is possible to use a white point conversion technique to estimate the characteristics of the image under a different illuminant.

Various considerations must be addressed when processing a digital image obtained with a digital camera or other imaging device. One consideration involves preserving the spatial quality and the detail of the digital image while another consideration involves sufficiently representing the color of the digital image. In many ways these two considerations are interrelated.

While the subject matter of the present disclosure is susceptible to various modifications and alternative forms specific embodiments thereof have been shown by way of example in the drawings and are herein described in detail. The figures and written description are not intended to limit the scope of the inventive concepts in any manner. Rather the figures and written description are provided to illustrate the inventive concepts to a person skilled in the art by reference to particular embodiments as required by 35 U.S.C. 112.

Referring to one embodiment of a system for generating and processing RAW images is schematically illustrated. The system includes an imaging device and a general purpose processing device . In general the imaging device and the general purpose processing device may integrated together into one device such as a digital still camera or the like that has requisite processing and memory capabilities. Alternatively the devices and may be separate components as shown in . For example the imaging device can be a digital still camera a camera phone or the like and the processing device can be a computer laptop or the like. In the description that follows the imaging device is referred to as a camera and the processing device is referred to as a computer for illustrative purposes.

The camera includes an imaging sensor for capturing digital images. The imaging sensor can be any of the various types of imaging sensors known and used in the art such as a charged coupled device. The camera also has processor and memory hardware and an interface for communicating data to the computer . In turn the computer has an interface processor and memory hardware operating system and software and an output device .

In use the imaging sensor of the camera captures a RAW image . As discussed previously the imaging sensor has a color filtered array that can be arranged in an RGB Bayer pattern. Therefore the color value at each photo site in the RAW image represents either a red intensity value a green intensity value or a blue intensity value and each value is typically 10 bits in the range of 0 to 4095. Other camera manufactures may have other color filter arrays so that the RAW image may have different information. For a Sony 828 digital camera for example the color values are in four channel RGBE coordinates representing Red Green Blue and Emerald. For descriptive purposes the discussion that follows will refer to the RAW image having RGB values.

After capturing the RAW image the processor memory hardware can output the RAW image via the interface which can be a Universal Serial Bus USB interface. The computer can then receives the RAW image with its interface . Preferably the computer stores the RAW image as a master file to maintain an original copy of the RAW image while pre processing described below is performed on a copy of the RAW image . In an alternative to importing the RAW image from the camera it can be imported from a network a memory card another computer or an external storage media such as a flash memory external hard disk drive or CDROM.

Using techniques discussed in detail later the processor memory hardware and operating system software pre process the copy of the RAW image and ultimately convert it into an absolute color space e.g. sRGB Adobe RGB or the like which is also referred to as a rendered color space suitable for display and viewing. After pre process the RAW image and converting it the absolute color space the processor memory hardware and operating system software make the resulting image available to various types of software and applications on the computer . For example the resulting pre processed image may be made available to imaging applications such as Apple Computer s Preview iPhoto and Aperture software components where the image can be processed viewed manipulated etc.

As noted above the operating system software is used to pre process the RAW image . Accordingly we now turn to a description of a software architecture for pre processing the RAW image .

The O S kernel layer provides core O S functions in a highly protected environment. Above the O S kernel layer the O S services layer extends functional services to the layers above it. The O S service layer for the operating system of a computer offers a number of functional services while an in camera operating system for a digital camera may offer services that are more limited. In the present embodiment the O S services layer has a RAW image processing service for performing pre processing on the RAW image according to the various teachings discussed below.

The resource layer is above the O S services layer and shows graphics resources such as Open Graphics Library OpenGL Apple Computer s PDF kit etc. OpenGL developed by Silicon Graphics Inc. is a specification for various graphics functions. A RAW processing Application Programming Interface API is positioned between the resource layer and the RAW image processing service in the O S Service Layer . Layer is an amalgamation of functions typically expressed as two layers applications frameworks and application services. This layer provides high level and often functional support for application programs that reside in the highest layer shown here as application layer .

The RAW processing service leverages the Central Processing Unit CPU of the general purpose processing device to pre process RAW images. The RAW processing service can also use OpenGL to leverage the Graphics Processing Unit GPU associated with the general purpose processing device. In turn the RAW processing API makes the pre processed images of the service available to the various application programs in the application layer . Thus the RAW image can be pre processed using the RAW image processing service in the O S services layer and hardware e.g. CPU GPU of the general purpose processing device. In this way RAW processing can be focused in the OS services layer to produce higher quality images with increased editability compared to regular JPEG files provided by a camera because RAW images contain more information.

In addition the pre processed images can be made available to various applications that in the application layer that may or may not be capable of processing RAW images. The application programs in layer that can use the pre processed images can include word processing applications photo applications address book applications e mail applications or any other applications that may use images. In addition the RAW processing techniques disclosed herein may be applicable to RAW video as well so that applications that use video may also benefit from the techniques disclosed.

As alluded to in the above embodiment the pre processing of RAW images can be performed in the RAW image processing service that operates in the O S service layer of the general purpose computer. A pipeline is one way of characterizing image processing. illustrates one embodiment of a RAW image processing pipeline according to certain teachings of the present disclosure. The pipeline is illustrated in a plurality of stages and for pre processing a RAW image from a digital camera or other source and for outputting a resulting image for use by various applications not shown . In an initial image capture stage the camera or other imaging device captures an image and stores it in the RAW format as a RAW image or file as discussed previously. At this point the RAW image may be communicated from the camera to a general purpose processing device or computer not shown or may remain at the camera for RAW processing depending on the processing capabilities of the camera .

To enable later processing the white balance and other metadata for the RAW image and the particular camera used to obtain the RAW image are identified because later processing stages will depend on attributes of the image and the camera . The metadata can be in an Exchangeable Image File EXIF format and can include information about how the RAW image image was captured including shutter speed aperture white balance exposure compensation metering setting ISO setting date and time for example.

In a RAW processing stage RAW processing is performed on the RAW image using one or more various processes including a black subtraction process a highlight recovery process a stuck pixel elimination process an auto exposure adjustment process a demosaic de Bayer process and a chroma blur process . One purpose of the RAW processing stage is to preserve spatial quality and detail of the captured scene in the RAW image .

To implement these processes the metadata is used to obtain various raw processing algorithms associated with the processes Block . Although shown in a particular order one or more of these processes may be rearranged depending on specific requirements such as any particulars associated with the camera. In addition one or more of these processes may or may not be used for various reasons and other processes commonly used in image processing can be used. Furthermore some of these various processes may actually operate in conjunction with one another in ways not necessarily expressed herein.

At the end of the RAW processing stage the demosaic process produces a camera RGB image from the initial RAW image . Here camera RGB indicates that the image has an unrendered RGB color space such as ISO RGB defined in ISO 17321. The camera RGB image embodies an estimate of the original colorimetry of the scene captured in the initial RAW image and the camera RGB image maintains the dynamic range and the gamut of the original. The camera RGB image must go through additional processing for it to be displayed viewed or printed with an output device.

In general the demosaic process transforms the device specific RAW data of the initial image to unrendered RGB data in the resulting image that is independent of the device e.g. camera used to capture the initial image . The actual implementation of the demosaic process can depend on specifics of the image and or the imaging sensor used to capture that image . For example actual colors for pixels in the camera RGB image are interpolated from the multiple photo sites in the RAW image because each photo site has only one color value for R G or B. This image reconstruction process is called demosaicing or De Bayering in the case of a Bayer Matrix filter . In general the demosaic process takes the plurality of color values represented in either R G or B at the photo sites in the RAW image and outputs pixels having R G and B values in the camera RGB image . Typically components of a given pixel are computed from intensities of its neighbors. Therefore the demosaic process is typically device specific because it depends on the arrangement of filters in a camera s color filter array.

Two conversion stages and follow the RAW processing stage . One purpose of these conversion stages and is to sufficiently represent the color of the original scene that was captured. In many ways the consideration in these stages and of sufficiently representing the color is interrelated with maintaining details of the RAW processing stage . The final result of these conversion stages and is a resulting image in a rendered color space such as Standard RGB or Adobe RGB. The resulting RGB image can then be made available to word processing applications photo applications address book applications e mail applications or any other applications that may use images as discussed previously with reference to .

In a first conversion stage the camera RGB image is automatically processed and converted into XYZ tristimulus values in the XYZ color space to produce an XYZ image . The XYZ color space is used for its various advantages and so that the image data will be compatible with the processes occurring in this stage . For example converting to the XYZ color space allows color values to be correlated with measurable color temperatures to sufficiently represent original colors of the scene captured by the camera .

In this stage a black compensation process adjusts the black level in the camera RGB image . Concurrently a matrix conversion process converts the camera RGB image into the XYZ image having XYZ tristimulus values. To do the processing during this stage certain metadata including the white balance is used to obtain a conversion matrix M Block and perform other necessary calculations detailed below.

In general the black compensation process subtracts a black level from the camera RGB image to provide more efficient coding and processing in later stages. In addition the black level to be subtracted depends on the image content and the camera model and the black level can be adjusted to reduce noise or produce better color rendition. The black compensation process is configured for the particular camera used to capture the RAW image . The process receives the pixels having R G and B values as input and outputs black compensated R G and B values. The black compensation process optimizes the black offset on each RGB channel such that the conversion from camera RGB tristimulus values to the XYZ tristimulus values will have minimal error. The black compensation process is derived by a process discussed in more detail with reference to .

The matrix conversion process incorporates the black compensation described above and takes the R G and B values of the camera RGB image as input and outputs XYZ tristimulus values. In general the process uses two or more camera characterization matrices Mand Mthat are specific to a camera model etc. and that are associated with predefined illuminants. Each of the camera characterization matrices Mand Mare derived by comparing measured color values of a color chart with the sensed data of a given camera when shooting the color chart. One embodiment for deriving characterization matrices is discussed below with reference to . In one advantage of this stage the pre calculation and derivation of the camera characterization matrices involves an automatic process that is independent of user subjectivity and of other implementations of RAW conversion. Thus the automated processing in the pipeline can provide a quality processed image for use by various applications of a computer.

Using camera characterization matrices specific to the camera used to obtain the RAW image the process derives a conversion matrix M that is then used to perform the black compensation and the conversion to XYZ tristimulus values on the camera RGB image . The conversion matrix M is used in conjunction with the white balance in the metadata to estimate the optimum matrix conversion from camera RGB to XYZ tristimulus values. The conversion matrix M is derived by a process discussed in more detail with reference to .

Finally in a second conversion stage the converted data of XYZ tristimulus values are processed and converted to values in an absolute color space such as Standard RGB sRGB or Adobe RGB. First a chromatic adaptation process reproduces the appearance of colors in the image by applying a chromatic adaptation transform. The chromatic adaptation transform converts the XYZ tristimulus values of input colors captured under an input illuminant to corresponding XYZ tristimulus values of output colors under a predicted output illuminant. Chromatic adaptation transforms are known in the art and most of them are based on the von Kries model.

Next a color tuning process adjusts the tone curve of the XYZ tristimulus values of the image. Finally a transformation process that incorporates the color tuning process transforms the XYZ tristimulus values to RGB tristimulus values of the absolute rendered color space such as sRGB or Adobe RGB so that a resulting image in a specific RGB color space results. The rendered color space e.g. sRGB or Adobe RGB is based on the colorimetry of real or virtual output characteristics. To do the processing during this second conversion stage certain metadata from the camera is used to obtain a tone curve Block and perform other necessary calculations detailed below.

In general the color tuning process sets a gain and output tone reproduction curve to optimize rendering of the image to an output media. The process automatically derives parameters for the conversion per camera type and manufacturer. Being automatic the process allows output to be derived based on a known target and without manual intervention by a user thereby eliminating the subjectivity associated with manual color tuning by a user. One embodiment of a process for deriving a tone reproduction curve for the automatic color tuning process is discussed in detail below with reference to .

The transformation process incorporates the color tuning process discussed above. In general the transformation process transforms the XYZ tristimulus values to RGB tristimulus values in the specified rendered color space such as sRGB and Adobe RGB. The resulting RGB image can then be accessed by applications or other software for further processing and manipulation.

As noted previously the RAW image processing pipeline of has RAW processing stage that includes various processes. illustrates details of one embodiment of a RAW processing stage . Although the stage is illustrated as a series of steps it will be appreciated that a given implementation may used a different ordering of steps may eliminate certain steps or may add additional steps not shown. The input for the RAW processing stage is the RAW image and the metadata. The RAW image is Bayer packed e.g. the photo sites in the image are arranged in a Bayer Pattern or the like and each have one R G or B value .

In a first step of the RAW processing stage black subtraction is performed on the input RAW image . In this step the color samples R G B of the photo sites are scaled and biased based on camera specific factors. For most cameras even an image taken with the lens cap on will result in an image with small non zero color values for the photo sites. The non zero color values correspond to a black offset or bias value of the camera that can be caused by noise or other reasons. For the final image to be correct the black offset value must be removed from the values of the color samples.

The RAW image from the camera may or may not be black compensated when it is generated. For some cameras the sensor data is automatically black compensated after it is generated. In this case the metadata of the RAW image may have an offset value of 0.

In some situations the camera may provide the offset value in the metadata of the image below which there is no sensor data. In this case the black offset value is subtracted from the color values of the RAW image in this step . For example the black offset value for the RAW image from some cameras may be estimated from an average of values on the margin of the imaging sensor that are masked from incident light. For camera models that do not use a sensor mask at the margins the black offset value can be a fixed value for the camera model. Either way the black offset value can then be subtracting from each intensity value in the image . Generally the R G and B photo sites of the camera may have different offset values. G generally may have two offset values. Cameras may also provide a row dependent offset value.

As seen above the metadata associated with the RAW image is one source of the black offset value. An override file may be a secondary source of the black offset value. The override file may include black offset values that have been predetermined on a per camera model basis. This secondary black offset can be used to either augment or replace the value associated with the metadata and the secondary black offset can also be different for each color channel R G B .

After subtracting the black offset values the resulting values are preferably clipped at zero to eliminate negative values in the RAW image . In addition it may be necessary to scale the values into a common range because different cameras have ranges of raw sensed values. This can be accomplished by multiplying all the black subtracted values by a predetermined value and then dividing by a white value for the current camera. The white value can be predetermined and can be obtained from an override file or metadata.

In a second step of the RAW processing stage the RAW image undergoes highlight recovery to correct sensed values in the image that have been clipped at a maximum level of the sensor s response. The imaging sensor used to obtain the RAW image is capable of responding to light up to a maximum value referred to herein as a max out value . Thus any received light above that max out value is not detected by the sensor element. To recover highlights in the image where light values have exceeded the max out value of the sensor the sensed values in the image that are clipped at the max out value are replaced by new estimated values using neighboring unclipped values of different color channels R G B .

In one embodiment values for neighboring sensors near to a maxed out sensor value are located. These neighboring sensors may have R B or G values that are about 85 to 90 of the max out value and are used to determine what value should be used for the maxed out sensor. This may work well for images where there are isolated sensor values that are maxed out and may be used within the RAW image. In general however a given image may have clusters or regions where the sensor values have maxed out so that averaging neighboring pixels is not practical.

In another embodiment shows a highlight recovery process that is applied to recover the luminance and hue of pixels where one or two of the channels are clipped by the limits of the sensors. This process is implemented very early in the demosaicing de Bayering process. First the R G and B sensor channels are scaled so that they are neutral Block . The scaling factor is determined using the white balance obtained from the metadata associated with the RAW image. Second a value is calculated at which each channel will become saturated Block . The calculation is performed using correlation information that correlates channel saturation values to over exposed images that have been examined for the given camera make and model. Thus the correlation information may be predetermined and stored for a plurality of camera makes and models for later access by the highlight recovery process . In this way the camera information obtained from the metadata associated with the RAW image is used to access the corresponding correlation information for the camera make or model that obtained the RAW image.

The saturation values are then scaled by the same factor used in Block Block . For most cameras the green channel has the smallest saturation value and this behavior may be exploited by the highlight recovery process . Each pixel in the image is sampled and analyzed by the process Block . If all three channels R G and B of a given pixel are not clipped Block the pixel is left as is. If all three channels of the given pixel are clipped Block the pixel is replaced by a neutral grey value Block .

If at least one but less than all of the channels is clipped Block the process looks at the saturation of the various channels. As detailed below a channel of a given pixel having a saturated value is replaced by an estimated value. The replacement of the saturated value with the estimated value is graduated and depends on how close the original value is to saturation. The estimated value is calculated based on the original RGB values at the pixel.

If the red value in the given pixel is at or near saturation Block for example then the original red value is replaced with an estimated red value calculated from the original R G and B values Block . The estimated red value is

If the green value in a given pixel is at or near saturation Block then it is gradually replaced with an estimated green value calculated from the original R G and B values Block . The estimated value for the G channel is based on a weighted average of the original R G B values. The R G B weights that are used range from 0.5 0 0.5 to 0.375 0.625 0.0 depending on a calculation of how green the original pixel is. The replacement also uses the weighted average discussed above.

If the blue value in a given pixel is at or near saturation Block then it is gradually replaced with an estimated blue value calculated from the original R G and B values Block . As with the R channel the estimated value for the blue value is

As a final step when one or more of the channels has been estimated if the recovered RGB values are very bright and saturated the color is moved toward neutral Block . Magenta hues that are less likely to be seen in bright areas of the image are more aggressively moved towards neutral. Moving colors toward neutral may be affected by user preferences. After analyzing the given pixel a determination is made if additional pixels in the image need to be analyzed Block and the process returns to Block to sample and analyze additional pixels of the image.

Returning to the RAW processing stage includes a third step where those photo sites in the RAW image that have aberrant values are changed using stuck pixel elimination and or noise processing techniques. In a given RAW image various photo sites may have atypical values because the sensor did not have a proper response e.g. the sensor is stuck or has accumulated charge at the photo site .

For these stuck photo sites a neighborhood of 3 3 like photo sites is evaluated and a range of the 8 surrounding values taken from that neighborhood is determined resulting in a minimum and a maximum value. For example if we are at a green pixel only green photo sites are examined. The size of the range is determined equal to the maximum minus the minimum of the range. The value at the stuck photo site is compared against the neighboring like photo sites range. If the value of the selected photo site is greater than the maximum by a predetermined factor times the size of the range or if the value of the selected photo site is less than the minimum by a predetermined factor times the size of the range then the value for that photo site is set to the average of the 3 3 neighborhood. The predetermined factor can be configured based on the camera and other factors. Replacing the atypical values with the average of their neighbors will reduce the number of stuck pixel values apparent in the RAW image .

Noise in a given image produces channel samples with aberrant values. This noise can be amplified when an RGB image is constructed from the Bayer encoded RAW image. Typically the rendered noise is undesirably colorful. This noise can be correlated to the ISO setting i.e. the gain on the sensor readout and the amount of exposure time used when the RAW image is captured. Therefore the noise processing in sub step preferably uses a correlation between ISO setting exposure time and amount of noise to reduce or control the colorfulness of noise for the RAW image being processed.

First a digital camera is secured with the lens cap and eyepiece closed and a plurality of images are taken under a wide range of ISO settings and exposure times for that particular camera Block . The RAW images are then grouped into a number of organized sets Block . For example a first organized set can have images with a low range of exposure times and with a wide range of ISO settings. In addition a second organized set can have images with a low range of ISO settings and with a wide range of exposure times and a third organized set can have images with a high range of exposure times and with a wide range of ISO settings.

The RAW images are then processed to quantify the noise response e.g. amount of noise and to quantify the stuck pixel response in each RAW image Block . Then noise profiles and stuck pixel profiles are calculated for the grouped images Block . For each of the organized sets for example a quadratic regression creates a noise profile of the camera s noise response relative to the ISO settings and exposure times for the given set. For each of the organized sets for example a quadratic regression model also creates a stuck pixel profile of the camera s stuck pixel response relative to the ISO settings and to exposure times. The resulting profiles for the camera are stored in the form of a small number of floating point values in a database for later use during pre processing in the RAW processing stage of Block . Then the previous steps are repeated for one or more additional cameras so that profiles can be stored for those particular cameras Block .

Ultimately the stored profile data characterizing various cameras can be used to decide how much to adjust a given RAW image for noise. When a RAW image is being processed in the RAW processing stage of for example the camera model the ISO setting and the exposure time associated with the RAW image is obtained from the associated metadata Block . The appropriate profile data for the camera model is located in memory Block . The given ISO setting and exposure time information is plugged into the formula for the noise profile for that particular camera model and an estimated amount of noise for that image is determined from the calculation Block . Then the given ISO setting and exposure time information are plugged into the formula for the stuck pixel profile for that camera model and a decision is made whether to enable or disable Stuck Pixel Elimination for that image Block .

When processing the RAW image in the RAW processing stage of the estimated information for noise and stuck pixels determined in the previous steps can be used to adjust processing. For example the information can determine whether or not and to what extent to perform noise reduction and stuck pixel elimination while reconstructing the RAW image. In addition the information can determine whether or not and to what extent to perform ramping down sharpening during or after reconstructing the RAW image. Finally the information can determine whether or not and to what extent to control colorfulness of noise during conversion of the RAW image to RGB at the end of reconstruction using chroma blur explicit de saturation in deep shadow areas or tonal reproduction curves for saturation control during contrast boost. These processes will be discussed in turn later in the present disclosure.

Returning to the RAW processing stage includes a fourth step where the RAW image undergoes an auto exposure adjustment. The auto exposure adjustment adjusts luminance of the RAW image so that its exposure meets predetermined criteria. Preferably the adjustment uses predetermined luminance variables that are already stored for the adjustment in the RAW processing stage . The predetermined luminance variables are based on survey information obtained from a plurality of people viewing various images with adjusted exposure. The survey uses reference images generated with various cameras at a plurality of exposures. The average luminance of these references images is computed. Using a Monte Carlo simulation to vary the luminescence variables of these reference images survey participants are asked to select examples of the images that are most visually pleasing. Then the survey results are converged to an acceptable resulting exposure having luminance variables correlated to the original input luminance.

The results are associated with the particular camera and are stored for later processing. When the RAW image is received for processing the auto exposure adjustment computes the average luminance of the RAW image and determines the exposure from the associated metadata. Then using the stored survey results auto exposure adjustment determines which luminance variables to apply to the image based on the average luminance and the exposure used so a pleasing result can be achieved. Finally the luminance of the RAW image is adjusted by the determined luminance variables in the automatic processing.

A fifth step of the RAW processing stage involves an interpolation process which is part of a demosaic de Bayering process. The interpolation process uses a number of sub steps that are culminated in a final chroma blur operation to produce a resulting RGB image. These sub steps includes creating a half size green edge image determining an interpolation direction map constructing a green reconstruction image sharpening the green reconstruction image creating a blurred half size RGB image and constructing a red reconstructed image and a blue reconstructed image . Each of these steps and will be discussed below. Ultimately the green red blue and blurred RGB images from these sub steps are combined in a chroma blur operation which is also discussed below.

In general the interpolation process uses gradient based interpolation by first interpolating the luminance channel i.e. the green channel for the photo sites of the RAW image then interpolating the chrominance channels i.e. the red and blue channels for the photo sites of the RAW image and finally combining the channels to create the resulting RGB image having pixels with R G and B values.

Because the human eye is sensitive to changes in luminance edges are found in the green channel so that interpolation can be performed based on those edges. Sub step constructs a half size green edge image from the RAW image so the green edge image can indicate edges caused by changes in luminance of the original RAW image . shows a process for creating a half size green edge image from Bayer encoded data of a RAW image only a portion of which is shown . The RAW image has a plurality of 2 2 cells with each cell having two green samples G G one red sample R and one blue sample B that are photo cites arranged in a Bayer pattern. Various patterns are known and used in the art and the portion of the pattern shown here is only exemplary.

To create the half size green edge image only the green samples G G of the Bayer encoded RAW image are used and the red and blue samples are ignored. First the two green samples G G in each of the 2 2 cells of the RAW image are averaged to produce a corresponding green value in an intermediate half size green image . Thus the half size green image has only green values and is half of the size of the original RAW image . Next first derivatives i.e. rates of change for the values of the green samples G are calculated in both horizontal and vertical directions. Then squares of the horizontal and vertical first derivatives for each of the values are summed to produce an edge image . Then two passes of a single pixel blur process are performed on the edge image to produce a resulting half size green edge image that is blurred.

In the single pixel blur process a first pass of the process is performed over the edge image with a center weight of 0.5 for the four neighbors of each pixel. Then a second pass of the process is performed over the previously blurred edge image with a center weight of 0.125 for the four neighbors of each pixel to produce the resulting half size green edge image . As shown in the resulting half size green edge image of this step will be used in later processing by other steps.

Returning to sub step of the interpolation process creates an interpolation direction decision map that is used to decide how to fill in missing green values for those photo sites having R or B channel located in the RAW image . In one embodiment adjacent red and blue samples in each of the 2 2 cells of the Bayer pattern can be averaged to determine the missing green value for the photo sites using standard demosaicing techniques. In a preferred embodiment either vertical or horizontal neighboring samples in the RAW image are used to determine the green channel for those samples that are red or blue. If a region of a generic image has horizontally oriented edges or stripes for example then it is preferred that horizontal neighbors of the R and B photo sites in that region are used to determine their green channel value. If on the other hand the region of the generic image has vertically oriented edges or stripes then it is preferred that the vertical neighbors of the R and B photo sites in that region are used to determine their green channel values.

To determine the horizontal or vertical direction to be used for interpolation the gradients of the R G and B channels of the RAW image are calculated for each photo site. Then the gradients are mapped in an interpolation direction decision map that is used to decide how to fill in the missing green channel values at a given R or B photo site in the RAW image . In general the gradient for the green channel has the strongest influence in determining how to fill in the missing green channel values for the R or B photo site if the gradient for the green channel shows that it is locally not changing. If the gradient of the green channel is changing locally then the gradient of the non green channel for the given photo site is used to determine how to fill in its missing green value. The gradient of the green values at the photo site can be determined by looking into the half size green edge image at that location.

The determined directions for each photo site are then input into the map as voting values. For example a voting value of 1 corresponds to a decision for a horizontal direction in the map to be used in interpolating the missing green channel at the photo site while a voting value of 0 is a decision for a vertical direction to be used in interpolating the missing green channel at the photo site. These voting values are then used in a voting process used to fill in the missing green channel values. The actual steps for filling in the missing green channel values are discussed with reference to the green reconstruction step below.

Sub step of the interpolation process creates a green reconstruction image using the original Bayer packed image from step the half size green edge image from sub step and the interpolation direction decision map of sub step . In one embodiment of a process for creating a green reconstruction image is diagrammatically illustrated. To create the image each of the green channels G for the photo sites in the RAW image are maintained for the corresponding photo sites in the green reconstruction image . For example the value for the green sample Gin the RAW image is the same as that used in the green reconstruction image . However the photo sites for the R and B channels in the RAW image do not have green values so their values in the green reconstruction image must be interpolated using the interpolation direction decision map . For example the selected photo site in the green reconstruction image does not have a green value because it corresponds to the blue sample Bin the RAW image .

As alluded to above a voting process is used to determine which direction to use when filling in the missing green values of the R and B samples. The voting process allows the neighborhood of the sample in the interpolation direction decision map to be analyzed so that averages of the direction decisions within the neighborhood can be used to determine which direction to use for filling the missing green value.

The interpolation direction decision map has a directionality vote at each pixel location. The voting process uses the decision map and can be implemented in a number of ways. For example the various direction decisions D in the map can be 1 for horizontal and 0 for vertical. In one alternative the single vote decision corresponding to the selected photo site can be used alone to determine the direction for interpolation. In another alternative a consensus of several votes from a neighborhood of the decisions can be taken and the consensus of the several decisions can decide which direction to use for interpolating the green value for the selected photo site .

A full vote is one kind of consensus voting that can be used. The full vote uses a neighborhood of decisions consisting of an odd number of samples for example a 3 3 or a 5 5 sample . A sum is computed of the decision values over the full neighborhood . If the sum is greater than half the total number of photo sites in the neighborhood then a 1 horizontal direction is used for the decision of the selected photo site . Otherwise a 0 vertical direction is used.

Pass band voting is another kind of voting that can be used. In this kind of voting a sum of neighboring photo sites decision values is computed excluding the central decision of the selected photo site . If this sum is less than a predetermined threshold the decision value is set to 0 vertical direction for the selected photo site . If the sum is greater than another predetermined threshold then the decision value is set to 1 horizontal direction for the selected photo site . The thresholds depend on the size of the neighborhood used. If the sum is equal to or between the thresholds then the central decision of the selected photo site is left alone.

With a 3 3 neighborhood there are 8 neighboring decisions when we exclude the center decision . With wide pass band voting the lower threshold can be 2 and the upper threshold can be 6. With narrow pass band voting the upper threshold can be 4 and the lower threshold can also be 4 indicating that the neighboring decisions must be exactly tied for the central decision to be used. Wide pass band voting is preferred because it may not affect the natural directional preference of the selected photo site .

After determining the direction vertical or horizontal to use to determine the green value for the selected photo site then mathematical calculations use the samples in the RAW image to determine the green value for the selected photo site in the green reconstruction image . First an average of the two green samples in the chosen direction is used for the base result of the green value for the selected photo site . In for example the determined direction is vertical so that the average of the green values for the green samples Gand Gare determined and used as the base of the result e.g. Green Result for the selected photo site . However an adjustment is made to the Green Result based on the value of the corresponding sample either red or blue in the RAW image . First the Average of the corresponding sample s two like neighbors in the sampling direction is calculated

The entire process is repeated until the green reconstruction image is entirely filled with green values i.e. those green values obtained directly from the RAW image and those green values interpolated using the decision map and voting discussed above.

Returning to a subsequent sub step of the interpolation process performs a green sharpening operation on the green reconstruction image from above. First the green image is converted into a space as close as possible to perceptual gradation. Then the perceptually graded reconstructed green image is blurred by the radius of the sharpening operation to produce the blurred reconstructed green image. The blurred reconstructed green image is subtracted from the perceptually graded reconstructed green image to produce the green high pass image. The green high pass image contains the high frequency information of the green reconstruction image. The green high pass image is used for two purposes. The first purpose is to sharpen the image. A provisionally sharpened green reconstruction image is produced by adding a predetermined sharpening factor times the green high pass image to the original image. The second purpose is to compute an edge mask so that the sharpening operation can be constrained to only operate on the edges of the image. The edge mask is produced by taking the absolute value of the green high pass image blurring it by a small amount and then increasing its contrast by a large factor. The result is threshold at a predetermined level and clamped to the range 0 . . . 1 to produce the edge mask image. The edge mask image is used as a mask to decide which areas of the provisionally sharpened green reconstruction image are mixed with the green reconstruction image to form the sharpened green reconstruction image . Later this sharpened green reconstruction image will be combined with a red reconstruction image and a blue reconstruction image to produce an image having RGB values at each sample.

Returning to sub step of the interpolation process creates a blurred half size RGB image which will be used in the chroma blur operation discussed later. shows an example of portion of Bayer encoded data of a RAW image used as input to this step. First green values G contributed to each pixel in an intermediate half size RGB image are determined using an average of the two green photo sites G G in the 2 2 cells of the original full size RAW image . The red and blue samples R B are contributed to each pixel based primarily on the single sample of those colors R B found in the 2 2 cells of the original full size image along with a small contribution added from neighboring like color R B samples using convolution resampling or similar technique. Finally the intermediate half size RGB image is blurred with a Gaussian kernel to produce the resulting half size blurred RGB image . One or more other blur operations could also be used including but not limited to a Gaussian blur a selective blur a bilateral filter a median filter and a box filter. This resulting image has a blur to it so that it can be used in the chroma blur step of discussed later.

In the interpolation process of sub step creates a red reconstruction image and a blue reconstruction image. diagrammatically shows processes for creating a red reconstruction image and a separate blue reconstruction image which are only shown partially in .

In the reconstruction processes each photo site in the RAW image is sampled. If the current sample of the RAW image is the right channel i.e. R or B then the value of the resulting sample in the reconstruction image or equals the value of the original sample plus a green sharpening difference. At this point during operation the RAW image process of has developed a sharpened green reconstruction image sub step and an unsharpened green reconstruction image sub step . The green sharpening difference is the difference between the green channel at that sample point in the sharpened green reconstructed image from sub step and the green channel at that sample point in the unsharpened green reconstruction image from sub step . The green sharpening difference has the effect of applying any green sharpening to the red and blue channels as well when reconstructing them. In reconstructing the red reconstruction image in for example the current sample is the right R channel and the resulting red photo site in the red reconstruction image is the value of the original sample plus the sharpening difference. 

If the current sample in the RAW image is not the right channel R or B then a determination is made whether the current sample has direct horizontal or vertical neighbors of the desired channel. If so the average of the horizontal or vertical neighbors is taken and the green sharpening difference is added on to produce the resulting sample in the reconstruction image. In reconstructing the red reconstruction image in for example the current sample is not the right channel i.e. not R . Therefore the resulting red photo site in the red reconstruction image is the average of the horizontal neighbors of the right channel i.e. R plus the sharpening difference discussed previously.

If the current sample in the RAW image is not the right channel and does not have direct horizontal or vertical neighbors of the right channel the average of all four diagonal neighbors is calculated and the green sharpening difference is added on to produce the resulting sample in the reconstruction image. In reconstructing the blue reconstruction image in for example the current sample is the not the right channel i.e. B for the blue reconstruction and does not have horizontal or vertical neighbors of the correct B channel. Therefore the resulting blue photo site in the blue reconstruction image is the average of the diagonal neighbors of the right channel i.e. B plus the green sharpening difference discussed previously. The end result of the reconstruction process is a red reconstruction image and a blue reconstruction image that can be stored in buffers until combined in later processes to produce a resulting RGB image.

Finally a chroma blur operation in uses the sharpened green reconstructed image from sub step the half size blurred RGB image from sub step and the red and blue reconstructing images from sub step to produce a full RGB image. illustrates one embodiment of a chroma blur operation for the automated processing of . Initially the red reconstruction image the sharpened green reconstruction image and the blue reconstruction image from previous processing are obtained Block and a reconstructed RGB image is created by combining each of the individual R G and B sample values from them as RGB pixels in a reconstructed RGB image Block . The luminance of the reconstructed RGB image at each pixel is then computed Block . Then bilinear interpolation is used to resize the half sized blurred RGB image into a full size blurred image Block and the luminance of each pixel in the full sized blurred image is computed Block . Then the luminance of the blurred color in the full size blurred image at each pixel is scaled to match the luminance of the reconstructed RGB image Block . Finally a full size RGB image that has reduced color fringing results from this fast chroma blur operation Block .

As alluded to previously in Block of characteristic matrices for converting the camera RGB image to XYZ tristimulus values are pre calculated and used in conjunction with the white balance of the image to estimate the optimum matrix conversion from camera RGB to XYZ tristimulus values. In one embodiment of a process for deriving characteristic matrices for conversion in automated RAW processing is illustrated in flowchart form. In the process a plurality of camera characterization matrix e.g. M M etc. are derived for a plurality cameras types of cameras models of cameras manufactures imaging sensors or other categories. The process is intended to eliminate user intervention and subjectivity in choosing interpolation matrices during the RAW processing of and automatically derives parameters for RAW conversion per camera type manufacturer or other category.

Initially a reference image of a color chart e.g. a Macbeth Color Checker or the like is obtained using a specific camera and one known illuminant e.g. illuminants A D65 etc. Block . This will not be the only reference image generated by this camera. As will be detailed below a plurality of such reference images are obtained for a given camera a type of camera a model of camera a camera manufacturer type of imaging sensor of the camera or other category. The reference image may be made in a lab and the color chart may contains various color patches highlights shadows etc. and may be positioned in a lighting box having a plurality of standard illuminants.

Under the same illuminant XYZ color values are measured for various color regions or patches from the color chart using a standard instrument such as a colorimeter Block . Then the RGB values for the regions are mathematically fit to the corresponding XYZ tristimulus values measured for those regions Block . The fitting process involves solving for matrix coefficients in a matrix that will correlate the RGB values of the reference image with the XYZ tristimulus values measured from the chart. Due to the tristimulus color spaces involved the matrix will likely be 3 3 so that there are nine variables involved in solving for the fit. In the fitting process the original white balance of the reference image is ignored. Thus the fitting process does not assume the camera is calibrated because the original white balance is ignored. Instead a fixed white balance is used in solving for the matrix coefficients. White balance is typically expressed as a vector right arrow over p having three value. For the fitting process each value of the vector right arrow over p is equal to one so that any influence of white balance in the fitting process is not recognized.

In the fitting process a number of color patches or regions of the chart are preferably used to derive the coefficients of the matrix. Accordingly the process can be repeated for different color patches of the same color chart under the same illuminant and or different color charts under the same illuminants Block . Then a balance is found for the coefficients of the derived matrix so that they will have a proper weighting to correlate RGB values of various colors to measured XYZ tristimulus values measured for those various colors using the same illuminant Block . Ultimately the process derives a first characterization matrix Mfor this first camera and this first illuminant. This first matrix Mis a 3 N matrix with n corresponding to the number of channels e.g. R G B for the camera.

After deriving the first camera characterization matrix M the Blocks of through are repeated one or more times for one or more additional illuminants using the same camera previously used Block . Previous measurements of the one or more color charts can be reused. The result of repeating these steps is a plurality of camera characterization matrices for different illuminants but the same camera. The matrix coefficients of these matrices are optimized or adjusted to reduce color error when various images are matched to corresponding measurements of one or more color chart under the various illuminants Block . This can be done using tests and adjustments. Once optimized the coefficients of each matrix are normalized using a normalization factor for that particular matrix Block . Preferably the normalization factor for a given matrix is the sum of the diagonal coefficients of the matrix so that each coefficient of the matrix is divided by this sum.

Finally the entire process of blocks through can be repeated for one or more additional cameras models etc. Block . The result will be a plurality of camera characterization matrices. Each of these camera characterization matrices will be associated with a corresponding illuminant and one camera type model manufacture or the like. These camera characterization matrices are then stored for later access when interpolation is performed in subsequent processing discussed below with reference to Block .

As noted above the plurality of camera characterization matrices can be made for the camera in a lab and stored for later access during the automatic RAW image processing of the present disclosure. Thus a plurality of cameras can have their own camera characterization matrices stored in memory. During the automatic pre processing the metadata for a RAW image can then be used to select the appropriate camera characterization matrices for the image form memory. In addition to having predetermined matrices for cameras users can independently create characterization matrices for their cameras by using the techniques above and carefully constructing a lighting environment and using a color calibration target.

As noted previously in a black compensation process is performed on the camera RGB image within the first conversion stage to XYZ color space. The process uses a derived black level adjustment to make the compensation. The black level adjustment is embodied in the camera characterization matrices Mand M derived for the camera and is performed by the conversion matrix M generated from those characterization matrices and from the white balance information received from the camera. illustrates one embodiment of a process for deriving a black level adjustment for use in the automated RAW processing of the present disclosure.

The process of deriving black compensation values is incorporated into the process of deriving the camera characterization matrices discussed above with reference to . As discussed previously the matrix derivation process of involves obtaining a reference image of a chart using a known illuminant. Part of that step further involves subtracting the standard black level off set if any for the captured image Block . Then as before the RGB tristimulus values for regions in the reference image are fit to the measured XYZ tristimulus values from the color chart to derive the characterization matrix Block . This fitting process produces an amount of fitting error regardless of how many color patches are analyzed because of the number of variable of the coefficient of the characterization matrix i.e. the nine variable for a 3 3 matrix.

To reduce the error variables for black compensation are used in the fitting process. One each of these black compensation variables is subtracted from one of the color channels R G B of the reference image. In this way Block will fit the RGB tristimulus values of the reference image to the measured XYZ tristimulus values using 12 variables i.e. nine from the matrix and three for the offsets of the RGB channels . Derivations are then performed with values of the black compensation variables in the fitting process of Block until the color error is reduced to a threshold level Block . In this way an optimum black compensation variable for each color channel is derived by minimizing the color error between the two color sets the measured one and the estimated one in the reference image. These black compensation variables represent an addition black adjustment in addition to the standard black offset.

The black compensation variables can then be organized and stored for later access during processing Block . To organize the black compensation variables for example they can be associated with the characterization matrix for the corresponding camera and illuminant used to derive them. Thus each characterization matrix can have associated black compensation variables. In addition the black compensation variables can be associated with the different conditions used to derive them. These conditions can include but may not be limited to the illuminants used the image content the white point camera settings and ISO sensitivities involved. Different dependencies between black compensation variables can be averaged between groupings if the values of the variables exhibit characteristics of clustering. Moreover the different black compensation variables can be differentiated and classified based on these various conditions if the variables are divergent.

The process provides more objective and consistent determination of an optimum black level compensation to be used in RAW image processing. In the pipeline of for example these black compensation variables can be initially subtracted from the RGB tristimulus values of the camera RGB image at step before matrix conversion is performed so as to reduce color error in the conversion. Selection of which set of black compensation variables to use can be based on the camera type model etc. in the metadata . In addition selection can be based on the different internal shooting conditions such as camera ISO setting exposure time etc. or external conditions such as illuminants which can also be indicated in the information of the metadata .

In addition selection of which black compensation variables to use during processing can be based on an average or an interpolation between sets of variables. As noted previously a first set of black compensation variables are derived for a first illuminant and associated with a first characteristic matrix M and a second set of black compensation variables are derived for a second illuminant and associated with a second characteristic matrix M. The two sets can be stored and associated with the characteristic matrices Mand Mused to calculate an optimum conversion matrix M discussed in the next section. In one embodiment optimum black compensation variables are calculated from a fix average between the first and second sets of variables associated with the characteristic matrices Mand Mused to calculate the optimum conversion matrix M discussed below. Alternatively the optimum black compensation variables are calculated using linear interpolation between the sets of black compensation variables. The linear interpolation applies the same interpolation factor used to calculate the optimum conversion matrix M discussed below.

As alluded to previously in the RAW image processing of a matrix conversion process is performed on the camera RGB image to covert it to XYZ tristimulus values. The process uses a conversion matrix M to make the transformation. The conversion matrix M is 3 N matrix where n is the number of channels for the camera. The conversion matrix M depends on the white balance and at least two pre computed camera characteristic matrices corresponding to two reference illuminants and is calculated based on the particular camera type of camera manufacture etc. used.

In particular the conversion matrix M is derived from characterization matrices Mand Mand the camera s unique white balance information right arrow over p which is image dependent and is provided by the camera as metadata e.g. of . The white balance information right arrow over p is a vector of three values which when normalized has only two significant elements because the third one equals 1. Through an iterative process a matrix equation for the image s white point W is solved using M Mand using the white balance right arrow over p of the image. The solution converges to a unique matrix M and a unique white point W.

The process solves for a conversion matrix M that can be specified for a given type of camera a given style of camera an individual camera a manufacturer or other characteristic and finds a chromaticity scaling factor and suitable white balance that will derive an optimal conversion matrix M. In the process discussed below only two characterization matrices M Mare used. Other embodiments can use more than two characterization matrices which may generally improve the interpolation.

As noted above with reference to a plurality of characterization matrices M M . . . Mfor the given camera are produced by tacking images of the same scene under different illuminants. For example Mmay be the camera characterization matrix made with illuminant A xa 0.4476 ya 0.4074 and Mmay be the camera characteristic matrix made with illuminant D65 xd 0.3127 yd 0.3290 . The illuminants can be represented as vectors in either XYZ space or CIE 1931 chromaticity space x y.

The plurality of characterization matrices M M . . . Mare ordered based on the correlated color temperature from 1 to n. The most distant illuminants are then used to estimate the actual illuminant of the scene by deriving a preferred conversion matrix M for the given camera that converges to the estimated illuminant in the RAW image being processed. Initially previously determined camera characterization matrices Mand Mare input Block and initial values are set Block . The initial values include a first intermediate matrix ma set equal to the first characterization matrix Mand a second intermediate matrix ma set equal to the second characterization matrix M. These intermediate matrices ma and ma initialized to M Mwill eventually be converged to a preferred conversion matrix M through an iterative process described below. A maximum number of iterations K is set to zero and a limit of total iterations N is set to 20 for example. In addition a first scaling factor fa for the chromaticity of the white point is set equal to 1 and a second scaling factor fa for the chromaticity of the white point is set equal to 0.

A threshold T used to determine convergence is also set to a value such as 0.001 so that the process will be repeated until the absolute value of the difference between the first and second chromaticity scaling factors fa and fa is under the threshold. Using this threshold the iterative process may typically require between 3 and 5 interpolation steps for most cameras. In an alternative embodiment the difference between the norm of the matrices ma and ma can be calculated for comparison to a threshold. In the example below however the first and second chromaticity scaling factors fa and fa are used to measure convergence because they are already computed scalars.

Initially a determination is made whether the absolute value of the difference between the first and second chromaticity scaling factors fa and fa is greater than the preset threshold T Block . For the first iteration of course the chromaticity scaling factors fa and fa are 1 and 0 respectively which will be greater then the threshold T. Processing then starts with the first characteristic matrix M Block .

For the first characteristic matrix M the inverse of the intermediate matrix ma is calculated Block . Then weights in the XYZ color spaces are calculated based on the white balance of the image Block . For example Xw mi 0 0 p 0 mi 0 1 p 1 mi 0 2 p 2 Yw mi 1 0 p 0 mi 1 1 p 1 mi 1 2 p 2 and Zw mi 2 0 p 0 mi 2 1 p 1 mi 2 2 p 2 . Next x y weights are calculated based on the xyz weights Block . For example xw Xw Xw Yw Zw and yw Yw Xw Yw Zw . X Y weighting factors are calculated based on the x y weights and specific illuminant values associated with the image Block . For example fx xw xa xd xa and fy yw ya yd ya .

Then the first chromaticity factor fa for intermediate matrix ma is calculated as fa sqrt fx fx fy fy Block . Finally a new value is calculated for the first characteristic matrix M as M 1 fa ma fa ma Block .

A similar process is then repeated for second characteristic matrix M Blocks using the following equations Inverse 2 0 0 0 0 1 1 0 2 2 1 0 0 1 1 1 1 2 2 2 0 0 2 1 1 2 2 2 2 2 2 sqrt 2 2 2 2 2 1 2 1 2 2 

Once the first and second characteristic matrices M and M have been calculated as above the intermediate matrices are set equal to corresponding characteristic matrix ma m and ma m Block .

Finally the process may repeat as long as the convergence threshold T Block or total allowed iterations Block has not been met. If the process has exceeded a set total iterations K Block then the process is forced to stop and proceeds to Block . Otherwise the process will return to Block to see if the threshold T has been exceeded and repeat another iteration of converging the characteristic matrices Mand Mfurther if needed. If the threshold T has not been exceeded then Blocks through are repeated for another iteration to converge the characteristic matrices.

In any event once convergence has been achieved or the total number of iterations exceeded a final chromaticity scaling factor is calculated as fa fa fa 2 Block . Finally a resulting conversion matrix M is determined by first calculating m 1 fa ma fa ma and then calculating M Inverse m Block . This resulting conversion matrix M can then be used in automatic processing of a RAW image for the particular camera regardless of the particular light source used by transforming the camera RGB image with the conversion matrix M Block .

As alluded to previously in the RAW image processing pipeline of a color tuning process and a transformation process are used to convert the XYZ tristimulus values to produce a resulting RGB image . The conversion is done using a conversion matrix M of 3 n where n is the number of channels per camera e.g. R G and B as discussed above. In addition the conversion is done using a tone reproduction curve that is intended to optimize the image rendition from a linear space to a display or output space. Therefore the tone reproduction curve used in the color tuning process affects the appearance of the image contrast shadow highlight detail and the overall image quality.

Initially in the derivation process a number of reference images are selected. In only two reference images are shown for convenience. A first of the reference images is associated with camera information i.e. a particular camera style of camera manufacture etc. This reference image is referred to as the target reference image. One or more other reference images can be produced by the same or a different camera and may have been processed using different techniques and processing software. Each of the reference images contain the same scene e.g. color chart or the like using the same settings illuminant etc. Preferably the other reference image contains highlights shadow a good distribution of tones and a distribution of under exposed to over exposed images. Accordingly the other reference image may have undergone some automatic and or manual manipulation using image processing software. In general the reference images can be generated in a lab so that the scene exposure size etc. between the images can be substantially the same. However embodiments of the RAW processing disclosed herein may allow for a user to independently generate reference images to generate tone reproduction curves.

The color of each pixel in the reference images depends on the sensor of the camera used the demosaicing process applied the noise reduction algorithm applied and other details used to produce them. Consequently the two images will not likely have a pixel to pixel correspondence of color even though they are of the same scene because there still will be differences in the imaging sensors used to capture the images differences in noise reduction difference in demosaicing processes used and other possible reasons.

In a first step the images are scale down to a size intended to reduce the effects of demosaicing differences and the noise artifacts between them. Scaling will usually be necessary because debayering highlight recovery size of the images and other details will likely not be the same for the two images . A low pass filter is preferably used to averages pixel colors into regions. The result is two scaled reference images having substantially the same size. In one optional step not shown gamma correction of the reference images can be performed if necessary.

In a second step the scaled images are transformed to grayscale images . The images can have a gray ramp patch that may help in the conversion of the images from color to grayscale. The scaling down of the images and the reduction from color to grayscale is intended to reduce effects or differences of noise and demosaicing between the original images .

In a third step a gain factor is determined that matches the maximum brightness of the second image to the maximum brightness of the first target image . Then the gray levels of the second image are scaled with this gain factor. In a fourth step a pairing between the grayscale images is done that compares the pixel to pixel grayscale values between the images . In a fifth step the one to one pixel pairing between the grayscale target image and the grayscale reference image is then plotted as a rough tone curve of y f x where x is the gray level of the reference image and y is the gray level of the target image . An example of such a rough tone curve is shown in graph of . In a sixth step the rough tone curve is refined using interpolation averaging and smoothing for missing gray levels of x or for discontinuities of the curve. In addition any plotted values that are aberrations or due to error can be removed during this refinement. In a final step a final tone reproduction curve is generated. An example of such a final tone curve is shown in graph of . Typically the tone reproduction curve has an S shape that boosts color saturation and luminance contrast of the image to produce pleasing results for each camera model. In one example the tone reproduction curve can be described by four third degree polynomials that are continuous.

This tone reproduction curve can then be used in the automatic color tuning process of when the camera for which RAW processing is done meets the criteria of the initial camera information associated with the target reference image used to generate the tone reproduction curve . Accordingly a plurality of tone reproduction curves are generated for a plurality of different cameras types of cameras and other criteria. In this way when a tone reproduction curve is obtained in Block of the process of the metadata from the camera for the image being processed can be used to select the appropriate tone curve from a pool of pre stored and configured tone curves.

In the present embodiment of the process of the final tone reproduction curve was generated using grayscale instead of distinct color channels e.g. R G B . When this tone reproduction curve is used in the automatic color tuning process of it will essentially be applied to each channel in the same way. In an alternative embodiment a similar process as that disclosed above can be used to generate a separate tone reproduction curve for each color channel of the image so that the automatic color tuning process of can then apply the separate tone reproduction curves to each channel independently for the color tuning. For example the reference images may undergo filtering to produce separate images for each channel. Then the images associated with each channel can under the stages of the process to produce an independent tone reproduction curve for the associated channel that can then be applied to each channel of an image being processed.

In a plurality of additional automated processes are shown for the RAW processing of an original RAW image to a resulting RGB image . These automated processes include a deep shadow de saturation process a luminance boost and an RGB separable boost . When and where these processes are actually implemented in the RAW image processing pipeline of depends on specifics of the camera the RAW image and other characteristics.

In one embodiment the automated deep shadow de saturation process can be implemented in the first conversion stage of from camera RGB to the XYZ color space. This process focuses on the RGB color values in the camera RGB image with luminance values below a shadow de saturation threshold. graphically shows a shadow de saturation threshold by line T in an input output luminance graph . The desaturation process reduces the saturation of those deep shadow RGB color values below the threshold T in proportion to their closeness to black. The shadow de saturation threshold T may correspond to luminance levels within 1 of black.

In the de saturation process the original luminance Lfor each pixel of the RGB image is computed using standard techniques. The computed luminance Lis compared to the shadow de saturation threshold T to determine if the computed luminance Lis below the threshold T. If it is below the threshold T then an interpolated grayscale luminance Lis calculated using an interpolation function. Then this interpolated grayscale luminance Lreplaces each of the original color values of R G and B of the deep shadow pixel. In the interpolation the grayscale luminance Lis preferably made proportional to the closeness of the pixel s original luminance Lto black so that the interpolated grayscale luminance Ltransitions smoothly from the deep shadow de saturation threshold T to black. Replacing deep shadow luminance with the interpolated grayscale values can reduce colorful noise in the deep shadows of the image.

The automated RGB separable boost can be performed in the pipeline of when the image is in the RGB color space. In general an RGB separable boost is preferred as a technique for increasing image contrast and adjusting the image s tonal curve by modifying the luminance of each R G and B channel individually. When increasing contrast using an RGB separable boost in the shadow and lower midtone areas of an image the saturation of the color can be increased as a side effect of an RGB separable boost. When an image has a considerable amount of noise this increase in contrast will make the noise more colorful and less desirable. The more monochromatic the noise appears in line with the local colors of the image the more the noise resembles traditional analog film grain. If the colorfulness of the noise is increased by an RGB separable boost then the noise will exhibit red and blue color shifts from the local color of the image which is not desirable. In the tonal regions where this undesirable artifact occurs a luminance boost discussed below is used instead of the RGB separable boost . Accordingly in the present embodiment the RGB separable boost focuses mainly on higher tones e.g. those tones above a mid tone level indicated by line in the graph of . When applied to these higher tones the RGB separable boost tends to reduce the colorfulness in the highlight regions of the image.

The automated luminance boost can also be performed in the pipeline of when the image is in the RGB color space. Like the RGB boost the luminance boost can also be used to control contrast in the RGB image . Unlike the RGB separable boost however the luminance boost does not modify the luminance of each R G and B channel individually. Instead the luminance boost is applied to all three channels at the same time. In addition the luminance boost focuses on a transition tonal region that is below the mid tone level used for the RGB separable boost . As roughly indicated in the graph of the transition tonal region for the luminance boost is a region between a quartertone level and the mid tone level . When applied to this tonal region the luminance boost tends to reduce some of the shortcomings associated with separable boosting each channel R G B individually in this tonal region .

When processing is performed using a GPU of a processing device an interpolation function which may be embodied in a cubic equation is used for the interpolation. When processing is performed using a CPU on the other hand the interpolation is preferably embodied in a look up table having a plurality of entries computed by a cubic equation. Such a look up table may have approximately 65 000 entries. For example a pixel having R G Bchannel values in the original image is computed to have a luminance value Lusing standard techniques.

In the interpolation of Block luminance values within the tonal region preferably transition smoothly with those luminance values outside the tonal region. To determine a new luminance value L the computed luminance Lis used in a piecewise cubic function of luminance or in a look up table e.g. L Table L that embodies evaluations of the piecewise cubic function of luminance. In one embodiment the piecewise cubic equation used by the GPU and used to construct the look up table for the CPU can be characterized by 

Any two abutting cubics are preferably designed to have matching values at the luminance threshold that separates them. In addition the abutting cubics are designed to have matching slopes at the same luminance threshold that separates them. Typically the luminance threshold1 is 0.1 the luminance threshold2 is 0.2 and the luminance threshold3 is 0.5. In addition the value of y at x 0 is zero. This implies that the coefficient d is identically equal to 0. The value of y at x 1 is 1 as well. This has implications for the cubic defined by the coefficients m n o and p.

After interpolating the new luminance values a ratio of the new luminance Lto the old luminance Lis calculated Block and the factor is multiplied to the original color values R G Bto obtain the new color values R G Bfor the image to produce a pixel having color values modified by the luminance boost Block . For example the modified pixel will have RGB color values characterized by

As shown in the transitional tonal region is defined by a minimum luminance value and a maximum luminance value . Below the minimum luminance value of the transition tonal region the luminance boost discussed above is used directly to compute the boosted RGB values for this region. Within the transitional tonal region however a hybrid boost is used depending on the amount of noise in the image. In the hybrid boost both the luminance boost and the RGB separable boost are evaluated.

Thus at Block of a determination is made whether the luminance value for the selected pixel lies within the transitional tonal region. If not then the boosted RGB values from the previously calculated luminance boost are used for the given pixel and the process passes to Block .

If the original luminance value for the selected pixel lies within the transitional tonal region at Block then the RGB separable boost for the luminance value is calculated Block . Then the boosted RGB values for those luminance values within the tonal region are computed by interpolating between the previously calculated luminance boost and the RGB separable boost based on where the original luminance value lies within the transition tonal region Block . The interpolant is computed using a smooth function that varies from 0 at the minimum luminance value of the transition tonal region to 1 at the maximum luminance value of the transitional tonal region . As discussed previously the RGB separable boost is used alone for values above the maximum luminance level and the luminance boost is used alone for values below the minimum luminance level .

Preferably the hybrid boost interpolated between the RGB separable and luminance boosts is only employed when the image has a given amount of noise. When the image has little noise for example the RGB separable boost is preferably used for the entire tonal range. If however the image has an amount of noise above a threshold the hybrid boost discussed above is used. If the image has an intermediate amount of noise then both the straight RGB separable boost and the hybrid boost are both calculated and an interpolation between the straight RGB separable boost and the hybrid boost is performed to determine the resulting boost to be used. Once the selected pixel is boosted the process at Block repeats Blocks through for additional pixels of the original image. Finally a resulting image having modified luminance values is produced Block .

As noted previously a processing device e.g. computer imaging device camera etc. having an operating system can perform the automated RAW image processing methods services and techniques disclosed herein. In addition a program storage device readable by a programmable processing device can have instructions stored on the program storage device for causing the programmable processing device to perform the automated RAW image processing methods and techniques disclosed herein.

The foregoing description of preferred and other embodiments is not intended to limit or restrict the scope or applicability of the inventive concepts conceived of by the Applicants. As one example although the present disclosure focused on RGB Bayer pattern and the RGB color space it will be appreciated that the teachings of the present disclosure can be applied to other implementations of colored filter arrays and color spaces. In exchange for disclosing the inventive concepts contained herein the Applicants desire all patent rights afforded by the appended claims. Therefore it is intended that the appended claims include all modifications and alterations to the full extent that they come within the scope of the following claims or the equivalents thereof.

