---

title: Private ethernet overlay networks over a shared ethernet in a virtual environment
abstract: Methods, systems, and computer programs for implementing private Ethernet overlay networks over a shared Ethernet infrastructure in a virtual environment are presented. In one embodiment, a method includes an operation for sending a packet on a private virtual network from a first virtual machine (VM) in a first host to a second VM. The first and second VMs are members of a fenced group of computers that have exclusive direct access to the private virtual network, where VMs outside the fenced group do not have direct access to the packets that travel on the private virtual network. Further, the method includes encapsulating the packet at the first host to include a new header as well as a fence identifier for the fenced group. If the encapsulated packet is too big for the underlying network, the packet is fragmented for transmission between hosts. The packet is received at a host where the second VM is executing and the packet is de-encapsulated to extract the new header and the fence identifier. Additionally, the method includes an operation for delivering the de-encapsulated packet to the second VM after validating that the destination address in the packet and the fence identifier correspond to the destination address and the fence identifier, respectively, of the second VM. The private virtual network scheme is transparent to the VM's operating system, and unicast messaging within the fenced group improves network efficiency.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08892706&OS=08892706&RS=08892706
owner: Vmware, Inc.
number: 08892706
owner_city: Palo Alto
owner_country: US
publication_date: 20100621
---
This application is related by subject matter to U.S. patent application Ser. No. 12 510 072 filed Jul. 27 2009 and entitled AUTOMATED NETWORK CONFIGURATION OF VIRTUAL MACHINES IN A VIRTUAL LAB ENVIRONMENT U.S. patent application Ser. No. 12 510 135 filed Jul. 27 2009 and entitled MANAGEMENT AND IMPLEMENTATION OF ENCLOSED LOCAL NETWORKS IN A VIRTUAL LAB and U.S. patent application Ser. No. 12 571 224 filed Sep. 30 2009 and entitled PRIVATE ALLOCATED NETWORKS OVER SHARED COMMUNICATIONS INFRASTRUCTURE U.S. patent application Ser. No. 11 381 119 filed May 1 2006 and entitled VIRTUAL NETWORK IN SERVER FARM all of which are incorporated herein by reference.

The present invention relates to methods systems and computer programs for deploying fenced groups of Virtual Machines VMs in a virtual infrastructure and more particularly to methods systems and computer programs for private networking among fenced groups of VMs executing in multiple hosts of the virtual infrastructure.

Virtualization of computer resources generally involves abstracting computer hardware which essentially isolates operating systems and applications from underlying hardware. Hardware is therefore shared among multiple operating systems and applications wherein each operating system and its corresponding applications are isolated in corresponding VMs and wherein each VM is a complete execution environment. As a result hardware can be more efficiently utilized.

The virtualization of computer resources sometimes requires the virtualization of networking resources. To create a private network in a virtual infrastructure means that a set of virtual machines have exclusive access to this private network. However virtual machines can be located in multiple hosts that may be connected to different physical networks. Trying to impose a private network on a distributed environment encompassing multiple physical networks is a complex problem. Further sending a broadcast message in a private network presents two problems. First the broadcast may be received by hosts which do not host any VMs in the private network thus reducing the scalability of the entire distributed system. Second if hosts are not located on adjacent layer 2 networks the broadcast may not reach all hosts with VMs in the private network.

Virtual Local Area Networks VLAN are sometimes used to implement distributed networks for a set of computing resources that are not connected to one physical network. A VLAN is a group of hosts that communicate as if the group of hosts were attached to the Broadcast domain regardless of their physical location. A VLAN has the same attributes as a physical Local Area Network LAN but the VLAN allows for end stations to be grouped together even if the end stations are not located on the same network switch. Network reconfiguration can be done through software instead of by physically relocating devices. Routers in VLAN topologies provide broadcast filtering security address summarization and traffic flow management. However VLANs only offer encapsulation and by definition switches may not bridge traffic between VLANs as it would violate the integrity of the VLAN broadcast domain. Further VLANs are not easily programmable by a centralized virtual infrastructure manager.

Virtual labs such as VMware s vCenter Lab Manager from the assignee of the present patent application enable application development and test teams to create and deploy complex multi tier system and network configurations on demand quickly. Testing engineers can set up capture and reset virtual machine configurations for demonstration environments in seconds. In addition hands on labs can be quickly configured and deployed for lab testing hands on training classes etc.

The creation of virtual lab environments requires flexible tools to assist in the creation and management of computer networks. For example if a test engineer decides to perform different tests simultaneously on one sample environment the test engineer must deploy multiple times the sample environment. The multiple deployments must coexist in the virtual infrastructure. However these environments often have network configurations that when deployed multiple times would cause networking routing problems such as the creation of VMs with duplicate Internet Protocol IP addresses an impermissible network scenario for the proper operation of the VMs and of the virtual lab environments.

Existing solutions required that VMs within the same private environment be executed on the same host using virtual switches in the host. However the single host implementation has drawbacks such as a maximum number of VMs that can be deployed on a single host inability to move VMs to different hosts for load balancing unexpected host shutdowns etc.

Methods systems and computer programs for implementing private networking within a virtual infrastructure are presented. It should be appreciated that the present invention can be implemented in numerous ways such as a process an apparatus a system a device or a method on a computer readable medium. Several inventive embodiments of the present invention are described below.

In one embodiment a method includes an operation for sending a packet on a private virtual network from a first virtual machine VM in a first host to a second VM. The first and second VMs are members of a fenced group of computers that have exclusive direct access to the private virtual network where VMs outside the fenced group do not have direct access to the packets that travel on the private virtual network. Further the method includes encapsulating the packet at the first host to include a new header as well as a fence identifier for the fenced group. The packet is received at a host where the second VM is executing and the packet is de encapsulated to extract the new header and the fence identifier. Additionally the method includes an operation for delivering the de encapsulated packet to the second VM after validating that the destination address in the packet and the fence identifier correspond to the destination address and the fence identifier respectively of the second VM.

In another embodiment a computer program embedded in a non transitory computer readable storage medium when executed by one or more processors for implementing private networking within a virtual infrastructure includes program instructions for sending a packet on a private virtual network from a first VM in a first host to a second VM. The first and second VMs are members of a fenced group of computers that have exclusive direct access to the private virtual network where VMs outside the fenced group do not have direct access to packets on the private virtual network. Further the computer program includes program instructions for encapsulating the packet at the first host to include a new header and a fence identifier for the fenced group and for receiving the packet at a host where the second VM is executing. Further yet the computer includes program instructions for de encapsulating the packet to extract the new header and the fence identifier and program instructions for delivering the de encapsulated packet to the second VM after validating that a destination address in the packet and the fence identifier correspond to the second VM.

In yet another embodiment a system for private networking within a virtual infrastructure includes a first VM and a first filter in a first host in addition to a second VM and a second filter in a second host. The first and second VMs are members of a fenced group of computers that have exclusive direct access to a private virtual network where VMs outside the fenced group do not have direct access to packets on the private virtual network. The first filter encapsulates a packet sent on a private virtual network from the first VM by adding to the packet a new header and a fence identifier for the fenced group. The second filter de encapsulates the packet to extract the new header and the fence identifier and the second filter delivers the de encapsulated packet to the second VM after validating that a destination address in the packet and the fence identifier correspond to the second VM.

Other aspects of the invention will become apparent from the following detailed description taken in conjunction with the accompanying drawings illustrating by way of example the principles of the invention.

The following embodiments describe methods and apparatus for implementing private networking within a virtual infrastructure. Embodiments of the invention use Media Access Control MAC encapsulation of Ethernet packets. The hosts that include Virtual Machines VM from fenced groups of machines implement distributed switching with learning for unicast delivery. As a result VMs are allowed to migrate to other hosts to enable resource management and High Availability HA . Further the private network implementation is transparent to the guest operating system GOS in the VMs and provides an added level of privacy.

With a host spanning private network HSPN VMs can be placed on any host where the private network is implemented. The HSPN may span hosts in a cluster or clusters in a datacenter allowing large groups of VMs to communicate over the private network. Additionally VMs may move between hosts since VMs maintain private network connectivity. A VM can also be powered on in a different host after failover and still retain network connectivity. Further VMs get their own isolated private level connectivity without the need to obtain Virtual Local Area Networks VLAN ID resources or even setup VLANs. Creating a HSPN is therefore simpler because there is no dependency on the network administrator. The HSPN can be deployed on either a VLAN or an Ethernet segment.

It should be appreciated that some embodiments of the invention are described below using Ethernet Internet Protocol IP and Transmission Control Protocol TCP protocols. Other embodiments may utilize different protocols such as an Open Systems Interconnection OSI network stack and the same principles described herein apply. The embodiments described below should therefore not be interpreted to be exclusive or limiting but rather exemplary or illustrative.

It will be obvious however to one skilled in the art that the present invention may be practiced without some or all of these specific details. In other instances well known process operations have not been described in detail in order not to unnecessarily obscure the present invention.

Many applications run on more than one machine and grouping machines in one configuration is more convenient to manage the applications. For example in a classic client server application the database server may run on one machine the application server on another machine and the client on a third machine. All these machines would be configured to run with each other. Other servers may execute related applications such as LDAP servers Domain Name servers domain controllers etc. Virtual lab server allows the grouping of these dependent machines into a Configuration which can be checked in and out of the library. When a configuration is checked out all the dependent machines configured to work with each other are activated at the same time. Library configurations can also store the running state of machines so the deployment of machines that are already running is faster.

Virtual lab networks also referred to herein as enclosed local networks can be categorized as private networks and shared networks. Private networks in a configuration are those networks available exclusively to VMs in the configuration that is only VMs in the configuration can have a Network Interface Controller NIC or VNIC connected directly to a switch or virtual switch VSwitch for the private network. Access to data on a private network is restricted to members of the configuration that is the private network is isolated from other entities outside the configuration. In one embodiment a private network in the configuration can be connected to a physical network to provide external connectivity to the VMs in the private network. Private networks in a configuration are also referred to herein as Configuration Local Networks CLN or virtual networks. Shared networks also referred to herein as shared physical networks or physical networks are available to all VMs in the virtual infrastructure which means that a configuration including a shared network will enable VMs in the shared network to communicate with other VMs in the virtual infrastructure connected directly or indirectly to the shared network. In one embodiment a shared network is part of a Virtual Local Area Network VLAN .

Deploying a configuration causes the VMs and networks in the configuration to be instantiated in the virtual infrastructure. Instantiating the VMs includes registering the VMs in the virtual infrastructure and powering on the VMs. When an individual VM from a configuration is deployed virtual lab deploys all shared networks and CLNs associated with the configuration using the network connectivity options in the configuration. Undeploying a configuration de instantiates the VMs in the configuration from the virtual infrastructure. De instantiating VMs includes powering off or suspending the VMs and un registering the VMs from the virtual infrastructure. The state of the deployment can be saved in storage or discarded. Saving the memory state helps debugging memory specific issues and makes VMs in the configuration ready for deployment and use almost instantly.

Virtual lab server manages and deploys virtual machine configurations in a collection of hosts . It should be appreciated that not all hosts need to be part of the scope of virtual lab server although in one embodiment all the hosts are within the scope of virtual lab server . Virtual lab server manages hosts by communicating with virtual infrastructure server and by using virtual lab server agents installed on those hosts. In one embodiment virtual lab server communicates with virtual infrastructure server via an Application Programming Interface API for example to request the instantiation of VMs and networks.

Although virtual lab server is used to perform some management tasks on hosts the continuous presence of virtual lab server is not required for the normal operation of deployed VMs which can continue to run even if virtual lab server becomes unreachable for example because a network failure. One or more users interface with virtual lab server and virtual infrastructure via a computer interface which in one embodiment is performed via web browser.

The virtual computer system supports VM . As in conventional computer systems both system hardware and system software are included. The system hardware includes one or more processors CPUs which may be a single processor or two or more cooperating processors in a known multiprocessor arrangement. The system hardware also includes system memory one or more disks and some form of Memory Management Unit MMU . The system memory is typically some form of high speed RAM random access memory whereas the disk is typically a non volatile mass storage device. As is well understood in the field of computer engineering the system hardware also includes or is connected to conventional registers interrupt handling circuitry a clock etc. which for the sake of simplicity are not shown in the figure.

The system software includes VMKernel which has drivers for controlling and communicating with various devices NICs and disk . In VM the physical system components of a real computer are emulated in software that is they are virtualized. Thus VM will typically include virtualized guest OS and virtualized system hardware not shown which in turn includes one or more virtual CPUs virtual system memory one or more virtual disks one or more virtual devices etc. all of which are implemented in software to emulate the corresponding components of an actual computer.

The guest OS may but need not simply be a copy of a conventional commodity OS. The interface between VM and the underlying host hardware is responsible for executing VM related instructions and for transferring data to and from the actual physical memory the processor s the disk s and other devices.

Another feature of virtual lab server is the ability to use multiple copies of VMs simultaneously without modifying them. When machines are copied using traditional techniques the original and the copy cannot be used simultaneously due to duplicate IP addresses MAC addresses and security IDs in the case of Windows . Virtual lab server provides a networking technology called fencing that allows multiple unchanged copies of virtual lab server VMs to be run simultaneously on the same network without conflict while still allowing the VMs to access network resources and be accessed remotely.

Deployment is deployed in fenced mode including private networking module which performs among other things filtering and encapsulation of network packets before sending the packets on the physical network. This way there is no duplication of addresses in the physical network.

A Distributed Virtual DV Filter is associated with VNIC and performs filtering and encapsulation of packets originating in VNIC before the packets reach distributed vSwitch . On the receiving side DV Filter performs filtering and de encapsulation stripping when needed. Distributed vSwitch is connected to one or more physical NICs PNIC that connect host to physical network .

The use of a DV Filter enables the implementation of the cross host private virtual network. The DV filter is compatible with VLAN and other overlays solutions as the encapsulation performed by DV Filter is transparent to switches and routers on the network. More details on the operation of DV Filter are given below in reference to .

Since the new encapsulated packet includes additional bytes it is possible that the resulting encapsulated packet exceeds the Maximum Transmission Unit MTU of layer 2. In this case fragmentation is required. The encapsulated packet is fragmented in 2 different packets transmitted separately and the DV filter at the receiving host de encapsulates the two packets by combining their contents to recreate the encapsulated packet. In one embodiment fragmentation is avoided by increasing the uplink MTU. In another embodiment the VM is configured by the user with an MTU that is smaller from the MTU on the network such that encapsulation can be performed on all packets without fragmentation.

Because of the MAC in MAC Ethernet frame encapsulation the traffic of the private virtual network is isolated from other traffic in the sense that the Ethernet headers of the private network packets are hidden from view. Also the private network packets terminate in hosts that implement the private networking allowing an additional level of control and security. Switches and routers on the network do not see or have to deal with this encapsulation because they only see a standard Ethernet header which is processed the same as any standard Ethernet header. As a result no network infrastructure or additional resources are required to implement private networking there no MAC addressing collisions and VLANs are interoperable with the private virtual network scheme. Also a large number of private networks is possible i.e. 16 million or more per VLAN.

The fence OUI is a dedicated OUI reserved for private virtual networking. Therefore there will not be address collisions on the network because nodes that are not part of the private networking scheme will not use the reserved fence OUI. The destination address in the encapsulating header can also be a broadcast address and all the hosts in the network will receive this packet.

The virtual lab server installation ID is unique on a LAN segment and is managed by virtual lab server . The fence identifier uniquely identifies a private network within the virtual lab server. Fence IDs can be recycled over time. Further the T L field in the encapsulating header includes the fence Ethernet type which is an IEEE assigned number in this case assigned to VMware the assignee of the present application that identifies the protocol carried by the Ethernet frame. More specifically the protocol identified is the Fence protocol i.e. the protocol to perform MAC in MAC framing. The Ethernet type is used to distinguish one protocol from another.

The fence protocol data includes a version ID of the private network implementation or protocol 2 bits a fragment type 2 bits a fragment sequence number and a fence identifier. The fragment type and sequence number indicate if the original packet has been fragmented and if so which fragment number corresponds to the packet. The fence identifier indicates a value assigned to the private virtual network. In one embodiment this field is 24 bits which allows for more than 16 million different private networks per real LAN.

It should be appreciated that the embodiments illustrated in are exemplary data fields for encapsulating network packets. Other embodiments may utilize different fields or may arrange the data in varying manners. The embodiments illustrated in should therefore not be interpreted to be exclusive or limiting but rather exemplary or illustrative.

Packet is a standard ARP broadcast packet including VM A s address as the source address. VM A sends the message through port which is associated with Fence 1. DV Filter receives packet associated with Fence 1 and adds the encapsulating header as described above in reference to to create encapsulated packet . The destination address of the encapsulating header is also an Ethernet broadcast address. DV Filter sends packet to distributed vSwitch for transmittal over the network via physical NIC .

Host 2 receives packet referred to as packet and the Distributed vSwitch forwards packet to the DV Filters for all VNICS since it is a broadcast packet. DV Filter associated with VM B examines the source address. It determines that packet is a private virtual network packet because of the unique fence OUI. This packet comes from Host 1 because the source address includes Host 1 s ID and it is originally from VM A because VM A s Ethernet address is in the original Ethernet header. Since DV Filter did not have an entry for VM A in that private network an entry is added to bridge table mapping VM A with Host 1. More details on the structure of bridge table are given below in reference to .

DV Filter de encapsulates the packet by stripping the encapsulating headers and added data to create packet which is associated with Fence 1 as indicated in the Fence ID of the fence protocol data. DV Filter then checks for ports associated with Fence 1 and the destination address of packet which is every node since it is a broadcast address. Since VM is associated with Fence 1 packet is delivered to VM B . On the other hand VM B will not get delivery of the packet or frame because the DV Filter for VM B not shown will detect that the frame is for Fence 1 nodes and will drop the frame because VM B does not belong to Fence 1. It belongs to Fence 2.

It should be noted that this mechanism provides an added level of security by assuring that the fence is isolated. Packets that have no Fence ID will be dropped and will never make it inside the fence.

After packet is unicast via the physical network Host 1 receives packet which is processed in similar fashion as described in reference to except that the destination address is not a broadcast address. DV Filter determines that the packet is from VM B in Fence 1. Since there is not an entry for VM B in bridge table a new entry for VM B is added to bridge table indicating that VM B is executing in Host 2. Additionally DV Filter proceeds to strip packet to restore original packet sent by VM B by taking out the added header and the additional payload ahead of the original packet. This results in packet which is associated with Fence 1 because the payload in packet indicates that the packet is for a Fence 1 node. Since VM A s port is associated with Fence 1 and the Ethernet destination address packet is successfully delivered to VM A .

It should be noted that although packets are described herein as travelling sent and received among the different entities of the chain of communication it is not necessary to actually transfer the whole packet from one module to the next. For example a pointer to the message may be passed between VNIC and DV filter without having to actually make a copy of the packet.

DV filter for VM A checks bridge table and determines that the destination VM C is executing in Host 1. The corresponding encapsulation is performed to create packet which is forwarded to distributed vSwitch via output leaf . VSwitch determines that the destination address of packet is for a VM inside the host and turns the packet around by forwarding packet to the DV Filter for VM C not shown via input leaf . The DV Filter for VM C strips the headers and after checking the destination address and the Fence ID delivers the packet to VM C s port in VNIC .

Thus encapsulated packet leaving DV Filter includes source and destination address associated with the IDs of hosts 1 and 2 respectively. When DV Filter for VM B receives packet it does not create a new entry in the bridge table because the entry for VM A already exists. Packet is forwarded to VM B via the distributed switch and the VNIC port as previously described.

It should be noted that packet is an Ethernet frame and that the scenario described in is for VMs that are executing in hosts with layer 2 connectivity. If the destination VM were in a host executing in a different LAN segment i.e. a different data link layer segment then MAC in MAC encapsulation would not work because the packet would be sent to a router in the network which may not be aware of the private networking scheme for fencing and would not work properly as the IP header is not where the router would expect it. In this case other fencing solutions for hosts on different networks can be combined with embodiments of the inventions. Solutions for internetwork fencing are described in U.S. patent application Ser. No. 12 571 224 filed Sep. 30 2009 and entitled PRIVATE ALLOCATED NETWORKS OVER SHARED COMMUNICATIONS INFRASTRUCTURE which is incorporated herein by reference. Also a VLAN network can be used to provide layer 2 connectivity to hosts in different networks.

Since packet is a broadcast packet packet will reach all nodes in the same private network as VM A . When packet is received by DV Filter in Host 2 DV Filter detects that message is from VM A in Host 3. Since the bridge table entry for VM A has Host 1 as the host for VM A and the new packet indicates that VM A is now executing in Host 3 the entry for VM A in bridge table is updated to reflect this change. The packet is then delivered to VM B because VM B is part of the private network in Fence 1 and this is a broadcast packet.

As previously described a way to avoid fragmentation is by reducing the MTU in the network configuration of the host. For example if the MTU of a network is 1 500 the network can be configured in the VM as having an MTU of 1 336 reserving 144 bits for the encapsulation by the DV Filter.

The inner MAC address corresponds to the Ethernet of another VM in the same private network. The outer MAC address corresponds to the Ethernet of the host that the VM is on and includes the address that would be added in an encapsulating header to send a message to the corresponding VM. Of course the address may be constructed as described in reference to . For example the entry in DV filter of holds the inner MAC address of VM A and the outer MAC address for Host 1. The used flag indicates if the entry is being used the age flag indicates if the entry has been updated in a predetermined period of time and the seen flag indicates if the entry has been used recently.

The tables in are interrelated. For example the second entry in active ports table of is for port 0x4100b9f86d40. The OPI is 3e 0000fb which means that the installation ID is 3e and the fence ID is 0000fb. In the bridge table of it can be observed that outer MAC addresses for port 0x4100b9f86d40 have the same OUI 00 13 f5 and the same installation ID . The remainder of the outer MAC address corresponds host IDs for different hosts 02 c2 02 e2 03 02 and 02 f2 .

The packet is received at a host where the second VM is executing in operation and the method continues in operation for de encapsulating the packet to extract the new header and the fence identifier. In operation the de encapsulated packet is delivered to the second VM after validating that the destination address in the packet and the fence identifier correspond to the address of the second VM and the fence identifier of the second VM.

Embodiments of the present invention may be practiced with various computer system configurations including hand held devices microprocessor systems microprocessor based or programmable consumer electronics minicomputers mainframe computers and the like. The invention can also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a network.

With the above embodiments in mind it should be understood that the invention can employ various computer implemented operations involving data stored in computer systems. These operations are those requiring physical manipulation of physical quantities. Any of the operations described herein that form part of the invention are useful machine operations. The invention also relates to a device or an apparatus for performing these operations. The apparatus may be specially constructed for the required purpose such as a special purpose computer. When defined as a special purpose computer the computer can also perform other processing program execution or routines that are not part of the special purpose while still being capable of operating for the special purpose. Alternatively the operations may be processed by a general purpose computer selectively activated or configured by one or more computer programs stored in the computer memory cache or obtained over a network. When data is obtained over a network the data maybe processed by other computers on the network e.g. a cloud of computing resources.

The invention can also be embodied as computer readable code on a computer readable medium. The computer readable medium is any data storage device that can store data which can be thereafter be read by a computer system. Examples of the computer readable medium include hard drives network attached storage NAS read only memory random access memory CD ROMs CD Rs CD RWs magnetic tapes and other optical and non optical data storage devices. The computer readable medium can include computer readable tangible medium distributed over a network coupled computer system so that the computer readable code is stored and executed in a distributed fashion.

Although the method operations were described in a specific order it should be understood that other housekeeping operations may be performed in between operations or operations may be adjusted so that they occur at slightly different times or may be distributed in a system which allows the occurrence of the processing operations at various intervals associated with the processing as long as the processing of the overlay operations are performed in the desired way.

Although the foregoing invention has been described in some detail for purposes of clarity of understanding it will be apparent that certain changes and modifications can be practiced within the scope of the appended claims. Accordingly the present embodiments are to be considered as illustrative and not restrictive and the invention is not to be limited to the details given herein but may be modified within the scope and equivalents of the appended claims.

