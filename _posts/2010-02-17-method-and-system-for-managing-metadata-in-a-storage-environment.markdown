---

title: Method and system for managing metadata in a storage environment
abstract: Method and system is provided for managing metadata for a plurality of data containers that are stored at one or more storage volumes in a storage system. The metadata is collected from one or more storage volumes and then provided to a catalog module. The catalog module pre-processes the metadata and then generates a searchable data structure. The searchable data structure may then be used to respond to a user request for information regarding the storage system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08595237&OS=08595237&RS=08595237
owner: NETAPP, Inc.
number: 08595237
owner_city: Sunnyvale
owner_country: US
publication_date: 20100217
---
This patent application is related to U.S. patent application entitled METHOD AND SYSTEM FOR MANAGING METADATA IN A CLUSTER BASED STORAGE ENVIRONMENT Ser. No. 12 706 953 filed on even date herewith and the disclosure of which is incorporated herein by reference in its entirety.

Various forms of storage systems are used today. These forms include direct attached storage DAS network attached storage NAS systems storage area networks SANs and others. Network storage systems are commonly used for a variety of purposes such as providing multiple users with access to shared data backing up data and others.

A storage system typically includes at least one computing system executing a storage operating system for storing and retrieving data on behalf of one or more client processing systems clients . The storage operating system stores and manages shared data containers in a set of mass storage devices such as magnetic or optical disks or tapes.

In traditional storage environments the operating system is typically geared towards handling access to one object at a time. Access to a group of data containers within a file system is difficult because the operating system layout is such that metadata for data containers for example a file name attributes access control lists and information regarding an owner of the data container may not be stored contiguously at a storage device and may be stored at different locations. Therefore it is difficult for an operating system to respond to user queries for information regarding a data container or a group of data containers because one typically has to traverse through a namespace and perform an extensive directory search. The term namespace refers to a virtual hierarchical collection of unique volume names or identifiers and directory paths to the volumes in which each volume represents a virtualized container storing a portion of the namespace descending from a single root directory. This is inefficient because metadata information is stored at various locations and also a directory may have a large number of files within a namespace. Continuous efforts are being made to integrate managing data containers and the metadata for the data containers.

In one embodiment a method and system is provided for managing metadata for a plurality of data containers that are stored at one or more storage volumes in a storage system. The metadata is collected from one or more storage volumes and then provided to a catalog module. The catalog module pre processes the metadata and then generates a searchable data structure. The searchable data structure may then be used to respond to a user request for information regarding the storage system.

In another embodiment a machine implemented method for a storage system is provided. The method includes configuring a data storage volume for collecting metadata for a plurality of data containers stored at the data storage volume. The metadata includes at least an attribute that is associated with the plurality of data containers. A storage volume is configured to operate as a catalog volume for storing metadata associated with the plurality of data containers. The metadata for the plurality of data containers is collected and pre processed by extracting one or more fields. The pre processed metadata is stored in a searchable data structure at the catalog volume for responding to a user query requesting information regarding the plurality of data containers.

In yet another embodiment a machine implemented method for a storage system for storing a plurality of data containers at one or more storage volumes is provided. The method includes pre processing metadata associated with the plurality of data containers where the metadata includes an attribute that is associated with the plurality of data containers. A searchable data structure is then generated by indexing the pre processed metadata such that information related to the plurality of data containers is obtained regardless of a storage volume location.

In another embodiment a machine implemented method for a storage system for storing a plurality of data containers at one or more storage volumes is provided. The method includes indexing metadata associated with the plurality of data containers where the metadata includes an attribute that is associated with the plurality of data containers and the metadata is collected from at least one storage volume. The indexed metadata is then stored in a searchable data structure which may be used for obtaining information regarding the plurality of data containers. The searchable data structure stores a snapshot table identifier for identifying a snapshot when the plurality of data containers were replicated and a time stamp when the snapshot was taken.

This brief summary has been provided so that the nature of this disclosure may be understood quickly. A more complete understanding of the disclosure can be obtained by reference to the following detailed description of the various embodiments thereof in connection with the attached drawings.

The following definitions are provided as they are typically but not exclusively used in the computing storage environment implementing the various adaptive embodiments described herein.

 Aggregate is a logical aggregation of physical storage i.e. a logical container for a pool of storage combining one or more physical mass storage devices e.g. disks or parts thereof into a single logical storage object which includes or provides storage for one or more other logical data sets at a higher level of abstraction e.g. volumes .

 CIFS means the Common Internet File System Protocol an access protocol that client systems use to request file access services from storage systems over a network.

 FC means Fibre Channel a high speed network technology primarily used for storage networking. Fibre Channel Protocol FCP is a transport protocol similar to Transmission Control Protocol TCP used in Internet Protocol IP networks which predominantly transports SCSI commands over Fibre Channel networks.

 iSCSI means the Internet Small Computer System Interface an IP based storage networking standard for linking data storage facilities. The standard allows carrying SCSI commands over IP networks. iSCSI may be used to transmit data over local area networks LANs wide area networks WANs or the Internet and can enable location independent data storage and retrieval.

 Metadata refers to one or more attributes for a data container for example a directory or data file. The attributes include a a unique data container identifier for example an inode number b a data container type i.e. if the data container is a directory file and others c information regarding whether the data container was created modified or deleted d a data container name for example NFS file name and CIFS file name and path e an owner identifier for example an NFS user identifier or a CIFS owner identifier f a group identifier for example an NFS group identifier GID g a data container size h permissions associated with the data container for example NFS permission bits that provide information regarding permissions associated with the data container i time the data container was accessed access time j time the data container was modified modification time k time the data container was created creation time when applicable and l any other custom fields that may be specified by a user or a storage system for example access control lists ACLS or a named stream which is a CIFS level feature that connects a file to a directory or any other attribute.

 Namespace refers to a virtual hierarchical collection of unique volume names or identifiers and directory paths to the volumes in which each volume represents a virtualized container storing a portion of the namespace descending from a single root directory. For example each volume associated with a namespace can be configured to store one or more data files scripts word processing documents executable programs and others. In a typical storage system the names or identifiers of the volumes stored on a storage server can be linked into a namespace for that storage server. The term global namespace refers to a virtual hierarchical collection of unique volume names or identifiers and directory paths to the volumes in which the volumes are stored on multiple server nodes within a clustered storage server system. The term virtual in this context means a logical representation of an entity.

 Snapshot without derogation to any trademark rights of NetApp Inc. means a point in time copy of a storage file system. A snapshot is a persistent point in time image of an active file system that enables quick recovery of data after data has been corrupted lost or altered. Snapshots can be created by copying the data at each predetermined point in time to form a consistent image or virtually by using a pointer to form the image of the data.

 Volume is a logical data set which is an abstraction of physical storage combining one or more physical mass storage devices e.g. disks or parts thereof into a single logical storage object and which is managed as a single administrative unit such as a single file system. A volume is typically defined from a larger group of available storage such as an aggregate.

As used in this disclosure the terms component module system and the like are intended to refer to a computer related entity either software executing general purpose processor hardware firmware and a combination thereof. For example a component may be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer.

By way of illustration both an application running on a server and the server can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers. Also these components can execute from various computer readable media having various data structures stored thereon. The components may communicate via local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems via the signal .

Computer executable components can be stored for example on computer readable media including but not limited to an ASIC application specific integrated circuit CD compact disc DVD digital video disk ROM read only memory floppy disk hard disk EEPROM electrically erasable programmable read only memory memory stick or any other storage device in accordance with the claimed subject matter.

Storage environment may include a plurality of storage systems each coupled to a storage subsystem . A storage subsystem may include multiple mass storage devices may also be referred to as that may be used to store a plurality of data containers for example directory files and data files as well as the searchable data structure as described below. The mass storage devices in each storage subsystem may be for example conventional magnetic disks optical disks such as CD ROM or DVD based storage magneto optical MO storage flash memory storage device or any other type of non volatile storage devices suitable for storing data.

Each storage subsystem is managed by a corresponding storage system . The storage devices in each storage subsystem can be organized into one or more redundant array of inexpensive disks RAID groups in which case the corresponding storage system accesses the storage subsystem using an appropriate RAID protocol.

Each storage system may operate as a NAS based file server a block based storage server such as used in a storage area network SAN or a combination thereof or a node in a clustered environment described below with respect to or any other type of storage server. Note that certain storage systems from NetApp Inc. in Sunnyvale Calif. are capable of providing clients with both file level data access and block level data access.

Storage environment may also include a plurality of client systems . . may also be referred to as a management console executing a catalog module and at least one network communicably connecting the client systems . . storage system and management console . The client systems . . may be connected to the storage systems via the computer network such as a packet switched network.

Clients . . may be general purpose computers having a plurality of components. These components may include a central processing unit CPU main memory I O devices and storage devices for example flash memory hard drives and others . The main memory may be coupled to the CPU via a system bus or a local memory bus. The main memory may be used to provide the CPU access to data and or program information that is stored in main memory at execution time. Typically the main memory is composed of random access memory RAM circuits. A computer system with the CPU and main memory is often referred to as a host system.

Processors executing instructions in storage system and client systems . . communicate according to well known protocols such as the NFS protocol or the CIFS protocol to make data stored on disk appear to users and or application programs as though the data were stored locally on the client systems . .. The storage system can present or export data stored on disks as a volume or one or more qtree sub volume units to each of the client systems . .. Each volume may be configured to store data files scripts word processing documents executable programs and the like. As described below in more detail a volume may be configured to operate as a catalog volume that stores a searchable data structure with metadata information regarding directories and data files stored on disks .

From the perspective of one of the client systems . . each volume can appear to be a single disk drive. However each volume can represent the storage space in one disk an aggregate of some or all of the storage space in multiple disks a RAID group or any other suitable set of storage space.

Specifically each volume can include a number of individually addressable files. For example in a NAS configuration the files of a volume are addressable over the computer network for file based access. In addition an aggregate is a fixed sized volume built on top of a number of RAID groups containing one or more virtual volumes or FlexVol flexible volumes.

In a typical mode of operation one of the client systems . . transmits one or more input output commands such as an NFS or CIFS request over the computer network to the storage system . Storage system receives the request issues one or more I O commands to storage device to read or write the data on behalf of the client system . . and issues an NFS or CIFS response containing the requested data over the network to the respective client system.

The management console that executes storage management application may also be referred to as management application may be for example a conventional PC workstation or the like. In another embodiment management application may also be executed by storage system . The management application may be a module with executable instructions typically used by a storage network administrator to manage a pool of storage devices. Management application enables the administrator to perform various operations such as monitoring and allocating storage space in the storage pool creating and deleting volumes directories and others.

In one embodiment management application includes a catalog module that interfaces with storage system for receiving metadata pre processes the collected metadata and then stores it in a searchable structure for example a relational database . Although catalog module is shown as a part of management application it may operate as a standalone application or may also be integrated with the operating system of storage system . Furthermore although catalog module is shown in the context of a NAS in it can be used effectively in a direct attached storage system not shown as well.

Communication between the storage management application clients and storage systems may be accomplished using any of the various conventional communication protocols and or application programming interfaces APIs the details of which are not germane to the technique being introduced here. This communication can be implemented through the network or it can be via a direct link not shown between the management console and one or more of the storage systems .

One or more other storage related applications may also be operatively coupled to the network residing and executing in one or more other computer systems . Examples of such other applications include data backup software snapshot management software and others. It is noteworthy that these applications may also be running at storage system .

The communication module implements one or more conventional communication protocols and or APIs to enable the storage management application to communicate with the storage system and cluster system .

The storage management application may also maintain policies a list of all volumes in a storage pool as well as a table of all free space on a per disk basis in a storage pool. Policies may be used to store configuration information based on which metadata is collected pre processed indexed and then stored in database . Details regarding database are provided below.

The following describes a cluster based storage system may also be referred to as clustered storage system or cluster storage system in a storage environment of . The clustered system is a scalable distributed architecture that stores data containers at different storage devices that are managed by a plurality of nodes. When configured metadata for each node is collected and provided to an instance of catalog module executed at each node. The metadata is pre processed and then stored in a searchable format. More details regarding processing of metadata is provided below.

Storage environment may include a plurality of client systems . . may also be referred to as a cluster storage system management console and at least one computer network similar to network communicably connecting the client systems . . and a clustered storage system .

The clustered storage system includes a plurality of nodes . . may also be referred to as a cluster switching fabric and a plurality of mass storage devices such as disks . . may also be referred to as disks similar to storage . Each of the plurality of nodes . . in the clustered storage system provides the functionality of a storage server. Clustered storage systems like the clustered storage system are available from NetApp Inc.

Each of the plurality of nodes . . may be configured to include an N module a D module and an M host each of which can be implemented as a separate software module. Specifically node . includes an N module . a D module . and an M host . node . includes an N module . a D module . and an M host . and node . includes an N module . a D module . and an M host ..

The N modules . . may also be referred to as include functionality that enables the respective nodes . . to connect to one or more of the client systems . . over the computer network while the D modules . . may also be referred to as connect to one or more of the disks . .. The D modules interface with a metadata collection module See and provides metadata for a plurality of data containers stored at one or more of disks .

The M hosts . . may also be referred to as provide management functions for the clustered storage server system . In one embodiment each M host includes or interfaces with an instance of catalog module similar to for receiving collected metadata pre processing the collected metadata and then storing the information is a searchable data structure.

A switched virtualization layer including a plurality of virtual interfaces VIFs may also be referred to a logical interfaces LIFs is provided between the respective N modules . . and the client systems . . allowing the disks . . associated with the nodes . . to be presented to the client systems . . as a single shared storage pool.

In one embodiment the clustered storage system can be organized into any suitable number of virtual servers also referred to as vservers in which each vserver represents a single storage system namespace with separate network access. Each vserver has a user domain and a security domain that are separate from the user and security domains of other vservers. Moreover each vserver is associated with one or more VIFs and can span one or more physical nodes each of which can hold one or more VIFs and storage associated with one or more vservers. Client systems can access the data on a vserver from any node of the clustered system but only through the VIFs associated with that vserver. The interaction between a vserver and catalog module is described below with respect to .

Each of the nodes . . is defined as a computer adapted to provide application services to one or more of the client systems . .. In this context a vserver is an instance of an application service provided to a client system. The nodes . . are interconnected by the switching fabric which for example may be embodied as a Gigabit Ethernet switch. Although depicts an equal number i.e. 3 of the N modules . . the D modules . . and the M Hosts . . any other suitable number of N modules D modules and M Hosts may be provided. There may also be different numbers of N modules D modules and or M Hosts within the clustered storage server system . For example in alternative embodiments the clustered storage server system may include a plurality of N modules and a plurality of D modules interconnected in a configuration that does not reflect a one to one correspondence between the N modules and D modules.

The clustered storage server system can include the NETAPP DATA ONTAP storage operating system available from NetApp Inc. that implements the WAFL storage system or any other suitable storage operating system.

The client systems . . of may be implemented as general purpose computers configured to interact with the respective nodes . . in accordance with a client server model of information delivery.

Each client system . . may request the services of one of the respective nodes . . . and that node may return the results of the services requested by the client system by exchanging packets over the computer network which may be wire based optical fiber wireless or any other suitable combination thereof. The client systems . . may issue packets according to file based access protocols such as the NFS protocol or the CIFS protocol when accessing information in the form of files and directories.

In a typical mode of operation one of the client systems . . transmits an NFS or CIFS request for data to one of the nodes . . within the clustered storage server system and the VIF associated with the respective node receives the client request. It is noted that each VIF within the clustered system is a network endpoint having an associated IP address and that each VIF can migrate from N module to N module. The client request typically includes a file handle for a data file stored in a specified volume on one or more of the disks . ..

Specifically each volume comprises a storage system subtree that includes an index node file an inode file having a root inode and a set of directories and files contained under the root inode. Each inode is a data structure allocated for a respective data file to store metadata that describes the data file. For example an inode can contain data and pointers for use in facilitating access to blocks of data within the data file and each root inode can contain pointers to a number of inodes.

Before describing the details of catalog module and how it interfaces with various components of storage environment and the following provides a description of a storage operating system that may be used in storage environment and according to one embodiment.

In one example operating system may include several modules or layers executed by one or both of N Module and D Module . These layers include a file system manager that keeps track of a directory structure hierarchy of the data stored in storage devices and manages read write operations i.e. executes read write operations on disks in response to client requests.

Operating system may also include a protocol layer and an associated network access layer to allow node . to communicate over a network with other systems such as clients and storage management application . Protocol layer may implement one or more of various higher level network protocols such as NFS CIFS Hypertext Transfer Protocol HTTP TCP IP and others as described below.

Network access layer may include one or more drivers which implement one or more lower level protocols to communicate over the network such as Ethernet. Interactions between clients and mass storage devices are illustrated schematically as a path which illustrates the flow of data through operating system .

The operating system may also include a storage access layer and an associated storage driver layer to allow D module to communicate with a storage device. The storage access layer may implement a higher level disk storage protocol such as RAID while the storage driver layer may implement a lower level storage device access protocol such as FC or SCSI. In one embodiment the storage access layer may implement the RAID protocol such as RAID 4 or RAID DP RAID double parity for data protection provided by NetApp Inc. the assignee of the present disclosure .

In one embodiment storage access layer obtains metadata for various data containers that may be stored in a data volume and provides that information to catalog module . The information is processed and then stored in a searchable data structure as described below.

N blade and D blade may interface with each other using CF protocol . Both blades may also include interface and to communicate with other nodes and systems.

The multi protocol engine includes a media access layer part of layer of network drivers e.g. Gigabit Ethernet drivers that interfaces to network protocol layers part of layer such as the IP layer and its supporting transport mechanisms the TCP layer and the User Datagram Protocol UDP layer .

A file system protocol layer provides multi protocol file access and to that end includes support for the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the HTTP protocol .

A virtual interface VI layer implements the VI architecture to provide direct access transport DAT capabilities such as RDMA Remote Direct Memory Access as required by the DAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the node. The FC and iSCSI drivers provide FC specific and iSCSI specific access control to the blocks and thus manage exports of LUNS to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing the blocks on the node ..

The storage server includes a file system module in cooperating relation with a volume stripped module VSM a RAID system module and a disk driver system module .

The VSM illustratively implements a striped volume set SVS . The VSM cooperates with the file system to enable storage server to service a volume of the SVS.

The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with I O operations while the disk driver system implements a disk access protocol such as e.g. the SCSI protocol.

The file system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustratively embodied as e.g. a virtual disk vdisk module not shown and a SCSI target module . The SCSI target module is generally disposed between the FC and iSCSI drivers and the file system to provide a translation layer of the virtualization system between the block lun space and the file system space where luns are represented as blocks.

The file system is illustratively a message based system that provides logical volume management capabilities for use in access to the information stored on the storage devices such as disks.

The file system illustratively may implement a write anywhere file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using index nodes inodes to identify data containers and metadata for the data container such as creation time access permissions size and others . The file system uses data containers to store metadata describing the layout of its file system these metadata data containers include among others an inode data container. A data container handle i.e. an identifier that includes an inode number inum may be used to retrieve an inode from disk.

Typically the metadata as handled by file system may not be stored contiguously and may be spread out among different storage volumes. This makes it difficult for the file system to provide user requested information that can be derived from the metadata. Hence as described below in more detail the present catalog module is being introduced to manage organize and use the metadata for the data containers.

Broadly stated all inodes of the write anywhere file system are organized into the inode data container. A file system fs info block specifies the layout of information in the file system and includes an inode of a data container that includes all other inodes of the file system. Each logical volume file system has an fsinfo block that is preferably stored at a fixed location within e.g. a RAID group. The inode of the inode data container may directly reference point to data blocks of the inode data container or may reference indirect blocks of the inode data container that in turn reference data blocks of the inode data container. Within each data block of the inode data container are embedded inodes each of which may reference indirect blocks that in turn reference data blocks of a data container.

Operationally a request from the client is forwarded as a packet over the computer network and onto the node .. A network driver processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the write anywhere file system . Here the file system generates operations to load retrieve the requested data from disk if it is not resident in core i.e. in memory .

If the information is not in memory the file system indexes into the inode data container using the inode number inum to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system the logical vbn is mapped to a disk identifier and disk block number disk dbn and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client .

It should be noted that the software path through the operating system layers described above needed to perform data storage access for a client request received at node . may alternatively be implemented in hardware. That is in an alternate embodiment of the disclosure the storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an ASIC. This type of hardware implementation increases the performance of the file service provided by node . in response to a file system request issued by client .

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node . implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this disclosure can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and a disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present invention may be utilized with any suitable file system including a write in place file system.

As shown in the global namespace includes the volume RT i.e. the root volume which has three junctions linking the volume RT to the volumes vol vol vol. The global namespace further includes the volume vol which has two junctions linking the volume vol to the volumes vol vol.

As shown in data containers and the metadata associated with the data containers may be spread out among various volumes. In order to get information regarding data containers storage usage and other user queries that rely on metadata information one has to traverse the namespace and evaluate individual directory entries. Catalog module as described below in detail efficiently organizes the metadata in a searchable data structure such that metadata can be easily searched and hence utilized to process user requests.

System includes a catalog module similar to catalog module that may be executed by or integrated with M host for a clustered environment or operates as module for example of management application for storage environment . Catalog module includes a catalog controller module also referred to as catalog controller that interfaces with various modules and implements various cataloging related process steps as described below.

Catalog controller interfaces with a configuration module that stores configuration information regarding cataloging metadata for a plurality of data containers at one or more data volumes. Configuration information may include information regarding how often metadata may be collected frequency and manner for indexing the collected data as well as details regarding any actions reports that a user may seek based on the collected metadata. Configuration module may be a memory module that is accessible by catalog controller .

Catalog module may also include a catalog scheduler that interfaces with catalog controller and schedules cataloging jobs. The cataloging jobs may include collecting metadata arranging or indexing the collected metadata generating reports based on collected and indexed metadata data performing a search based on a user request as well as taking an action based on the search results.

In one embodiment catalog scheduler receives a client request or may create a job request based on configuration information stored at configuration module . The job request may be for collecting metadata arranging or indexing the collected metadata generating reports based on the collected and indexed metadata data and performing a search based on a user request.

System may further include metadata collection module may also be referred to as a metadata collector module a pre processing module and a database engine . Metadata collection module is used to collect metadata from operating system for a plurality of data containers stored at a data volume for example . The structure and operation of metadata collection module depends on the storage environment. For example in one embodiment in storage environment an instance of metadata collection module may be a part of storage system . In this example metadata collection module interfaces with the file system and obtains metadata regarding a plurality of data containers stored in within volume .

In another embodiment for storage environment metadata collection module may be executed at each node . In this example metadata collection module interfaces with each D blade to collect metadata for a plurality of data containers that may be stored within a volume accessible to each node .

The information collected by metadata collector depends on user needs and how system is configured. An example of the type of information that is collected is provided below.

In one embodiment metadata is collected for an initial version of the plurality of data containers. This may be referred to as baseline metadata information or baseline image . Storage environments typically maintain a snapshot of the file system and the associated data containers. A file system manager or any other module may take the actual snapshot and communicate it to catalog controller . The snapshots being a point in time copy of the file system may be used to restore a storage file system to an instance when the snapshot was taken.

A first snapshot for a data volume operates as a starting point and once that is created metadata for data containers that may have changed after the first snapshot is collected and processed. One process that may be used to obtain differential information is called SnapDiff that is provided by NetApp Inc. the assignee of the present application. Metadata collection module may use the SnapDiff process to first obtain baseline metadata information for the plurality of data containers that may be stored in data volume . Once the baseline is established metadata collection module may only collect information for data containers that may have been created modified or deleted from the baseline snapshot. If there are no changes to data containers after the baseline image then metadata for those data containers is not collected. It is noteworthy that system may establish any snapshot to be a baseline and then collect incremental metadata for data containers that are modified or created after the baseline is established.

Metadata collection module provides the collected metadata to catalog controller via an interface similar to interface . The collected metadata is initially handled by pre processing module that receives the metadata and stores it in an intermediate data structure may also be referred to as staging table or intermediate table . Information from the intermediate table is then used by database engine for populating database in a catalog volume . It is noteworthy that although pre processing module is shown as a separate module it could be implemented as part of database engine .

The following provides an example of what information is collected pre processed and stored in intermediate table and then stored in database as a searchable data structure.

 a Unique data container identifier for example an inode number b a data container type i.e. if the data container is a directory file and others c information regarding whether the data container was accessed created modified or deleted d a data container name for example NFS file name and CIFS file name and path e an owner identifier for example an NFS user identifier UID or a CIFS owner identifier f a group identifier for example an NFS group identifier GID g a data container size h permissions associated with the data container for example NFS permission bits that provide information regarding permissions associated with the data container i time the data container was accessed access time j time the data container was modified modification time k time the data container was created creation time when applicable and l any other user specified fields.

The pre processing module takes the foregoing information extracts a plurality of fields and populates them in intermediate data structure . For example pre processing module extracts the unique identifier value the NFS and CIFS accessible path where the data container resides data container name i.e. NFS and CIFS accessible name and extension of the data container that identifies a property of the container for example if a data container is a data file xyz.doc then the pre processing module extracts the .doc .

The pre processing module also extracts information to identify the data container type i.e. a file directory or others creation time of the data container last time it was accessed and modified if applicable. The pre processing module separates UID GID permission bits and the size of the data container.

In case the data container is a part of a directory and a snapshot the pre processing module generates a unique identifier that identifies the snapshot. The pre processing module also generates a flag that identifies whether the data container was created modified or deleted.

Once the intermediate table is populated database engine takes that information and then either creates database if one does not exist or modifies an exiting database . In one embodiment database may be a relational database that includes one or more components. Database may include a plurality of searchable segments that are described below in detail. A user may request information regarding data containers and catalog module provides user requested information using database .

A reporting module is also provided such that user requested information may be compiled into reports. The layout and structure of the report will depend on the user needs and the user preferences. The user may set these reporting preferences using management application via a user interface.

Before describing the details of database the following provides an example of using catalog system in storage environment similar to as shown in according to one embodiment. Each node in storage environment may execute an instance of catalog module that is described above with respect to . Each node may also execute an instance of metadata collection module shown as C . . to collect metadata from D blades . .

Storage environment includes a plurality of volumes namely . Volumes and are managed by D . volumes are managed by D . and volumes and are managed by Dn .. The volumes in storage environment may be provided to different virtual servers via VIFs and . For example VIF provides access to volume VIF provides access to volume VIF provides access to volume and VIF provides access to volume . Catalog module manages metadata for the various vservers as if they were individual nodes.

Metadata collection module . collects metadata for volumes and . Catalog module at node . then preprocesses the metadata and stores it at catalog volume . Metadata collection module . collects metadata for volumes . The collected and pre processed metadata is then stored at catalog volume . Similarly metadata collection module .collects metadata for volumes and which is then stored at catalog volumes

In one embodiment a query involving metadata stored at different catalog volumes for example and may be generated. The catalog module at the node where the query is generated gathers metadata from different catalog volumes and then the results are aggregated together and presented as requested by the query. For example when catalog module at node . receives a request for information regarding data containers stored at volumes then catalog module gathers information from catalog volumes and and presents the aggregated information to the user.

It is noteworthy that the systems disclosed herein for example are scalable. Based on storage space utilization and overall performance one can assign any volume to operate as a catalog volume. One can also add new catalog volumes to store metadata. Furthermore a same volume may be configured to store data containers and metadata.

Snapshot table may include a plurality of fields A F. Field A ID may be used to identify a snapshot itself. Field B NAME may be used to name the snapshot. Field C Creation Time may be used to store the time when the snapshot was taken. Field D Index Start Time stores a time when metadata collected for a particular snapshot was indexed. Field E Index End Time may be used to store a time when information for the snapshot was indexed.

The metadata for a particular snapshot may be indexed based on a schedule that may be established by a user during storage system configuration a request generated by the user initiated by a management application based on whether the overall storage system is busy doing other tasks or if the system is idle. The indexing itself can be optimized such that it does not negatively impact the overall performance of the storage environment.

Field F ATTR may be used to store attribute information regarding a snapshot. For example field F may include a snapshot version indicator indicating a snapshot when a change in status for the data container was discovered.

Besides the fields shown in other fields may also be added. One such field may be referred to as a tag . A tag is a user defined field that one can add for example a user may want to identify all files that are labeled as confidential by using a confidential tag. The systems and processes described herein allow one to search for metadata based on the tags.

Table may be used to determine if there are any snapshots. The snapshots itself may be taken by the file system manager or any other module and communicated to the catalog module via catalog interface See . In one embodiment whenever a snapshot is taken file system manager may send a notification to the catalog module.

When the first snapshot is taken then metadata collected for that snapshot may be used as a baseline image for database table as described below. As more snapshots are taken metadata for data containers that were created modified or deleted from the initial snapshot is collected and indexed as described below. If there is no change in the data containers after the initial snapshot then no metadata is collected for the unchanged data containers.

Field A Identifier may be a unique identifier to identify a directory for example an inode number an inode generation number or both. Field B Parent identifies a parent for the directory. The parent in this case is an upper level directory to which the directory identified by A may belong. Field C provides a directory path.

Field D provides a name for the directory. Field E provides a directory size. Field F Mode provides the permissions associated with a directory. The permissions indicate what level of authority a user has with respect to a particular directory. Permissions may range from being able to read the directory entry to be able to create modify or delete the entry and other permission types.

Field G identifies the owner of the directory shown as uid . Field H identifies a group to which the directory may belong to shown as Gid . In an enterprise having different business groups for example engineering sales marketing legal and others a storage system may be divided among different entities. Field H identifies the group to which a particular directory belongs.

Field I Atime provides a time when the directory was last accessed while field J Ctime provides a time when the directory was created. Field K Mtime includes a time when the directory was modified. Field L includes a flag that indicates whether an entry was added by using a flag A modified by using a flag M or deleted by using a flag D .

Field M identifies a snapshot to which the directory may belong. This may be similar to field A shown in .

Database may also include a second searchable segment for example a data container table that may store metadata information regarding a plurality of data containers. shows an example of data container table that stores information regarding a plurality of data containers for example files. Each file in the data container table is associated with an entry in the directories table . This allows one to include a path for a file only once in the directories table and one does not have to copy the path in data container table every time the metadata for the file is indexed.

Data container table may include various fields A L. Field A identifies the file with a unique identifier for example an inode number an inode generation number or both. Field B associates a parent to the data container identified by field A. This field maps to an entry in the directory table . Because of this cross reference to the directory table one does not have to enter the data container path for all individual data container entries. This saves memory space and processing time. For example if there are one million files in a storage system if one tried to save the paths for all one million files it would take space and processing time. Instead in one embodiment field B cross references to a directory entry in data structure where the path for each entry in data structure is located.

In another embodiment the structure of cross referencing files to directory entry also reduces processing time when a directory is renamed. For example if each file had an entry that provided the storage path and directory name then one would have to go and change entries for each individual files. Using the foregoing scheme one only has to update directory names and individual path entries do not need to be updated.

Field C includes a data container name for example a file name while field D includes a size of the data container. Field E Mode identifies the permissions that may be associated with the data container. This includes whether a user is permitted to simply read the data container content modify it or delete it.

Field F UID identifies the owner of the data container while field G GID identifies the group to which the data container belongs.

Field H Atime identifies the time the data container is accessed field I Ctime identifies the time it was created while field J Mtime identifies the time the data container was modified if applicable. Field K is a flag that indicates whether the data container was created A modified M or deleted D . Field L identifies the snapshot if applicable to which the file belongs. This identifier is similar to M in table .

The following example explains the various entries of Directories a and b are identified as and by identifier A in . Directories a and b are parent directories as shown by directory path C entry . Directories c and d are identified as and and are sub directories under parent directory a .

File f.txt as identified by file name C is stored at a c . The path can be obtained by using the cross referenced parent directory entry under B . Files g.txt h.doc i.epp and j.pdf as identified by file name B are stored at a d as shown by the parent identifier . File e.jpeg is stored under sub directory b based on parent identifier .

It is noteworthy that although show examples of different database tables and the adaptive embodiments are not limited to having separate tables. In one embodiment the files and directory tables and may be included in a single table but differentiated by an identifier for example a Snapshot identifier.

The files or data containers at time t are also shown in the data container table labeled as Files. For example field A provides the inode numbers 31 41 42 43 44 and 51 for files f.txt g.txt h.doc i.cpp j.pdf and e.jpeg respectively. Each file is associated with a parent under field B i.e. 30 40 40 40 40 and 20 respectively.

At time t another snapshot is taken and metadata for the snapshot at time t may be referred to as Snap is shown as and . Under Snap directory z gets created under b as indicated by the flag A which means added directory c is moved from a c to b c and directory a d is modified.

In the Files table at Snap file y.txt is created under b z file j.pdf is modified and file h.doc is deleted.

In one embodiment database segments and may be used efficiently to respond to user queries for information regarding data containers that can obtained by searching metadata information. Since metadata fields are organized in a relational database one can search through the database to provide user requested information. The information type of course may vary based on a user request.

As shown above database is split into multiple logical tables and . This is efficient and saves disk space because the data container tables or file tables do not include the path for every file entry and this saves storage space. Instead each data container for example a file is associated with a parent or directory identifier in a directories table. To access a data container one simply has to look at the parent entry and ascertain the path where the data container is stored.

The storage administrator may also configure one or more data volumes for example or such that metadata for the data containers stored at the data volumes can be collected indexed and then stored at the catalog volume. The storage administrator may associate one or more data volumes to a particular catalog volume. The storage administrator may specify a collection frequency which determines how often the metadata is collected. The storage administrator may also specify certain events based on which the metadata may be collected. For example the storage administrator may specify that when a new snapshot is taken metadata should be collected for the data containers that may have changed from a previous snapshot of the same data volume.

In block S metadata is collected by metadata collection module . In one embodiment metadata is collected based on a user specified schedule as described above. In another instance metadata may be collected based on an event for example a snapshot. In yet another embodiment a user may send a request to collect metadata for a data volume.

The metadata that is collected by metadata collection module may be for a baseline snapshot. This means that metadata is collected for all the data containers stored at the data volume. When there are changes to the data containers and a snapshot is taken at a later instance then metadata is collected for only the changed data containers. Incremental metadata collection is efficient because one does not have to repeat the metadata collection step for all the data containers including data containers that may not have changed from a previous instance.

In one embodiment for a clustered environment the metadata collection module is executed at one or more nodes and collects metadata associated with data volumes that are accessible to the node. The metadata may be collected from operating system that maintains information regarding all the data containers at the selected data volume.

After the metadata is collected it is pre processed and placed at intermediate table in block S. One reason for pre processing the metadata is because the metadata received from the operating system may be of a different format and one may have to extract one or more fields so that the information can be placed in database and used efficiently to respond to user requests as described below. An example of how fields are extracted from the collected metadata and placed at intermediate table has been described above.

After the metadata is pre processed the information from intermediate table is indexed. The indexing is based on one or more fields that have been described above with respect to the database tables.

The indexing in block S may be based on a policy that is set up by a user and stored in configuration module . The policy allows a user to set indexing of metadata collected after each snapshot. The indexing may be on demand i.e. based on when a user or storage administrator sends a request to start indexing. In another embodiment indexing may be time based such that catalog controller starts indexing based on a set schedule. The indexing policy settings make the system and process flexible because users in different storage environments may use different polices for indexing metadata based on user needs.

After the pre processed metadata is indexed in block it is stored in database . In one embodiment the stored metadata is placed in a searchable relational database . An example of searchable database is described above with respect to .

In one embodiment for a clustered environment database may be stored at one or more volumes that may be referred to as catalog volumes. Metadata collected from different nodes may be stored at the catalog volumes. Catalog controller can access a volume locator database VLDB or that identifies different volumes and their locations. This allows the catalog controller to cross reference the volume identifiers with the collected metadata.

In block S the query is forwarded to database engine so that user requested information can be obtained from database . Catalog controller parses the user request to ascertain what fields in database may need to be searched. For example if the user wants to know how many .pdf files belong to a particular group then catalog controller will search file name C and group identifier G to respond to the query.

In block S the user requested information is presented to the user. The information may be displayed in a user interface on a display device. The information may be presented as a report by reporting module .

In block S an action that may need to be taken based on the search results is performed. The nature and action type may be based on user request. For example a user request may be to obtain information regarding certain file types for example video files. The action associated with the file type may be to move the certain file type from one volume to another volume. Catalog controller obtains the file types by searching database that stores information regarding file types. Thereafter catalog controller communicates with operating system to move the files from the first location to one or more locations. This example is provided to illustrate the adaptive nature of the various embodiments and not to limit the various embodiments shown herein.

In block S metadata is collected from a plurality of nodes. The metadata is collected by metadata collection module executed by the plurality of nodes and then stored at one or more catalog volumes and . An example of this is shown in where metadata collection modules . .N are executed at each node and collect metadata for data volumes that are configured in block S.

In block S metadata collected from different volumes and controlled by different nodes is pre processed and stored in database at catalog volume . The pre processing is performed so that information from the collected metadata can be used to populate database . The collected metadata may arrive in an order determined by the storage operating system. The collected metadata may include more information than what may be needed by catalog module . The pre processing is performed such that catalog module can extract the relevant fields and values that are used in database . Details regarding pre processing and database are provided above with respect to and .

In block S a user query for information regarding a plurality of data structures that may be stored at different volumes and controlled by different nodes is received. The user query is received by catalog module via a user interface provided by management application . A user may request different information types for the plurality of data structures. The type of user query and the nature of information that the user may seek depends on how a user is using storage environment .

In block S database is used to search for information requested by the user. Searching database is faster and less taxing on computing resources vis vis performing a directory walk analyzing metadata for millions of files. For example to determine how many files were accessed within certain duration one only has to search using field G and ascertain the number of files within the specified duration. One is able to do that because of the way database is structured and built.

In some instances an action may be associated with a search query. When an action is associated with a search query then the requested action associated with the search results is performed in block S. For example a user may configure a volume such that after every snapshot certain file types may be moved to another location. To accommodate this action after every snapshot first database is searched to ascertain the file types and then operating system is notified to move the file types.

In one embodiment using catalog system and the process steps described above one can efficiently search metadata for data containers stored at one or more data volumes both in a clustered environment and non cluster environment . In traditional storage environments the operating system is typically geared towards handling access to one object at a time. Access to a group of files within a file system is difficult. Furthermore the operating system layout is such that metadata for a data container for example a file name attributes access control lists information regarding the owner may not be stored contiguously at the storage devices. Therefore to access information regarding a data container or a group of data containers one has to traverse through a namespace and perform a directory search.

The embodiments disclosed herein efficiently search for data containers using relational database and its associated tables. For example one can search for all files greater than size 1 MB that were not accessed within the last year by searching data structure . One can use the size field D and access time field H to filter all files that may be greater than 1 MB and were not accessed within one year without having to do an extensive namespace based directory search.

In one embodiment catalog system integrates metadata management related operations as well as data container related operations within a storage environment. In conventional systems typically one vendor provides an operating system and a different vendor provides a separate system for handling metadata related operations. Catalog system is integrated with operating system and management application . Hence one does not need to use another third party module for handling metadata related operations.

In one embodiment metadata related operations are executed efficiently because catalog system is integrated with operating system . This allows one to use operating system s ability to collect metadata efficiently. If one were to use an external third party system then one will have to scan an entire file system using other techniques compared to the techniques that are integrated with the operating system.

In one embodiment because metadata is handled efficiently one can provide useful reports to users such that users can efficiently use the storage space. The reports are provided by reporting module and management application via a user interface. The data for the reports is provided by catalog module and formatted and presented by management application .

Reports can be configured based on user specified parameters for example users may want to know what different types of files are being used for example media files .doc files and others. In conventional systems to gather that information one will have to traverse through a namespace directory that may include millions of files. In the embodiments disclosed herein one can obtain this information from database by searching field C that includes the file type. This is faster and more efficient than searching through a directory that may include millions of files.

The embodiments herein also allow a user to generate reports based on different users that use the storage space. For example by searching database using fields C and owner identification field F one can ascertain which users are using a certain file type. One can also view usage of storage space based on groups by using the group identifier G. One can do this efficiently because of the manner in which the relational database is structured.

In another embodiment reports can be generated based on volumes that are spread out in a clustered environment . Because metadata is collected for different nodes and efficiently cataloged at one or more catalog volumes for example one is able to obtain an overall view of the clustered system as well as node based view. A storage administrator can issue cluster wide requests and catalog module can obtain information regarding the entire cluster or for specific volumes. One can obtain all this information without having to perform an entire file system search that can be resource intensive and inefficient.

In yet another embodiment not only one can generate reports and perform fast queries one can also perform actions that may be related to the search results. For example a user may want to know how many files of a certain type for example .mp3 are saved in the storage system and then move the files to a different storage environment. One can conduct an efficient search using database and then perform the appropriate action. This allows a user to efficiently use storage space. Continuing with the foregoing example if the .mp3 files are not being accessed or used frequently and the user has access to secondary storage that is also not used frequently then the user can move the files to the secondary storage that is used infrequently.

This allows a user to efficiently manage and use storage resources. The user can obtain storage system usage views efficiently by using database and based on user needs perform the appropriate actions for moving information around.

The embodiments disclosed herein allow a user to search for data containers based on a data container owner name of the data container modification time access time and type of data container and other fields. The search may be performed by combining different fields. For example a user can search which owners and groups use the highest amount of storage as well as the least amount of storage. One can then apportion storage cost to individuals teams and business units.

Since metadata is collected incrementally for different snapshots one can look at the growth of storage between snapshots. This will allow storage administrators to plan better for upgrading or downgrading storage space based on business need.

It is noteworthy that the systems and processes described herein are not limited to collecting metadata for Snapshots but instead catalog module may catalog metadata for an active file system.

The cluster access adapter comprises a plurality of ports adapted to couple node . to other nodes of cluster . In the illustrative embodiment Ethernet may be used as the clustering protocol and interconnect media although it will be apparent to those skilled in the art that other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternate embodiments where the N modules and D modules are implemented on separate storage systems or computers the cluster access adapter is utilized by the N D module for communicating with other N D modules in the cluster .

Each node . is illustratively embodied as a dual processor storage system executing a storage operating system that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks on disks .. However it will be apparent to those of ordinary skill in the art that the node . may alternatively comprise a single or more than two processor systems. Illustratively one processor A executes the functions of the N module . on the node while the other processor B executes the functions of the D module ..

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing programmable instructions and data structures. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the programmable instructions and manipulate the data structures. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the invention described herein.

The storage operating system portions of which is typically resident in memory and executed by the processing elements functionally organizes the node . by inter alia invoking storage operations in support of the storage service implemented by the node. An example of operating system is the DATA ONTAP Registered trademark of NetApp Inc. operating system available from NetApp Inc. that implements a Write Anywhere File Layout WAFL Registered trademark of NetApp Inc. file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such where the term ONTAP is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.

The network adapter comprises a plurality of ports adapted to couple the node . to one or more clients . . over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Illustratively the computer network may be embodied as an Ethernet network or a Fibre Channel FC network. Each client . . may communicate with the node over network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system executing on the node . to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on disks .. The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC link topology.

Storage of information on each array . is preferably implemented as one or more storage volumes that comprise a collection of physical storage disks . cooperating to define an overall logical arrangement of volume block number vbn space on the volume s . Each logical volume is generally although not necessarily associated with its own file system. The disks within a logical volume file system are typically organized as one or more groups wherein each group may be operated as a RAID. Most RAID implementations such as a RAID 4 level implementation enhance the reliability integrity of data storage through the redundant writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of parity information with respect to the striped data. An illustrative example of a RAID implementation is a RAID 4 level implementation although it should be understood that other types and levels of RAID implementations may be used in accordance with the inventive principles described herein.

The processing system includes one or more processors and memory coupled to a bus system . The bus system shown in is an abstraction that represents any one or more separate physical buses and or point to point connections connected by appropriate bridges adapters and or controllers. The bus system therefore may include for example a system bus a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus sometimes referred to as Firewire .

The processors are the central processing units CPUs of the processing system and thus control its overall operation. In certain embodiments the processors accomplish this by executing executable instructions stored in memory . A processor may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices.

Memory represents any form of random access memory RAM read only memory ROM flash memory or the like or a combination of such devices. Memory includes the main memory of the processing system . Instructions may be used to implement the techniques introduced above e.g. catalog module may reside in and executed by processors from memory .

Also connected to the processors through the bus system are one or more internal mass storage devices and a network adapter . Internal mass storage devices may be or may include any conventional medium for storing large volumes of data in a non volatile manner such as one or more magnetic or optical based disks. The network adapter provides the processing system with the ability to communicate with remote devices e.g. storage servers over a network and may be for example an Ethernet adapter a Fibre Channel adapter or the like. The processing system also includes one or more input output I O devices coupled to the bus system . The I O devices may include for example a display device a keyboard a mouse etc.

Thus a method and apparatus for managing metadata for data containers have been described. Note that references throughout this specification to one embodiment or an embodiment means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Therefore it is emphasized and should be appreciated that two or more references to an embodiment or one embodiment or an alternative embodiment in various portions of this specification are not necessarily all referring to the same embodiment. Furthermore the particular features structures or characteristics being referred to may be combined as suitable in one or more embodiments of the invention as will be recognized by those of ordinary skill in the art.

While the present disclosure is described above with respect to what is currently considered its preferred embodiments it is to be understood that the disclosure is not limited to that described above. To the contrary the disclosure is intended to cover various modifications and equivalent arrangements within the spirit and scope of the appended claims.

