---

title: Incremental restore of data between storage systems having dissimilar storage operating systems associated therewith
abstract: A method includes quiescing a file system of source storage system through a backup engine associated with the source storage system and a destination storage system and capturing, at the source storage system, a point-in-time image of the file system thereof through the backup engine. The method also includes sharing the captured point-in-time image of the file system of the source storage system with the destination storage system to enable the storage systems to have a common base data and negotiating between the storage systems for the common base data. Further, the method includes applying, to the common base data at the source storage system, a differential change corresponding to a difference between the common base data and a point-in-time image of the file system of the source storage system backed up at the destination storage system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08688645&OS=08688645&RS=08688645
owner: NetApp, Inc.
number: 08688645
owner_city: Sunnyvale
owner_country: US
publication_date: 20101130
---
This disclosure relates generally to restoring data in storage systems and more particularly to a method an apparatus and or a system of incrementally restoring data between storage systems having dissimilar storage operating systems associated therewith.

Backing up of data e.g. remote back up associated with a source storage system at a destination storage system may aid recovery of lost data at the source storage system. The lost data may signify data accidentally erroneously deleted from the source storage system and or data corrupted at the source storage system. The aforementioned backing up of the data may be performed using a backup engine e.g. NetApp s SnapVault associated with the source storage system and the destination storage system.

When there is no difference between data associated with the source storage system and the data backed up at the destination storage system save for a folder of data and or a file accidentally deleted from the source storage system an incremental restore process may be performed whereby a differential data corresponding to the data accidentally deleted alone may be transferred from the destination storage system to the source storage system. The aforementioned incremental restore process may be possible when the source storage system and the destination storage system have the same storage operating systems associated therewith.

For example when the source storage system is a third party application server the storage operating system associated therewith may be different from that of the destination storage system. Here the granularity of source data at the destination storage system may be different from the granularity at the source storage system. Thus even if a mere single file is deleted from the source storage system an entire container associated with the single file may need to be restored.

The time consumed for the full restore may be large compared to the time consumed during a corresponding incremental restore process if possible to perform. Moreover as the source storage system and the destination storage system may be configured to communicate with each other through a computer network bandwidth associated therewith may increase. Further storage requirements at the source storage system may increase for the full restore process. The expenses associated with the aforementioned time increase and bandwidth storage requirement increase during data recovery may be prohibitive for a concerned organization backing up data thereof at the destination storage system.

Disclosed are a method an apparatus and or a system of performing an incremental restore of data between storage systems having dissimilar storage operating systems associated therewith.

In one aspect a method of restoring data associated with a source storage system configured to be backed up at a destination storage system to the source storage system includes quiescing a file system associated with the source storage system through a backup engine associated with the source storage system and the destination storage system and capturing at the source storage system a point in time image of the file system associated therewith through the backup engine. The source storage system and the destination storage system have different storage operating systems associated therewith.

The method also includes sharing the captured point in time image of the file system associated with the source storage system with the destination storage system to enable the source storage system and the destination storage system to have a common base data therein and negotiating between the source storage system and the destination storage system for the common base data therein. Further the method includes applying to the common base data at the source storage system a differential change corresponding to a difference between the common base data at the destination storage system and a point in time image of the file system of the source storage system backed up at the destination storage system.

The point in time image of the file system of the source storage system backed up at the destination storage system is configured to be in an immediate temporal past relative to a back up of the data to be restored to the source storage system at the destination storage system and in a temporal past relative to the common base data at the destination storage system.

In another aspect a method of performing an incremental restore of a differential data to a source storage system configured to have data associated therewith backed up at a destination storage system includes quiescing a file system associated with the source storage system through a backup engine associated with the source storage system and the destination storage system capturing at the source storage system a point in time image of the file system associated therewith through the backup engine and sharing the captured point in time image of the file system associated with the source storage system with the destination storage system to enable the source storage system and the destination storage system to have a common base data therein.

The source storage system and the destination storage system have different storage operating systems associated therewith. The method also includes negotiating between the source storage system and the destination storage system for the common base data therein and obtaining at the destination storage system a difference between the common base data and a point in time image of the file system of the source storage system backed up at the destination storage system to configure a quota tree data a volume data or a sub directory data associated with the point in time image of the file system of the source storage system backed up at the destination storage system to be in a same state as a corresponding quota tree data a volume data or a sub directory data corresponding to the common base data at the destination storage system.

The point in time image of the file system of the source storage system backed up at the destination storage system is configured to be in an immediate temporal past relative to a back up of data to be restored to the source storage system at the destination storage system and in a temporal past relative to the common base data at the destination storage system. Further the method includes traversing an appropriate quota tree a volume or a sub directory corresponding to the quota tree data the volume data or the sub directory data associated with the point in time image of the file system of the source storage system backed up at the destination storage system starting from a root inode thereof in a tree order and fetching an inode associated with the common base data at the destination storage system corresponding to an inode associated with the point in time image of the file system of the source storage system backed up at the destination storage system based on an inode identifier thereof.

Still further the method includes obtaining a difference between data associated with the two inodes as a difference between the common base data at the destination storage system and the point in time image of the file system of the source storage system backed up at the destination storage system and applying to the common base data at the source storage system the differential change corresponding to the difference between the common base data at the destination storage system and the point in time image of the file system of the source storage system backed up at the destination storage system.

In yet another aspect a storage environment configured to enable an incremental data restore includes a source storage system and a destination storage system configured to back up data associated with the source storage system. The source storage system and the destination storage system have different storage operating systems associated therewith. The destination storage system includes a processor and a memory. The memory includes storage locations configured to be addressable by the processor and the memory is configured to store instructions associated with a backup engine associated with the source storage system and the destination storage system.

The backup engine is configured to enable quiescing a file system associated with the source storage system capturing at the source storage system a point in time image of the file system associated therewith and sharing the captured point in time image of the file system associated with the source storage system with the destination storage system to enable the source storage system and the destination storage system to have a common base data therein. The backup engine is also configured to enable negotiating between the source storage system and the destination storage system for the common base data therein and applying to the common base data at the source storage system a differential change corresponding to a difference between the common base data at the destination storage system and a point in time image of the file system of the source storage system backed up at the destination storage system.

The point in time image of the file system of the source storage system backed up at the destination storage system is configured to be in an immediate temporal past relative to a back up of the data to be restored to the source storage system at the destination storage system and in a temporal past relative to the common base data at the destination storage system.

In a further aspect a method of restoring data between storage systems having different storage operating systems associated therewith based on a commonality of point in time images is disclosed. The method includes generating a point in time image of a file system at a source storage system having a first storage operating system associated therewith. The source storage system is configured to have data associated therewith backed up at a destination storage system having a second storage operating system associated therewith. The first storage operating system is configured to implement the file system at the source storage system.

The method also includes providing through a computer network the point in time image to the destination storage system. The second storage operating system is configured to implement a file system different from that implemented through the first storage operating system. Further the method includes establishing a common base data between the source storage system and the destination storage system through sharing the point in time image with the destination storage system and incrementally restoring data associated with the source storage system through the destination storage system based on the common base data. The common base data is referenced by both the source storage system and the destination storage system based on an identification data.

The methods and systems disclosed herein may be implemented in any means for achieving various aspects and may be executed in a form of a machine readable medium embodying a set of instructions that when executed by a machine cause the machine to perform any of the operations disclosed herein. Other features will be apparent from the accompanying drawings and from the detailed description that follows.

Other features of the present embodiments will be apparent from the accompanying drawings and from the detailed description that follows.

Example embodiments as described below may be used to perform an incremental restore of data between storage systems having dissimilar storage operating systems associated therewith. Although the present embodiments have been described with reference to specific example embodiments it will be evident that various modifications and changes may be made to these embodiments without departing from the broader spirit and scope of the various embodiments.

In one or more embodiments host devices may indicate customers of services provided through network or users departments associated with an organization e.g. an Information Technology IT organization . In one or more embodiments each host device may have storage associated therewith. For the aforementioned purpose in one or more embodiments isolated logical virtual storage partitions not shown may be created on storage system through an operating system associated therewith. In one or more embodiments therefore each virtual storage partition may be associated with a host device . In one or more embodiments information on a secured virtual storage partition may solely be accessed by the host device associated therewith.

In one or more embodiments storage system may include a storage server e.g. file server and may be associated with a set of mass storage devices e.g. disks . In one or more embodiments file server may be configured to receive read write requests from host devices and to respond appropriately. In one or more embodiments the requests may be directed to data stored in to be stored in disks . Disks may be for example conventional magnetic disks optical disks such as compact disks read only memory CD ROM or digital versatile video disks DVD based storage magneto optical MO storage or any other type of non volatile storage devices suitable for storing large quantities of data. In one or more embodiments file server may be construed as the manager of storage system . In one or more embodiments a main purpose of file server may be to provide a location or location s for shared disk access e.g. shared storage of computer data configured to be accessed by host devices .

In one or more embodiments the operating system associated with storage system may support data sets associated with protocols including but not limited to Network File System NFS protocol Common Internet File System CIFS protocol Internet Small Computer System Interface iSCSI protocol Hypertext Transfer HTTP protocol File Transfer Protocol FTP FTP Secure FTPS protocol Secure File Transfer Protocol SFTP and Network Data Management Protocol NDMP .

In one or more embodiments each of source storage system and destination storage system may be a computing device configured to organize information on disks analogous to disks . In one or more embodiments each storage system may include a processor a memory a network adapter a non volatile memory and a storage adapter configured to be interconnected through a system bus . Here the constituents of destination storage system have been left out for clarity sake because destination storage system as discussed above is analogous to source storage system .

In one or more embodiments each storage system may include storage operating system configured to implement a file system as a hierarchical structure of files directories and blocks on disks . In one or more embodiments memory may include a buffer cache e.g. volatile memory configured to store data structures passed between disks and network during operation. In one or more embodiments memory may also be configured to store software code e.g. instructions associated with a backup engine to be discussed below and may include storage locations configured to be addressable by processor . In one or more embodiments processor and the adapters may include processing elements and or logic circuitry configured to execute instructions in memory and to manipulate the data structures. In one or more embodiments each storage system may also include non volatile memory e.g. Non Volatile Random Access Memory NVRAM configured to provide fault tolerant back up of data to enable survival of storage system during power failure and or other faults.

In one or more embodiments network adapter may be configured to couple storage system to host devices through network . In one or more embodiments storage adapter may be configured to communicate with storage operating system to access information requested through host devices . In one or more embodiments host devices may be configured to execute applications . In one or more embodiments a host device may be configured to interact with storage system according to a client server model of information delivery. For example host device may request the services of storage system and storage system may return the results e.g. through packets through network .

In one or more embodiments storage of information may be implemented as volumes not shown and may include configuring disks to implement a logical arrangement of volume block number VBN space on the volumes. In one or more embodiments each logical volume may be associated with the file system as discussed above. In one or more embodiments the file system may include a continuous range of VBNs starting from 0 to n for a file system of n 1 blocks. In one or more embodiments disks within a volume may be organized as one or more Redundant Array of Independent Disks RAID groups. In one or more embodiments to facilitate access to disks storage operating system may implement a write anywhere file system e.g. NetApp s WAFL file system to map the storage space provided by disks to host devices .

In one or more embodiments source storage system and destination storage system may both include file servers implemented through a same type of storage operating system e.g. NetApp s Data ONTAP operating system . In one or more embodiments source storage system and destination storage system may also have different operating systems associated therewith. For example storage operating system of destination storage system may be NetApp s Data ONTAP operating system and the operating system associated with source storage system may be Microsoft s Windows UNIX Linux or VMWare s ESX . In one or more embodiments here the operating system associated with source storage system may implement a file system that is different from a file system implemented through the operating system associated with destination storage system .

In one or more embodiments storage on destination storage system associated with storage operating system may be sold to a customer for back up purposes e.g. a customer at source storage system . Thus in the case of source storage system being associated with an operating system different from that of destination storage system source storage system may be the third party client. shows an expanded view of communication between the third party source storage system here as application server and destination storage system according to one or more embodiments. It is to be noted that elements of application server and obviously the operating system associated therewith are different from the corresponding elements of destination storage system and the operating system associated with destination storage system .

In one or more embodiments data on source storage system or the third party client may be backed up through the utilization of an open system backup engine e.g. NetApp s Open Systems SnapVault OSSV . In one or more embodiments data on the third party source storage system may reside on locally attached disk s or SAN attached disks and may include a disk a file system or a sub directory. In one or more embodiments the open system backup engine may be configured to enable the sole backup of blocks of data that have changed since the previous backup. In one or more embodiments a full backup may be performed only during the first backup.

In one or more embodiments the full backup may involve invoking a baseline transfer through destination storage system contacting the third party source storage system to request a baseline transfer of the requisite data on source storage system . In one or more embodiments the baseline transfer may be invoked through the creation of an open system backup engine relationship between source storage system and destination storage system . In one or more embodiments the aforementioned relationship may be created at destination storage system through issuing one or more command s associated with the open system backup engine at source storage system . In one or more embodiments an agent associated with the open system backup engine may be installed on source storage system requiring backup.

In one or more embodiments after the baseline transfer is complete the open system backup engine may be configured to perform block level incremental transfers through calculating checksums for every incremental block size e.g. 4 kB of data from source storage system . In one or more embodiments information associated with the checksums may be stored in a database associated with source storage system . In one or more embodiments the data on source storage system may then be transferred to a quota tree qtree associated with a volume on destination storage system . In one or more embodiments following the completion of the baseline transfer a snapshot copy e.g. a read only point in time image of the file system associated with source storage system may be taken at destination storage system .

In one or more embodiments backups performed after the initial baseline transfer through the open system backup engine may be block level incremental in nature. In one or more embodiments through the incremental block size checksums calculated as discussed above the open system backup engine may be configured to enable the sole backup s of blocks within the files of source storage system that have changed. Thus in one or more embodiments backups through the open system backup engine may be suited to a slow network may be faster and or may result in bandwidth savings associated with network .

In one or more embodiments information about file deletions in the third party source storage system may be communicated to destination storage system . In one or more embodiments checksum calculations may then be performed on the modified files following which the backup data may be transferred to destination storage system .

In one or more embodiments as discussed above backups of data of the third party source storage system may map to a qtree within a volume of destination storage system . In one or more embodiments a number of open system backup engine relationships may share the same volume of destination storage system . However in one or more embodiments each relationship may have a unique qtree associated therewith. In one or more embodiments data of source storage system may also be a member of multiple open system backup engine relationships.

In one or more embodiments backups may be captured in destination storage system as snapshot copies as discussed above. In one or more embodiments the retention of the aforementioned copies may depend on the level of retention e.g. a number of hourly backups a number of daily backups a number of weekly backups a number of monthly backups . In one or more embodiments during recovery from destination storage system the data backed up may be restored directly from the corresponding copy.

In one or more embodiments the contents of the snapshot copies may represent a full data backup and may be completely accessible readable because of the open system backup engine being configured to store backups in a native format.

In one or more embodiments data e.g. applications of the third party source storage system may be backed up at destination storage system such that the data granularity at the backup on the file system associated with destination storage system may not be a one on one mapping of the data granularity of source storage system . For example virtual machines VMs of source storage system may be backed up at destination storage system . Eventually Virtual Machine Disks VMDK and vmdk files associated therewith may also be backed up at destination storage system . In an instance of a particular folder including data e.g. files or a particular file being erroneously deleted from the operating system executing on the VMDKs associated with source storage system the aforementioned erroneously deleted folder file may need to be restored at source storage system . Due to the abovementioned difference in data granularity of source data between source storage system and destination storage system the VMDKs associated with source storage system may be seen as for example individual files at destination storage system .

Thus in one or more embodiments even to restore an erroneously deleted file restoring a full backup of the VMDK may need to done. In one or more embodiments even restoring associated with the smallest available full storage container e.g. sub directory including the erroneously deleted file may prove to be time consuming network bandwidth wasting and or inefficient.

In one or more embodiments as discussed above source storage system and destination storage system may execute the same operating systems e.g. storage operating system therein. Here the data granularity of the source data at destination storage system may be the same as that at source storage system . For example in order to restore an erroneously deleted folder on source storage system note here source storage system is associated with the same operating system as destination storage system . viz. storage operating system the backup engine not the open system backup engine an example of the backup engine would be NetApp s SnapVault associated therewith may be configured to perform a differential update on a common base data e.g. snapshot associated with both source storage system and destination storage system at source storage system .

It is to be noted that source storage system is configured to be the source and destination storage system is configured to be the destination during the backup process and source storage system is configured to be the destination and destination storage system is configured to be the source during the restore process.

In one or more embodiments operation may involve checking as to whether the common snapshot between source storage system and destination storage system has been determined. If yes in one or more embodiments operation may involve configuring destination storage system to roll back to the common snapshot through the file system thereof. In one or more embodiments the rolling back of destination storage system may include performing a difference operation between the common snapshot and the incremental version thereof of destination storage system at destination storage system so as to configure the qtree data associated therewith to be in the same state as the qtree data corresponding to the common snapshot. For example the incremental version of the common snapshot at destination storage system may be associated with the corresponding backup of the immediately previous version of the file system prior to the erroneous deletion of the folder of source storage system at destination storage system .

In one or more embodiments if the result of the checking in operation indicates that the common snapshot has not been determined control may be transferred to the negotiation in operation . In one or more embodiments the negotiation may continue until the common snapshot is determined. Else in one or more embodiments a full restore process may be attempted.

In one or more embodiments operation may then involve updating the common snapshot e.g. an immediate previous version of the file system prior to the erroneous deletion of the folder at source storage system with the differential data corresponding to the difference between the common snapshot e.g. the backup of the immediate previous version of the file system of source storage system prior to the erroneous deletion of the folder at destination storage system and the incremental version thereof at destination storage system . For example the common snapshot may be updated with the differential data to restore the erroneously deleted folder.

In one or more embodiments the aforementioned restoring process may be initiated by source storage system which serves as the destination during the process. In one or more embodiments when source storage system is a third party system as in source storage system may not be configured to operate on with snapshots. In one or more embodiments source storage system may not even be configured to retain snapshots. Thus in one or more embodiments it may not be possible to negotiate a common snapshot between source storage system and destination storage system using the open system backup engine as discussed above. Additionally the difference in granularity of source data at source storage system and destination storage system may necessitate a full restore using the open system backup engine.

In one or more embodiments utilizing an appropriate interface at source storage system may enable remote triggering of an update associated with the open system backup engine backup engine through destination storage system . Note that destination storage system is the source here. For example an Application Programming Interface API at source storage system may be configured to invoke an update command associated with the open system backup engine backup engine. In one or more embodiments the update command may trigger an update from destination storage system . Alternately in one or more embodiments the update associated with the open system backup engine backup engine may directly be triggered from destination storage system . It is for the advantageous purpose of remotely triggering the update that the API may be provided at source storage system . An example of the API may be the Zephyr API ZAPI a proprietary API of Network Appliance Inc.

In one or more embodiments quiescing may be pausing and or altering a state of processes executing on a computer particularly those that may modify information stored on disk during a backup in order to guarantee a consistent and usable backup. In one or more embodiments following the triggering of the update from destination storage system the file system of source storage system may be quiesced and a point in time image of the file system may momentarily be captured. In one or more embodiments the quiescing of the file system may imply that the file system of source storage system may be frozen i.e. no further updates thereto e.g. no further Input Output I O may occur. In an example embodiment the quiescing of the file system of application server example of source storage system of may trigger a consistency point CP therein. In the example embodiment the CP may constitute an operation during which the operating system of application server flushes in core data e.g. data in a buffer cache all data not residing on disks changes associated with source storage system to corresponding locations on disk .

In one or more embodiments as part of the update command discussed above the operating system of source storage system may then be configured to capture the point in time image of the file system associated therewith. Although the operating system of source storage system e.g. application server may not be configured to support snapshots the operating system may hold onto the point in time image during the quiescing of the file system. For example the triggering of the update discussed above may involve the operating system of source storage system associating an identification ID data with the captured point in time image of the file system analogous to a snapshot snap ID being used to tag snapshots in an operating system configured to support snapshots. For instance the aforementioned identification data may be an integer which may be utilized to track the point in time image.

Alternately in one or more embodiments a series of point in time images may be captured and the last point in time image that is captured immediately prior to the quiescing may be chosen as the snapshot to be common with destination storage system as seen below. In one or more embodiments the point in time image of the file system at source storage system may momentarily be held on to following the quiescing of the file system of source storage system . In one or more embodiments in order for there being a common base data between source storage system and destination storage system the point in time image of the file system at source storage system may then be shared with destination storage system . For the aforementioned purpose for example the point in time image of the file system at source storage system may be transmitted to destination storage system through network .

Thus in one or more embodiments both source storage system and destination storage system may now include a common snapshot as in the case where source storage system and destination storage system have the same operating systems e.g. storage operating system associated therewith. In one or more embodiments destination storage system may have the appropriate license s by way of the open system backup engine e.g. NetApp s OSSV to receive data from the third party source storage system . In one or more embodiments the negotiation process between source storage system and destination storage system for the common snapshot therebetween may yield the abovementioned point in time image as the common snapshot. 

In one or more embodiments a negotiation process may be a dialogue of information exchanges between source storage system and destination storage system intended to resolve disputes and to produce an agreement for a course of action with respect to a point in time image. In one or more embodiments the negotiation process may involve the open system backup engine searching in source storage system and destination storage system for the common snapshot. In one or more embodiments as the operating system of destination storage system may be configured to tag an appropriate identification data to the point in time image of the file system of source storage system shared therefrom and source storage system may include just the aforementioned point in time image the negotiation process may result in the quick selection of the common snapshot. In one or more embodiments the negotiation process may occur through network .

In one example embodiment destination storage system may be configured to send a list of snapshots to source storage system and source storage system may be configured to select the shared point in time image as the common snapshot. 

In one or more embodiments source storage system may be configured to copy the data associated with the file system at another location therein after the quiescing thereof for redundancy purposes. In case the quiescing fails and or updates to the file system of source storage system occur source storage system may have the backup of the original data. It is to be noted that here in one or more embodiments as destination storage system includes the shared point in time image destination storage system may not be required to roll back to the common snapshot as in .

In one or more embodiments source storage system may then be configured to request for an incremental restore from an appropriate back in time version or snapshot of the backed up data e.g. a destination storage system backup of the file system of source storage system immediately prior to the erroneous deletion of the folder through destination storage system . In one or more embodiments the back in time snapshot may be the snapshot to which source storage system requires the restored data to be synchronized to.

It is obvious that the appropriate backed up data is always in a temporal past or back in time relative to the common snapshot because the common snapshot is captured after the back up of for example the immediate previous version of the file system of source storage system prior to the erroneous deletion of the folder at destination storage system . Again in one or more embodiments a difference operation between the common snapshot and the back in time snapshot at destination storage system may be performed so as to configure the qtree data associated therewith to be in the same state as the qtree data corresponding to the common snapshot. 

In one or more embodiments the differential data changes between the corresponding common snapshot and the back in time snapshot may be obtained at destination storage system . In one or more embodiments the common snapshot at source storage system may then be updated with the differential data corresponding to the difference between the common snapshot and the incremental version thereof at destination storage system . For example the common snapshot may be updated with the differential data to restore the erroneously deleted folder at source storage system . For the aforementioned restoring purpose in one or more embodiments the differential data may be transmitted e.g. through network from destination storage system to source storage system .

According to one or more embodiments capturing a point in time image of the file system associated with the source storage system may mean the storage operating system associated with the source storage system generating a point in time image of the file system and associating a new identification data with the point in time image. In one or more embodiments operation may then involve capturing a point in time image of the file system associated with source storage system at source storage system and holding on to the captured point in time image momentarily in memory.

In one or more embodiments sharing the point in time image of the file system associated with the source storage system e.g. source storage system may mean forwarding the point in time image to the destination storage system e.g. destination storage system whereby the destination storage system manages the received point in time image based on a snapshot identification data e.g. snap ID tagged onto the point in time image by the storage operating system associated therewith. In one or more embodiments operation may then involve sharing e.g. through network the captured point in time image of the file system associated with source storage system with destination storage system e.g. through the reference identification data initially generated by the source storage system to enable source storage system and destination storage system to have a common snapshot therebetween.

In one or more embodiments operation may then involve negotiating between source storage system and destination storage system for the common snapshot therebetween. For instance the negotiation process may involve destination storage system forwarding a list of snapshots therein to source storage system . Upon the receipt of the snapshots source storage system may be configured to compare the list of snapshots with the point in time image therein based on the identification data. In an example embodiment destination storage system may further associate a common base data identifier with the common snapshot at source storage system and destination storage system .

In one or more embodiments operation may then involve checking as to whether the common snapshot between source storage system and destination storage system has been determined. In an example embodiment determining the common snapshot may involve destination storage system transmitting a message to source storage system wherein the message includes the identification data of the common snapshot as determined by destination storage system . Upon receipt source storage system may be configured to select the common snapshot based on the identification data tagged therethrough and the identification data from destination storage system .

In one or more embodiments if the common snapshot is determined a difference operation between the common snapshot and an appropriate back in time snapshot may be performed at destination storage system so as to configure the qtree data associated therewith to be in the same state as the qtree data corresponding to the common snapshot. In one or more embodiments source storage system may then be configured to request for the incremental restore from the appropriate back in time version or snapshot of the backed up data e.g. back up of an immediate previous version of the file system at source storage system prior to the deletion of the folder through destination storage system . In one or more embodiments the back in time snapshot may be the snapshot to which source storage system requires the restored data to be synchronized to.

In one or more embodiments if No control may be transferred to operation so that the negotiation process may be continued. In one example embodiment if the negotiation is still unsuccessful the full restore process may be performed.

In one or more embodiments applying a differential data may mean appending a differential data to the common snapshot at the source storage system e.g. source storage system and or the destination storage system e.g. destination storage system . Thus in one or more embodiments operation may involve applying the differential data changes corresponding to the difference between the corresponding common snapshot and the back in time snapshot of destination storage system to the common snapshot at source storage system . For example the common snapshot may be updated with the differential data to restore the erroneously deleted folder or sub directory at source storage system .

Tree structures of file systems inodes inode files volumes sub directories and qtrees are known to one skilled in the art. It is obvious that qtree data associated with the snapshots has been discussed above because of destination storage system being configured to back up data in qtrees. In one or more embodiments destination storage system may also be configured to back data up in volumes and other sub directories. The aforementioned modifications are within the scope of the exemplary embodiments. Further it is obvious that the open system backup engine is merely used for illustrative clarity with regard to and that a mere backup engine will suffice. Although a qtrees is special sub directory of the root directory of a volume a qtree may be considered distinct from other sub directories for illustrative convenience.

In one or more embodiments snapshots may include an ordered list of inodes associated therewith In one or more embodiments when source storage system and destination storage system are implemented with the same operating system e.g. storage operating system the differential data processing between the common snapshot refer to and the incremental version thereof at destination storage system may involve scanning through the inode files associated with the ordered list of inodes of both the snapshots. In one or more embodiments inodes of interest may be chosen and the remaining inodes filtered out. In one or more embodiments the difference between the corresponding each filtered out inode of both the snapshots may then be obtained.

In one or more embodiments the abovementioned scanning process may not guarantee order in processing inodes from the perspective of a parent child relationship between directories files. For example the child sub directory may be processed prior to the parent directory. In one or more embodiments utilizing the same approach in the case of source storage system being a third party application server implemented with an operating system different from that of destination storage system may force the third party source storage system to mimic the unordered processing which may lead to complexities involved therein.

Therefore in one or more embodiments in the case of performing an incremental restore to a third party source storage system the scanning process may being with the root inode of a subtree directory and may be followed by processing of all the children thereof until the bottom of the tree is reached. Here in one or more embodiments in order to perform the incremental restore to the third party source storage system refer to a qtree in the back in time snapshot at destination storage system may be traversed starting from the root inode thereof. In one or more embodiments for each inode number or inode identifier associated with an inode of the back in time snapshot a corresponding inode i.e. corresponding to the same inode number of the common snapshot at destination storage system may be fetched. In one or more embodiments the differential data e.g. newly created modified files and or directories between the two inodes may then be obtained through the processing.

In one or more embodiments the differential data may include attributes that are incompatible with the operating system e.g. storage operating system at destination storage system and therefore may be sent as is to the third party source storage system . For example if the third party source storage system is Microsoft s Windows based meta files associated therewith may be incompatible with destination storage system implemented with NetApp s Data ONTAP operating system. Although there may be a one on one correspondence for the meta files at destination storage system the operating system at destination storage system may not be able to interpret the files.

It is obvious that when incremental restore is associated with a single file the scanning process beginning with the root inode of a subtree directory may not be required. The single file may merely be tracked and then restored through the incremental restore process.

Thus in one or more embodiments performing the incremental restore to the third party source storage system using the scanning order discussed above and the process described in may not require any special design of the third party source storage system . In one or more embodiments if the third party source storage system and destination storage system include data that are almost similar except for a small difference the incremental restore process discussed above may consume a mere fraction of the time required to perform a full baseline restore. In one or more embodiments due to the lesser amount of data required to be stored at the third party source storage system there may be space savings associated with the incremental restore process.

In one or more embodiments as a lot of third party source storage systems are deployed in WAN environments network bandwidth may be saved due to the incremental restore process. In one or more embodiments the incremental restore process discussed above may enable efficient application restores. In one or more embodiments applications may use incremental restore with finer data granularity instead of resorting to the full back up restore.

In one or more embodiments an alternate approach to performing sub file level restores may include implementing a selective block restore process. In one or more embodiments here destination storage system may solely restore the blocks of data as requested required by source storage system . In one or more embodiments source storage system may be configured to track changes of data e.g. files therein and request for specific blocks associated with the data as and when required as part of the restore process.

For example source storage system may create a list of inode number range set of file block numbers FBNs and transmit the list to destination storage system . In one or more embodiments destination storage system may then transmit the requested blocks for the corresponding data e.g. files along with the extended attributes associated therewith. In one or more embodiments the abovementioned approach may require a redirecting mechanism e.g. a filter driver associated with the one or more platforms that source storage system is configured to support.

In one or more embodiments operation may involve capturing at source storage system a point in time image of the file system associated therewith through the backup engine. In one or more embodiments operation may involve sharing the captured point in time image of the file system associated with source storage system with destination storage system to enable source storage system and destination storage system to have a common base data therein. In one or more embodiments operation may involve negotiating between source storage system and destination storage system for the common base data therein.

In one or more embodiments operation may then involve applying to the common base data at source storage system a differential change corresponding to a difference between the common base data at destination storage system and a point in time image of the file system of source storage system backed up at destination storage system . In one or more embodiments the point in time image of the file system of source storage system backed up at destination storage system may be configured to be in an immediate temporal past relative to a backup of the data to be restored to source storage system at destination storage system and in a temporal past relative to the common base data at destination storage system .

In one or more embodiments operation may involve capturing at source storage system a point in time image of the file system associated therewith through the backup engine. In one or more embodiments operation may involve sharing the captured point in time image of the file system associated with source storage system with destination storage system to enable source storage system and destination storage system to have a common base data therein. In one or more embodiments operation may involve negotiating between source storage system and destination storage system for the common base data therein.

In one or more embodiments operation may involve obtaining at destination storage system a difference between the common base data and a point in time image of the file system of source storage system backed up at destination storage system to configure a qtree data a volume data or a sub directory data associated with the point in time image of the file system of source storage system backed up at destination storage system to be in a same state as a corresponding qtree data a volume data or a sub directory data corresponding to the common base data at destination storage system . In one or more embodiments the point in time image of the file system of source storage system backed up at destination storage system may be configured to be in an immediate temporal past relative to a backup of data to be restored to source storage system at destination storage system and in a temporal past relative to the common base data at destination storage system .

In one or more embodiments operation may involve traversing an appropriate qtree a volume or a sub directory corresponding to the qtree data the volume data and the sub directory data associated with the point in time image of the file system of source storage system backed up at destination storage system starting from a root inode thereof in a tree order. In one or more embodiments operation may involve fetching an inode associated with the common base data at destination storage system corresponding to an inode associated with the point in time image of the file system of source storage system backed up at destination storage system based on an inode identifier thereof.

In one or more embodiments operation may involve obtaining a difference between data associated with the two inodes as a difference between the common base data at destination storage system and the point in time image of the file system of source storage system backed up at destination storage system . In one or more embodiments operation may then involve applying to the common base data at source storage system the differential change corresponding to the difference between the common base data at destination storage system and the point in time image of the file system of source storage system backed up at destination storage system .

In one or more embodiments operation may involve providing through a computer network e.g. network the point in time image to the destination storage system. In one or more embodiments the second storage operating system may be configured to implement a file system different from that implemented through the first storage operating system. In one or more embodiments operation may involve establishing a common base data between the source storage system and the destination storage system through sharing the point in time image with the destination storage system. In one or more embodiments the common base data may be referenced by both the source storage system and the destination storage system based on an identification data.

In one or more embodiments operation may then involve incrementally restoring data associated with the source storage system through the destination storage system based on the common base data.

It should be noted that in one embodiment the destination storage system tagging snapshot identification data e.g. snap IDs to snapshots therein may force the source storage system also to use an identifier with respect to the point in time image therein which is common to both the source storage system and the destination storage system . For example as referenced previously the source storage system may associate an identification ID data with the captured point in time image of the file system analogous to a snapshot snap ID being used to tag snapshots in an operating system configured to support snapshots. In this example the identification data may refer to the identifier of the point in time image captured at the source storage system. The destination storage system may use snap IDs with regard to snapshots therein including the point in time image shared therewith .

Although the present embodiments have been described with reference to specific example embodiments it will be evident that various modifications and changes may be made to these embodiments without departing from the broader spirit and scope of the various embodiments. Also for example the various devices and modules described herein may be enabled and operated using hardware circuitry e.g. CMOS based logic circuitry firmware software or any combination of hardware firmware and software e.g. embodied in a machine readable medium . For example the various electrical structure and methods may be embodied using transistors logic gates and electrical circuits e.g. application specific integrated ASIC circuitry and or in Digital Signal Processor DSP circuitry .

In addition it will be appreciated that the various operations processes and methods disclosed herein may be embodied in a machine readable medium and or a machine accessible medium compatible with a data processing system e.g. a computer devices and may be performed in any order e.g. including using means for achieving the various operations . Accordingly the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.

