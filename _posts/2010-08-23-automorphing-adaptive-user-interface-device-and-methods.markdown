---

title: Auto-morphing adaptive user interface device and methods
abstract: An adaptive user interface device capable of implementing multiple modes of input and configuration may adapt to current user inputs, and may include configuration changes. In an aspect, an adaptive user interface device may be configured for a finger sensing in a touchpad mode, and configured for stylus sensing in a digital tablet mode. In another aspect, surface features of the adaptive user interface device may change shape, such as by raising buttons in response to entering a keyboard or keypad mode. Various mechanisms may be used for raising buttons, and may enable presenting buttons in a variety of shapes and locations on the interface. The configuration of the adaptive user interface device may depend upon the user actions and user identity. Configuration modes may be organized according to many levels enabling a single user interface to support a large number of input options functionality within a limited surface area.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08836643&OS=08836643&RS=08836643
owner: QUALCOMM Incorporated
number: 08836643
owner_city: San Diego
owner_country: US
publication_date: 20100823
---
This application claims the benefit of priority to U.S. Provisional Application No. 61 353 577 entitled Auto morphing Adaptive User Interface Device and Methods filed Jun. 10 2010 the entire contents of which are hereby incorporated by reference.

The present invention relates generally to computer user interface systems and more particularly to a user interface capable of adapting between alternate configurations.

Many computer systems rely upon user interface devices to receive commands and data inputs from users. A few types of user interface devices have become standard including the keyboard computer mouse touchpads touchscreen displays and trackballs. Such conventional user interface devices are specialized for particular types of input tasks such as entering text or type commands e.g. a keypad or keyboard and navigating within a graphical user interface e.g. a computer mouse or trackball . Users typically have to switch between two input devices in the course of daily tasks such as between a keyboard and a mouse

Touchscreen displays have become popular for some computing devices since they enable navigation and data input e.g. via a virtual keyboard . However touchscreen user interfaces do not provide raised keys and therefore are ergonomically and haptically less satisfying than a physical keyboard for text entry. Onscreen keyboards are not as easy to use and do not provide the tactile feedback of hard keys.

The various aspects include an adaptive user interface device for receiving user inputs to a computing device in which a user interface surface is configured to change operating modes in response to user inputs or operating states of applications or the computing device. Such changing operating modes may include changing functionality and or the shape of the user interface surface. An aspect includes a method of configuring such an adaptive user interface device configured to receive user inputs that includes adapting a mode of operation of the adaptive user interface device in which adapting a mode of operation may be selected from the group consisting of adjusting a sensitivity of the user interface device to user inputs and changing a shape of the adaptive user interface device. In an aspect the method may further include sensing a user input on the adaptive user interface device determining an operating mode compatible with the sensed user input and implementing the identified compatible user input mode by configuring the adaptive user interface device. In a further aspect adapting a mode of operation of the adaptive user interface device may include selecting one of a touchpad mode and a stylus activated digital tablet mode.

In another aspect adapting a mode of operation of the adaptive user interface device may include changing a shape of a portion of the surface of the adaptive user interface device. The method of this aspect may further include determining an acceptable location for a user s finger on the surface of the adaptive user interface device determining a current location of the user s finger on the surface of the adaptive user interface device and raising a portion of the surface of the adaptive user interface device so as to guide the user s finger in the direction of the determined acceptable location when it is determined that the current location of the user s finger is not at the determined acceptable location. In an aspect changing a shape of the adaptive user interface device may include raising a button on the surface of the adaptive user interface device in a first mode and lowering the button flush with the surface of the adaptive user interface device in a second mode and raising a button on the surface of the adaptive user interface device may include raising a plurality of buttons in a form a keyboard keypad game interface or other form of user interface structure. In a further aspect raising a button on the surface of the adaptive user interface device may include raising a plurality of buttons in a form of a keyboard configured in size and orientation based on an identity of a user of the adaptive user interface device. In a further aspect the adaptive user interface device may include one or more visual elements and the method may include activating the one or more visual elements on the surface of the adaptive user interface device.

In a further aspect the method may include determining an operating state of a computing device coupled to the adaptive user interface device determining a suitable configuration mode for the adaptive user interface based upon the determined current operating state and implementing the determined configuration mode by raising a plurality of buttons on the surface of the adaptive user interface device with shapes and locations defined by the determined configuration mode. In this aspect the shapes and locations of the plurality of raised buttons may convey information regarding a functionality associated with each of the raised buttons. In this aspect the method may further include activating one or more visual elements on the surface of the adaptive user interface device in a manner that communicates information regarding the functionality associated with each of the raised buttons.

In a further aspect the method may include determining a menu state of a computing device coupled to the adaptive user interface device in which raising a button on the surface of the adaptive user interface device includes raising a plurality of buttons in a format defined by the menu state. In this aspect the menu state may be specified in an application executing on the computing device. In this aspect the method may further include sensing a press of one of the plurality of buttons determining a functionality associated with the pressed one of the plurality of buttons executing a command in the computing device if the functionality associated with the pressed one of the plurality of buttons is determined to be an executable command and changing the menu state and raising a plurality of buttons in a format defined by the changed menu state if the functionality associated with the pressed one of the plurality of buttons is determined to be a selection of another user interface menu. In this aspect the shapes and locations of the plurality of raised buttons may convey information regarding the functionality associated with each of the raised buttons. In a further aspect the method may further include activating a plurality of visual elements on the surface of the adaptive user interface device in a manner that communicates information regarding the functionality associated with each of the plurality of buttons.

In further aspect a computing device may include a processor coupled to an adaptive user interface surface configured to send signal to the processor in response to receiving a user touch to the user interface surface in which the processor may be configured with processor executable instructions to perform operations including adapting a mode of operation of the user interface surface in which adapting a mode of operation may be selected from adjusting a sensitivity of the adaptive user interface surface to user inputs and changing a shape of the adaptive user interface surface. In a further aspect the processor may be configured with processor executable instructions to perform operations further including receiving a signal from the adaptive user interface surface indicating a user input on the user interface surface determining an operating mode compatible with the indicated user input and implementing the identified compatible user input mode by configuring the adaptive user interface surface. In a further aspect the processor may be configured with processor executable instructions to perform operations such that adapting a mode of operation of the adaptive user interface surface includes selecting one of a touchpad mode and a stylus activated digital tablet mode.

In a further aspect the adaptive user interface surface may be configured to be raised and lowered in response to commands received from the processor and the processor may be configured with processor executable instructions to perform operations such that adapting a mode of operation of the adaptive user interface surface includes sending signals to the adaptive user interface surface to cause it to change a shape of a portion of the user interface surface. In an aspect the processor may be configured with processor executable instructions to perform operations including determining an acceptable location for a user s finger on the user interface surface determining a current location of the user s finger on the user interface surface and sending signals to the adaptive user interface surface to cause it to raise a portion of the adaptive user interface surface so as to guide the user s finger in the direction of the determined acceptable location when it is determined that the current location of the user s finger is not at the determined acceptable location. In a further aspect the processor may be configured with processor executable instructions to perform operations and the adaptive user interface surface may be configured such that changing a shape of the adaptive user interface surface includes raising a button on the adaptive user interface surface in a first mode and lowering the button flush with the adaptive user interface surface in a second mode. In a further aspect the processor may be configured with processor executable instructions to perform operations and the adaptive user interface surface may be configured such that raising a button on the adaptive user interface surface includes raising a plurality of buttons in a form of one of a keyboard a keypad and a game interface. In a further aspect the processor may be configured with processor executable instructions to perform operations and the adaptive user interface surface may be configured such that raising a button on the adaptive user interface surface includes raising a plurality of buttons in a form of a keyboard configured in size and orientation based on an identity of a user of the computing device. In a further aspect the adaptive user interface surface may further include one or more visual elements coupled to the processor and the processor may be configured with processor executable instructions to perform operations further including activating the one or more visual elements on the user interface surface.

In a further aspect the computing device processor may be configured with processor executable instructions to perform operations further including determining an operating state of the computing device determining a suitable configuration mode for the user surface based upon the determined current operating state and implementing the determined configuration mode by sending signals to the adaptive user interface surface to cause it to raise a plurality of buttons with shapes and locations defined by the determined configuration mode. In the aspect the processor may be configured with processor executable instructions to perform operations such that the shapes and locations of the plurality of raised buttons conveys information regarding a functionality associated with each of the raised buttons. Further in this aspect the adaptive user interface surface may further include one or more visual elements coupled to the processor and the processor may be configured with processor executable instructions to perform operations further comprising activating the one or more visual elements on the adaptive user interface surface in a manner that communicates information regarding the functionality associated with each of the raised buttons.

In a further aspect the computing device processor may be configured with processor executable instructions to perform operations further comprising determining a menu state of the computing device in which raising a button on the adaptive user interface surface includes raising a plurality of buttons in a format defined by the menu state. In this aspect the processor may be configured with processor executable instructions to perform operations such that the menu state may be specified in an application executing on the computing device.

In a further aspect the computing device processor may be configured with processor executable instructions to perform operations further including receiving a signal from the adaptive user interface surface indicating a press of one of the plurality of buttons determining a functionality associated with the pressed one of the plurality of buttons executing a command in the computing device if the functionality associated with the pressed one of the plurality of buttons is determined to be an executable command and changing the menu state and sending signals to the adaptive user interface surface to raise a plurality of buttons in a format defined by the changed menu state if the functionality associated with the pressed one of the plurality of buttons is determined to be a selection of another user interface menu. In this aspect the processor may be configured with processor executable instructions to perform operations such that the shapes and locations of the plurality of raised buttons conveys information regarding the functionality associated with each of the raised buttons. Further in this aspect the adaptive user interface surface may include a plurality of visual elements coupled to the processor and the processor may be configured with processor executable instructions to perform operations further comprising activating the plurality of visual elements on the adaptive user interface surface in a manner that communicates information regarding the functionality associated with each of the plurality of buttons.

In a further aspect computing device adaptive user interface surface includes a piezoelectric actuator configured to raise a portion of the adaptive user interface surface in response to signals received from the processor. In this aspect the adaptive user interface surface may include Macro Fiber Composite elements comprising rectangular piezo ceramic rods sandwiched between layers of adhesive and electroded polyimide film.

In a further aspect computing device adaptive user interface surface includes a fluid pocket a valve coupled to the fluid pocket and to the processor and configured to open and close in response to signals received from the processor and a fluid pump fluidically coupled to the valve.

In a further aspect computing device adaptive user interface surface includes an electrostatically activated portion electrically coupled to the processor. In this aspect the electrostatically activated portion may include first and second surface layers separated by an insulator layer in which the first and second surface layers may be electrically coupled to the processor so that the processor can selectively apply same or different voltages to the first and second surface layers and in which the processor may be configured with processor executable instructions to perform operations further comprising applying voltages of a same polarity to the first and second surface layers to raise the electrostatically activated portion.

In a further aspect computing device adaptive user interface surface includes a magnetically activated portion electrically coupled to the processor. In this aspect the magnetically activated portion comprises a permanent magnet and an electromagnet separated by a separator layer in which the electromagnet may be electrically coupled to the processor so that the processor can generate a magnetic field by applying a current to the electromagnet and the processor may be configured with processor executable instructions to perform operations further comprising applying a current to the electromagnet to raise the magnetically activated portion of the user interface surface.

A further aspect includes a computing device including a means for accomplishing some or all of the functions of the aspect method described above.

A further aspect includes a non transitory processor readable storage medium having stored thereon processor executable instructions configured to cause a processor of a computing device coupled to an adaptive user interface surface to perform operations of the aspect method described above.

In a further aspect an adaptive user interface device includes a user interface surface configured to change a shape in response to a signal from a processor coupled to the adaptive user interface device. In this aspect the user interface surface may include one or more visual elements configured to generate a visual image in response to a signal from a processor coupled to the adaptive user interface device. In this aspect the user interface surface may include an actuator configured to raise a portion of the user interface surface in a first mode and lowering the portion of the surface flush with the user interface surface in a second mode. In this aspect the user interface surface may include a plurality of actuators in a form of a keyboard a keypad a game interface or another user interface device in an aspect the plurality of actuators may be configured to raise a plurality of buttons in a form of a keyboard configured in size and orientation based on an identity of a user of the adaptive user interface device. In an aspect the user interface surface may include a plurality of actuators controllable by a processor coupled to the adaptive user interface device and configured to be individually actuated to each raise a portion of an exterior of the user interface surface in response to signals from a processor coupled to the adaptive user interface device. In an aspect the user interface surface may be configured to detect a press of a raised portion of the user interface surface and send a signal to the processor coupled to the adaptive user interface device indicating that the raised portion of the user interface surface was pressed. In a further aspect the actuator may be one of a piezoelectric actuator such as a Macro Fiber Composite element including rectangular piezo ceramic rods sandwiched between layers of adhesive and electroded polyimide film. In another aspect the actuator may be a fluidic actuator including a fluid pocket a valve coupled to the fluid pocket and configured to open and close in response to signals received from the processor coupled to the adaptive user interface device and a fluid pump fluidically coupled to the valve. In another aspect the actuator may be an electrostatic actuator which may include first and second surface layers separated by an insulator layer in which the first and second surface layers may be electrically coupled to the processor so that the processor can selectively apply same or different voltages to the first and second surface layers. In another aspect the actuator may be a magnetic actuator which may include a permanent magnet and an electromagnet separated by a separator layer in which the electromagnet is electrically coupled to the processor so that the processor can generate a magnetic field by applying a current to the electromagnet.

In another aspect an adaptive user interface device may include a user interface surface and a means for changing shape of the user interface surface in response to a signal from a processor coupled to the adaptive user interface device and or a means for generating a visual image in response to a signal from a processor coupled to the adaptive user interface device. In an aspect the user interface surface may include a means for raising a portion of the user interface surface in a first mode and lowering the portion of the surface flush with the user interface surface in a second mode. In an aspect the user interface surface may include a means for raising a plurality of buttons in a form of one of a keyboard a keypad a game interface and or another user interface device. In an aspect the user interface surface may include a means for raising a plurality of buttons in a form of a keyboard configured in size and orientation based on an identity of a user of the adaptive user interface device. In an aspect the user interface surface may include a means for individually raising small portions of the user interface surface in response to signals received from the processor coupled to the adaptive user interface device. In an aspect the user interface surface may include a means for detecting a press of a raised portion of the user interface surface and a means for sending a signal to the processor coupled to the adaptive user interface device indicating that the raised portion of the user interface surface was pressed.

The various aspects will be described in detail with reference to the accompanying drawings. Wherever possible the same reference numbers will be used throughout the drawings to refer to the same or like parts. References made to particular examples and implementations are for illustrative purposes and are not intended to limit the scope of the invention or the claims.

The word exemplary is used herein to mean serving as an example instance or illustration. Any implementation described herein as exemplary is not necessarily to be construed as preferred or advantageous over other implementations.

The various aspects include systems and methods that provide an adaptive user interface device which can be automatically configured to receive a variety of different types of user input. In a first aspect the adaptive user interface device can function as a touchscreen responsive to finger touches and when touched with a stylus adapt to function as a digital tablet receiving input from the stylus. In a second aspect the adaptive user interface device may raise or lower buttons in fixed positions enabling it to transform between a flat touch surface and a keypad or keyboard of raised buttons. In a third aspect the adaptive user interface device may raise or lower buttons anywhere on the surface of the interface with the shape configuration and orientation of the raised button being configurable by application or operating system software. In an aspect the reconfiguration of the adaptive user interface may be accomplished automatically in response to the sensed user inputs and or operating system e.g. available button function options or a menu state . In an aspect a computing device may anticipate a mode of user interactions based upon a received input e.g. a touch on the touch surface and present a configuration e.g. a particular set of raised buttons consistent with that anticipated interaction. The various aspects may be implemented in a variety of applications particularly in applications where a variety of different user input interfaces and configurations would be desirable. Additionally aspects which enable the dynamic placement of buttons anywhere on the surface of the interface may enable menu button tree configurations which can be interpreted by users simply by feeling the location and shapes of the raised buttons thereby providing users with a physical user interface with which users can interact with sophisticated systems without the need to look at a graphical user interface display. In a further aspect the adaptive user interface surfaces may be positioned on any touchable surface of a computing device or apparatus connected to a computing device and not just on a flat e.g. tablet surface or visual display surface.

In the various aspects the adaptive user interface device may adapt the user interface within a current mode of input or morph the user interface into a totally different mode of input. In an aspect illustrated in the adaptive user interface may be in the form of a tablet that can function as a touchpad that can sense user finger touches operating in a touch pad mode and can provide such inputs to an attached computer and upon sensing the touch of a stylus to the surface transform into a digital writing pad configured to receive inputs from the stylus . In this aspect the adaptive user interface may exhibit a first level of sensitivity to touches suitable for receiving user inputs in the form of finger strokes on the surface and a second level of sensitivity suitable for receiving user inputs in the form of stylus strokes on the surface. For example the sensitivity of the touch surface may be reconfigured to accommodate the smaller contact surface with higher local contact pressure associated with a stylus pressing against the surface of the adaptive user interface device. Additionally the sensing mechanisms may be modified or recalibrated to take advantage of a different type of physical interaction provided by a stylus as compared to a finger touch. For example an adaptive user interface may be configured to sense a user s finger touch using capacitive sensors in a touchpad mode and transform to recognizing stylus strokes based on a different set of electrical signals that may be provided by a metal stylus in a digital tablet mode .

In another aspect illustrated in the adaptive user interface may be configured to function as a touchpad or digital tablet in a first mode and transform or morph into the a keyboard mode by raising preconfigured buttons that are actuated to rise up to form a keyboard interface . Such buttons may be actuated by any of the actuation mechanisms described herein. By providing actuating buttons that raise up in a keyboard mode but retract to form a smooth surface in a touchpad mode the adaptive user interface device can function as a touch surface or digital tablet to receive touch type user inputs and as a conventional keyboard to receive conventional data entry user inputs. Further the keyboard configuration mode may be activated only when certain user inputs are to be expected such as when a user is authorized to use the computer or make data entry sent to the computer. When not in use this aspect of the adaptive user interface provides a smooth touchpad surface which may be suitable for other tasks such as receiving pointing and drawing inputs while providing a surface that can easily be cleaned. For example this aspect of the adaptive user interface device may be particularly useful in a hospital setting since the interface device surface returns to a flat configuration when not in the keyboard mode enabling it to be easily cleaned and sterilized. Since the buttons rise up from the surface level there may be no gaps or grooves in which germs can hide. This aspect of the adaptive user interface device may similarly be useful in a factory setting in which manufacturing machine control buttons may be raised only when the machine is in an operating state to receive user inputs. At other times adaptive user interface may present a smooth surface that easily can be cleaned of dirt and grime. Since there are no openings or grooves between buttons there are far less opportunities for dirt and machining particles to become lodged in the keyboard. By providing raised keys the keyboard may be easier to use than current user interface devices deployed in factory setting which typically use a plastic sheet over a flat keyboard.

In a third aspect described in more detail below with reference to the adaptive user interface may be configured with a plurality of actuating elements that can be actuated individually to generate raised portions or buttons anywhere on the interface surface and in a variety of shapes. By adjusting the position in shapes of raised buttons the adaptive user interface can accommodate a wide variety of different operating modes or functionalities and provide keys that can be have meaning based upon their tactically recognizable shapes. For example as discussed below with reference such a generally configurable adaptive user interface may be able to actuate buttons whose shape can be recognized by the user s fingers to convey information regarding the functionality associated with each button. This may allow users to accurately select a particular button without having to look at the keypad keyboard or a graphical user interface display. Such capabilities may be useful in applications where users cannot divert their attention from a visual activity e.g. driving a vehicle flying an airplane or operating heavy machinery . In a further aspect the button positions may be responsive to user inputs enabling users to reposition buttons as if moving sliding levers on a conventional physical display.

In some aspects the configuration or operating mode of the adaptive user interface may be selected by a user such as by positioning a switch on the computing device or interface device or by selecting an option on a user interface menu. In other aspects a computing device or the adaptive user interface device may be configured to automatically determine an appropriate configuration or operating mode based on a user s interaction with the interface. Various mechanisms for recognizing a user s intended use of the interface may be used. Input may be received directly via the user interface such as input by touchpad stylus or various button modes. Input may be voice or audio commands received over a microphone or receiver. Input may also include gestures or postures of fingers hands or other objects where the gestures or postures are recognized by processing data received from cameras proximity sensors or other devices. For example if a user touches the surface of the adaptive user interface with fingertips of both hands as illustrated in the attached computing device or adaptive user interface device may recognize this touch event as indicating that a keyboard mode should be activated as illustrated in . As another example user interactions with the interface may be received by a camera as illustrated in . For example the attached computing device or the adaptive user interface device may be configured to process images received from the camera to determine an orientation of the user s hand s and compare this image to images stored in memory to select a corresponding image which may be useful for determining the appropriate configuration or operating mode for the interface device.

To enable applications to interact with an adaptive user interface device which may provide a wide range of command signals the computing device using such an interface may be configured with an adaptive tablet application interface software module as part of its operating system or a driver for the interface device. Such an adaptive tablet application interface software module may include an application interface portion configured to pass key event notifications to and receive configurations signals and commands from applications . The application interface module may serve as a translator so that application developers need not be concerned with the details of receiving commands from an adaptive user interface device. The adaptive tablet application interface software module may also include a tablet controller module which is configured to pass configuration and actuation commands to and receive touch event notifications from the adaptive user interface device . The tablet controller module may be configured to specify the coordinates on the tablet surface that are to be actuated or raised in order to generate buttons the particular configuration. Such configuration and actuation commands may be passed to surface actuators such as circuitry which drives individual button raising actuators. Since the location of buttons and the nature of the user inputs may vary based upon the implemented device configuration the tablet controller module may also be configured to translate received touch event coordinates or button actuations into corresponding commands e.g. a letter or number associated with a particular pressed button which can be interpreted by applications. The tablet controller module may also include a state machine configured to support keyboard operations when the adaptive user interface device is configured as a keypad or keyboard.

In operation when a user touches or otherwise interacts with the adaptive user interface device such as positioning fingertips of both hands in a manner suitable for typing on the keyboard such interactions may be passed to the tablet controller module . The tablet controller may be configured to recognize such an input as indicating that the user desires to enter data into the computer via a keyboard. In response the tablet controller may transmit configuration and actuation commands to the surface actuators to cause portions of the surface to be raised in the form of a keyboard. Information regarding the locations of finger tips touching the tablet surface may be used by the tablet controller module to determine the appropriate orientation and size of the generated keyboard.

In various aspects the adaptive tablet application interface may be implemented within software operating in a computing device coupled to the adaptive user interface device or may be implemented within a processor that is part of the adaptive user interface device .

If the received user input is recognized as meaningful user input i.e. determination block Yes the received input may be correlated with a corresponding interface device mode or configuration in block . Different input modes may require different sensor configurations e.g. sensitivity sensor interpretation etc. and different configurations may require the touch interface to physically change shape such as raising portions of the surface in order to generate physical buttons. In block the processor may determine whether the detected user interface device is already configured in the correlated input mode or configuration. If the correlated mode or configuration is already in effect i.e. determination block Yes the received input may be processed according to the implemented motor configuration in block with the resulting command information provided to the attached computing device. After the user input is processed in block the processor may return to block to await the next received user input. However if the correlated mode or configuration is not already in effect i.e. determination block No the adaptive user interface may transform itself into the corresponding mode or configuration in block . As described herein such transformation may include the adaptive user interface physically changing the shape of the surface e.g. by raising or lowering buttons recalibrating input sensors or both.

The various aspects may use a variety of mechanisms for achieving shape changes in the surface of the adaptive user interface device . Such shape changing mechanisms may include pneumatic hydraulic electric actuator electrostatic and magnetic mechanisms. Such shape changing driving actuators are tied to the processor within the adaptive user interface device itself or within a computing device coupled to the interface device so that surface shape changes can be controlled by the processor. Examples of various shape change driving actuators are described below however different technologies and drive mechanisms may be used with the various aspects.

An example of a piezoelectric element that could be used in various aspects is Macro Fiber Composite MFC which is manufactured and sold by Smart Material Corp. of Sarasota Fla. The MFC comprises rectangular piezo ceramic rods sandwiched between layers of adhesive and electroded polyimide film. This film contains interdigitated electrodes that transfer applied voltages directly to and from the ribbon shaped rods. This assembly enables in plane poling actuation and sensing in a sealed durable ready to use package. When embedded in a surface or attached to flexible structures the MFC can provide distributed solid state deflection and vibration control or strain measurements.

A further example shape changing actuation system is illustrated in . In this example aspect the surface of an adaptive user interface device may be raised by a plurality of pins individually driven by linear actuator motors. Referring to a single actuator may include a pliable surface layer which can be displaced vertically by an underlying pin which is held in place laterally by parallel plates and and driven vertically by a linear actuator . The linear actuator may be any suitable linear motor including linear piezoelectric actuators solenoid actuators linear motors linear stepper motors etc. Each linear actuator may be driven by an individual electric signal such as is delivered by a connector which may be coupled to a processor within the attached computing device or within the adaptive user interface device. illustrates the actuator in the down or deenergized configuration. In this configuration the pliable surface layer lies flat. illustrates the actuator in the up or energized configuration in which the pliable surface layer is raised by pressure applied by the pin .

As shown in a plurality of the actuators may be closely spaced and configured in an array in order to be able to raise different portions of the pliable surface layer in response to individual signals applied by the processor via connectors . illustrates the adaptive user interface device in a tablet mode in which none of the actuators are energized.

A further example shape changing actuation system utilizing electrostatic forces is illustrated in . In this example aspect the adaptive user interface device may include a top surface layer and a bottom support layer which may be selectively energized by a processor such as applying a voltage e.g. Vcc . If the top surface layer and bottom support layer are separated by an insulator layer charges applied to the top and bottom layers may result in electrostatic repulsive or attractive forces. By configuring the top surface layer with a pliable portion when the voltages of the same polarity are applied to both the top surface layer and bottom server support layer as illustrated in the electrostatic forces may cause the pliable portion to raise above the rest of the top surface layer . This raised surface may provide a tactile button . Pressing of this button may be detected by the change in the capacitance or voltage between the two layers as may be measured by the electrical sensor . As illustrated in the button may be retracted and the adaptive user interface device may return to a smooth surface by coupling one or both of the top surface layer and bottom support layer to ground or by applying voltages of opposite polarities to the top surface layer and bottom support layer . By organizing the pliable portion in an array such as is illustrated in a wide variety of different button shapes can be generated.

A further example of a shape changing actuation system utilizing magnetic forces is illustrated in . In this example aspect the adaptive user interface device may include permanent magnets embedded within the pliable top surface layer positioned opposite of pancake electromagnets position within a bottom support plate . A separator layer may be provided to prevent the permanent magnet and electromagnet from sticking together. By applying a voltage of the right polarity to the electromagnet a magnetic field may be generated in a manner that repels the permanent magnet . If the permanent magnet is coupled to the top surface layer with pliable materials it may raise above the surface layer to a sufficient degree so as to create a button as illustrated in . Actuation of this button may be detected such as by measuring a current induced in the pancake electromagnets sixteenths such as by an electrical sensor . By reversing the polarity of the current applied to the pancake electromagnet an attractive force can be established between the permanent magnet and the electromagnet causing the permanent magnet to rest on the separator layer resulting in a smooth surface of the adaptive user interface device . By organizing the permanent magnet and electromagnet pairs in an array such as illustrated in a wide variety of different button shapes can be generated by selectively energizing individual electromagnets .

In a further aspect physical distortion of the surface of the interface may not be required. Instead other techniques such as vibrations temperature or electric charges may provide haptic feedback to users in control of regions of the user interface such as applying force creating vibrations or changing temperature when input is received.

The controllability of the shape changing surface of an adaptive user interface device can provide a large number of useful applications not available with currently known interface technologies. The ability to raise buttons of recognizable shapes anywhere on the adaptive user interface device surface may enable the interface to convey information regarding available command options i.e. functions associated with the buttons in a manner that is currently accomplished using visual displays. Raising buttons of recognizable shapes in different locations on the interface can communicate tactically to the user the available button functions. Such buttons can then be pressed by a user to select the corresponding functions.

In a manner similar to how graphical user interfaces and voice activated telephone interface systems operate an aspect can raise buttons of shapes and locations that communicate their functionality in a menu tree organization. Thus a series of different raised button configurations may be presented in response to user button presses with the configuration changing depending upon the user inputs and the current functional options available. For example a first configuration of raised buttons may enable the user to select among different classes of functionality e.g. flight controls engine controls navigation controls communication controls etc. . Pressing one of such general menu buttons to select a class of functionality may lead to a second button configuration relevant to the selected class in which the button shapes and locations convey the functionality associated with each button. Pressing one of those buttons may actuate a corresponding function or select a further configuration of buttons associated with additional function options. Such layering of command options may continue through multiple layers in order to provide a wide range of functions that users can select via a single adaptive user interface device. Since the shape and location of buttons can change a user can determine the function of a button based on feel without looking either at it or at a graphical user interface display. In this manner a very sophisticated user control interface can be provided within a small interface surface area.

Conveying the functionality of a button in its shape and location may have a wide range of useful applications. One example application of such an adaptive user interface device is for control panels aircraft cockpits. Aircraft cockpits typically are filled with a multitude of buttons for controlling the various aircraft systems. Pilots must memorize the location of each switch or button which take up large portions of the cockpit real estate. Recent avionics systems are replacing many buttons with a computer graphical user interface that can present menus identify functions assigned to fewer buttons. However pilots much divert their attention to the display to activate a virtual key or button tied to a function identified on the display screen. The various aspects enable a different type of control system for use in cockpits in which many buttons can be replaced by a single adaptive user interface device that transforms the buttons presented to correspond with a multi layered menu system of alternative functions.

In a further aspect the adaptive user interface device may be configured with sensors to recognize the user based upon sensible parameters such as the shape of the user s hand. Upon recognizing a user the adaptive user interface device may change configuration and shape to provide the user with an interface consistent with the user s preferences and authorizations. Such capabilities may be combined with the transforming user interface to provide a further layer of security since the keys will not be raised and made available for use if the user is not authorized. Further the particular keys presented on the interface may depend upon the functionality that the user is authorized to activate.

An example of an adaptive user interface device functioning to provide a menu of alternative keys based upon user inputs is illustrated in B and C. For example illustrates an adaptive user interface display in which the user is presented with two function options represented by two raised buttons . This may reflect an initial menu configuration in which the user can select between a second layer of menus. In response to the user pressing button a second interface configuration may be presented including buttons and with button having been withdrawn as illustrated in . As a further example if the user presses button the third alternative button configuration may be generated as illustrated in including buttons and with button withdrawn.

The capabilities of the various aspects may be useful in tailoring keyboards to a variety of different users as illustrated in . For example a young child may prefer a smaller keyboard whereas an adult with large fingers may prefer larger keys that are more spread out. Thus as illustrated in the adaptive user interface device may transform into a keyboard suitable for a user with small hands and transform into a larger key board configuration if the user has large size hands as illustrated in .

In a further example user may select a particular type of keyboard layout such as illustrated in . Thus a single adaptive user interface device may enable users to select from a wide range of alternate keyboard arrangements such as the Dvorak layout and include a variety of keys for supporting various languages including additional keys for language specific diacritic marks. Each of these layouts may be set as preferred modes associated with a user identity or as different modes within the same aspect.

The physical configuration of the adaptive user interface device may be accomplished automatically such as to adjust the topography of buttons to fit the size of a user s hand based upon their sensed touch locations on the surface or they may be set by users. Users may program or set an adaptive user interface device to automatically react with a particular mode or topography based on the user s identity such as in the form of personal preferences. For example a user may prefer a keyboard layout including additional buttons as illustrated in . When that particular user is identified to or recognized by the detected user interface device or a computer coupled to the device the user setting may be automatically implemented resulting in keys rising in the desired locations.

The capabilities of the various aspects may not be limited to conventional keypad or keyboard layouts and may accommodate a variety of user interface it is appropriate to particular applications . For example illustrates a configuration of an adaptive user interface device in which the device has assumed a button topography specialized for computer games. For example a game interface configuration may include a set of function control buttons a set of menu buttons and a motion or direction controller . As an example of an application of an adaptive user interface device to a game application a computing device may be configured to enable users to play a game with moving objects similar to the game Pong by interacting with the surface to cause a raised portion to move about. For example two players could sit on opposite sides of an adaptive user interface device surface and take turns hitting the raised portion of the surface with their hands. The touch of the user s hand on the raised portion of the surface would be detected by the adaptive user interface device to communicate to a processor which would cause the raised surface to move in response to the detected touch e.g. location and pressure . Thus the user touching a moving raised portion could cause that raised portion to travel in the opposite direction. A user might win such a game if the raised portion of the surface reaches a boundary of the adaptive user interface device. In a further example a processor could cause the adaptive user interface device to raise portions of surface in order to propel an object such as a ping pong ball poker chip or miniature toy across the surface such as in response to game inputs or games states. Other forms of physical interactions between moving surface portions and users may be implemented to enable a wide variety of computer games.

As discussed above the shape of buttons that are generated by the adaptive user interface display may be used to convey meaning to users that can be sensed without looking at the keypad. Examples of a few tactically recognizable button configurations are illustrated in . Other complex button shapes are also possible.

Various aspects are not limited to generating buttons. For example complex topographies may be presented on the adaptive user interface device such as to convey geographic information identity information and generally the kind of information that are currently communicated to users on visual displays. For example the surface of an adaptive user interface device may morph into the contour of a person s face enabling users to recognize an individual based on feel.

In further aspect the position of buttons presented on an adaptive user interface display may change in response to user inputs. For example illustrate a configuration in which buttons are positioned along the raised lines such as in a manner similar to how letters may be positioned in an audio equalizer a sliding electronic control or a linear potentiometer. illustrates a first position of interface buttons . The user may touch one of the buttons and without lifting a finger from the surface push it vertically along one of control lines . In response to sensing the applied pressures the adaptive user interface device may reposition the buttons along the vertical axis MDII as if the user were physically moving a button. illustrates a second position of the interface buttons after the user has moved one of the buttons along one of the vertical lines . This form of control provides both visual and tactile feedback regarding the position of the buttons along the vertical scale such as may be useful for controlling systems that respond to such a linear inputs. The sliding motion of the generated button may signal any input with a continuous range of values including behavior similar to the sliding electronic control or linear potentiometers that the surfaces resemble.

In a similar manner users may reposition buttons on the adaptive user interface device such as to suit their personal preferences or working needs. For example as illustrated in a user may touch a button in one position and drag it to a second position where it may remain as illustrated in . In addition to input sliding surfaces may also be a way of manipulating an aspect into a preferred topography. Such personalized topographies may be saved as user preferences that may then be reproduced when the user s identity is made known to the adaptive user interface device or an attached computing device.

As discussed above the flexibility of presenting buttons in various locations and various shapes enables a neat new form of interface with users not tied to visual displays. An example method that may be implemented in a computing device or within an adaptive user interface device equipped with a processor for implementing a variety of menu button configurations is illustrated in . In method in block a processor may set the menu state at the lowest level in a layered menu organization. For example the lowest layer level menu may provide options for selecting categories of menus from which the user can select. In block the adaptive user interface device may raise the buttons in the shapes and locations corresponding to the currently set menu state. In block the adaptive user interface device may sense a button press and in block an attached processor may determine the function or menu selection that linked to the pressed button. In blocks and the linked functionality may be determined based upon a map of functions correlated to button locations on the surface area of the adaptive user interface device defined in the current menu state. Thus determining the function or menu selection options linked to particular buttons may involve correlating the location of the button press to options presented in the current menu state. In determination block the processor may determine whether the selection indicated by the button press has selected an executable function or another menu. If the functionality corresponding to the button press is a selection of another menu to be implemented i.e. determination block Menu selection the processor may set the menu state to the selected menu in block . Thereafter the adaptive user interface device may return to block to raise buttons in shapes and locations corresponding to the newly selected menu state. If the processor determines that the pressed button corresponds to an executable function i.e. determination block Function the selected function may be executed in block . If as a result of executing the function the menu options should return to the base menu the processor may return to block to set the menu state at the lowest level. If the executed function results in a different menu state the processor may optionally return to block to implement the current menu state.

The menu states defined and implemented in method may be specified in user applications and or within a computing device operating system. For example many states may specify the physical location on the interface and shape of buttons along with their associated functionality organized in terms of menu states. Any number of menu states may be so defined by application developers. Thus method enables application developers to implement a wide variety of physical keypad configurations and menus.

While some of the figures illustrate the adaptive user interface device configured as a flat surface such as a tablet surface the interface surfaces may be configured in any form or contour and positioned on any accessible surface of a computing device or apparatus coupled to a computing device. For example as mentioned above the adaptive user interface device may be incorporated into surfaces of an aircraft cockpit or an automobile dashboard or console. As another example adaptive user interface device elements may be incorporated on any surface of a mobile device e.g. a smart phone smart book or laptop such as a side or the back surface. In this manner for example a user interface set of buttons may be activated and raised on the back of a mobile device enabling the user to provide inputs by pressing buttons of recognizable shapes on the back surface while viewing images on the visual display on the front surface.

Although the adaptive user interface device enables users to interact with a system without a graphical display in some aspects the interface may also incorporate display elements. In addition to the raised button configuration communicating the state of the system flexible display elements may be integrated into the top layer of the adaptable user interface device which can be controlled by a processor to convey visual information. An example of a suitable display technology that may be used for this aspect is flexible organic light emitting diode OLED technology an example of which has recently been announced by Sony Corporation. Displaying visual information on top of morphing surfaces may confirm the functions of the raised buttons such as displaying letters numbers and or colors on the raised surface. In this way the actuated buttons may be dynamically labeled with a visual identifier such as a color letter or number and or text label. This aspect of an adaptive user interface device equipped with a dynamic display is illustrated in .

Referring to an adaptive user interface device with integrated visual display elements may be formed by including processor activated display elements such as flexible OLED elements within or on top of the top surface . The display elements may be flexible in structure so they can bend with the top surface when various configurations of raised buttons or shapes are activated. The display elements may be arranged in a variety of shapes and layouts in order to accommodate a wide assortment of deployable buttons and shapes. A variety of display element components may be implemented in the various aspects including for example liquid crystal display LCD elements and light emitting diodes LED of the various types. Other components illustrated in are described above with reference to .

In some aspects the processor activated display elements may be configured as set display features such as letters numbers or text for underlying buttons which are illuminated or otherwise activated when the underlying button is raised. In this manner the keys of a keyboard are identified when the keyboard mode is activated i.e. when the keys are raised and configured to receive user inputs but not illuminated when the adaptive user interface device is in a table or deactivated mode.

In other aspects the processor activated display elements may be positioned on the adaptive user interface display device so that they can be activated by a processor in order to generate any display consistent with the element patterns. For example referring to the processor activated display elements may be positioned on the adaptive user interface display device in segments and patterns so that any number or character may be illuminated by the processor selectively darkening e.g. by activating an LCD or illuminating e.g. energizing an LED particular display element segments. In this aspect the display elements may be in the form of short strips which are configured in an array formatting a number character pattern . As illustrated in cross section A that is shown in the display elements may be partially or fully embedded within the external surface . By selectively activating specific display elements the processor can generate any number or character within the number character pattern as illustrated in number character pattern . A wide variety of different display element patterns may be implemented such as providing diagonal display elements as illustrated in number character pattern . In this aspect each of the display elements are coupled electronically to the processor so that they may be selectively energized to illuminate the letter or character corresponding to an activated button.

In another example illustrated in the processor activated display elements may be configured as an array of dots or LEDs across the surface of the adaptive user interface display device . In this configuration any form of display may be generated by selectively illuminating or darkening selected display elements in order to form a pattern. This aspect may be particularly useful for adaptive user interface displays that can activate a wide range of buttons and raised shapes since any pattern of letters numbers shapes or text may be illuminated or darkened on the surface. While shows a pattern of round dots the display elements may be of any shape including linear segments crosses ovals etc.

In use a processor coupled to the adaptive user interface display device may activate display elements in conjunction with activating buttons or raised portions of the device surface such as part of the operations performed in blocks and described above with respect to . In this manner the shape and location of the raised buttons may depend upon or convey the meaning of their functionality which may be expressed in a visual display on the surface of the buttons raise portions.

In a further aspect an application programming interface API may be provided to assist application developers in configuring and implementing an adaptive user interface device. For an adaptive user interface device capable of actuating any portion of a surface such as described above with reference to the individual surface actuators may be treated in a manner like display pixels with the exception that the actuation involves specifying a height to which the surface should be elevated. To reflect this the smallest actuation element units may by referred to as hixels. 

There are several ways that a C programmer could configure an application to actuate collections of hixels so as to generate a particular user interface surface configuration. In a first example applications could be configured to draw into some location or buffer in memory which might be called a Height Canvas . The adaptive user interface device may then use the data stored in such a Height Canvas to determine which hixels to raise and how high. The adaptive user interface device then raises or lowers each hixel or set of pixels if the device does not have pixel to hixel resolution . For example hixels with value of 0 black may not be raised while hixels with a value of 255 white may be extended to their maximum height. By sequentially loading new hixel values into the Height Canvas the adaptive user interface device may be cause to move or animate with surface features the rising and falling with time.

In a second example a function or API could be provided by the manufacturer of the adaptive user interface device that can be called by any application program to actuate hixels. Such a function or API could receive an image from a graphics buffer such as some or all of a displayed image transform the received image into grayscale possibly modify the image to render it suitable for use in actuating particular surface actuation elements and copy the result into a Height Canvas as described above. In this manner an animated image processed by such a function or API could generate a moving surface features.

In a third example graphical user interface GUI toolkits like Qt GTK or Windows may be modified to provide a look up table whereby for every widget buttons checkbox slider etc that can associate interface features with hixels of particular positions and heights. Thus if a developer is using Visual C to drag and drop buttons to create a calculator application a GUI toolkit like Microsoft may determine the height of buttons on the calculator interface should be of hixel value 128 with the buttons configured to be round and determine that the height of text boxes should be of hixel value 96 around the border with the text having a hixel value of 48. Such GUI toolkits could work with a wide array of software applications and programming languages and be used to support application development for a variety computing devices.

In a further aspect the height to which buttons and features are raised in the may be adjustable or user definable such as adjusted according to a scale factor based on user preferences measurements of the user s hand and or fingers or other information. Such an actuation height adjustment scale factor for particular users may be recorded in memory so that the height of actuated buttons and surfaces will depend upon who his logged into the system. Such an actuation height adjustment scale factor may be applied to hixel values store in a height canvas or may be applied to inputs that are used to populate the height canvas.

In a further aspect the processor of the computing device may control and adaptable user interface device in a manner that interacts with the user by moving the raised surfaces in response to the user or based on operations or operational states of an executing application. Thus the computing device processor and the adaptive user interface device may be configured to raise a portion of the device surface to alert the user to a condition such as an incoming telephone call an alarm or a current or pending operating state. For example the adaptive user interface device could be used by a processor to touch tap or poke the user such as the user s leg when the computing device is in the user s pocket to alert the user to an incoming telephone call email or text message. Such a touch tap or poke could take the place of or augment a vibration mode alarm. As another example a portion of the interface surface could be raised up to provide a visual indication of the alarm such as to simulate raising a hand or a flag.

As another example the computing device processor and the adaptive user interface device may be configured to determine whether a current position of a user s hand on the device surface is suitable for a current operating state and when it is not in an suitable position raise portions of the surface so as to guide the user s hand to a better location.

A method for guiding a user s finger to a new location is illustrated in . In method in block the processor may determine an acceptable location for the user s finger or hand. This determined acceptable position may depend upon a configuration of buttons the actuated for current operating state or application or a displayed image that would otherwise be blocked by the user s hand. In block the adaptive user interface device may detect the touch of a user s finger on the interface surface and communicate the location of the touch to the processor of a computing device coupled to the user interface device. In determination block the processor may determine whether the user s finger or hand is in an acceptable position on the surface of the adaptive user interface device. For example if the adaptive user interface device includes a display element as described above with reference to the processor may determine whether the position of the user s finger on the adaptive user interface device surface would block the user s view of an important part of the display. As another example the processor may determine that the user s hand in an unsuitable position when a current operating state of the device requires a raised button configuration that cannot be sensed or actuated by a user when the user s hand is in the determined position. If the processor determines that the user s fingers or hand are in an acceptable location on the user interface surface i.e. determination block Yes the processor and adaptive user interface device may proceed with method or as described above with reference to or .

If the processor determines that the user s finger or hand is not in an acceptable location i.e. determination block No the processor may direct the adaptive user interface device to raise a portion of the actuator surface that is connected to the location of the user s finger or hand in block . In block the processor device may then actuate adjoining surface actuator elements in order to move the raised surface so as to direct the user s finger towards the determined acceptable location. For example the actuator surface may provide a ridge or raised surface that presses against the tip of the user s finger so as to provide a simple push in the direction of the acceptable location. With each incremental movement of the raised surface the processor may determine whether the user s finger or hand has moved to the acceptable location by returning to block to determine its current position on the surface. This movement of a portion of the surface may continue until the processor determines that the user s finger or hand are in an acceptable location on the user interface surface i.e. determination block Yes the processor and adaptive user interface device may proceed with method or as described above with reference to or . In this manner the computing device and associated adaptive user interface device can provide haptic feedback to the user to guide the user s finger or hand to a more suitable location on the device surface.

Along the lines of prompting a user to move fingers to a more suitable location a processor of the computing device may also be configured to similarly move a user s finger to a location that may improve operation such as by providing an additional heat sink. For example the processor may be configured to detect when the device is running above a threshold temperature and raise portions of the actuator surface so as to reposition one or more of the user s fingers to a location where it can serve as an additional heat sink.

In a further aspect a processor of a computing device may be configured to use the ability to move raised portions of the adaptive user interface device to generate movement of the computing device itself. For example the processor may be configured to move raised portions across the interface surface that enables the device to walk across a table or other surface. In another example the processor may be configured to move raised portions across the interface surface to cause the device to crawl within a user s pocket. For example the processor may be configured to recognize when the devices in a user s pocket e.g. by detecting low or no ambient light the device being in a sleep mode the device being in a vertical orientation etc. and activate the movable membrane in a manner consonant to crawl deeper into the user s pocket i.e. in the direction of gravity in order to prevent the device from falling out of the pocket.

In a further aspect the adaptive user interface display configuring with actuator elements that enable a large actuation could be configured to pop open a cover of the surface such as to alert the user to a condition requiring a user input. Such an actuation may be configured as an alarm condition or actuation that may be activated by an application.

The aspects described above may be implemented on any of a variety of computing devices example components of which are illustrated in . Example components and modules of an exemplary non limiting aspect of a computing device is illustrated in . A computing device may include circuit board of electronic components e.g. a motherboard some or all of which may be integrated into an on chip system that includes a control processor coupled to memory . The control processor may further be coupled to a digital signal processor and or an analog signal processor which also be coupled together. In some aspects the control processor and a digital signal processor may be the same component or may be integrated into the same processor chip. A display controller and a touchscreen controller may be coupled to the control processor and to display or touchscreen display within or connected to the computing device .

In the various aspects the control processor may be coupled to an adaptive user interface device . In some aspects the adaptive user interface device may be integrated within the computing device structure such as a portion of the device exterior surfaces while in others the adaptive user interface device may be a separate device such as a digital tablet touch pad or machine control interface. In some aspects the adaptive user interface device may include a controller circuit that may function as a control interface between the actuators and sensors of the adaptive user interface device and the control processor . In other aspects the controller circuit may be included within the computing device components such as on the motherboard or even within the control processor . In some aspects the controller circuit may include a processor not shown separately to control operations performed by the adaptive user interface device in response to control signals from the control processor and or in response to user interactions with the device.

Additionally the control processor may also be coupled to removable memory e.g. an SD memory or SIM card in the case of mobile computing devices and or to external memory such as one or more of a disk drive CD drive and a DVD drive. The control processor may also be coupled to a Universal Serial Bus USB controller which couples to a USB port . Also a power supply may be coupled to the circuit board through the USB controller or through different electrical connections to provide power e.g. DC power to the various electronic components.

The control processor may also be coupled to a video encoder e.g. a phase alternating line PAL encoder a sequential couleur a memoire SECAM encoder or a national television system s committee NTSC encoder. Further the video encoder may be coupled to a video amplifier which may be coupled to the video encoder and the display or touchscreen display . Also a video port may be coupled to the video amplifier to enable connecting the computing device to an external monitor television or other display not shown .

In some aspects particularly mobile computing devices the control processor may be coupled to a radio frequency RF transceiver such as via an analog signal processor . The RF transceiver may be coupled to an RF antenna for transmitting and receiving RF signals. The RF transceiver may be configured to transmit and receive communication signals of one or more different wireless communication protocols including for example cellular telephone e.g. G 3 UMTS CDMA etc. WiFi WiMax and BlueTooth.

The control processor may further be coupled to a network card which may be coupled to a network connector and or the RF transceiver and configured to enable communications via an external network e.g. local area networks the Internet an intranet WiFi networks BlueTooth networks personal area network PAN etc. The network card may be in the form of a separate chip or card or may be implemented as part of the control processor or the RF transceiver or both as a full solution communication chip.

A number of analog devices may be coupled to the control processor via the analog signal processor such as a keypad as shown in . In other implementations a keypad or keyboard may include its own processor so that the interface with the control processor may be via direct connection not shown via a network connection e.g. via the network card or via the USB port .

In some implementations a digital camera may be coupled to the control processor . In an exemplary aspect the digital camera may be a charge coupled device CCD camera or a complementary metal oxide semiconductor CMOS camera. The digital camera may be built into the computing device or coupled to the device by an external cable.

In some implementations an audio CODEC e.g. a stereo CODEC may be coupled to the analog signal processor and configured to send sound signals to one or more speakers via an audio amplifier . The audio CODEC may also be coupled to a microphone amplifier which may be coupled to a microphone e.g. via a microphone jack . A headphone jack may also be coupled to the audio CODEC for outputting audio to headphones.

In some implementations the computing device may include a separate RF receiver circuit which may be coupled to an antenna for receiving broadcast wireless communication signals. The receiver circuit may be configured to receive broadcast television signals e.g. FLO TV broadcasts and provide received signals to the DSP for processing. In some implementations the receiver circuit may be configured to receive FM radio signals in which case the received signals may be passed to the Audio CODEC for processing.

In an aspect processor executable instructions for accomplishing one or more of the method operations described above may be stored in the internal memory removable memory and or non volatile memory e.g. as on a hard drive CD drive or other storage accessible via a network . Such processor executable instructions may be executed by the control processor in order to perform the methods described herein.

An example of a multipurpose computer suitable for use with the various aspects is illustrated in . Such a multipurpose computer typically includes a processor coupled to volatile memory and a large capacity nonvolatile memory such as a disk drive . The computer may also include a floppy disc drive and a compact disc CD drive coupled to the processor . In the aspects the computer is coupled to an adaptive user interface device such as described above. Additionally computer may also be coupled to conventional user interface devices such as a keyboard computer mouse and a display . The computer may also include a number of connector ports coupled to the processor for establishing data connections or receiving external memory devices such as a USB or FireWire connector sockets or other network connection circuits for coupling the processor to a network.

An example of a mobile device suitable for use with the various aspects is illustrated in . Such a mobile device may include the components described above with reference to within the case of the device as illustrated. Additionally one or more of the surfaces of the mobile device may be configured as an adaptive user interface device . For example a back portion of the case may include an adaptive user interface device . So configured user inputs may be provided on both a back side adaptive user interface device and a touchscreen display . Additionally or alternatively a normal user interface portion may be configured with an adaptive user interface device such as a portion of the device where a keyboard might otherwise be provided as illustrated. In the example illustrated in an adaptive user interface device may be provided on a portion revealed by opening or sliding the display portion. So configured the mobile device may be opened to reveal an adaptive user interface device that can function either as a touch pad or as a keypad depending upon the operating mode and user input.

The foregoing method descriptions and the process flow diagrams are provided merely as illustrative examples and are not intended to require or imply that the steps of the various aspects must be performed in the order presented. As will be appreciated by one of skill in the art the order of steps in the foregoing aspects may be performed in any order. Words such as thereafter then next etc. are not intended to limit the order of the steps these words are simply used to guide the reader through the description of the methods. Further any reference to claim elements in the singular for example using the articles a an or the is not to be construed as limiting the element to the singular.

The various illustrative logical blocks modules circuits and algorithm steps described in connection with the aspects disclosed herein may be implemented as electronic hardware computer software or combinations of both. To clearly illustrate this interchangeability of hardware and software various illustrative components blocks modules circuits and steps have been described above generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application but such implementation decisions should not be interpreted as causing a departure from the scope of the present invention.

The hardware used to implement the various illustrative logics logical blocks modules and circuits described in connection with the aspects disclosed herein may be implemented or performed with a general purpose processor a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or other programmable logic device discrete gate or transistor logic discrete hardware components or any combination thereof designed to perform the functions described herein. A general purpose processor may be a microprocessor but in the alternative the processor may be any conventional processor controller microcontroller or state machine A processor may also be implemented as a combination of computing devices e.g. a combination of a DSP and a microprocessor a plurality of microprocessors one or more microprocessors in conjunction with a DSP core or any other such configuration. Alternatively some steps or methods may be performed by circuitry that is specific to a given function.

In one or more exemplary aspects the functions described may be implemented in hardware software firmware or any combination thereof. If implemented in software the functions may be stored on or transmitted over as one or more instructions or code on a computer readable medium. The steps of a method or algorithm disclosed herein may be embodied in a processor executable software module executed which may reside on a tangible non transitory computer readable medium or processor readable medium. Non transitory computer readable and processor readable media may be any available media that may be accessed by a computer or processor. By way of example and not limitation such computer readable media may comprise RAM ROM EEPROM CD ROM or other optical disk storage magnetic disk storage or other magnetic storage devices or any other medium that may be used to carry or store desired program code in the form of instructions or data structures and that may be accessed by a computer. Disk and disc as used herein includes compact disc CD laser disc optical disc digital versatile disc DVD floppy disk and blu ray disc where disks usually reproduce data magnetically while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer readable media. Additionally the operations of a method or algorithm may reside as one or any combination or set of codes and or instructions on a non transitory processor readable medium and or computer readable medium which may be incorporated into a computer program product.

The preceding description of the disclosed aspects is provided to enable any person skilled in the art to make or use the present invention. Various modifications to these aspects will be readily apparent to those skilled in the art and the generic principles defined herein may be applied to other aspects without departing from the scope of the invention. Thus the present invention is not intended to be limited to the aspects shown herein but is to be accorded the widest scope consistent with the principles and novel features disclosed herein.

