---

title: Systems and methods for data migration in a clustered file system
abstract: Systems and methods for providing more efficient handling of I/O requests for clustered file system data subject to data migration or the like. For instance, exemplary systems can more quickly determine if certain files on primary storage represent actual file data or stub data for recalling file data from secondary storage. Certain embodiments utilize a driver cache on each cluster node to maintain a record of recently accessed files that represent regular files (as opposed to stubs). A dual-locking process, using both strict locking and relaxed locking, maintains consistency between driver caches on different nodes and the data of the underlying clustered file system, while providing improved access to the data by the different nodes. Moreover, a signaling process can be used, such as with zero-length files, for alerting drivers on different nodes that data migration is to be performed and/or that the driver caches should be flushed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08209307&OS=08209307&RS=08209307
owner: Commvault Systems, Inc.
number: 08209307
owner_city: Oceanport
owner_country: US
publication_date: 20100311
---
This application claims the benefit of priority under 35 U.S.C. 119 e of U.S. Provisional Patent Application No. 61 165 109 filed Mar. 31 2009 which is hereby incorporated herein by reference in its entirety to be considered part of this specification.

Embodiments of the invention relate to data migration and in particular to systems and methods for managing access to primary or migrated data in a clustered file system environment.

Current information management systems employ a number of different methods to perform storage operations on electronic data. For example data can be stored in primary storage as a primary copy or in secondary storage as various types of secondary copies e.g. backup copies archive copies hierarchical storage management HSM copies which are typically intended for long term retention before some or all the data is moved to other storage or discarded.

In certain storage systems when the data of a file is moved from primary to secondary storage the file in primary storage is replaced with a stub file that indicates the new location of the migrated data on secondary storage. In certain examples the stub comprises a relatively small truncated file e.g. several kilobytes having the same name as the original file. The stub file can also include metadata that identifies the file as a stub and that can be used by the storage system to locate and restore the migrated data to primary storage. This stubbing process is generally performed transparently to the user by a storage service and file system driver.

Reading each file following a file system operation e.g. read write rename request to identify if the file is a stub or an actual file can be unduly time consuming. As a result certain stand alone file systems can utilize an index or cache to record whether or not a recently accessed file is a stub. However such a configuration becomes unworkable in a clustered file system as the same file can be independently accessed and modified e.g. migrated by any one of various cluster nodes. That is in the cluster configuration caching of file stub information becomes more cumbersome because a file system driver associated with one node of the cluster does not necessarily control or monitor all I O paths to the stored files data and does not know when a file has been modified or migrated by another node.

In view of the foregoing a need exists for improved systems and methods that manage access to shared data that can be moved within a storage environment such as a clustered file system. For instance there is a need for more efficient systems and methods for handling I O requests for shared file system data after such data has been migrated to secondary storage.

In certain embodiments of the invention systems and methods are disclosed that utilize a driver based inode cache on each cluster node to maintain a record of recently accessed files that represent regular files as opposed to stubs . Certain further embodiments of the invention implement a dual locking process for maintaining consistency between driver caches on different nodes and the data of the underlying clustered file system. In yet further embodiments a signaling process is used by migration systems disclosed herein for alerting file system drivers on different cluster nodes that data migration is to be performed and or that driver caches should be flushed.

In certain embodiments a method is disclosed for coordinating access to data in a storage management system. The method includes storing in a shared file system on a primary storage device a plurality of files comprising a first plurality of regular files and a plurality of stub files the plurality of stub files being associated with a second plurality of regular files having been migrated to secondary storage. The method also includes maintaining in a first driver cache of a first computer a first indication of first inodes associated with at least one of the first plurality of regular files on the primary storage device and maintaining in a second driver cache of a second computer a second indication of second inodes associated with at least one of the first plurality of regular files on the primary storage device. The method further includes requesting with each of the first and second computers a read only lock on a signal file on the shared file system to determine whether or not a migration operation is about to occur with respect to at least one of the first plurality of regular files on the primary storage device. When the request for the read only lock on the signal file is denied of at least one of the first and second computers the method includes clearing the respective first and or second indications from the respective first and or second driver caches of the at least one of the first and second computers. When the request for the read only lock on the signal file is granted to at least one of the first and second computers the method further includes unlocking the signal file and re requesting the read only lock on the signal file after a predetermined period of time.

In certain embodiments a system is disclosed for coordinating access to data in a shared storage environment. The system comprises a first computer including a first memory storing copies of first inodes in a shared file system that represent first data files that have not been migrated from primary storage to secondary storage and a second computer including a second memory storing copies of second inodes in the shared file system that represent second data files that have not been migrated from primary storage to secondary storage the first and second memories not storing copies of inodes that represent stub files. The system also comprises first and second file system drivers executing on respectively said first and second computers each of the first and second file system drivers being configured to request a shared lock on a signal file on the shared file system. When the request for the shared lock on the signal file is denied of at least one of the first and second file system drivers the respective copies of the first and or second inodes are cleared from the respective first and or second memories. When the request for the shared lock on the signal file is granted to at least one of the first and second file system drivers the signal file is unlocked and a request for the shared lock on the signal file is made again after a predetermined period of time.

In certain embodiments a system is disclosed for coordinating access to data in a shared storage environment. The system comprises first means for storing on a first computing device a first indication of first inodes in a shared file system that represent first data files that have not been migrated from primary storage to secondary storage and second means for storing on a second computing device a second indication of second inodes in the shared file system that represent second data files that have not been migrated from primary storage to secondary storage the first and second means for storing not storing indications of inodes that represent stub files. The system also includes third means for requesting a non exclusive lock on a signal file on the shared file system to determine whether or not a migration operation is about to occur on data in the shared file system and fourth means for requesting a non exclusive lock on the signal file on the shared file system to determine whether or not a migration operation is about to occur at least on at least one of the first and second data files. When the request for the non exclusive lock on the signal file is denied of at least one of the third and fourth means for requesting the first and or second indications of the respective first and or second means for storing are cleared. When the request for the non exclusive lock on the signal file is granted to at least one of the third and fourth means for requesting the signal file is unlocked and another request is made with the at least one of the third and fourth requesting means after a predetermined period of time for the non exclusive lock on the signal file.

In certain embodiments a method is disclosed for coordinating access to data in a storage management system. The method comprises maintaining in a first memory of a first computer a first indication of first inodes in a shared file system that contain entire data of one or more first files. The method further includes maintaining in a second memory of a second computer a second indication of second inodes in a shared file system that contain entire data of one or more second files. The method also includes requesting with each of the first and second computers a non blocking lock on a signal file on the shared file system the signal file being indicative of whether or not a migration operation is occurring and or is to occur and when the request for the non blocking lock on the signal file is denied of at least one of the first and second computers clearing the respective first and or second indication from the respective first and or second memory of the at least one of the first and second computers and when the request for the non blocking lock on the signal file is granted to at least one of the first and second computers unlocking the signal file and re requesting the non blocking lock on the signal file after a predetermined period of time.

For purposes of summarizing the disclosure certain aspects advantages and novel features of the inventions have been described herein. It is to be understood that not necessarily all such advantages may be achieved in accordance with any particular embodiment of the invention. Thus the invention may be embodied or carried out in a manner that achieves or optimizes one advantage or group of advantages as taught herein without necessarily achieving other advantages as may be taught or suggested herein.

Systems and methods disclosed herein provide for improved data access management in a shared storage system. In particular certain embodiments of the invention provide for more efficient handling of I O requests for clustered file system data that is subject to data migration or other information life cycle management ILM services. For instance such systems can more quickly determine whether or not certain files on primary storage represent actual file data or stub data for recalling the file data from a secondary storage location.

Certain embodiments of the invention utilize a driver based inode cache on each cluster node to maintain a record of recently accessed files that represent regular files as opposed to stubs on primary storage. A dual locking process using a combination of strict locking and relaxed locking procedures can be implemented for maintaining consistency between driver caches on different nodes and the data of the underlying clustered file system while also providing improved access to the data by the different nodes. Moreover a signaling process can be used such as with zero length files for alerting file system drivers on different cluster nodes that data migration is to and or can be performed and or that the driver caches should be flushed.

The features of the systems and methods will now be described with reference to the drawings summarized above. Throughout the drawings reference numbers are re used to indicate correspondence between referenced elements. The drawings associated descriptions and specific implementation are provided to illustrate embodiments of the invention and not to limit the scope of the disclosure.

In addition methods and functions described herein are not limited to any particular sequence and the blocks or states relating thereto can be performed in other sequences that are appropriate. For example described blocks or states may be performed in an order other than that specifically disclosed or multiple blocks or states may be combined in a single block or state.

As shown the data migration system comprises a shared storage configuration such as a clustered file system environment. For instance the data migration system can comprise a distributed file system in which a plurality of servers transparently provides services to a plurality of clients. In certain embodiments the data migration system comprises a UNIX clustered file system that operates for example according to the Portable Operating System Interface POSIX standard. In certain embodiments the data migration system can comprise a General Parallel File System GPFS a PolyServe file system a common file system CFS combinations of the same or the like.

The illustrated data migration system includes a plurality of client systems or devices that communicate with a primary storage and a secondary storage through a switch . In certain embodiments the client devices can comprise any computing device capable of accessing and or processing data on a storage volume. In certain embodiments the client devices comprise server computers. In yet other embodiments each client device can comprise a workstation a personal computer a cell phone a portable computing device a handheld computing device a personal digital assistant PDA combinations of the same or the like.

The primary storage can include any type of media capable of storing data. For example the primary storage can comprise magnetic storage such as a disk or a tape drive or other type of mass storage. In certain embodiments the primary storage can comprise one or more storage volumes that include physical storage disks defining an overall logical arrangement of storage space. For instance disks within a particular volume may be organized as one or more groups of redundant array of independent or inexpensive disks RAID . In certain embodiments the primary storage can include multiple storage devices of the same or different media. Although the primary storage is illustrated separate from the client devices it will be understood that at least a portion of the primary storage can be internal and or external e.g. remote to the one or more of the client devices .

In certain embodiments data stored on the primary storage is advantageously organized and or cataloged through a file system . In yet further embodiments the client devices of the data migration system can access the data on the primary storage and or secondary storage either directly or through means other than the switch .

As shown each of the client devices comprises one or more applications residing and or executing thereon. In certain embodiments the applications can comprise software applications that interact with a user to process data on primary storage and may include for example database applications e.g. SQL applications word processors spreadsheets financial applications management applications e commerce applications browsers combinations of the same or the like. In particular the applications are able to interface with the file system and data on the primary storage through a corresponding file system driver .

Each of the client devices further comprises a migration service configured to transfer data from primary storage to secondary storage . In certain embodiments the migration service comprises a userspace backup application. For instance the migration service can be configured migrate data from primary storage to secondary storage e.g. tape or magnetic storage based on one or more storage polices criteria defined by the user combinations of the same or the like. In yet other embodiments the migration service is configured to perform one or more of the following copy operations creation storage retrieval auxiliary copies incremental copies differential copies HSM copies archive copies ILM copies or the like.

In certain embodiments when the migration service moves data from primary storage to secondary storage the migration service replaces the original data on the primary storage with a stub file in order to conserve space on the primary storage . This stub as discussed above can comprise metadata that allows for the actual data to be restored from secondary storage when requested for example by the application .

For instance in certain embodiments of the invention components of the data migration system can use Data Management API DMAPI persistent file attributes to store metadata and or mark files as stubs. Other embodiments can store metadata as plain text data surrounded by signatures and or checksums inside the stub file.

In certain embodiments the file system driver is located between the application and the file system and intercepts file system operations such as read write and rename operations from the application . When the file system driver detects that the application wants to read from a file that has been replaced by a stub the file system driver posts a recall request to the migration service and blocks the application from accessing the data until the file data has been restored back from secondary storage . In certain embodiments the migration service advantageously restores the migrated data in a manner that is transparent to the application that requested the data.

Although the migration service has been described with reference to both migrating data from primary storage to secondary storage placing stubs and communicating with the file system driver to restore migrated data it should be understood that multiple modules can be used to perform such functions. For example the data migration system can comprise one or more backup modules that migrate the data and place stubs in the primary storage while one or more restore modules are configured to receive the recall requests when the file system drivers detect that an I O request is directed to a stubbed file. Such restore modules can be configured to obtain the migrated data and restore the data to primary storage .

In certain embodiments reading each requested file to determine if it represents a stub or an actual file data can be a slow and costly approach. Thus as shown the file system driver of the data migration system further comprises a driver cache such as a driver based inode cache. In certain embodiments the driver cache is configured to store information regarding the most recently accessed inodes such as whether or not a particular inode represents an actual file and or a stub.

For instance whenever a file is being accessed or requested the file system driver can first perform a look up of the file s inode in the driver cache . If the inode is present in the driver cache the file system driver can quickly determine that the file is a regular file without needing to directly access the file.

If information about the file is not present in the driver cache the file system driver can then access the file directly to determine if the file is a regular file or a stub file. In certain embodiments the driver reads a portion of the file and parses the metadata to determine if the file is a regular file or a stub file. If the file is a regular file the driver adds the file s inode to the driver cache . In certain further embodiments cache pruning and or adaptive hashing can be used to improve system performance by tracking the locations of the files and or whether or not a particular file is an actual file or a stub file.

However in a clustered file system environment such as the data migration system the information between driver caches can become inconsistent. Because multiple applications on multiple client devices can access the same data on the primary storage the file system driver on an individual client device no longer has exclusive control over the underlying files and may not be aware of file changes made through other file system drivers . Thus in such circumstances it becomes important to prevent other applications from accessing the file when that file is being recalled or migrated e.g. preventing one application from reading a file at the same time another client device is replacing the file with a stub .

In view of the foregoing certain methods are disclosed hereinafter that can be used by the data migration system to further address such races to file data between different nodes in a clustered file system and to maintain driver cache consistency. In particular illustrate flowcharts of exemplary processes usable by the data migration system in managing access to data in a clustered file system.

As illustrated the process begins at Block wherein the file system driver intercepts an I O call e.g. a read or write operation for a particular inode. The file system driver then locks the corresponding file for read only RO or read write RW access Block . In certain embodiments the file system driver utilizes the file system locks available through an fcntl interface.

After the file is locked the file system driver analyzes the contents and or size of the inode Block to determine if the inode contains the actual data file or a stub Block . For instance the file system driver can read a portion such as the first block s of the file to determine if the file is a regular file. In other embodiments the file system driver can determine that the file is a stub if the size of the file is on the order of several kilobytes. In yet other embodiments the file system driver can access other metadata relating to the file e.g. file attributes to determine if the file is a stub.

If the inode does not represent a stub the file system driver forwards the I O call to the file system to perform the requested operation Block . Once the operation is complete the file system driver unlocks the file Block and the process returns to Block .

On the other hand if at Block the inode does represent a stub the file system driver posts a recall or restore request to the migration service to obtain and restore the file data from secondary storage to primary storage Block . At Block the file system driver unlocks the file and waits for the recall process to complete Block and the process returns to Block .

The locking process can be tolerable in environments such as GPFS where file system locking is relatively fast. As can be appreciated however such an indiscriminate locking process can be relatively slow in other file system environments such as in PolyServe file systems or CFS environments where file access speed can be impacted dramatically e.g. up to ten times slower . Moreover the locking process does not enjoy the performance benefits of utilizing the driver cache to identify stubs.

As illustrated at Block the file system driver receives an I O call for a particular inode. The process then determines if the file system driver for the particular client device is operating in a relaxed locking mode Block . In certain embodiments as discussed in more detail below with respect to such a determination can be based on a signaling process that alerts the driver to proceed with the strict locking mode or the relaxed locking mode. In yet other embodiments other types of signaling notifications user selectable options can be used to determine in which mode the driver is to operate.

If the driver is operating in a relaxed locking mode the file system driver determines if the requested inode is present in the driver cache Block . In certain embodiments the driver cache advantageously maintains only information that indicates which inodes are known to be regular files not stubs and is used to expedite access to files on primary storage that have not yet been migrated to secondary storage .

If the requested inode is present in the driver cache the file system driver immediately forwards the I O call to the file system to perform the requested operation Block . The process then returns to Block .

If the file system drivers are not operating in a relaxed locking mode determined at Block or if at Block the requested inode is not present in the driver cache the process commences a strict locking procedure. In particular the strict locking procedure defined by Blocks through of the process generally corresponds to the strict locking process of .

That is at Block the file system driver locks the file for RO or RW access and then the contents and or size of the inode are analyzed Block to determine if the inode contains the actual data file or a stub Block . If the inode does not represent a stub the file system driver adds an entry to the driver cache identifying the inode as having a regular file Block .

The file system driver then forwards the I O call to the file system to perform the requested operation Block . Once the operation is complete the file system driver unlocks the data file Block and the process returns to Block .

On the other hand if at Block the inode does represent a stub the file system driver posts a recall request to the migration service to obtain and restore the file data from secondary storage to primary storage Block . At Block the file system driver unlocks the file and waits for the recall process to complete Block and the process returns to Block .

In certain embodiments of the process the file system driver utilizes byte range locking to avoid interference with locks created by user applications. For instance during one or more blocks of the process instead of locking the actual file access to which is being intercepted by the file system driver the file system driver can lock a corresponding byte range in a dedicated zero length file. In certain embodiments such files are maintained in a directory of a root folder of each shared volume of the primary storage . As one example if a file being accessed has inode number inode num the file system driver can lock byte range inode num inode num 1 in the zero length file.

As can be appreciated the process can significantly increase the speed of processing I O calls by using the driver cache to identify inodes corresponding to regular files since no locking takes place as long as the file s inode is present in the driver cache and the relaxed mode is active. When the information in the driver cache can no longer be trusted e.g. when migration is in progress the process will generally follow the strict locking branch e.g. Blocks .

Moreover when operating with two locking modes it can become important to implement a signaling procedure that notifies the file system drivers on each of the client devices that migration is taking place and or is about to take place. With such signaling the file system drivers can flush their respective driver caches since the migration can cause information in the driver caches to no longer be consistent with the contents of the file system .

The process utilizes two additional files for facilitating the management of data migration information a token file and a signal file. The token file is generally RO locked e.g. a non exclusive or shared lock by file system drivers on each node and as long as the particular driver holds this lock the driver works in the relaxed mode under the assumption that no migration service from any client device will try to perform migration on files in primary storage .

The signal file is used by the migration services to notify the file system drivers on all the client devices that migration is about to begin. In certain embodiments each of the file system drivers has a dedicated thread that periodically attempts to RO lock the signal file. As long as such RO locking is possible the file system driver assumes that there in no migration service currently operating in the data migration system and the file system driver operates in a relaxed locking mode.

However when the thread of the file system driver determines that the signal file is read write RW locked e.g. an exclusive lock the file system driver unlocks the token file and begins to operate in a strict locking mode. In certain embodiments the file system driver also at this time flushes its driver cache because the file system driver can no longer be certain that the driver cache is consistent with the data on the primary storage . In certain embodiments when the file system driver on the final client device detects the locked signal file and unlocks the token file the migration service obtains a RW lock of the token file and begins the migration stubbing process.

In certain embodiments one or both of the token and signal files comprises a zero length file stored in a subdirectory of the root directory of the file system . Working with such zero length files can avoid interference with application level locks that may be imposed by user applications on the same files.

As shown the process begins by the migration service placing a RW lock on the signal file. This action can alert the file system drivers that migration stubbing is about to begin. In certain embodiments as discussed above each of the file system drivers includes a dedicated thread that periodically attempts to RO lock the signal file e.g. approximately every five seconds . Through these periodic lock attempts once a file system driver discovers that the signal file is RW locked the file system driver determines that migration is to begin and releases any RO lock on the token file.

At Block the migration service monitors the token file to determine when all the file system drivers have released their RO locks on the token file Block . In general the duration of this monitoring can take up to the time that is established for the file system drivers to periodically check the signal file e.g. approximately five seconds .

Once all the locks are released from the token file the migration service RW locks the token file Block . At Block the migration service also RW locks a particular data file for writing. This can be especially advantageous in certain embodiments wherein the file system driver does not trust the contents of its driver cache and begins to lock data files during each I O request. RW locking the data file with the migration service can avoid interference with the user I O requests through the file system driver .

At Block the migration service stubs the particular data file. After stubbing the migration service unlocks the data file Block and determines if there are additional files to be stubbed Block . If there are additional files the process returns to Block to lock the additional data file for stubbing.

On the other hand if migration and stubbing are complete the process proceeds with Block and the migration service releases its locks on both the signal and token files. When through the repeated lock attempts of the signal file the file system driver recognizes that migration has complete i.e. there is no longer the RW lock on the signal file the file system drivers again RO lock the token file mark the driver caches as valid and begin to repopulate the driver cache entries as files are re accessed.

As shown at Block the file system driver obtains or maintains a RO lock of the token file. In certain embodiments during this block the file system driver advantageously operates in a relaxed locking mode. Moreover as long as the file system driver holds the RO lock on the token file the migration service is prevented from migrating data files and the driver cache can be assumed to be consistent with the data of the file system .

At Block the file system driver performs a test lock such as a RO lock of the signal file to determine if the migration service is preparing to perform migration. As discussed above in certain embodiments the file system driver includes a dedicated thread that periodically performs this locking attempt.

If at Block the file system driver is able to obtain a lock e.g. RO lock of the signal file the file system driver proceeds to release the lock of the signal file Block . The file system driver then waits a predetermined time period Block and returns to Block to perform another test lock of the signal file. In certain embodiments the predetermined time period is between approximately two and ten seconds. In further embodiments the predetermined time period is approximately five seconds. In yet other embodiments the predetermined time period is less than two seconds or more then ten seconds or may vary based on a storage policy a user defined preference or other criteria.

On the other hand if at Block the file system driver is not able to lock the signal file e.g. the signal file is already RW locked by the migration service the file system driver flushes its driver cache Block and releases its lock on the token file Block . At this point the file system driver operates under a strict locking mode with respect to any I O requests.

The file system driver continues to monitor for when the signal file is unlocked signifying that the migration service has completed the stubbing process Block . As discussed above this monitoring can be performed by the dedicated thread of the file system driver that periodically checks the signal file to see if it is unlocked. If it is determined that the signal file is unlocked the process returns to Block wherein the file system driver again RO locks the token file.

The processes described above with respect to can in certain embodiments provide one or more advantages in managing access to data. For instance the processes and are independent of the number of computer nodes or client devices in the cluster. Such data migration systems do not require a list of active cluster nodes and notifications do not need to be sent to each node regarding migration of data.

The processes can also adequately respond to application crashes. For example if the migration service crashes after causing a cluster to operate in a strict locking mode the signal and token files that were locked by the migration service can automatically be unlocked by the file system . When the file system drivers are alerted to this they can return to operating in the relaxed locking mode.

Moreover if an entire node crashes this does not cause other file system drivers to remain in a strict locking mode. That is the file system can be configured to automatically release all locks held by a crashed node.

The above described processes and methods are also file system and or operating system independent if the file system operates a fcntl locking application programming interface API or an equivalent locking API.

Although data migration systems and methods have been disclosed herein with respect to particular configurations other embodiments of the invention may take on different arrangements. For instance other embodiments of the disclosed data migration systems can implement an alternative method for synchronizing driver cache contents. For instance in certain embodiments each of the driver caches can communicate with each other to achieve consistency between their contents and the underlying clustered file system . Such a communication mechanism can in certain embodiments be implemented inside the file system drivers and or kernel e.g. the file system drivers opening socket connections directly to services running on other cluster nodes . In such embodiments the synchronization communications between the driver caches can be performed substantially simultaneously.

In yet other embodiments flags associated with shared files on the file system could be checked periodically by the file system drivers to determine if migration has occurred. In yet other embodiments the file system drivers could access a shared driver cache accessible to all the drivers that represents the contents of the primary storage.

Moreover in certain embodiments of the invention data migration systems and methods may be used in a modular storage management system embodiments of which are described in more detail in U.S. Pat. No. 7 035 880 issued Apr. 5 2006 which is hereby incorporated herein by reference in its entirety. For example the data migration system may include multiple clients and or media agents for accessing data on a common storage device. Moreover one or more portions of the data migration system may be part of a storage operation cell that includes combinations of hardware and software components directed to performing storage operations on electronic data. Exemplary storage operation cells usable with embodiments of the invention include CommCells as embodied in the SIMPANA QNET and or QINETIX storage management systems by CommVault Systems Inc. Oceanport N.J. and as further described in U.S. Pat. No. 7 454 569 issued Nov. 18 2008 which is hereby incorporated herein by reference in its entirety.

Systems and modules described herein may comprise software firmware hardware or any combination s of software firmware or hardware suitable for the purposes described herein. Software and other modules may reside on servers workstations personal computers computerized tablets PDAs and other devices suitable for the purposes described herein. Software and other modules may be accessible via local memory via a network via a browser or via other means suitable for the purposes described herein. Data structures described herein may comprise computer files variables programming arrays programming structures or any electronic information storage schemes or methods or any combinations thereof suitable for the purposes described herein.

Embodiments of the invention are also described above with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams may be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable memory that can direct a computer or other programmable data processing apparatus to operate in a particular manner such that the instructions stored in the computer readable memory produce an article of manufacture including instruction means which implement the acts specified in the flowchart and or block diagram block or blocks. The computer program instructions may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operations to be performed on the computer or other programmable apparatus to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide steps for implementing the acts specified in the flowchart and or block diagram block or blocks.

While certain embodiments of the inventions have been described these embodiments have been presented by way of example only and are not intended to limit the scope of the disclosure. Indeed the novel methods and systems described herein may be embodied in a variety of other forms furthermore various omissions substitutions and changes in the form of the methods and systems described herein may be made without departing from the spirit of the disclosure. The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of the disclosure.

