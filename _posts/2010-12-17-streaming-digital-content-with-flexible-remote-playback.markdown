---

title: Streaming digital content with flexible remote playback
abstract: Disclosed herein are representative embodiments of methods, apparatus, and systems for facilitating the streaming of digital media content to a remote device. In one exemplary embodiment, a translation layer translates markup language code and/or scripting language code (e.g., code that complies with the HTML5/W3C standard) to comply with a streaming protocol (e.g., a streaming protocol specified by the Digital Living Network Alliance (DLNA)) to facilitate streaming of digital content (e.g., digital video, digital audio, or digital images) to remote devices (e.g., a digital television, digital audio player, game console, etc.). In some embodiments, a translation layer translates streaming protocol events at a remote device into other events (e.g., events specified in the HTML5 standard) at a local computing device. Local/remote playback switching logic can also facilitate switching between local playback and remote playback.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09451319&OS=09451319&RS=09451319
owner: Microsoft Technology Licensing, LLC
number: 09451319
owner_city: Redmond
owner_country: US
publication_date: 20101217
---
As digital content becomes more diverse and readily available on the Internet consumers are looking for more convenient ways to access such content. A modern home typically has several devices e.g. PCs digital televisions game consoles smart phones and other such digital content playback devices that can be used to access edit store or play digital content such as video audio or images. Some devices provide greater convenience and ease of movement. For example smart phones are highly portable and provide a wide variety of functionality. Other devices provide a more desirable user experience for certain kinds of content. For example wide screen digital televisions are well suited for viewing digital video content in a home theater arrangement. Today s consumer wants to leverage the advantages of each of her devices when accessing the wide variety of digital content available on the Internet.

Disclosed herein are representative embodiments of methods apparatus and systems for facilitating streaming of digital content and remote playback of digital content at a remote device. The disclosed methods apparatus and systems should not be construed as limiting in any way. Instead the present disclosure is directed toward all novel and nonobvious features and aspects of the various disclosed embodiments alone and in various combinations and subcombinations with one another. Furthermore any features or aspects of the disclosed embodiments can be used in various combinations and subcombinations with one another. The disclosed methods apparatus and systems are not limited to any specific aspect or feature or combination thereof nor do the disclosed embodiments require that any specific advantage be present or problem be solved.

In examples described herein code received at a local computing device can be translated to comply with a streaming protocol in order to facilitate the processing of content associated with the code by a remote playback device. For example a translation layer on a PC that receives markup language code and or scripting language code e.g. code that complies with the HTML5 W3C standard from a remote server e.g. via a web browser can automatically convert such code into corresponding calls in a streaming protocol e.g. a streaming protocol specified by the Digital Living Network Alliance DLNA to facilitate streaming of digital content e.g. digital video digital audio or digital images to a wide range of remote devices e.g. a digital television digital audio player game console etc. . Translations also can be performed in the other direction. For example a translation layer at a local computing device can automatically translate streaming protocol events from a remote device into other events e.g. events specified in the HTML5 standard at a local computing device. Local remote playback switching logic can provide flexible remote playback by facilitating switching between local playback of content on a local computing device and remote playback at another device. Described translation and playback switching technology can be used for example to allow users to combine the convenience of web browsing on a local computing device e.g. a laptop computer with the viewing experience provided by other devices e.g. a large screen TV .

The foregoing and other objects features and advantages of the invention will become more apparent from the following detailed description which proceeds with reference to the accompanying figures.

Disclosed herein are representative embodiments of methods apparatus and systems for facilitating streaming of digital content and remote playback of digital content at a remote device. The disclosed methods apparatus and systems should not be construed as limiting in any way. Instead the present disclosure is directed toward all novel and nonobvious features and aspects of the various disclosed embodiments alone and in various combinations and subcombinations with one another. Furthermore any features or aspects of the disclosed embodiments can be used in various combinations and subcombinations with one another. The disclosed methods apparatus and systems are not limited to any specific aspect or feature or combination thereof nor do the disclosed embodiments require that any one or more specific advantages be present or problems be solved.

Although the operations of some of the disclosed methods are described in a particular sequential order for convenient presentation it should be understood that this manner of description encompasses rearrangement unless a particular ordering is required by specific language set forth below. For example operations described sequentially may in some cases be rearranged or performed concurrently. Moreover for the sake of simplicity the attached figures may not show the various ways in which the disclosed methods apparatus and systems can be used in conjunction with other methods apparatus and systems.

The disclosed methods can be implemented using computer executable instructions stored on one or more computer readable media e.g. non transitory computer readable media such as one or more optical media discs volatile memory components e.g. DRAM or SRAM or nonvolatile memory or storage components e.g. hard drives and executed on a computer e.g. any commercially available computer or a computer or image processor embedded in a device such as a laptop computer desktop computer net book web book tablet computing device smart phone or other mobile computing device . Any of the intermediate or final data created and used during implementation of the disclosed methods or systems can also be stored on one or more computer readable media e.g. non transitory computer readable media .

For clarity only certain selected aspects of the software based embodiments are described. Other details that are well known in the art are omitted. For example it should be understood that the software based embodiments are not limited to any specific computer language or program. Likewise embodiments of the disclosed technology are not limited to any particular computer or type of hardware. Exemplary computing environments suitable for performing any of the disclosed software based methods are introduced below.

The disclosed methods can also be implemented using specialized computing hardware that is configured to perform any of the disclosed methods. For example the disclosed methods can be implemented by an integrated circuit e.g. an application specific integrated circuit ASIC a graphics processing unit GPU or programmable logic device PLD such as a field programmable gate array FPGA specially designed to implement any of the disclosed methods e.g. dedicated hardware configured to perform any of the disclosed translations .

In examples described herein code received at a local computing device can be translated to comply with a streaming protocol in order to facilitate processing of content at a remote device. Translation can include converting code into a different kind of code. For example markup language code or scripting language code can be converted into translated code that complies with a streaming protocol. Translation also can include other interpretations of code. For example different kinds of processing and data handling e.g. in compliance with a streaming protocol can occur e.g. at a remote device in response to received markup language code or scripting language code. Translation can also be used as part of the process of streaming content and or related information to a remote device e.g. a digital television digital audio player game console or other digital content playback device .

For example a translation layer on a PC that receives markup language code and or scripting language code e.g. code that complies with the HTML5 W3C standard from a remote server via a web browser can automatically convert such code into corresponding calls in a streaming protocol e.g. a streaming protocol specified by the Digital Living Network Alliance DLNA in order to stream digital content e.g. digital video digital audio or digital images to a wide range of remote devices e.g. a digital television digital audio player game console or other digital content playback device . A translation layer also can perform translations in the other direction. For example a translation layer at a local computing device can automatically convert streaming protocol events received from a remote device into other events e.g. events specified in the HTML5 standard .

As further described herein local remote playback switching logic at a local computing device can be used to switch between local playback of acquired content on the local computing device and remote playback at another device. Described translation and playback switching technology can be used for example to allow users to combine the convenience of web browsing on a local computing device e.g. a laptop computer with the viewing experience provided by other devices e.g. a large screen TV .

As applied to devices described herein the term remote is used to refer to devices other than a local computing device. Remote devices can be accessible by a local computing device over the Internet a wide area network a local network e.g. an Ethernet network Wi Fi network or other network covering a small geographic area such as a home or office or other some other network. As applied to playback or other processing described herein the term remote is used to refer to the playback or other processing at a remote device.

In the example shown in computing device receives input . Input comprises information representing digital content such as digital video digital audio or digital images. Input also can include other types of input such as user input. For example computing device can accept user input that indicates a user s selection of local playback or remote playback of the digital content.

Local remote playback switching logic allows the system to select local playback at computing device or remote playback at a remote device. For example in response to user input the system can use local remote playback switching logic to switch between local playback using local playback logic or remote playback at a remote device e.g. a remote DLNA compliant device such as a digital television game console or other digital content playback device. Local remote playback switching logic can be implemented in different ways. For example local remote playback switching logic can be included in a software element at computing device . In a described example local remote playback switching logic is included in a software element that represents digital content for playback e.g. local playback or remote playback . A software element that represents media data e.g. video data and or audio data for playback can be referred to as a media element. A software element that represents image data for playback can be referred to as an image element. Local playback logic and playback logic at remote devices not shown can include for example video audio or image decoders that decode encoded video audio or image information respectively.

In translation layer translation is performed. For example translation layer can automatically translate markup language code and or scripting language code that complies with the HTML5 W3C standard into translated code that complies with a streaming protocol e.g. a streaming protocol specified by DLNA . The translated code can then be sent via remote playback byte stream to another device such as a digital television game console or other digital content playback device for remote playback.

In practice the systems shown herein such as system can include additional system components additional relationships between system components and the like. For example the system can include one or more transcoders that transcode video data audio data or image data. Transcoders can be used for example to convert media data to a different coded format when the media data is received in a coded format that is not supported by a remote device. The exact operations performed by such transcoders can vary depending on input and output compression formats.

The relationships shown between modules within the system indicate general flows of information in the system other relationships are not shown for the sake of simplicity. Depending on the implementation and the type of processing desired modules of the system can be added omitted split into multiple modules combined with other modules and or replaced with like modules. Generally the technologies described herein are generic to different operating systems or hardware and can be applied in any variety of environments to take advantage of the described features.

At the system receives markup language code or scripting language code e.g. code that complies with HTML5 associated with digital content. For example a user of a local computing device navigates to a web page via a web browser and the local computing device receives markup language code and or scripting language code via the web browser.

At the system parses the received markup language code or scripting language code to obtain information representing an interface call associated with an element representing digital content. In described examples elements representing digital content can include software elements e.g. media elements that represent digital media content or image elements that represent digital still image content that implement interfaces. For example markup language code can include an interface call to a media element e.g. a element or an element that implements a media element interface e.g. an HTMLMediaElement interface or an image element e.g. an element that implements an image element interface e.g. an HTMLImageElement interface .

At the system automatically translates the interface call into translated code that complies with a streaming protocol. For example a translation layer at the local computing device automatically translates a call to an HTMLMediaElement interface or a call to an HTMLImageElement interface into translated code that complies with a streaming protocol e.g. a UPnP streaming protocol specified by DLNA . The translation layer can be implemented in a software element e.g. a media element . As described herein translations involve receiving code to be translated and interpreting the received code in a prescribed manner. Exemplary translations for an HTMLMediaElement interface are shown in Tables 4 7 below. Exemplary translations for an HTMLImageElement interface are shown in Table 9 below. Exemplary translations described herein can be performed in different ways e.g. by code in a media element or an image element that includes a translation layer by code in software that is separate from a media element or an image element by referring to a table or other data structure in which translation information is stored or in some other way . Alternatively the system translates interface calls differently or translates other code.

At the system sends information representing the translated code to a remote device operable to render the digital content e.g. via a local network . For example the local computing device sends information representing calls to an action of a UPnP AVTransport service via a byte stream to a remote device that is operable to render the digital content. Alternatively the system translates interface calls differently or translates other code.

At the system receives markup language code or scripting language code associated with digital media content from outside a local network. For example a user of a local computing device on a local network navigates to a web page on the Internet via a web browser and the local computing device receives markup language code and or scripting language code via the web browser.

At the system parses the received markup language code or scripting language code to obtain information representing an interface call associated with a media element representing the digital media content. For example markup language code can include an interface call to a element or element that implements an HTMLMediaElement interface.

At the system automatically translates the interface call into translated code that complies with a streaming protocol. For example a translation layer at the local computing device automatically translates a call to an HTMLMediaElement interface into translated code that complies with a streaming protocol e.g. a UPnP streaming protocol specified by DLNA . Exemplary translations for an HTMLMediaElement interface are shown in Tables 4 7 below. Exemplary translations described herein can be implemented in different ways such as by being implemented in code in a software element that includes a translation layer. Alternatively the system translates interface calls differently or translates other code.

At the system receives information indicating a streaming protocol event associated with the digital media content. For example a local computing device receives information indicating a streaming protocol event such as a pause play stop playback rate change or error event from an audio visual transport service e.g. a UPnP AVTransport service or a change event e.g. a volume change event from a rendering control service e.g. a UPnP RenderingControl service . The streaming protocol event can be generated in response to user input e.g. video playback control input at a remote device that renders the digital media content.

At the system automatically translates information indicating the streaming protocol event into a markup language event associated with the media element. For example a translation layer at the local computing device automatically translates a streaming protocol event to a corresponding HTML5 event associated with the media element. Exemplary translations for streaming protocol events are shown in Table 8 below. Exemplary translations described herein can be implemented in different ways such as by being implemented in code in a software element that includes a translation layer. Alternatively the system translates streaming protocol events differently or translates other streaming protocol events.

In examples described herein markup language code and or scripting language code can be used to deliver content to a local computing device. For example markup language and or scripting language code is provided by a remote server and processed by a web browser on a local computing device.

Some examples of markup language code and scripting language code described herein comply with HTML5. HTML5 is a revision of the HTML standard. However exemplary technology described herein does not require compliance with any particular guideline or standard. For example modified versions of exemplary interfaces attributes methods or events described herein or different interfaces attributes methods or events can be used.

HTML5 includes several types of elements including the element for video content the element for audio content and the element for still image content.

The and elements are examples of media elements. Media elements are used to represent media data e.g. video data audio data . A complete audio file or video file that includes media data can be referred to as a media resource. Media data represented by a media element can be identified by an address e.g. a valid URL of a media resource e.g. an MPEG 4 video file an MP3 audio file or some other media resource in an attribute of the media element e.g. a src attribute as shown in Table 1 below .

In HTML5 media elements implement the HTMLMediaElement interface. The HTMLMediaElement interface exposes methods and attributes relating to different states and aspects of the media element. In HTML5 the HTMLMediaElement interface includes the methods and attributes shown in Table 1 below.

In HTML5 and elements also implement an HTMLVideoElement and HTMLAudioElement interface respectively. The HTMLVideoElement interface derives from the HTMLMediaElement interface while adding some methods and properties. The HTMLAudioElement interface also derives from the HTMLMediaElement interface without adding methods or properties. The HTMLMediaElement interface derives from the HTMLElement interface. The HTMLSourceElement interface also relates to and elements and derives from the HTMLElement interface. For brevity the HTMLElement HTMLVideoElement HTMLAudioElement and HTMLSourceElement interfaces are not described in detail herein. For further information on these and other interfaces see the HTML5 standard.

HTML5 also defines several events that relate to media elements such as and elements. In HTML5 events occur or fire on media elements as shown in Table 2 below.

The element implements the HTMLImageElement interface. The HTMLImageElement interface exposes attributes relating to an image resource. In HTML5 the HTMLImageElement interface includes the attributes shown in Table 3 below.

The HTMLImageElement interface derives from the HTMLElement interface. For brevity the HTMLElement interface is not described in detail herein. For further information on these and other interfaces see the HTML5 standard.

In examples described herein devices comply with device models specified by the DLNA. However exemplary technology described herein does not require compliance with any particular guideline or standard.

DLNA interoperability guidelines use a device model having several device classes that can be divided into one or more device categories. Each device class includes a set of device functions. The set of device functions is not tied to any particular type of device devices having different physical attributes e.g. different form factors may possess the same set of device functions. Under the DLNA interoperability guidelines a DLNA compliant device supports at least one device class. A single DLNA compliant device can support more than one device class.

One category of device classes is home network devices HND . Some examples of FIND device classes are digital media server DMS digital media player DMP digital media renderer DMR and digital media controller DMC . The DMS class is for devices that expose and distribute digital media content to other devices. Typical examples of DMS devices include PCs DVRs and smartphones. The DMP class is for devices that find and render or play content on a digital media server. Typical examples of DMP devices include digital televisions game consoles and smartphones. The DMC class is for devices that find digital media content and match them with appropriate media players or renderers. The DMR class is for devices that render or play digital media content but do not independently find content on the network. DMC devices can be used to find content to be rendered or played on DMR devices.

Another category of device classes is mobile handheld devices MHD . Some examples of MHD device classes are mobile digital media server M DMS mobile digital media player M DMP and mobile digital media controller M DMC . These classes share usage models with their HND counterparts DMS DMP and DMC respectively but have different media format and network requirements. Other MHD classes include mobile digital media uploader M DMU and mobile digital media downloader M DMD . M DMU devices can upload digital media to M DMS devices. M DMD devices can download digital media from M DMS devices.

The device model specified in the DLNA interoperability guidelines is based on the Universal Plug and Play UPnP AV architecture specification. The UPnP AV architecture specification defines interaction between UPnP control points and UPnP AV devices. shows an arrangement according to a UPnP AV architecture. Control point typically includes a user interface and coordinates interaction between digital media server DMS and digital media renderer DMR . Control point can control e.g. through user interaction with a user interface flow of media resource information between DMS and DMR using a UPnP AVTransport service. In the example shown in DMS also communicates directly with DMR . For example DMS transfers media resource information to DMR using a media format and a transfer protocol e.g. an isochronous push transfer protocol such as IEC61883 IEEE1394 for real time transfer or an asynchronous pull transfer protocol such as HTTP GET supported by DMS and DMR . DMS and or DMR can send event notifications e.g. event notifications relating to the state of the DMR as it plays the media resource back to control point .

Control point DMS and DMR can each be embodied in different devices e.g. a remote control DVR and digital television respectively or two or more of the control points DMSs and DMRs can be embodied in the same device. For example a PC with a graphical user interface GUI can include control point and DMS . A user can interact with the GUI on the PC to select digital video to be played and to select a digital television that acts as the DMR to play the selected video. Alternative arrangements also are possible. For example arrangement can include multiple control points multiple digital media servers and or multiple digital media renderers or other types of components.

DLNA interoperability guidelines draw on industry standards developed and managed by other organizations. For example DLNA interoperability guidelines require DLNA compliant devices to have a network connection e.g. Ethernet BlueTooth WiFi that uses TCP IP which is maintained by the Internet Engineering Task Force IETF . For streaming of media content DLNA interoperability guidelines specify a protocol maintained by the Universal Plug and Play UPnP Forum. DLNA devices typically handle some standardized media formats e.g. JPEG MPEG 2 by default while other digital media formats are optional.

In examples described herein an exemplary protocol that can be used to stream media content to devices on a local network is specified by UPnP and used by the DLNA. However exemplary technology described herein does not require compliance with any particular guideline or standard. For example modified versions of exemplary services state variables actions or events described herein or different services state variables actions or events can be used.

In the example shown in DMS includes several UPnP services ContentDirectory ConnectionManager and AVTransport which is optional . The ContentDirectory service allows control point to select a media resource to be played on DMR . The ConnectionManager service in the context of DMS allows DMS to prepare and or terminate transfer of digital media to DMR such as by using the PrepareForConnection and ConnectionComplete actions respectively although actions such as PrepareForConnection need not be used in exemplary technologies described herein. The AVTransport service allows control point to select the desired media resource to be transferred to DMR e.g. by using the SetAVTransportURI action . The AVTransport service also allows control point to control playback of the media resource e.g. by using Play Stop Pause Seek or other actions . DMR also includes several UPnP services RenderingControl ConnectionManager and AVTransport. DMR can include multiple instances of these services e.g. to support rendering of more than one media resource at the same time . The ConnectionManager service in the context of DMR allows DMR to enumerate supported transfer protocols and media formats using the GetProtocolInfo action. The ConnectionManager service also allows DMR to prepare for receiving media resource information from DMS and or terminate a connection such as by using the PrepareForConnection and ConnectionComplete actions respectively although actions such as PrepareForConnection need not be used in exemplary technologies described herein. The RenderingControl service allows control point to control the rendering of a media resource at DMR . Alternative arrangements also are possible. For example DMS and or DMR can include different combinations of services such as a combination that omits the AVTransport service from DMS .

This section provides further details of the AVTransport service including AVTransport state variables and AVTransport actions.

The TransportState variable is a string that indicates whether the media resource associated with the AVTransport instance is playing stopped etc. Exemplary values for TransportState include STOPPED PLAYING TRANSITIONING PAUSED PLAYBACK PAUSED RECORDING RECORDING and NO MEDIA PRESENT.

The TransportStatus variable is a string that indicates whether asynchronous errors e.g. network congestion server errors etc. have occurred during operation of the AVTransport service. Exemplary values for TransportStatus include ERROR OCCURRED and OK.

The TransportPlaySpeed variable is a string representation of a fraction that indicates playback speed relative to normal speed. Exemplary values for TransportPlaySpeed include 1 normal speed half of normal speed etc.

The AVTransportURI variable is a uniform resource identifier URI of the media resource that corresponds to the AVTransport instance. AVTransportURI also allows a control point to obtain metadata for the media resource.

The LastChange variable is a string that allows receipt of event notifications when the state of the AVTransport instance changes. LastChange contains a list of pairs e.g. that indicate the respective state changes.

Other AVTransport state variables include PlaybackStorageMedium RecordStorageMedium PossiblePlaybackStorageMedia PossibleRecordStorageMedia CurrentPlayMode TransportPlaySpeed RecordMediumWriteStatus CurrentRecordQualityMode PossibleRecordQualityModes NumberOfTracks CurrentTrack CurrentTrackDuration CurrentMediaDuration CurrentTrackMetaData CurrentTrackURI AVTransportURIMetaData NextAVTransportURI NextAVTransportURIMetaData RelativeTimePosition AbsoluteTimePosition RelativeCounterPosition AbsoluteCounterPosition CurrentTransportActions A ARG TYPE SeekMode A ARG TYPE SeekTarget and A ARG TYPE InstanceID.

The SetAVTransportURI action specifies the URI of a media resource e.g. a video resource corresponding to the AVTransport instance. Input arguments for SetAVTransportURI include InstanceID which corresponds to A ARG TYPE InstanceID CurrentURI which corresponds to AVTransportURI and CurrentURIMetaData which corresponds to AVTransportURIMetaData . SetAVTransportURI changes TransportState to STOPPED if the media resource cannot be located at the specified URI or if the current value of TransportState is NO MEDIA PRESENT. If TransportState is PLAYING SetAVTransportURI may also change TransportState to TRANSITIONING such as where buffering is occurring before actual playback begins before returning TransportState to PLAYING.

The GetPositionInfo action returns information that describes the current position of a media resource e.g. track number track duration etc. corresponding to the AVTransport instance. The input argument for GetPositionInfo is InstanceID which corresponds to A ARG TYPE InstanceID . Output arguments include Track which corresponds to CurrentTrack TrackDuration which corresponds to CurrentTrackDuration TrackMetaData which corresponds to CurrentTrackMetaData TrackURI which corresponds to CurrentTrackURI RelTime which corresponds to RelativeTimePosition AbsTime which corresponds to AbsoluteTimePosition RelCount which corresponds to RelativeCounterPosition and AbsCount which corresponds to AbsoluteCounterPosition .

The Stop action stops playback of a current media resource corresponding to the AVTransport instance. The input argument for Stop is InstanceID which corresponds to A ARG TYPE InstanceID . Stop changes TransportState to STOPPED unless the current value of TransportState is NO MEDIA PRESENT. Stop may also cause changes in the current position of the media resource which can be discovered using the GetPositionInfo action .

The Play action starts playback of a current media resource e.g. at a specified speed and starting position according to a current play mode corresponding to an AVTransport instance. The input arguments for Play are InstanceID which corresponds to A ARG TYPE InstanceID and Speed which corresponds to TransportPlaySpeed . Play changes TransportState to PLAYING and updates TransportPlaySpeed e.g. normal speed in forward direction . Play may also change TransportState to TRANSITIONING such as where buffering is occurring before actual playback begins.

The Pause action pauses playback of a current media resource corresponding to an AVTransport instance. The input argument for Pause is InstanceID which corresponds to A ARG TYPE InstanceID . Pause changes TransportState to PAUSED PLAYBACK if TransportState is PLAYING or to PAUSED RECORDING if TransportState is RECORDING when the action is performed. Pause causes the media resource to remain at its current position.

The Seek action moves the current position of a current media resource corresponding to an AVTransport instance to a target position. Input arguments for Seek include InstanceID which corresponds to A ARG TYPE InstanceID Unit which corresponds to A ARG TYPE SeekMode and Target which corresponds to A ARG TYPE SeekTarget . Seek temporarily changes TransportState to TRANSITIONING if TransportState is PLAYING or STOPPED when the action is performed before returning to the previous state when the new position is reached.

Other AVTransport actions include SetNextAVTransportURI GetMediaInfo GetTransportInfo GetDeviceCapabilities GetTransportSettings Record Next Previous SetPlayMode SetRecordQualityMode and GetCurrentTransportActions .

This section provides further details of the ConnectionManager service including ConnectionManager state variables and ConnectionManager actions.

ConnectionManager state variables include SourceProtocolInfo SinkProtocolInfo CurrentConnectionIDs A ARG TYPE ConnectionStatus A ARG TYPE ConnectionManager A ARG TYPE Direction A ARG TYPE ProtocolInfo A ARG TYPE ConnectionID A ARG TYPE AVTransportID and A ARG TYPE ResID.

The GetProtocolInfo action returns protocol related information for protocols supported by an instance of ConnectionManager. Output arguments for GetProtocolInfo include Source which corresponds to SourceProtocolInfo and Sink which corresponds to SinkProtocolInfo .

Other ConnectionManager actions include PrepareForConnection ConnectionComplete GetCurrentConnectionIDs and GetCurrentConnectionInfo .

This section provides further details of the RenderingControl service including RenderingControl state variables and RenderingControl actions.

The LastChange variable is a string that conforms to an XML schema and allows receipt of event notifications when the state of the device as indicated by the state of a RenderingControl instance changes. The Mute variable is a Boolean value that represents the current mute setting of an associated audio channel with TRUE indicating that the channel has been muted . The Volume variable is an unsigned integer value that represents a current volume level with 0 representing silence of an associated audio channel.

Other RenderingControl state variables include PresetNameList Brightness Contrast Sharpness RedVideoGain GreenVideoGain BlueVideoGain RedVideoBlackLevel GreenVideoBlackLevel BlueVideoBlackLevel ColorTemperature HorizontalKeystone VerticalKeystone VolumeDB Loudness A ARG TYPE Channel A ARG TYPE InstanceID and A ARG TYPE PresetName.

The SetMute action sets the Mute state variable of a RenderingControl instance and audio channel. Input arguments include InstanceID which relates to A ARG TYPE InstanceID Channel which relates to A ARG TYPE Channel and DesiredMute which relates to the Mute state variable .

The SetVolume action sets the Volume state variable of a corresponding RenderingControl instance and audio channel. Input arguments include InstanceID which relates to A ARG TYPE InstanceID Channel which relates to A ARG TYPE Channel and DesiredVolume which relates to the Volume state variable .

Other RenderingControl actions include ListPresets SelectPreset GetMute GetVolume GetVolumeDB SetVolumeDB GetVolumeDBRange GetLoudness SetLoudness and Get and Set actions for display related state variables e.g. Brightness Contrast Sharpness RedVideoGain etc. .

Digital video described herein can be represented in a variety of formats e.g. MPEG 2 MPEG 4 H.264 AVC VC 1 and or other formats for raw uncompressed video data or compressed video data. Some video formats are specified by international standards. For example the VC 1 standard sets forth requirements for decoders to decode video encoded in a VC 1 format. A VC 1 compliant encoder and decoder codec can typically provide high quality video with good compression efficiency. Described techniques and tools can handle standard definition video or high definition video 2 D or 3 D video etc.

Digital audio described herein can be represented in a variety of file formats e.g. WAY MP3 AAC WMA and or other formats for raw uncompressed audio data or compressed audio data. Some audio formats are specified by international standards. For example the WMA standards set forth requirements for decoders to decode video encoded in a WMA format e.g. WMA WMA Pro WMA Lossless etc. A WMA compliant encoder and decoder codec can typically provide high quality audio with good compression efficiency. Described techniques and tools can handle audio having different sample rates channel configurations etc.

Digital images described herein can be color grey scale or other types of images and can be represented in a variety of file formats e.g. GIF PNG BMP TIFF TIFF Float32 JPEG JPEG XR and or other formats for raw uncompressed image data or compressed image data. For example described techniques and tools can handle standard dynamic range SDR images in an SDR format such as JPEG or high dynamic range HDR images in an HDR format such as PEG XR. Some image formats are specified by international standards. For example the JPEG XR standard sets forth requirements for decoders to decode images encoded in JPEG XR format. A JPEG XR compliant encoder and decoder codec can typically provide high quality images with good compression efficiency. The Exchangeable Image File EXIF format specifies a structure for image files. Image data in an EXIF file can be compressed e.g. in JPEG format . Alternatively image data in an EXIF file can be uncompressed e.g. in TIFF format .

This section describes detailed examples of technology described herein. Described examples implement features such as translation and local remote playback switching. Described examples can allow playback of digital content at remote devices while shielding content providers from the intricacies of described streaming protocols. For web developers content providers and others allowing remote playback of content can be as simple as inserting a or tag in HTML code when performed in accordance with technologies described herein. Technologies described herein can be used in combination with customizations such as scripts e.g. JavaScript scripts and cascading style sheets CSS .

This section describes detailed examples of media elements e.g. audio and video elements and related features interfaces and arrangements.

Media element receives code . Code can include a script that is not aware of the potential for remote playback of a media resource. In order for a transition between local and remote playback to be transparent to scripts not aware of remote playback an extra level of indirection can be added between the script and the components implementing playback. Accordingly media element has a local remote playback switch between local playback component and remote playback component . Local playback component and remote playback component can implement a common interface. Using a common interface for both local and remote playback allows smooth transitions from local playback to remote playback and vice versa. Playback state e.g. current position playback speed volume etc. can be preserved in such a transition. In the example shown in only one component is active at a time. However when a switch happens the playback state of the element can be preserved by reading attributes from the current playback component and setting them on the new playback component. For example the following attributes from the HTMLMediaElement interface can be read on a local playback component and then set on a remote playback component preload defaultPlaybackRate playbackRate autoplay loop muted volume currentTime paused and src. Source elements representing the media resources also can be transferred from one component to the other during a switch.

Media element can perform device selection e.g. when more than one device is available for remote playback based on information that describes remote devices available for remote playback. For example remote playback component can provide a list of remote devices and or other data structures to manage selection of and playback to remote device s .

Scripts which are aware of remote playback capabilities can subscribe to custom device connection and disconnection events e.g. msConnected and msDisconnected events to be notified of switching between local and remote playback. Media element can be for example a feature of the Trident rendering engine functionality for Microsoft Internet Explorer that is available via the MSHTML.dll dynamic link library.

Alternatively another arrangement for media elements can be used. For example components that act as translation layers or local remote switches can be implemented outside media elements.

A media resource can be provided by a server and indicated by a single URI or a set of URIs stored as source elements. During loading a byte stream which can be referred to as an original byte stream can be opened between a local computing device and the server providing the media resource. Opening the original byte stream on the local computing device can have advantages such as providing accurate values for time ranges e.g. via the buffered attribute of an HTMLMediaElement interface as shown in Table 4 below . A streaming protocol feature allows URIs to be sent directly to remote devices but without supporting transfer of HTTP headers to the remote device and in described examples this feature is not relied upon for streaming content.

The original byte stream can be parsed in part to determine codec parameters and extract metadata for the media resource. Based on this information and a call to ConnectionManager GetProtocolInfo a media element can determine whether encoded media data in the byte stream can be decoded by a remote device or if transcoding is needed. In either case another byte stream which can be referred to as a proxied byte stream can be opened between the local computing device and the remote device. The proxied byte stream can be created for example by opening a port on the local computing device and calling AVTransport SetAVTransportURI on the remote device with the URI of the local port and the metadata of the proxied byte stream.

When a remote connection is started the media element can subscribe to change events e.g. LastChange events fired by AVTransport and RenderingControl services provided by the remote device. When the media element is shut down it can unsubscribe from these services and call AVTransport Stop on the remote device to end playback.

Alternatively other arrangements and techniques for media element loading startup and shutdown can be used.

For media elements e.g. audio elements or video elements exemplary translations relating to the HTMLMediaElement interface are shown in Tables 4 7 below. In Tables 4 7 some methods or attributes of the HTMLMediaElement interface are not translated indicated by n a . Some translations in Tables 4 7 involve translated code e.g. a call to the src attribute leads to a call to AVTransport SetAVTransportURI while other translations involve other interpretations e.g. a call to the buffered attribute is interpreted as measuring an amount of data buffered in an original byte stream rather than a proxied byte stream . Calls to interfaces such as HTMLVideoElement HTMLElement and HTMLSourceElement do not directly affect the streaming protocol and need not be translated. Alternatively different translations more or fewer translations or different combinations of translations can be used.

Table 7 below shows two other attributes from the HTMLMediaElement interface the readyState attribute and the seeking attribute. In the example shown in Table 7 no specific translations are used that relate to the readyState attribute or the seeking attribute.

During playback at a remote device a user can perform actions to control playback. A user may for example use a digital television s remote control to stop playback on the digital television which can generate one or more streaming protocol events. Streaming protocol events can be translated into scripting events and or markup language events. Such translations can help to keep the state of a local media player and a remote device consistent.

Exemplary translations for streaming protocol events are shown in Table 8 below. Alternatively different translations more or fewer translations or different combinations of translations can be used.

This section describes detailed examples of image elements and related features interfaces and arrangements.

Local playback component can cause an image resource to be displayed at a local display device at the same time that remote playback component is causing the image resource to be displayed at a remote device. Alternatively the image resource can be displayed at one location at a time or at more than two locations. Transfer of playback state between local playback component and remote playback component is not needed. Image element receives code . Code can include a script that may or may not be aware of the potential for remote playback. A transition between local playback and remote playback can be transparent to a caller although custom device connection and disconnection events e.g. msConnected and msDisconnected events can still be fired to indicate when connections are made and terminated.

Alternatively another arrangement for image elements can be used. For example components that act as translation layers can be implemented outside image elements.

For image elements the loading process is similar to the loading process described for audio elements and video elements above. An image resource can be provided by a server and indicated by a single URI or a set of URIs. During loading a byte stream which can be referred to as an original byte stream can be opened between a local computing device and a server providing the image resource. The original byte stream can be parsed in part to determine codec parameters and extract metadata. Based on this information and a call to ConnectionManager GetProtocolInfo the image element can determine whether encoded data in the byte stream can be decoded by a remote device or if transcoding is needed. In either case another byte stream which can be referred to as a proxied byte stream can be opened between the local computing device and the remote device. The proxied byte stream can be created by opening a port on the local machine and calling AVTransport SetAVTransportURI on the remote device with the URI of the local port and the metadata of the proxied bytestream.

Because images do not exhibit dynamic playback behavior like video and audio on startup image elements can omit listening for LastChange events and can omit calling actions such as AVTransport Stop e.g. during shutdown on the remote device.

For image elements exemplary translations relating to the HTMLImageElement interface are shown in Table 9 below. In the examples shown in Table 9 no specific translations are used indicated by n a in the table for several attributes of the HTMLImageElement interface. In described examples calls to interfaces that do not directly affect the streaming protocol such as calls to HTMLElement are not translated and streaming protocol events are also not translated for image elements. Alternatively different translations more or fewer translations or different combinations of translations can be used.

This section describes exemplary extensions that can be implemented on image audio and video elements. Such extensions can allow scripts to control connections of such elements to remote devices and to obtain information about such connections. For example extensions that comply with the HTML5 standard are described that can provide more control over playback experience. Events e.g. msConnected and msDisconnected events sent to servers that provide web pages can indicate switches between local and remote playback. Methods e.g. JavaScript methods can be used to connect to and disconnect from remote devices test whether playback is local or remote and transfer connections between audio and video elements to allow pre buffering and faster transitions between pieces of media. Faster transitions can allow smoother operation of features such as playlists and smoother transitions between media segments e.g. smoother transitions between commercial segments and other segments in video .

In described examples a read only msPlayTo attribute is added to an interface of an element. The msPlayTo attribute returns an IPlayTo interface. Exemplary methods and attributes of the IPlayTo interface are described in Table 10 below.

Described technologies can increase security of computing devices and networks by removing the need for installing plugins of unknown origin while still controlling what remote servers that provide content via web pages can do with remote devices so that users can remain in control.

This section describes exemplary approaches to user interaction. shows an example user interface . User interface can be used to switch seamlessly between local and remote playback of content e.g. content provided by a media server over the Internet via a web page . Local playback can include playback at a local computing device. Remote playback can include playback at a remote device that communicates with the local computing device via a local network such as an Ethernet network Wi Fi network or other network covering a small geographic area such as a home or office. Allowing a user to switch seamlessly between local playback and remote playback can provide user experience advantages such as the ability to preview streaming video on a local computing device e.g. a laptop PC and then switch without interruption to viewing on a remote device with a larger screen such as a high definition wide screen television. For example user interface includes a local remote playback selection window with check box and button controls that can be used to select local or remote playback and if remote playback is selected to select a remote device e.g. TV or MOBILE DEVICE . User interface also includes a playback window comprising playback controls for viewing and controlling playback and a library window for selecting video content e.g. EPISODE 1 EPISODE 2 or EPISODE 3 to be played. User interface can simplify the user experience by allowing remote playback and local playback to be initiated from the same user interface. Alternatively a user interface can have more or fewer features or different combinations of features.

At the system receives markup language code or scripting language code associated with digital media content. At the system performs local playback of the digital media content. At the system receives user input via a user interface e.g. user interface . At the system switches from local playback to remote playback in response to the user input. At in response to the switching the system parses received markup language code or scripting language code to obtain information representing an interface call associated with an element e.g. a media element that implements an HTMLMediaElement interface representing the digital media content. At the system automatically translates the interface call into translated code that complies with a streaming protocol e.g. a UPnP protocol specified by DLNA . At the system sends information representing translated code to a remote device on a local network. The remote device is operable to render the digital media content.

With reference to the computing environment includes at least one processing unit coupled to memory . In this basic configuration is included within a dashed line. The processing unit executes computer executable instructions. In a multi processing system multiple processing units execute computer executable instructions to increase processing power. The memory may be non transitory memory such as volatile memory e.g. registers cache RAM non volatile memory e.g. ROM EEPROM flash memory etc. or some combination of the two. The memory can store software implementing any of the technologies described herein.

A computing environment may have additional features. For example the computing environment includes storage one or more input devices one or more output devices and one or more communication connections . An interconnection mechanism not shown such as a bus controller or network interconnects the components of the computing environment . Typically operating system software not shown provides an operating environment for other software executing in the computing environment and coordinates activities of the components of the computing environment .

The storage may be removable or non removable and includes magnetic disks magnetic tapes or cassettes CD ROMs CD RWs DVDs or any other non transitory computer readable media which can be used to store information and which can be accessed within the computing environment . The storage can store software containing instructions for any of the technologies described herein.

The input device s may be a touch input device such as a keyboard touchscreen mouse pen or trackball a voice input device a scanning device or another device that provides input to the computing environment . The output device s may be a display printer speaker CD or DVD writer or another device that provides output from the computing environment . Some input output devices such as a touchscreen may include both input and output functionality.

The communication connection s enable communication over a communication mechanism to another computing entity. The communication mechanism conveys information such as computer executable instructions audio video or other information or other data. By way of example and not limitation communication mechanisms include wired or wireless techniques implemented with an electrical optical RF infrared acoustic or other carrier.

The techniques herein can be described in the general context of computer executable instructions such as those included in program modules being executed in a computing environment on a target real or virtual processor. Generally program modules include routines programs libraries objects classes components data structures etc. that perform particular tasks or implement particular abstract data types. The functionality of the program modules may be combined or split between program modules as desired in various embodiments. Computer executable instructions for program modules may be executed within a local or distributed computing environment.

Any of the storing actions described herein can be implemented by storing in one or more computer readable media e.g. non transitory computer readable storage media or other tangible media . Any of the things described as stored can be stored in one or more computer readable media e.g. computer readable storage media or other tangible media .

Any of the methods described herein can be implemented by computer executable instructions in e.g. encoded on one or more computer readable media e.g. non transitory computer readable storage media or other tangible media . Such instructions can cause a computer to perform the method. The technologies described herein can be implemented in a variety of programming languages.

Any of the methods described herein can be implemented by computer executable instructions stored in one or more non transitory computer readable storage devices e.g. memory CD ROM CD RW DVD or the like . Such instructions can cause a computer to perform the method.

In example environment various types of services e.g. computing services which can include any of the methods described herein are provided by a cloud . For example the cloud can comprise a collection of computing devices which may be located centrally or distributed that provide cloud based services to various types of users and devices connected via a network such as the Internet. The cloud computing environment can be used in different ways to accomplish computing tasks. For example with reference to the described techniques and tools some tasks such as processing user input and presenting a user interface can be performed on a local computing device while other tasks such as storage of data to be used in subsequent processing can be performed elsewhere in the cloud.

In example environment the cloud provides services for connected devices with a variety of screen capabilities A N. Connected device A represents a device with a mid sized screen. For example connected device A could be a personal computer such as desktop computer laptop notebook netbook or the like. Connected device B represents a device with a small sized screen. For example connected device E could be a mobile phone smart phone personal digital assistant tablet computer and the like. Connected device N represents a device with a large screen. For example connected device N could be a television e.g. a smart television or another device connected to a television or projector screen e.g. a set top box or gaming console .

A variety of services can be provided by the cloud through one or more service providers not shown . For example the cloud can provide services related to mobile computing to one or more of the various connected devices A N. Cloud services can be customized to the screen size display capability or other functionality of the particular connected device e.g. connected devices A N . For example cloud services can be customized for mobile devices by taking into account the screen size input devices and communication bandwidth limitations typically associated with mobile devices.

The illustrated mobile device can include a controller or processor e.g. signal processor microprocessor ASIC or other control and processing logic circuitry for performing such tasks as signal coding data processing input output processing power control and or other functions. An operating system can control the allocation and usage of the components and support for one or more application programs . The application programs can include common mobile computing applications e.g. include email applications calendars contact managers web browsers messaging applications or any other computing application. The mobile computing applications can further include an application for performing any of the disclosed techniques.

The illustrated mobile device can include memory . Memory can include non removable memory and or removable memory . The non removable memory can include RAM ROM flash memory a disk drive or other well known non transitory storage technologies. The removable memory can include flash memory or a Subscriber Identity Module SIM card which is well known in GSM communication systems or other well known non transitory storage technologies such as smart cards. The memory can be used for storing data and or code for running the operating system and the application programs including an application program for performing any of the disclosed techniques. Example data can include web pages text images sound files video data or other data sets to be sent to and or received from one or more network servers or other mobile devices via one or more wired or wireless networks. The memory can be used to store a subscriber identifier such as an International Mobile Subscriber Identity IMSI and an equipment identifier such as an International Mobile Equipment Identifier IMEI . Such identifiers can be transmitted to a network server to identify users and equipment.

The mobile device can support one or more input devices such as a touchscreen microphone camera physical keyboard and or trackball and one or more output devices such as a speaker and a display device . Other possible output devices not shown can include a piezoelectric or other haptic output device. Some devices can serve more than one input output function. For example touchscreen and display can be combined in a single input output device.

Touchscreen can accept input in different ways. For example capacitive touchscreens can detect touch input when an object e.g. a fingertip distorts or interrupts an electrical current running across the surface. As another example resistive touchscreens can detect touch input when a pressure from an object e.g. a fingertip or stylus causes a compression of the physical surface. As another example touchscreens can use optical sensors to detect touch input when beams from the optical sensors are interrupted. Physical contact with the surface of the screen is not necessary for input to be detected by some touchscreens.

A wireless modem can be coupled to an antenna not shown and can support two way communications between the processor and external devices as is well understood in the art. The modern is shown generically and can include a cellular modem for communicating with the mobile communication network and or other radio based modems e.g. Bluetooth or Wi Fi . The wireless modern is typically configured for communication with one or more cellular networks such as a GSM network for data and voice communications within a single cellular network between cellular networks or between the mobile device and a public switched telephone network PSTN .

The mobile device can further include at least one input output port a power supply a satellite navigation system receiver such as a global positioning system GPS receiver an accelerometer a transceiver for wirelessly transmitting analog or digital signals and or a physical connector which can be a USB port IEEE 1394 FireWire port and or RS 232 port. The illustrated components are not required or all inclusive as components can be deleted and other components can be added.

Various alternatives to the examples described herein are possible. For example techniques described with reference to flowchart diagrams can be altered by changing the ordering of stages shown in the flowcharts by repeating or omitting certain stages etc. As another example although some examples are described with reference to specific digital media formats other formats also can be used.

The various examples described herein can be used in combination or independently. Technology described herein can be used in a computer system with software hardware or a combination of software and hardware for processing digital content such as digital video digital audio or digital images or in some other system not specifically limited to processing such digital content.

Having described and illustrated the principles of our invention with reference to described embodiments it will be recognized that the described embodiments can be modified in arrangement and detail without departing from such principles. It should be understood that the programs processes or methods described herein are not related or limited to any particular type of computing environment unless indicated otherwise. Various types of general purpose or specialized computing environments may be used with or perform operations in accordance with the teachings described herein. Elements of the described embodiments shown in software may be implemented in hardware and vice versa.

In view of the many possible embodiments to which the principles of our invention may be applied we claim as our invention all such embodiments as may come within the scope and spirit of the following claims and equivalents thereto.

