---

title: Systems and methods for improving throughput of a graphics processing unit
abstract: Systems and methods for improving throughput of a graphics processing unit are disclosed. In one embodiment, a system includes a multithreaded execution unit capable of processing requests to access a constant cache, a vertex attribute cache, at least one common register file, and an execution unit data path substantially simultaneously.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08564604&OS=08564604&RS=08564604
owner: VIA Technologies, Inc.
number: 08564604
owner_city: Taipei
owner_country: TW
publication_date: 20100421
---
This application is related to co pending U.S. patent application Ser. No. 11 954 621 filed Dec. 12 2007 and entitled TRIANGLE SETUP AND ATTRIBUTE SETUP INTEGRATION WITH PROGRAMMABLE EXECUTION UNIT which is incorporated by reference in its entirety into the present disclosure. This application is related to co pending U.S. patent application Ser. No. 12 764 243 filed Apr. 21 2010 and entitled SYSTEM AND METHOD FOR MANAGING THE COMPUTATION OF GRAPHICS SHADING OPERATIONS which is incorporated by reference in its entirety into the present disclosure.

As is known the art and science of three dimensional 3 D computer graphics concerns the generation or rendering of two dimensional 2 D images of 3 D objects for display or presentation onto a display device or monitor such as a Cathode Ray Tube CRT or a Liquid Crystal Display LCD . The object may be a simple geometry primitive such as a point a line segment a triangle or a polygon. More complex objects can be rendered onto a display device by representing the objects with a series of connected planar polygons such as for example by representing the objects as a series of connected planar triangles. All geometry primitives may eventually be described in terms of one vertex or a set of vertices for example coordinate X Y Z that defines a point for example the endpoint of a line segment or a corner of a polygon.

To generate a data set for display as a 2 D projection representative of a 3 D primitive onto a computer monitor or other display device the vertices of the primitive are processed through a series of operations or processing stages in a graphics rendering pipeline. A generic pipeline is merely a series of cascading processing units or stages wherein the output from a prior stage serves as the input for a subsequent stage. In the context of a graphics processor these stages include for example per vertex operations primitive assembly operations pixel operations texture assembly operations rasterization operations and fragment operations.

In a typical graphics display system an image database e.g. a command list may store a description of the objects in the scene. The objects are described with a number of small polygons which cover the surface of the object in the same manner that a number of small tiles can cover a wall or other surface. Each polygon is described as a list of vertex coordinates X Y Z in Model coordinates and some specification of material surface properties i.e. color texture shininess etc. as well as possibly the normal vectors to the surface at each vertex. For three dimensional objects with complex curved surfaces the polygons in general must be triangles or quadrilaterals and the latter can always be decomposed into pairs of triangles.

A transformation engine transforms the object coordinates in response to the angle of viewing selected by a user from user input. In addition the user may specify the field of view the size of the image to be produced and the back end of the viewing volume so as to include or eliminate background as desired.

Once this viewing area has been selected clipping logic eliminates the polygons i.e. triangles which are outside the viewing area and clips the polygons which are partly inside and partly outside the viewing area. These clipped polygons will correspond to the portion of the polygon inside the viewing area with new edge s corresponding to the edge s of the viewing area. The polygon vertices are then transmitted to the next stage in coordinates corresponding to the viewing screen in X Y coordinates with an associated depth for each vertex the Z coordinate . In a typical system the lighting model is next applied taking into account the light sources. The polygons with their color values are then transmitted to a rasterizer.

For each polygon the rasterizer determines which pixel positions are covered by the polygon and attempts to write the associated color values and depth Z value into a frame buffer. The rasterizer compares the depth values Z for the polygon being processed with the depth value of a pixel which may already be written into the frame buffer. If the depth value of the new polygon pixel is smaller indicating that it is in front of the polygon already written into the frame buffer then its value will replace the value in the frame buffer because the new polygon will obscure the polygon previously processed and written into the frame buffer. This process is repeated until all of the polygons have been rasterized. At that point a video controller displays the contents of a frame buffer on a display one scan line at a time in raster order.

The default methods of performing real time rendering typically display polygons as pixels located either inside or outside the polygon. The resulting edges which define the polygon can appear with a jagged look in a static display and a crawling look in an animated display. The underlying problem producing this effect is called aliasing and the methods applied to reduce or eliminate the problem are called anti aliasing techniques.

Screen based anti aliasing methods do not require knowledge of the objects being rendered because they use only the pipeline output samples. One typical anti aliasing method utilizes a line anti aliasing technique called Multi Sample Anti Aliasing MSAA which takes more than one sample per pixel in a single pass. The number of samples or sub pixels taken for each pixel is called the sampling rate and axiomatically as the sampling rate increases the associated memory traffic also increases.

Although the foregoing has only briefly summarized the operation of the various processing components persons skilled in the art will recognize that the processing on graphics data is quite intense. Consequently it is desired to improve processing design and manufacturing efficiency wherever possible. Similarly it is desired to improve throughput of the processing core of a graphics processing unit in order to maximize the performance of a given hardware design. Improvements in throughput can result in an increased pipeline efficiency which can reduce the required hardware requirements and gate counts for a given performance level.

The present disclosure is generally related to systems and methods for increasing throughput of a graphics processing unit. Briefly described in architecture one embodiment of the system among others can be implemented as follows a graphics processing unit comprising an execution unit configured to execute programmable shader operations. The execution unit is further configured to process operations for a plurality of threads. The apparatus further comprises memory forming a register file configured to accommodate register operations for all threads executed by the execution unit. The memory is organized in a plurality of banks with a first plurality of banks being allocated to a first plurality of the threads and a second plurality of banks being allocated to the remaining threads.

In certain embodiments the apparatus further comprises memory forming a constant cache configured to accommodate the fetching of certain constants used for a plurality of shader operations executed within the execution unit the constant cache configured to store a plurality of contexts of values corresponding to a plurality of types of shaders. The constant cache is further configured to store a plurality of contexts corresponding to a plurality of versions of values stored within the constant cache. The apparatus further includes memory forming a vertex attribute cache configured to accommodate the storing of certain vertex attributes used in programmable shader operations executed by the execution unit.

One embodiment of a method includes fetching a first instruction in an execution unit from an instruction cache on behalf of one of a plurality of active threads. The methods further include broadcasting the first instruction to the plurality of active threads and queueing the first instruction in an instruction queue corresponding to at least one of the plurality of active threads. The methods further include decoding a second instruction in the instruction queue of at least one of the plurality of active threads and submitting the second instruction to at least one of a constant cache a vertex attribute cache a first common register file a second common register file and an execution unit data path.

An alternative embodiment of a system can be implemented as follows a graphics processing unit comprising an execution unit capable of multi threaded operation the execution unit having a thread controller the thread controller including a first instruction fetch arbiter and a second instruction fetch arbiter. The first instruction fetch arbiter is configured to fetch instructions on behalf of at least half of a plurality of threads within the execution units and the second instruction fetch arbiter is configured to fetch instructions on behalf of the remainder of the plurality of threads.

Reference is now made in detail to the description of the embodiments as illustrated in the drawings. While several embodiments are described in connection with these drawings there is no intent to limit the disclosure to the embodiment or embodiments disclosed herein. On the contrary the intent is to cover all alternatives modifications and equivalents.

As summarized above the present invention is directed to a novel system and method for improving the throughput of a graphics processing unit. Before discussing implementation details of various embodiments reference is made to which is a block diagram illustrating certain components in a graphics pipeline which may be utilized by or in embodiments of the present invention. The principal components illustrated in are a vertex shader geometry shader triangle setup scan and tile generator and attribute setup block pixel shader and a frame buffer . The general function and operation of these components are known and understood by persons skilled in the art and therefore need not be described in detail herein. To summarize however graphics primitives may be defined by location data e.g. X Y Z and W coordinates as well as lighting and texture information. All of this information can be passed to a vertex shader . As is known the vertex shader may perform various transformations on the graphics data received from the command list. In this regard the data may be transformed from World coordinates into Model View coordinates into Projection coordinates and ultimately into Screen coordinates. The functional processing performed by the vertex shader is known and need not be described further herein. The vertex shader outputs a geometry primitive to a geometry shader .

Geometry and other graphics data generated by the geometry shader are communicated to logic for performing a triangle setup operation. Specific functions and implementation details of the triangle setup logic may vary from embodiment to embodiment. In general vertex information about triangle primitives may be passed to the triangle setup logic and operations may be performed on the various primitives defined by the graphics data that is passed to the triangle setup logic . Among other operations certain geometry transformations may be performed within the triangle setup logic .

By way of illustration for a given vertex geometry data such as x y z and w information may be provided where x y and z are geometric coordinates and w is the homogenous coordinate . As is known by persons skilled in the art various transformations may be made from for example model space to world space to eye space to projection space to homogeneous space to normalized device coordinates or NDC and finally to screen space performed by a viewport transformation . It would be appreciated that certain components of the depicted graphics pipeline have been omitted from the illustration for ease of depiction and clarity but are known to those skilled in the art. As a non limiting example certain stages of the rasterization pipe of a graphics pipeline have been omitted for the sake of clarity but a person of ordinary skill in the art would appreciate that the graphics pipeline may include those other stages.

The above described graphics pipeline is typically implemented within a graphics processing unit or graphics processing apparatus. Current graphics pipelines are typically guided by published application programming interfaces API or a collection of API s that impose certain requirements on the manufacturers of graphics processing units. A non limiting example of such an API is the Direct3D API. An alternative view of an implementation of a graphics pipeline is discussed below.

Reference is now made to which is a block diagram illustrating certain components or stages of a graphics pipeline . The first component is designated as a command stream processor which essentially receives or reads vertices out of memory which are used to form geometry primitives and create working items for the pipeline. In this regard the command stream processor reads data from memory and from that data generates triangles lines points or other primitives to be introduced into the pipeline. This geometry information once assembled is passed to the vertex shader . A requirement imposed by certain current graphics API s is that shaders such as the vertex shader are user programmable stages of the graphics pipeline. In other words a programmer utilizing such a graphics API is able to create a shader and program certain operations that shader execute. Accordingly the vertex shader is depicted in with rounded edges which is a convention hereinafter adopted to depict those stages of a graphics pipeline that are implemented by executing instructions in a programmable execution unit or pool of execution units in a processing core of a graphics processing unit. The vertex shader processes vertices by performing operations such as transformations scanning and lighting. Thereafter the vertex shader passes data to the geometry shader . The geometry shader receives as inputs vertices for a full primitive and is capable of outputting multiple vertices that form a single topology such as a triangle strip a line strip point list etc. The geometry shader may be further configured to perform the various algorithms such as tessellation shadow volume generation etc.

The geometry shader outputs information to a triangle setup stage which performs operations such as triangle trivial rejection determinant calculation culling pre attribute setup KLMN edge function calculation and guardband clipping. The operations necessary for a triangle setup phase should be appreciated by one of ordinary skill in the art and need not be elaborated further. The triangle setup stage outputs information to the span generator tile generator. Among the operations performed by the span generator tile generator is the rejection of triangles if it is not necessary to render such a triangle to the screen. It should be appreciated that other elements of a rasterization pipeline may operate such as for example a Z test or other fixed function elements of a graphics pipeline. For example a Z test may be performed to determine the depth of a triangle to further determine if the triangle should be rejected as unnecessary to render to the screen. However such elements are not further discussed herein as they should be appreciated by a person of ordinary skill in the art.

If a triangle processed by the triangle setup stage is not rejected by the span generator tile generator or other stage of the graphics pipeline then the attribute setup stage of the graphics pipeline will perform attribute setup operations. The attribute setup stage generates the list of interpolation variables of known and required attributes to be determined in the subsequent stages of the pipeline. Further the attribute setup stage as is known in the art processes various attributes related to a geometry primitive being processed by the graphics pipeline.

The pixel shader is invoked for each pixel covered by the primitive that is output by the attribute setup stage . As is known the pixel shader operates to perform interpolations and other operations that collectively determine pixel colors for output to a frame buffer . The operation of the various components illustrated in are well known to persons skilled in the art and need not be further described herein. Therefore the specific implementation and operation internal to these units need not be described herein to gain and appreciate a full understanding of the present invention.

Reference is now made to which is a block diagram showing an example processor environment for a graphics processor constructed in accordance with embodiments of the invention. While not all components for graphics processing are shown the components shown in should be sufficient for one having ordinary skill in the art to understand the general functions and architecture related to such graphics processors. The exemplary processor environment of is analogous to the environment disclosed in co pending U.S. patent application Ser. No. 11 954 621 filed Dec. 12 2007 which is incorporated by reference herein in its entirety.

Certain components of the graphics processing unit have been omitted from the illustration for clarity however a person of ordinary skill in the art would appreciate that other hardware and logical components not pictured may be present in a graphics processing unit . The depicted graphics processing unit includes a pool of multiple programmable execution units and an execution unit pool control and cache subsystem . The execution unit pool control and cache subsystem can handle thread management of the processing core of the pool of multiple programmable execution units as well as communication between a user of the system and other components within the graphics processing unit . A cache subsystem including one or more caches for use by the execution unit pool can also reside in the execution unit pool control and cache subsystem . The cache subsystem can be used for example by a vertex shader thread to store data for use by a subsequent thread executing triangle setup operations or for typical memory transactions. Alternatively each execution unit in the execution unit pool may include an execution unit buffer for the storage of data for use by a subsequent thread executing within the same execution unit.

As noted above user programmable stages of a graphics pipeline such as a geometry shader a vertex shader or a pixel shader are executed within the execution unit pool. Because the pool of execution units is preferably a processing core capable of multithreaded operation the execution unit pool control and cache subsystem is preferably responsible for scheduling of threads within the execution unit pool . When the execution unit pool control and cache subsystem receives a request for the execution of a programmable shader it will instruct an execution unit in the execution unit pool to create a new thread for execution of the shader. The execution unit pool control and cache subsystem can manage load across the execution unit pool as well as shift resources from one type of shader to another to efficiently manage throughput of the graphics pipeline. Such thread management techniques should be appreciated and need not be discussed in further detail herein. However by way of example if pixel shader operations are the source of a bottleneck in terms of throughput of the GPU then the execution unit pool control and cache subsystem can allocate more execution unit resources to pixel shader operations in order to improve throughput.

Reference is now made to which depicts a functional block diagram of relevant portions of an execution unit . An execution unit is capable processing multiple instructions simultaneously. Therefore a pool of execution units as noted above is capable of processing multiple shader threads at once. The execution unit includes a thread controller which further includes instruction fetch arbiter and instruction fetch arbiter . The thread controller manages tasks assigned to an execution unit and manages active threads and sleeping threads within the execution unit . As is known an active thread is a thread corresponding to a task that is ready for processing. In other words data required by the thread is available to the thread and thus the execution unit can process the thread. A sleeping thread is a thread assigned to a task by the thread controller that is not ready for processing. In other words a sleeping thread may exist in a state of where it is awaiting data from various other components of a graphics processing unit. The depicted thread controller includes two instruction fetch arbiters because the processing of threads is divided in the illustrated embodiments into even and odd numbered threads. For example if an execution unit is capable of processing sixteen threads at once half of the sixteen threads or even numbered threads can be assigned to instruction fetch arbiter while the remaining threads can be assigned to instruction fetch arbiter . Dividing the threads into two groups each with a separate instruction fetch arbiter can improve the throughput of the execution unit by reducing the latency related to instruction fetching. Of course threads may be divided or allocated in other ways as well.

The instruction fetch arbiters can independently arbitrate requests to fetch instructions on behalf of active threads executing within the execution unit based on the age of the requesting thread. Upon receiving a request for an instruction from a thread instruction fetch arbiter or instruction fetch arbiter can request an instruction from an instruction cache which can be a cache maintaining a cache of instructions. The instruction cache may include an instruction cache controller that performs a hit test to determine whether a requested instruction resides within the instruction cache . If a requested instruction does not reside within the instruction cache or if the hit test results in a miss then a requested instruction must be retrieved from level 2 cache or other memory via level 2 cache access . Fetched instructions are broadcast to even threads as well as odd threads on an instruction broadcast bus . Accordingly if more than one thread is requesting the same instruction from instruction cache this can reduce latency by eliminating at least one instruction fetch from the instruction cache . In other words if more than one thread is requesting the same instruction from instruction cache it is not necessary to individually fetch and deliver the instruction on behalf of each thread because the requested instruction is returned from instruction cache via an instruction broadcast bus which is available to all even and odd threads executing within the execution unit .

Following instruction fetch threads within even and odd thread groups can determine whether a fetched instruction requires interaction with a constant cache vertex attribute cache common register file or common register file . For example materials properties may be required that reside in a constant cache as well as other parameters which are not changed in a given context and constant for all vertices of an object being rendered. In addition light source properties may also reside in a constant cache as they are normally stable through the generation of a frame. As is shown the common register files are divided into an even and odd group just as threads are divided into an even and odd thread group. The structure and configuration of the common register files can be analogous to that disclosed in co pending application serial number TKHR ref. 252209 1560 S3U05 0029 entitled SYSTEM AND METHOD FOR MANAGING THE COMPUTATION OF GRAPHICS SHADING OPERATIONS filed Apr. 21 2010 which is incorporated by reference herein in its entirety. If an instruction requires data from constant cache the data is requested from the constant cache and the instruction is not issued until the requested data is ready. Similarly if an instruction requires data from vertex attribute cache the data is requested from vertex attribute cache and the instruction is not issued until the requested data is ready. Further if an instruction requires data from other components within a graphics processing unit but external to an execution unit the instruction is not issued until the data is received from the external component. For example an instruction may include a request to fetch texture data from components external to the execution unit and store in a register which may require the thread to wait for data to be requested and delivered.

When data required for the execution of an instruction is ready the thread controller can issue an instruction for execution by the execution unit data path EUDP which can include arithmetic logic unit arithmetic logic unit and interpolator . Upon completion of execution of the instruction by the EUDP resultant data can be output by the output buffer of the execution unit which can send data to components of the graphics processing unit external to the execution unit or alternatively to components within the execution unit including but not limited to the vertex attribute cache . For example if an instruction execution requires updating data within the vertex attribute cache the data can be send to the vertex attribute cache via the output buffer following execution by the EUDP . As another non limiting example the EUDP may calculate a texture coordinate or other data and output the texture coordinate or other data to a texture unit or other component external to the execution unit via the data output buffer.

It should be appreciated that certain components and data paths have been omitted from the above described illustration for sake of clarity and ease of depiction. For example it should be appreciated that the thread controller may be coupled to execution unit pool control and level 2 cache subsystem to receive tasks to be managed within the execution unit. Further certain components may require data from a level 2 cache which may reside external to the depicted execution unit. Accordingly level 2 cache access represents mechanisms for accessing a level 2 cache or other system memory.

Reference is now made to which depicts a more detailed view of the constant cache utilized within the execution unit of . Because the execution unit is capable of simultaneously processing multiple threads corresponding to different types of shader operations such as pixel shader vertex shader and geometry shader the execution unit must maintain more than one set of constants for use by the EUDP. For example an execution unit processing threads for pixel shader and vertex shader operations maintains constants for both pixel shader constants as well as vertex shader constants. The execution unit further maintains multiple contexts of constants as well as multiple versions of each constant context. For example two threads executing pixel shader operations within an execution unit may exist in different contexts so the execution unit maintains at least two different contexts of pixel shader constant values. Accordingly the depicted constant cache can be utilized to maintain at least two constant contexts for various types of shader threads. Similarly the execution unit is also required to maintain multiple versions of constants that may change within a context. For example if a constant within a vertex shader context in memory is altered by a vertex shader thread the constant cache can maintain a previous version of the altered constant as well as fetch from memory the altered version of the constant. Accordingly other vertex shader threads within the execution unit can access the previous version or altered version of the constant depending on the requirements of the thread.

The constant cache includes a header table a cache and a mapping table . Various constants for shader contexts are stored within the cache storage in according to a scheme defined by the header table . For example the header table can delineate groups of constants according shader type as well as context or context ID. In the depicted example constants corresponding to a shader type and context ID can be stored sequentially at a location in the cache specified by the base address. The header table can store the base address corresponding to the location of constants stored substantially contiguously in the cache. A pixel shader thread can request a constant from the constant cache by requesting a particular constant from a particular constant without containing knowledge of the actual location in the cache of the constant cache . A shader thread requesting a constant from the constant cache can request a constant only with knowledge of the constant s location within a particular context. In the hypothetical header table of with a non limiting exemplary addressing scheme a pixel shader thread existing in a context with a context ID of 0 can for example submit a request for the first pixel shader constant of context ID 0 and the constant cache will return the first constant located at or near the corresponding base address from the header table. Similarly a vertex shader thread existing in a context with a context ID of 1 can submit a request for the second pixel shader constant of context ID 1 and the constant cache will return the second constant located at or near the corresponding base address from the header table .

Further the constant cache can also maintain multiple versions of constant if its value is altered by a thread processed by the execution unit. For example in the non limiting exemplary mapping table of the depicted constant cache of the mapping table maintains data regarding constants that are altered by various shader threads and also tracks a version of each constant. For example the first entry of the depicted mapping table contains an exemplary entry for a vertex shader constant A that has been modified by a vertex shader thread. Therefore the constant cache can maintain the previous value of this constant in within the cache storage of the constant cache for use by other threads if necessary. It should be appreciated that multiple versions of a constant s value can be maintained in this fashion.

The constant cache also includes a FIFO which serves as a mechanism for delivering data to shader threads or other threads processed by an execution unit . The FIFO can be configured with varying sizes to accommodate a varying number of entries depending on the requirements of the execution unit the constant cache is operating within. When a shader thread for example requests a constant from the constant cache the constant is located utilizing the header table and mapping table and delivered to the FIFO . The FIFO can then broadcast a signal to other components within the execution unit indicating that it is ready to deliver a constant. Because the execution unit is capable of processing multiple instructions simultaneously the FIFO within the constant cache allows requests from other shader threads to fetch other constants prior to completing a previous request to fetch a constant and delivering it to a requesting thread. This enables to constant cache improve throughput of the execution unit by increasing throughput of the servicing requests to read data from or write data to the constant cache . It should be appreciated that the disclosed header table mapping table and FIFO of the constant cache can be implemented in various ways and that the disclosed implementation is but one of many possible examples which will be readily appreciated by persons skilled in the art.

Reference is now made to which illustrates an alternative view of the execution unit of . In addition to a thread controller and instruction fetch arbiter and instruction fetch arbiter also depicted are active threads and corresponding instruction queues being processed by the execution unit . For ease of depiction and clarity not all active threads and instruction queues are depicted as it should be appreciated that more or fewer active threads can be processed by the execution unit . The depicted execution unit can simultaneously process at least eight active threads that are separated into even and odd thread groups. Stated another way the execution unit can have at least eight instruction queues corresponding to at least eight active threads. The depicted active threads include instruction queues capable of queuing up to four instructions. The instruction fetch arbiters fetch instructions from instruction cache on behalf of the active threads. Instruction fetch arbiter fetches instructions on behalf of the even active threads while instruction fetch arbiter fetches instructions on behalf of the four odd active threads .

It should be appreciated that the instruction queues corresponding to the active threads can be configured with the ability to queue more or fewer instructions than the depicted four depending on the latency of fetching an instruction from the instruction cache or other memory system. Each instruction for each active thread is pre fetched prior to its actual execution to remove the latencies that exist in sending a request for an instruction to the instruction cache as well as retrieving an instruction from a level 2 cache or other memory system in the event that it does not exist in the instruction cache. It should now be appreciated that decoupling the fetching of an instruction from its decoding and execution among various active threads can improve throughput and performance of the execution unit. Fetched instructions can be broadcast to even threads as well as odd threads on an instruction broadcast bus . Accordingly if more than one thread is requesting the same instruction from instruction cache this can reduce latency by eliminating at least one instruction fetch from the instruction cache . In other words if more than one thread is requesting the same instruction from instruction cache it is not necessary to individually fetch and deliver the instruction on behalf of each thread because the requested instruction is returned from instruction cache via an instruction broadcast bus which is available to all even and odd threads executing within the execution unit .

Each active thread further includes an instruction pre decoder which determines whether the next instruction to be processed is an instruction including a fetch or a store of a constant an instruction including a fetch or a store of vertex attribute data or an instruction with interaction with the one of the common register files . If an instruction including a fetch or a store or constants or an interaction with the constant cache is being processed by the pre decoder then the instruction pre decoder corresponding to the active thread in which the instruction exists can submit the instruction request to the constant cache arbiter which arbitrates access to the constant cache . In the above example the constant cache arbiter will submit a request to the constant cache . As noted above the constant cache will process a request to fetch constants and store the requested constant in a FIFO within the constant cache .

Similarly if an instruction in an instruction queue requests to fetch or store vertex attribute data or to otherwise interact with the vertex attribute cache then the instruction pre decoder can submit the request to the vertex attribute cache arbiter which arbitrates access to the vertex attribute cache . The vertex attribute cache can process a request to fetch data from the vertex attribute cache and store the requested data in a FIFO coupled to the vertex attribute cache . If an instruction requests to interact with one of the common register files then the instruction pre decoder of an active thread can submit the instruction request to the common register arbiter which arbitrates requests to access either common register file or common register file . The common register arbiter can submit the instruction request to the common register file or common register file depending on whether the instruction originates from an even thread or an odd thread.

It should be appreciated that because the of depicted architecture of the execution unit of throughput of the execution unit can be improved because requests to the constant cache common register files and vertex attribute cache can be submitted and handled in parallel prior to execution of the instruction by an execution unit datapath EUDP with no one process acting as a bottleneck. For example if an instruction including a request to fetch a constant requires the constant cache to fetch the requested constant from a level 2 cache or other system memory this is an operation that may potentially require a few cycles to complete. However in the disclosed execution unit such a situation may not stall the remaining active threads because the execution unit is able to simultaneously process other instructions such as a request to interact with the common register files or the vertex attribute cache .

As noted above the execution unit also contains an execution unit datapath EUDP which executes a given instruction using data retrieved from at least the constant cache the common register file common register file or the vertex attribute cache . The thread controller can issue an instruction to the EUDP when all of the data required for execution of the instruction is ready. For example in the case of an instruction requesting a constant from the constant cache if the requested constant has been stored in the constant cache FIFO the thread controller can then issue the instruction to the EUDP which can read the data from the constant cache FIFO and process any necessary outputs via the data output buffer . Similarly if an instruction requiring interaction with either of the common register files is ready to be executed by the EUDP then the thread controller can issue the instruction to the EUDP . In other words if the data required by the instruction is ready then the thread controller can issue the instruction for execution.

In addition to further improve throughput of the execution unit measures to optimize the execution of instructions by the EUDP can be undertaken in accordance with the disclosure. For example if an arithmetic instruction on two values within one of the common register files followed by an instruction to store the result in another register of one of the common register files can be combined into one instruction to improve throughput of the execution unit . The resulting instruction can simply perform an arithmetic operation and store the result directly in the destination register removing the necessity of the execution unit to execute an additional instruction to store the result of an arithmetic instruction in a register of one of the common register files. This can be accomplished by analyzing instructions within the instruction queue or by a compiler when compiling software code into machine instructions executed by the depicted execution unit. For example a compiler translating software code into machine instructions can be configured to identify a case of the above exemplary arithmetic instruction followed by an additional instruction to move the result of the arithmetic instruction into a different register of the register file. In this case a compiler can generate one instruction incorporating an arithmetic operation that stores the result in a destination register instead of generating two separate instructions to accomplish the same result.

As another non limiting example a common arithmetic instruction executed by a thread within an execution unit includes the calculation of a texture coordinate and the storing of the texture coordinate in a register of one of the common register files. Often the next instruction executed by the thread following the calculation of a texture coordinate is a sample instruction or an instruction that outputs the resulting texture coordinate to a texture unit or other component of a graphics processing unit via the data output buffer of the execution unit. This pair of operations can be optimized utilizing the above architecture by combining the two instructions into one instruction whereby in one instruction the texture coordinate is calculated and the destination of the texture coordinate can be designated as a texture unit or other component external to the execution unit. Therefore the depicted execution unit architecture allows for at least 5 operations to be simultaneously processed by the execution unit. As a non limiting example the execution can simultaneously execute a constant cache fetch a vertex attribute cache fetch a fetch operation from constant register file a fetch operation from constant register file and the output of data from the EUDP such as the above noted sample instruction to output a texture coordinate to a texture unit or other component external to the execution unit.

Reference is now made to which depicts a flowchart of a method in accordance with the disclosure. depicts steps undertaken by an execution unit and components within the execution unit to execute instructions making up tasks assigned to the execution unit and embodied in threads corresponding to programmable shaders. The method begins at step where it branches to two parallel collections of method steps. The first collection beginning at step describes thread level arbitration for instruction fetch and queue. The second collection beginning at step describes instruction level execution arbitration and scheduling. The first parallel branch begins at step where it is determined on behalf of which active thread to prefetch an instruction. This determination can be made by analyzing the age of each active thread where an instruction is fetched on behalf of the oldest of the active threads in an execution unit. Alternatively an instruction can be fetched on behalf of an active thread that has waited the longest amount of time since an instruction was last fetched on its behalf. Other schemes which could be used to determine which active thread to fetch an instruction on behalf of will be appreciated by those skilled in the art.

In step an instruction is fetched from an instruction cache on behalf of an active thread. If the instruction is not located within the instruction cache then it may be necessary to retrieve the instruction from a level 2 cache or other memory system. The fetched instruction is delivered to an active thread by broadcasting it on a broadcast bus that all currently active threads can access in step . Accordingly if more than one thread is requesting the same instruction from instruction cache this can reduce latency by eliminating at least one instruction fetch from the instruction cache. In other words if more than one thread is requesting the same instruction from instruction cache it is not necessary to individually fetch and deliver the instruction on behalf of each thread because the requested instruction is returned from instruction cache via an instruction broadcast bus which is available to all even and odd threads executing within the execution unit.

An active thread requiring a fetched instruction will then queue the instruction in its instruction queue in step . As noted above in an execution unit each active thread can maintain a separate instruction queue that be sized to store an appropriate number of instructions given the latency of an instruction fetch from an instruction cache.

The second instruction level arbitration parallel branch begins at step where the next instruction to be executed in a particular active thread is decoded or pre decoded to determine what type of operations the instruction requires. In step the instruction type is determined. The instruction operand type can include but is not limited to a constant cache access request a vertex attribute access cache request and common register file access request or operations that can be immediately executed by the EUDP. If the instruction requests to fetch or store a constant or otherwise interact with a constant cache then in step the instruction is submitted to a constant cache. If the instruction requests to fetch or store vertex attribute data or otherwise interact with a vertex attribute cache then in step the instruction is submitted to a vertex attribute cache. If the instruction requests to interact with either common register file then the instruction is submitted to the common register file in step . If the instruction can be executed by the EUDP without interacting with any of the above then the instruction can be submitted directly to the EUDP in step . In step after execution of the instruction by the EUDP data is output to its destination whether it is a component external to an execution unit such as a texture unit or to other components within the execution unit.

The embodiments of the present invention can be implemented in hardware software firmware or a combination thereof. In some embodiments the compression of color data is implemented in software or firmware that is stored in a memory and that is executed by a suitable instruction execution system. If implemented in hardware as in an alternative embodiment the triangle setup and attribute setup stages can be implemented with any or a combination of the following technologies which are all well known in the art a discrete logic circuit s having logic gates for implementing logic functions upon data signals an application specific integrated circuit ASIC having appropriate combinational logic gates a programmable gate array s PGA a field programmable gate array FPGA etc.

Any process descriptions or blocks in flow charts should be understood as representing modules segments or portions of code which include one or more executable instructions for implementing specific logical functions or steps in the process and alternate implementations are included within the scope of the preferred embodiment of the present invention in which functions may be executed out of order from that shown or discussed including substantially concurrently or in reverse order depending on the functionality involved as would be understood by those reasonably skilled in the art of the present invention. Further such process descriptions or blocks in flow charts can also represent portions of hardware logic configured to operate in a manner consistent with implementing specific logical functions or steps in the process and alternate implementations included within the scope of embodiments of the invention.

It should be emphasized that the above described embodiments of the present invention particularly any preferred embodiments are merely possible examples of implementations set forth for a clear understanding of the principles of the invention. Many variations and modifications may be made to the above described embodiment s of the invention without departing substantially from the spirit and principles of the invention. All such modifications and variations are intended to be included herein within the scope of this disclosure and the present invention and protected by the following claims.

