---

title: Method and system for non-disruptive migration
abstract: Method and system for migrating a virtual storage system from a source storage system having access to a source storage device to a destination storage system having access to a destination storage device is provided. A processor executable management application estimates a likelihood of success for a migration operation before the migration operation enters a cut-over duration during which client access to the source storage system and the destination storage system is restricted. The migration operation enters the cut-over duration if there is high likelihood of success for completing the migration during the cut-over duration or aborted, if there is a low likelihood of success for completing the migration during the cut-over duration.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08812806&OS=08812806&RS=08812806
owner: Netapp, Inc.
number: 08812806
owner_city: Sunnyvale
owner_country: US
publication_date: 20101029
---
Various forms of storage systems are used today. These forms include network attached storage NAS systems storage area networks SANs and others. Network storage systems are commonly used for a variety of purposes such as providing multiple users with access to shared data backing up data and others.

A network storage system typically includes at least one computing system may also be referred to as a server which is a processing system configured to store and retrieve data on behalf of one or more client processing systems clients . In the context of NAS a storage server operates on behalf of one or more clients to store and manage shared files in a set of mass storage devices such as magnetic or optical disks or tapes.

In a SAN context the storage server provides clients with block level access to stored data. Some storage systems may be capable of providing clients with both file level access and block level access.

Storage systems may be presented as virtual storage systems to clients. A virtual storage system may be migrated from a source storage system to a destination storage system. While information is being migrated there may be applications that may still want to use the source storage system. It is desirable to migrate information in a non disruptive manner so that clients can continue to use the source storage system. Continuous efforts are being made to efficiently migrate a virtual storage system from the source storage system to the destination storage system.

In one embodiment a method and system for migrating a virtual storage system from a source storage system having access to a source storage device to a destination storage system having access to a destination storage device is provided. A processor executable management application estimates a likelihood of success for a migration operation before the migration operation enters a cut over duration. The cut over duration is a time slot during the migration operation when client access to the source storage system and the destination storage system is restricted.

The management application estimates a workload for the processors used by both the source storage system and the destination storage system. The management application also estimates utilization of destination storage if the migration operation were to be completed. The estimated workload for the processors and the utilization of destination storage are compared to a first threshold value and a second threshold value. Based on the comparison the management application predicts the likelihood of success for the migration operation. The migration operation enters the cut over duration if there is a high likelihood of success or aborted if there is a low likelihood of success.

In another embodiment a machine implemented method for migrating a virtual storage system in response to a migration request is provided. The virtual storage system is migrated from a source storage system having access to a source storage device to a destination storage system having access to a destination storage device.

The method includes determining a projected workload of the source storage system prior to a cut over duration when access to the source storage system is restricted and determining a projected work load of the destination storage system prior to the cut over duration when access to the destination storage system is restricted and determining projected utilization of the destination storage device if the virtual storage system were to be successfully migrated from the source storage system to the destination storage system.

The method further includes estimating a likelihood of success for completing a migration operation in response to the migration request and then proceeding with the migration operation in response to the migration request based on the estimated likelihood of success.

In yet another embodiment a method for migrating a virtual storage system managed by a source storage system having access to a source storage device to a destination storage system having access to a destination storage device is provided. The method includes estimating a likelihood of success for completing a migration operation based on a projected usage of the source storage system and the destination storage system prior to a cut over duration when access to the source storage system and the destination storage system is restricted and a projected utilization of the destination storage device if the migration operation were to be successfully completed. The method further includes aborting the migration operation if the estimated likelihood of success is unacceptable.

In another embodiment a machine implemented method for migrating a virtual storage system from a source storage system having access to a source storage device to a destination storage system having access to a destination storage device is provided. The method includes estimating a likelihood of success for migrating the virtual storage system from the source storage system to the destination storage system determining if the estimated likelihood of success is acceptable or unacceptable and aborting a migration operation if the estimated likelihood of success is unacceptable.

In yet another embodiment a system is provided. The system includes a virtual storage system managed by a source storage system having access to a source storage device and a destination storage system having access to a destination storage device. During a migration operation the virtual storage system is migrated from the source storage system to the destination storage system. The system also includes a management application executed by a computing system that interfaces with the source storage system and the destination storage system for estimating a likelihood of success for completing the migration operation based on i a projected usage of the source storage system and the destination storage system prior to a cut over duration when access to the source storage system and the destination storage system is restricted and ii a projected utilization of the destination storage device if the migration operation were to be successfully completed. The migration operation is aborted if the estimated likelihood of success is unacceptable.

This brief summary has been provided so that the nature of this disclosure may be understood quickly. A more complete understanding of the disclosure can be obtained by reference to the following detailed description of the various embodiments thereof in connection with the attached drawings.

In one embodiment a method and system for migrating a virtual storage system from a source storage system having access to a source storage device to a destination storage system having access to a destination storage device is provided. A processor executable management application estimates a likelihood of success for a migration operation before the migration operation enters a cut over duration. The cut over duration is a time slot during the migration operation when client access to the source storage system and the destination storage system is restricted.

The management application estimates a workload for the processors used by both the source storage system and the destination storage system. The management application also estimates utilization of destination storage if the migration operation were to be completed. The estimated workload for the processors and utilization of destination storage are compared to a first threshold value and a second threshold value. Based on the comparison the management application predicts the likelihood of success for the migration operation. The migration operation enters the cut over duration if there is a high likelihood of success or aborted if there is a low likelihood of success.

As a preliminary note the terms component module system and the like as used in this disclosure are intended to refer to a computer related entity either software executing general purpose processor hardware firmware and a combination thereof. For example a component may be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer.

By way of illustration both an application running on a server and the server can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers. Also these components can execute from various computer readable media having various data structures stored thereon. The components may communicate via local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems via the signal .

Computer executable components can be stored for example on computer readable media including but not limited to an ASIC application specific integrated circuit CD compact disc DVD digital video disk ROM read only memory floppy disk hard disk EEPROM electrically erasable programmable read only memory memory stick flash memory device or any other non volatile memory device or any other storage device in accordance with the claimed subject matter.

Each storage subsystem is managed by a corresponding storage system . The storage devices in each storage subsystem can be organized into one or more RAID redundant array of independent disks groups in which case the corresponding storage system accesses the storage subsystem using an appropriate RAID protocol.

Each storage system may operate as a NAS network attached storage based file server a block based storage server such as used in a storage area network SAN or a combination thereof or a node in a clustered environment described below with respect to or any other type of storage server.

The storage systems may be operationally coupled to a plurality of clients through a network . Each storage system receives and responds to various read and write requests from clients directed to data stored in or to be stored in a corresponding storage subsystem .

Each client may be for example a conventional personal computer PC workstation or the like. The network may be for example a local area network LAN a wide area network WAN or any other type of network or a combination of networks.

Each storage system may be presented as one or more virtual storage system A N also referred to as a vfiler . Each vfiler is associated with a physical storage system . This allows one to present a physical storage system as shared multiple virtual storage systems.

Each vfiler is addressable by the client systems and can handle input output commands just like storage systems . Each vfiler may be assigned a unique address that may be used by a client to access the storage system. For example each vfiler may be assigned an Internet Protocol IP address that is used by a client to send input output commands. The IP address may be assigned when the vfiler is configured using management application .

In some instances a vfiler may be migrated from one storage system also referred to as a source storage system to another storage system also referred to as a destination storage system . Details regarding vfiler migration are provided below in more detail.

Also connected to the network is a management console that may store and execute the management application may also be referred to as a storage management application . The management console may be for example a conventional PC a workstation or the like. The management application may be used by a storage network administrator to manage a pool of storage devices may also be referred to as a resource pool . The management application enables an administrator to perform various operations such as monitoring and allocating storage space in the storage pool creating and deleting volumes directories and others. The management application also allows an administrator to provision and configure storage space for non disruptive migration.

In one embodiment as described below in more detail a non disruptive migration operation for migrating a vfiler from a source storage system to a destination storage system is initiated. Before the migration operation is completed during a cut over duration access to the source storage system and the destination storage system is restricted.

Before proceeding with the migration operation during the cut over duration and then completing the migration operation the management application executing executable code analyzes the source storage system the destination storage system and the destination storage device to predict whether the migration operation is likely to succeed. The prediction is based on estimating processor utilization during the cut over duration for both the source storage system and the destination storage system as well as estimating destination storage device utilization if the migration operation were to be completed.

The estimates determined by management application are based on various parameters and data collected from the operating systems of the source and destination storage systems as well as commands and input from the clients. To obtain such information communication between the management application clients and storage systems may be accomplished using any of the various conventional communication protocols and or application programming interfaces APIs the details of which are not germane to the technique being introduced here. This communication can be done through the network or it can be via a direct link not shown between the management console and one or more of the storage systems .

Management Application illustrates the management application in greater detail for implementing the various processes described below according to one embodiment. Management application maintains various data structures for enabling non disruptive migration as described below in more detail.

In the illustrated embodiment the management application includes a graphical user interface GUI module to generate a GUI e.g. for use by an administrator a provisioning manager module may also be referred to as a provisioning manager for provisioning storage gathering information from the storage systems and predicting the likelihood of successfully completing a migration operation according to one embodiment one or more other management modules to perform various other storage management related functions and a communication module . In another embodiment the management application may provide a command line interface CLI for use by an administrator for managing and configuring storage systems. The communication module implements one or more conventional communication protocols and or APIs to enable the management application to communicate with the storage systems and clients .

Provisioning manager typically receives a provisioning request from a client . The provisioning manager scans a storage resource pool selects a storage space that best meets the user requirements provisions the storage space and configures the provisioned storage space for migration.

In one embodiment management application generates and maintains a migration data structure may also be referred to as data structure that may be used for configuring migration of a vfiler from one storage system to another storage system. Data structure may be a table a relational database a data object or any other logical component that may be used to implement the adaptive embodiments described herein. In one embodiment data structure may include various components A B that are now described below in detail.

In one embodiment data structure may include configuration information A for the provisioned storage. Configuration information A may include information regarding the storage devices used for migrating a vfiler. Configuration information A may include information regarding storage device type for example the storage device may be a Fibre Channel device SATA device SAS device and others. Configuration information A may also include serial numbers and firmware revision numbers for the storage devices. In one embodiment configuration information A may be a table with a plurality of fields that are populated by provisioning manager .

Configuration information A is generated and maintained by management application . The information included in configuration information A may be obtained from various sources including the storage systems clients and storage subsystems. Management application collects the information as part of a provisioning process or as a part of a configuration process for configuring the various components of the storage systems.

Data structure may also include storage system information B may also be referred to as data structure B . The storage system in this context means one or more storage systems or nodes for a cluster system that manage access to the provisioned storage. Storage system information B may be populated by management application .

Storage system information B includes processor utilization data obtained from a source storage system and a destination storage system. Storage system information B may also include information regarding a number of sequential write operations and random write operations at both the source and destination storage system before the cut over duration starts. Storage B also includes a number of sequential read operations and random read operations at the destination storage system before the cut over duration starts.

Storage system information B also includes various parameters that allow management application to estimate utilization of a destination storage device after a migration operation is completed. The various parameters are typically maintained by the operating systems of the respective storage systems and then obtained by management application . The use of storage system information B is described below in detail.

Storage system information B may also include information regarding the capabilities of the storage systems. For example storage system information B may include information regarding whether a storage system is capable of hosting a vfiler and whether the storage system is a part of a clustered system and uses a global namespace for communication.

Global namespace in this context refers to a virtual hierarchical collection of unique volume names or identifiers and directory paths to the volumes in which the volumes are stored on multiple server nodes within a clustered storage server system as described below with respect to . In the context of the present disclosure the global namespace of a clustered storage server system can be extended to include not only the identifiers of volumes stored on the multiple nodes of the clustered system but also the identifiers of volumes stored on one or more storage systems that are remote from and do not constitute a part of the clustered system.

The term volume as used herein is a logical data set which is an abstraction of physical storage combining one or more physical mass storage devices e.g. disks or parts thereof into a single logical storage object and which is managed as a single administrative unit such as a single file system. A volume is typically defined from a larger group of available storage such as an aggregate.

Storage system information B is generated and maintained by management application . The management application may obtain the foregoing information by polling the storage systems or may be stored when the storage system is configured and updated by management application .

In one embodiment although data structure is shown to have multiple components for example data structures A B each of these components may exist as separate modules or may be included in an integrated single data structure. In a distributed architecture different components of data structure may be stored in different locations and may be accessible to management application via network .

Data structure may be maintained by management application at a memory location that is accessible to management application . For example the memory location may be local to the management console that executes management application or may be remote to management console .

The management application may also maintain policies a list of all volumes in a storage pool as well as a data structure shown as free space and may also be referred to as free space with information regarding all free storage space in a storage pool. Free space may be a table database or any other data structure. Free space may be maintained by management application at a memory location that is accessible to management application . For example the memory location may be local to the management console that executes management application or may be remote to management console . Free space may be maintained and updated by management application by communicating with different storage systems via network or otherwise.

Free space may include details regarding storage space that may be available for a user at any given time. The details may include type of storage available storage space and any other information that can be used to provision and configure storage. In one embodiment management application may use policies volume list free space with access protocol information and data structure for configuring storage for migration.

In one embodiment the migration operation may be non disruptive. Non disruptive migration occurs when the first storage location i.e. A can be accessed while information is being migrated to another location i.e. B . As explained above even for non disruptive migration access to the source storage A may be restricted during cut over which is the last time slot in the migration timeline for completing a migration operation. To minimize disruption to the client the cut over duration should be short so that a client is not unduly limited to access storage. Therefore it is desirable to know if a migration operation can be completed before the migration operation enters the cut over duration. It is also desirable to know whether the destination storage B can handle post migration traffic so that client experience in accessing the migrated vfiler B is fast and efficient.

In one embodiment and as described below in more detail management application performs various executable process steps to predict the work load for both the source storage system A and the destination storage system B before the cut over duration. The management application also ascertains the projected utilization of destination storage B to ensure that the destination storage system and the destination storage devices can handle the post migration work load if the migration were to be completed.

The various adaptive embodiments of the present disclosure and the functionality of the management application are now described below with respect to the various process flow diagrams.

In block S the migration operation is initiated. During the migration operation all client data serviced by vfiler A is moved from source storage device A to destination storage device B. Configuration information associated with the vfiler A is also migrated to destination storage device B.

Source storage system A and or management application also estimate when the migration operation may enter the cut over duration. The estimate is based on the amount of information that is to be migrated and a rate at which the information is migrated during the migration operation.

In block S while the migration is taking place and before the migration operation enters the cut over duration management application estimates the likelihood of a successful migration operation. A successful migration operation is one that is completed within the cut over duration such that clients can easily send read and write requests. A successful migration operation also depends on whether destination storage device B can adequately support clients after vfiler A is migrated. Details regarding block S are provided below with respect to .

Based on the estimated likelihood of success either the migration operation is completed in block S or aborted and then re tried at a later time. If the migration operation is to be aborted management application notifies the client. In one embodiment management application notifies the client before the migration operation is aborted and provides the client with an option to override management application s decision. This allows the client to proceed with the migration operation even though management application may recommend that the migration operation be aborted.

To abort the migration operation management application sends a message to the source storage system A and the destination storage system B. Based on the message the migration operation is aborted.

In one embodiment the projected workload may be ascertained by using the following relationship Projected Processor Utilization PPU Current CPU busy SEQWriteOPs SeqFactor RndWriteOps RndFactor SeqReadOps SeqReadFactor RndReadOps RndReadFactor Equation I 

PPU is determined by management application and then compared to the threshold value designated as MaxBusy . Based on the comparison management application determines if the PPU is acceptable or unacceptable. For example when PPU is less than MaxBusy then one can assume that the storage system can handle the migration and the migration is likely to be successful. In this example the PPU is acceptable. If the PPU is greater than MaxBusy then the projected workload of the source and or destination systems will be such that the migration operation may not be successful or may be undesirably delayed. In this case the PPU is unacceptable.

 Current CPU busy is a factor that indicates how busy a processor may be before entering the cut over duration. This factor is used for both the source storage system and the destination storage system. Information regarding how busy a CPU maybe is obtained by management application from the operating systems of the source storage system and the destination storage system. In one embodiment provisioning manager makes a Zephyr Application Program Interface ZAPI call to the operating system of the source storage system A and the destination storage system B and gathers the status of the various processors used by the source and destination storage systems. The obtained information may be stored as part of storage system information B .

 SeQWriteOPs means a number of sequential write operations that the source storage system A and the destination storage system B have to process during the cut over duration. The term sequential means sequential logical block addresses LBA where data is written sequentially at storage devices A and B. The operating systems of the source storage system A and destination storage systems B maintain this information because they have to process write requests for clients. This information is obtained by provisioning manager and may be stored as part of storage system information B.

 SeqFactor is a numerical value that is used to quantify the affect of sequential write operations during cut over. The write operations may be performed by the source storage system A and or the destination storage system B.

The SeqFactor can be determined during storage system testing for example in a laboratory test environment where one can observe how sequential write operations increase or affect storage processor performance. Briefly fewer sequential operations will have less of an impact on processor utilization compared to the impact of a higher number of sequential write operations. The impact of write operations is simulated and then translated to a numerical value that is used in Equation I.

 RndWriteOPs in this context means random write operations at storage devices A and B. The term random as used herein means non sequential i.e. data is not written in sequential logical block addresses. The random write operations during cut over also impact processor utilization except the impact of random write operations may be more severe than sequential write operations. The number of random write operations is also maintained by the operating systems of both the storage system A and B respectively. This information is obtained by provisioning manager and then may be stored as part of storage system information B.

 RndFactor is a numerical factor used to quantify the affect of random write operations during cut over. The RndFactor is similar to the SeqFactor except it is used for quantifying the impact of random write operations instead of the impact of sequential write operations. This factor can also be derived in a system test environment and then stored as part of storage system information B.

 SeqReadOps means a number of sequential read operations that the source storage system A has to service during cut over. The read operations are for the original vfiler A before the migration is completed. Sequential read operations only impact the source storage system A and not the destination storage system B because clients only access vfiler A before it is migrated. The number of sequential read operations is also maintained by the operating system of source storage system A. This information can be obtained by provisioning manager and then stored as storage system information B.

 SeqReadFactor is similar to the SeqFactor except SeqReadFactor quantifies the impact of sequential read operations. This factor may also be ascertained by trial and error in a system test environment and then stored as part of storage system information B.

 RndReadOps is similar to RnDWriteOps except it quantifies the impact of random read operations compared to random write operations. The number of random read operations is also maintained by the operating system of source storage system A. This information can be obtained by provisioning manager and then stored as part of storage system information B.

 RndReadFactor is similar to Rndfactor except it is applicable to random read operations. This factor may also be ascertained by trial and error in a system test environment and then stored as part of storage system information B.

Management application uses the foregoing values factors stored as storage system information B to ascertain PPU. In one embodiment a processor executing executable instructions uses the various values factors to calculate PPU values for the source storage system A and the destination storage system B.

Referring back to in block S management application estimates destination storage B utilization if the migration operation were to be completed. One reason for this estimation is to ascertain the possible impact of the migration operation on destination storage B and its ability to service client requests.

In block S management application ascertains how much data the clients are writing to vfiler A before entering the cut over duration. During cut over information is written to both source storage A and destination storage B so that both storage systems are synchronized after the cut over duration and once the migration operation is completed management application determines how much of destination storage B is being used before cut over. Management application then ascertains the amount of work i.e. write operations that is being done on the source storage device A and projects how busy the destination storage device B would get once the write operation during the cut over duration are added to an existing work load of destination storage B.

The projected destination storage device utilization is then compared to a threshold value. The threshold value may be stored as part of configuration information A and or storage system information B.

The comparison with the threshold value is used to determine if the projected destination storage device utilization is acceptable or unacceptable. For example if projected utilization is over 75 then it may be too high and hence unacceptable. In that case the migration operation does not enter the cut over stage because the comparison provides the likelihood of the destination storage s inability to efficiently handle the post migration work load. Thereafter the migration operation is aborted.

The projected destination storage device utilization may be referred to as AggrBusy that is determined by the following equation AggrBusy CurrentAggrDiskBusy data throughput MB sec Write overhead data disk in aggr Disk factor Equation II 

 CurrentAggrDiskBusy is a numerical value that provides an indication of how busy destination storage B is before cut over. Provisioning manager obtains this information from the operating system of destination storage system B that maintains a count value of what is being written to destination storage B and how much free space may be available at destination storage B.

 Data throughput is a current amount of data that the source storage system A writes to the source storage device B. This information is also obtained by provisioning manager from the operating system of source storage system A that handles all write operations for vfiler A.

 Write overhead is a numerical factor that is used to account for any extra writing the source storage system A does for each client write operation. The extra writing may be to store any metadata for information that is written to source storage A.

 data disk in aggr is a number of storage devices or disks that the destination storage system B is using for handling the write operations during cut over. Higher the number of storage devices lesser the utilization for each storage device.

 Disk factor is a numerical value to account for different types of storage devices. For example a faster storage device will show less increase in utilization than slower disks for a same amount of workload.

Write overhead of Equation II may be determined by the following equation Write overhead RndWriteOps 6 SeqWriteOps 33 vfiler write bytes in KB sec Equation III 

The various parameters used for determining Write overhead have been described above with respect to Equation I .

Referring back to in block S management application compares the PPU value for both the source storage system and the destination storage system with a first threshold value and AggrBusy with a second threshold value. Based on the comparison management application automatically predicts the likelihood of success for the migration operation.

The foregoing process steps for migrating a vfiler may be implemented in a cluster environment that is now described below in detail with respect to .

As described above management application may interface with storage systems and client systems for performing the various process blocks of . The storage systems may be a part of a cluster or clustered storage system that is now described below in detail with respect to .

Cluster includes a plurality of interconnected nodes configured to provide storage services related to information stored at a plurality storage devices . Management application communicates with the plurality of nodes for generating and maintaining the data structures described above with respect to . A global namespace is used to uniquely identify cluster system . The global namespace information may be a part of data structure and may be used for migrating information from one storage volume to another storage volume within the cluster.

Nodes comprise various functional components that cooperate to provide distributed storage system architecture of cluster . Each node is generally organized as a network element N module and a disk element D module . N module includes functionality that enables node to connect to clients over a computer network and to management application while each D module connects to one or more storage devices may generically be referred to as disks or storage array similar to storage subsystem and may also be referred to as disk array . In one embodiment information regarding the storage devices may be collected by the D Module and then communicated to the management application by the N module.

Nodes may be interconnected by a cluster switching fabric which in the illustrative embodiment may be embodied as a Gigabit Ethernet switch. It should be noted that while there is shown an equal number of N and D modules in the illustrative cluster there may be differing numbers of N and or D modules in accordance with various embodiments of the present invention. For example there may be a plurality of N modules and or D modules interconnected in a cluster configuration that does not reflect a one to one correspondence between the N and D modules. As such the description of a node comprising one N module and one D module should be taken as illustrative only.

Clients similar to clients may be general purpose computers having a plurality of components. These components may include a central processing unit CPU main memory I O devices and storage devices for example flash memory hard drives and others . The main memory may be coupled to the CPU via a system bus or a local memory bus. The main memory may be used to provide the CPU access to data and or program information that is stored in main memory at execution time. Typically the main memory is composed of random access memory RAM circuits.

Clients may request management services from management application . In one embodiment clients may request migration of a vfiler A as described above.

Clients may be configured to interact with the node in accordance with a client server model of information delivery. That is each client may request the services of the node and the node may return the results of the services requested by the client by exchanging packets over the network .

The client may issue packets using application including file based access protocols such as the CIFS protocol or the NFS protocol over TCP IP when accessing information in the form of certain data containers. Data container means a block a file a logical unit of data or any other information. CIFS means the Common Internet File System Protocol an access protocol that client systems use to request file access services from storage systems over a network. NFS means Network File System a protocol that allows a user to access storage over a network.

Alternatively the client may issue packets using application including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over FCP when accessing information in the form of other data containers such as blocks.

A switched virtualization layer including a plurality of virtual interfaces VIFs is provided below the interface between the respective N module and the client systems allowing the storage associated with the nodes to be presented to the client systems as a single shared storage pool.

The clustered storage system can be organized into any suitable number of vfilers in which each vfiler represents a single storage system namespace with independent network access. Each vfiler has a user domain and a security domain that are separate from the user and security domains of other vfilers. Moreover each vfiler is associated with one or more VIFs and can span one or more physical nodes each of which can hold one or more VIFs and storage associated with one or more vfilers. Client systems can access the data on a vfiler from any node of the clustered system but only through the VIFs associated with that vfiler.

A vfiler can be migrated from one node to another in cluster as described above. Management application performs the executable process steps of for estimating a likelihood of success for a migration operation.

The cluster access adapter comprises a plurality of ports adapted to couple node to other nodes of cluster . In the illustrative embodiment Ethernet may be used as the clustering protocol and interconnect media although it will be apparent to those skilled in the art that other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternate embodiments where the N modules and D modules are implemented on separate storage systems or computers the cluster access adapter is utilized by the N D module for communicating with other N D modules in the cluster .

Each node is illustratively embodied as a dual processor storage system executing a storage operating system that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks on storage devices . However it will be apparent to those of ordinary skill in the art that the node may alternatively comprise a single or more than two processor systems. Illustratively one processor A executes the functions of the N module on the node while the other processor B executes the functions of the D module .

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing programmable instructions and data structures. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the programmable instructions and manipulate the data structures. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the invention described herein.

The storage operating system portions of which is typically resident in memory and executed by the processing elements functionally organizes the node by inter alia invoking storage operations in support of the storage service implemented by the node. An example of operating system is the DATA ONTAP Registered trademark of NetApp Inc. operating system available from NetApp Inc. that implements a Write Anywhere File Layout WAFL Registered trademark of NetApp Inc. file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such where the term ONTAP is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.

The network adapter comprises a plurality of ports adapted to couple the node to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Illustratively the computer network may be embodied as an Ethernet network or a FC network. Each client may communicate with the node over network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system executing on the node to access information requested by the clients and management application . The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory flash memory devices micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on the disks of storage array . The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC link topology.

Data structure may include different components. For example data structure may include CurrentCPUbusy data A SeqWriteOps B RndWriteOps C SeqReadOps D RndReadOps E CurrentAgrDiskBusy F data throughput G and data diskin aggr H. Other data may include any other information for example Disk factor that is used by management application to perform the various process steps described above. These factors have been described above with respect to . Operating system provides this information to management application for estimating processor utilization and storage device utilization in blocks S and S of .

In one example operating system may include several modules or layers executed by one or both of N Module and D Module . These layers include a file system manager that keeps track of a directory structure hierarchy of the data stored in storage devices and manages read write operations i.e. executes read write operations on disks in response to client requests.

Operating system may also include a protocol layer and an associated network access layer to allow node to communicate over a network with other systems such as clients and management application . Protocol layer may implement one or more of various higher level network protocols such as NFS CIFS Hypertext Transfer Protocol HTTP TCP IP and others as described below.

Network access layer may include one or more drivers which implement one or more lower level protocols to communicate over the network such as Ethernet. Interactions between clients and mass storage devices are illustrated schematically as a path which illustrates the flow of data through operating system .

The operating system may also include a storage access layer and an associated storage driver layer to allow D module to communicate with a storage device. The storage access layer may implement a higher level disk storage protocol such as RAID redundant array of inexpensive disks while the storage driver layer may implement a lower level storage device access protocol such as FC or SCSI.

It should be noted that the software path through the operating system layers described above needed to perform data storage access for a client request received at node may alternatively be implemented in hardware. That is in an alternate embodiment of the disclosure the storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an ASIC. This type of hardware implementation increases the performance of the file service provided by node in response to a file system request issued by client .

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this disclosure can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and a disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present invention may be utilized with any suitable file system including a write in place file system.

The processing system includes one or more processors and memory coupled to a bus system . The bus system shown in is an abstraction that represents any one or more separate physical buses and or point to point connections connected by appropriate bridges adapters and or controllers. The bus system therefore may include for example a system bus a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus sometimes referred to as Firewire .

The processors are the central processing units CPUs of the processing system and thus control its overall operation. In certain embodiments the processors accomplish this by executing programmable instructions stored in memory . A processor may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices.

Memory represents any form of random access memory RAM read only memory ROM flash memory or the like or a combination of such devices. Memory includes the main memory of the processing system . Instructions which implements the migration techniques introduced above e.g. the management application in may reside in and may be executed by processors from memory .

Also connected to the processors through the bus system are one or more internal mass storage devices and a network adapter . Internal mass storage devices may be or may include any conventional medium for storing large volumes of data in a non volatile manner such as one or more magnetic or optical based disks. The network adapter provides the processing system with the ability to communicate with remote devices e.g. storage servers over a network and may be for example an Ethernet adapter a FC adapter or the like. The processing system also includes one or more input output I O devices coupled to the bus system . The I O devices may include for example a display device a keyboard a mouse etc.

Thus a method and apparatus for vfiler migration have been described. Note that references throughout this specification to one embodiment or an embodiment mean that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Therefore it is emphasized and should be appreciated that two or more references to an embodiment or one embodiment or an alternative embodiment in various portions of this specification are not necessarily all referring to the same embodiment. Furthermore the particular features structures or characteristics being referred to may be combined as suitable in one or more embodiments of the invention as will be recognized by those of ordinary skill in the art.

While the present disclosure is described above with respect to what is currently considered its preferred embodiments it is to be understood that the disclosure is not limited to that described above. To the contrary the disclosure is intended to cover various modifications and equivalent arrangements within the spirit and scope of the appended claims.

