---

title: Touch inputs interacting with user interface items
abstract: Techniques for managing user interactions with items on a user interface are disclosed. In one aspect, a representation of an opening is presented in response to touch input. A display object is moved over the opening, and the display object is processed in response to the moving. In another aspect, touch input pinching two opposite corners of a display object followed by touch input flicking the display object is received and the display object is deleted in response to the inputs. In another aspect, touch input centered over a display object is received and the display object is deleted in response to the input. In another aspect, touch input corresponding to swiping gestures are received and a display object is securely deleted in response to the gestures.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08769443&OS=08769443&RS=08769443
owner: Apple Inc.
number: 08769443
owner_city: Cupertino
owner_country: US
publication_date: 20100211
---
This subject matter is generally related to user interactions with items displayed on a user interface of a device.

Graphical user interfaces present display objects to users. Each display object can represent a system object. For example a display object can be an icon that represents a file stored on a device. Users can interact with the display objects in various ways. Users can select e.g. with a mouse or other input device display objects to invoke functionality associated with the display objects. For example a user can select a file to cause a system window displaying the contents of the file to be presented in the user interface.

Users can also drag display objects around the user interface. For example a user can drag and drop a display object representing a file over a display object representing a folder. In response the file can be moved into the folder.

Techniques and systems supporting user interactions with items displayed on a user interface of a device are disclosed.

In one aspect touch input to a device is received from a user and the device presents a graphical representation of an opening in response to the touch input. Second touch input moving a display object over the opening is received and the device processes the display object in response to the moving. In another aspect a device receives touch input pinching two opposite corners of a display object presented on a user interface followed by touch input flicking the display object. The device deletes the display object in response to the inputs. In yet another aspect a device receives touch input centered over a display object presented on a user interface and deletes the display object in response to the input. In another aspect a device receives touch input corresponding to a swiping gestures made with two or more fingers of a single hand. The device determines that the swiping gestures are made over a display object presented in the user interface and deletes the display object in response to the gestures.

Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. Users can be provided with a more intuitive environment for interacting with items displayed on a user interface. Users can use real world analogies such as holes windows and doors to indicate desired processes to be applied to items. Users can use real world analogies such as shrinking and flicking an item or drawing an X or other symbol over an item to indicate that an item should be deleted. Users can use real word analogies such as a shredding motion to indicate that an item should be securely deleted.

In some implementations device includes touch sensitive display . Touch sensitive display can implement liquid crystal display LCD technology light emitting polymer display LPD technology or some other display technology. Touch sensitive display can be sensitive to haptic and or tactile contact with a user. In some implementations touch sensitive display is also sensitive to touch inputs received in proximity to but not actually touching display . In addition device can include a touch sensitive surface e.g. a trackpad or touchpad .

In some implementations touch sensitive display can include a multi touch sensitive display. A multi touch sensitive display can for example process multiple simultaneous points of input including processing data related to the pressure degree and or position of each point of input. Such processing facilitates gestures and interactions with multiple fingers chording and other interactions. Other touch sensitive display technologies can also be used e.g. a display in which contact is made using a stylus or other input tool.

A user can interact with device using various touch inputs e.g. when a user touches touch sensitive display . Gesture inputs can also be derived from multiple touch inputs e.g. where a user moves his or her finger or other input tool across touch sensitive display . An example gesture input is a swipe input where a user swipes his or her finger or other input tool across touch sensitive display . In some implementations device can detect inputs that are received in direct contact with display or that are received within a particular vertical distance of display e.g. within one or two inches of display . Users can simultaneously provide input at multiple locations on display . For example inputs simultaneously touching at two or more locations can be received.

In some implementations device can implement various device functionalities. As part of one or more of these functionalities device presents graphical user interfaces e.g. graphical user interface on touch sensitive display of device and also responds to touch input received from a user for example through touch sensitive display .

In some implementations the graphical user interface can include one or more display objects e.g. display objects and . Each display object is a graphical representation of a system object. For example a display object can be an icon corresponding to a system object or a system window displaying the contents of a system object e.g. the contents of a file or directory . Example system objects include device functions applications windows files directories alerts events or other identifiable system objects. In some implementations the display objects can be configured by a user e.g. a user may specify which display objects are displayed and or may download additional applications or other software that provides other functionalities and corresponding display objects.

A user can interact with the display objects using various touch inputs. For example users can use touch inputs to create openings in user interface and then move objects over the openings The device can then process the objects in response to the inputs. As another example users can use touch inputs to squeeze and flick objects to indicate that the objects should be deleted or can use touch inputs to draw an X or other symbol through objects to indicate that the objects should be deleted. As yet another example users can use touch inputs to mimic the action of a shredder indicating that objects should be securely deleted. Each of these scenarios will be described in more detail below.

In response to one or more touch inputs user interface displays a graphical representation of an opening in the user interface.

In some implementations user interface also presents a graphical representation of dirt or other material next to opening . This dirt or other material represents material that was removed to create the opening. The user interface can present an animation of more material appearing next to opening in response to each touch input . For example the user interface can present an animation of dirt flying out of opening and into the pile of material.

While opening illustrated in is shown as a depression in the screen of device other representations of opening can also be used. For example opening can be represented as a hole in the ground. The hole can be a graphical representation of a hole in dirt or rock. A pile of dirt or rocks can be displayed on the user interface next to the opening. As another example opening can be represented by a tunnel. The bottom of the tunnel can be illustrated as connecting to another location for example another device in communication with device . The sides of the tunnel can be illustrated as being made out of plastic metal or another material. As yet another example opening can be represented as a hole in device itself. The hole can be represented as having sides and a bottom that are made up of the inner components of an electronic device for example wiring and circuit boards. Other representations can also be used. For example opening can be represented as a black hole in space or as a worm hole in space.

As illustrated in once opening is displayed a user provides additional touch input selecting display object and dragging and dropping display object over opening . In response to this input device processes display object . Device can process display object in various ways. For example device can secure the system object corresponding to display object move the system object corresponding to display object to a new directory or copy the system object corresponding to display object to another device. Each of these actions is described in more detail below with reference to .

In response to input user interface presents a graphical representation corresponding to the pattern. The graphical representation can be for example a graphical representation of a portal. A portal is a way of moving between two points for example a door a window a trap door a tunnel a sliding panel or a gate.

A user can provide another touch input to open trap door . In this input is illustrated by swiping gesture along the y axis. However a user can alternatively use a swiping gesture along the x axis a lifting gesture along the z axis or a gesture along multiple axes. In response to input user interface presents a graphical representation of an opening. The opening corresponds to an open trap door.

As illustrated in a user provides additional touch input dragging and dropping display object over opening . In response to this input device processes the display object . Device can process display object in various ways. For example device can copy the system object corresponding to display object to another device make a backup of the system object corresponding to display object or secure the system object corresponding to display object . Each of these actions is described in more detail below with reference to .

The device can present a display object on a user interface . The user interface can be for example the user interface described above with reference to . The display object corresponds to a system object. For example the display object can be an icon representing a file or directory or a system window displaying the contents of a file or directory.

The device can receive a first touch input corresponding to a digging gesture at a location on the device user interface . Example digging gestures are described above with reference to .

The first touch input can be one or more touch input events received from a touch services module executing on the device. The touch services module is described in more detail below with reference to .

The first touch input can be represented as a single touch input event corresponding to touch inputs received at multiple locations on the drafting user interface e.g. simultaneously. Alternatively the first touch input can be represented as multiple touch input events. The device can determine that the multiple touch input events correspond to a single touch input for example if the time delay between the touch inputs is less than a threshold amount of time or if one touch input event still persists when another is touch input event is received.

The device can present a graphical representation of an opening at the location on the user interface where the first touch input was received . For example the graphical representation can be a hole corresponding to a depression in the user interface a hole in the ground a hole in the device a worm hole a black hole or a tunnel as described above with reference to . Representations of other openings can also be used.

The device can determine the appropriate graphical representation to present from device settings stored on the device. For example the device settings can specify that a particular representation should be used. Alternatively or additionally the device can determine the appropriate graphical representation to present from an orientation of the device. For example if the device is in a vertical position such that the screen of the device is perpendicular to the ground the appropriate representation can be a black hole or a worm hole. As another example if the device is in a horizontal position such that the screen of the device is parallel to the ground the appropriate representation can be a hole or a tunnel. The device can also determine the appropriate representation according to an orientation of the device relative to other devices. For example if the device is positioned above another device the appropriate representation can be a tunnel connecting to the other device.

The device can also determine the appropriate size of the representation from the number of touch inputs received as described above with reference to .

The device can receive second input moving the display object over the representation of the opening . The second input can be for example touch input dragging and dropping the display object over the representation as described above with reference to . Alternatively or additionally the display objects can be free to move around the device and the second input can be input tilting the device to cause the display object to move over the opening. The second input can also be touch input flicking the display object over the opening.

The device can process the display object in response to the second input . The device can process the display object in various ways. In some implementations the device can secure the display object in response to the second input. Securing the display object can involve securing the system object represented by the display object. For example if the display object represents a file or directory the device can create additional protection for the file or directory. This additional protection can be encrypting the file or directory for example using conventional encryption techniques. The additional protection can also include making backup copies of the file or directory and optionally encrypting each copy. Making backup copies of the file or directory makes it more likely that the contents of the file or directory will be preserved even if one copy of the file or directory is corrupted e.g. by a disk failure on the device. The additional protection can also include moving the system object to a particular location in the device file system for example a particular directory with particular permission settings. Other additional protections for example obfuscating the file name of the system object can also be used.

In some implementations the device can create a directory in the device file system that corresponds to the opening and move the system object corresponding to the display object into the directory in response to the second input. The device can prompt the user to enter a name and location for the directory once the first touch inputs are received and use the information received from the user to name the directory.

In some implementations the device can cause a copy of the system object corresponding to the display object to be sent to another device. The device can select a particular device to which to send the copy of the system object to as follows. First the device can broadcast a request for responses from devices in its vicinity. Each response can include a location of the responding device. The device can process any received responses to determine that a particular device is below the device and identify that particular device as the device to which the file should be sent. Once the device identifies the particular device the device can establish a communication link with the particular device for example using conventional device communication techniques. The device can then cause a copy of the system object to be sent through the communication link. The device can establish the communication link in response to the first touch input or in response to the second input.

The device can determine the appropriate way to process the display object from device settings stored on the device. Alternatively or additionally the device can determine the appropriate way to process the display object from the position of the device relative to another device. For example if the system detects that the other device is below the device the system can send a copy of the system object to the other device. Otherwise the device can process the display object in a different way.

In some implementations the device closes the opening in response to third touch input received from a user. The third touch input can be a sweeping gesture for example as described above with reference to .

The device can present a display object on a device user interface . The user interface can be for example the user interface described above with reference to . The display object can correspond to a system object. For example the display object can be an icon representing a file or directory stored on device or the display object can be a user interface window displaying the contents of a file or directory stored on device .

The device can receive first touch input drawing a pattern at a location on the device user interface . The first touch input can be one or more touch input events received from a touch services module executing on the device.

The device can optionally present a first graphical representation corresponding to the pattern . The graphical representation can be for example a portal. Example portals include windows doors trap doors and sliding panels as described above with reference to . The first graphical representation is presented at the location where the first touch input is received.

The device can determine the appropriate representation from settings stored on the user device. For example the device can store data that indicates that certain patterns correspond to certain graphical representations and display the graphical representation corresponding to the pattern. For example if the device stores data indicating that a square corresponds to a window and a rectangle corresponds to a trap door the device can display a graphical representation of a window if the pattern best fits a square and can display a graphical representation of a trap door if the pattern best fits a rectangle.

Alternatively or additionally the device can determine the appropriate representation from an orientation of the device. For example if the device is in a vertical position such that the screen of the device is perpendicular to the ground the appropriate representation can be a window or a door. As another example if the device is in a horizontal position such that the screen of the device is parallel to the ground the appropriate representation can be a trap door.

The device can also determine the appropriate size of the first representation from the first touch input. For example the size of the first representation can correspond to the size of the pattern indicated by the first input.

The device can receive second touch input at the location where the first touch input was received . For example the device can receive second touch input corresponding to a swiping gesture in an x or y direction or a lifting gesture in a z direction.

The system displays a second graphical representation corresponding to an opening in response to the second touch input for example as described above with reference to .

The system receives third input moving the display object over the opening . The third input can be for example touch input dragging and dropping the display object over the representation as described above with reference to . Alternatively or additionally the third input can be input tilting the device or input flicking the display object as described above with reference to .

The device processes the display object in response to the third input . The device can process the display object in various ways. In some implementations the device can secure the system object corresponding to the display object as described above with reference to . In some implementations the device can send a copy of the system object corresponding to the display object to another device as described above with reference to . In some implementations the device can cause the state of the system object corresponding to the display object to be captured by a backup program executing on the device.

In some implementations the device determines the appropriate way to process the display object from device settings stored on the device. In some implementations the device determines the appropriate way to process the display object from the position of the device relative to another device. For example if the system detects that the other device is below the device the system can send a copy of the system object to the other device. Otherwise the device can process the display object in a different way.

In some implementations the way the device processes the display object corresponds to the representation of the opening that is presented on the display. Different representations can have different associated actions. For example a graphical representation of a window can be associated with sending the system object corresponding to the display object to a backup program and a graphical representation of a trap door can be associated with sending a copy of the system object to another device or securing the system object.

In some implementations the device closes the opening in response to fourth touch input received from a user for example as described above with reference to .

The device can present a display object on a device user interface for example as described above with reference to . The device can receive first touch input at a location on the device user interface . The first touch input can be for example a digging gesture as described above with reference to . Alternatively the first touch input can be a compound input corresponding to input drawing a pattern and input interacting with a graphical representation of the pattern for example as described above with reference to . Alternatively the first touch input can be an input drawing a pattern.

The device can present a graphical representation of an opening at the location on the device user interface in response to the first touch input . The graphical representation of opening is space in something or space between two things. Example openings include holes tunnels and portals as described above with reference to . In some implementations if the first touch input is input drawing a pattern the device can present a graphical representation of an open portal without first requiring the input interacting with the portal.

The device can determine the appropriate representation from the first input that was received. For example if the first input is a digging gesture a hole or tunnel can be selected as the appropriate representation. As another example if the first input draws a pattern a portal can be selected as the appropriate representation. The system can also determine the appropriate representation from device settings and or an orientation of the device as described above with reference to .

The device can receive second touch input moving the display object over the opening . The device can process the display object in response to the second touch input . In general the system can process the display object by processing a system object associated with the display object. Example processing is described above with reference to .

In some implementations the device can receive input putting the device into a particular state before receiving the first and second touch inputs. The particular state causes the device to take the actions described above in response to the touch inputs described above.

User interface presents display objects and . A user can delete one of the display objects using a sequence of touch inputs to device . For example a user can delete display object as follows. First as illustrated in the user can provide touch input pinching two opposite corners of display object . In response to this input user interface can present an animation showing display object becoming smaller e.g. shrinking.

Then as illustrated in the user can provide another touch input corresponding to a gesture flicking object across user interface . The user interface can present an animation corresponding to object moving across the screen in response to touch input . An example animation is illustrated in where display object is shown moving across the user interface . Display object moves from its initial position to position to position and finally to position before it disappears from user interface .

In some implementations the animation incorporates one or more laws of physics. For example display object can continue to increase in velocity as it moves across user interface to simulate an acceleration of the object. As another example the display object can slow down as it moves across user interface to simulate the effects of friction. The animation can also indicate that the file is being deleted. For example the animation can show display object exploding as it moves across user interface or the animation can show display object becoming increasingly transparent as it moves across user interface .

In response to touch input the device also deletes the system object corresponding to display object . In some implementations the system performs a regular deletion operation. In some implementations the system performs a secure deletion operation. These operations are described in more detail below with reference to .

The device can present a display object on a user interface for example as described above with reference to . The device can receive touch input pinching two opposite corners of the display object followed by touch input flicking the file representation for example as described above with reference to . Each touch input can be one or more touch input events received from a touch services module executing on the device. The touch services module is described in more detail below with reference to .

The device can delete the display object in response to the first and second touch inputs . The device can require the second touch input to follow the first touch input with no intervening inputs and within an amount of time that is less than a pre determined threshold before it performs the deletion.

Deleting the display object can include deleting the system object represented by the display object. The device can perform either a secure deletion or a regular deletion of the system object. A regular deletion marks the portions of a computer readable medium storing the data for the system object as writeable but does not necessarily overwrite the data. A secure deletion overwrites the system object with alternating patterns of data that make it difficult to recover the file. An example standard for secure deletion is the U.S. Department of Defense standard 5220.22 M. The secure deletion can also delete all backup copies of the system object that are maintained by the device.

In some implementations the device presents a dialog box to the user to prompt the user to specify that either a secure deletion or a regular deletion should be performed and then performs the type of deletion specified by the user. In other implementations the device securely deletes or regularly deletes the system object according to device settings.

The device can require a user to first place the device into a deletion mode before performing the deletion. A user can place the device into a deletion mode using touch input. For example the user can provide touch input touching at a location of the user interface where no display objects are presented. If the input persists for a period of time that exceeds a threshold the device can be placed into a deletion state. Once in the deletion state the device can delete a display object in response to touch inputs as described above.

User interface presents display objects and . A user can delete one of the display objects using a sequence of touch inputs. For example a user can delete display object as follows. The user first provides input placing the device into a deletion state. The user then provides touch input drawing an symbol that is centered over display object .

Touch input consists of a first gesture drawing a first diagonal line through display object and a second gesture drawing a second diagonal line through display object . The second diagonal line is substantially perpendicular to the first diagonal line. Together the two gestures and form an X that is centered over the display object .

In response to touch input device deletes display object . The deletion can be a secure deletion or a regular deletion as described in more detail below with reference to . The user interface can present an animation corresponding to the deletion of display object . For example display object can be animated as shrinking and then disappearing as exploding or as becoming increasingly transparent.

The device can present a display object at a first location on a user interface . The user interface can be for example the user interface described above with reference to .

The device can optionally receive a first input and activate a deletion state of the device in response to the input . The first input can be touch input received as a touch event from a touch model. For example the first input can be generated when a user presses and holds in a single location on the user interface for more than a threshold amount of time.

The device receives second touch input centered at a second location on the user interface . The second touch input can correspond to a symbol for example an X or a circle with a diagonal line through it.

For example the device can receive touch input corresponding to an X when a user makes a first gesture along a first diagonal line and a second gesture along a second diagonal line that is substantially perpendicular to the first diagonal line as described above with reference to . The X is centered where the two gestures overlap. The device can require the two gestures to be received within a predetermined period of time of each other before they are interpreted as an X.

The device can determine in response to the second input that the first location overlaps with the second location . The first location overlaps with the second location if the two locations are identical or are within a threshold distance of each other.

The system can delete the display object presented at the first location in response to the determination . The system can perform a secure deletion or a regular deletion. In some implementations the system determines whether to perform a secure deletion or a regular deletion according to input received from a user for example as described above with reference to .

User interface presents display object corresponding to a system window presenting the contents of a file. A user can securely delete the display object by providing touch input with multiple e.g. two or more fingers of a single hand. The touch input corresponds to a swiping gesture made substantially simultaneously with each finger used to provide input. The swiping gesture is made over display object .

For illustrative purposes touch input is illustrated in as corresponding to three fingers providing a swiping input in the y direction. However a different number of fingers and or a different swiping direction could also be used.

In response to touch input the device securely deletes the display object by securely deleting the system object corresponding to the display object. The user interface can also present an animation indicating that the display object is being deleted. For example user interface can present an animation showing display object being torn into multiple strips. The number of strips can correspond to the number of points of input and the direction of the strips can correspond to the direction of touch input . The user interface can also remove display object as part of the deletion.

The device can present a display object at a first location on a user interface for example as described above with reference to . The device can receive touch input at a second location on the user interface . The touch input can correspond to swiping gestures made with two or more fingers of a single hand and made substantially simultaneously for example as described above with reference to .

The device can determine that the first location overlaps with the second location . The device can determine that the first location overlaps with the second location if the two locations are identical or are within a threshold distance of each other for example as described above with reference to . The device can perform a secure deletion of the display object in response to the determination . Secure deletions are described in more detail above with reference to .

In some implementations the device can require a user to first place the device into a shredding mode before performing the secure deletion. A user can place the device into a shredding mode using touch input. For example the user can provide touch input touching at a location of the user interface where no display objects are presented. If the input persists for a period of time that exceeds a threshold the device can be placed into a shredding state. Once in the deletion state the device can securely delete a display object in response to touch inputs as described above.

Software architecture can include operating system touch services module and object management engine . This architecture can conceptually operate on top of a hardware layer not shown .

Operating system provides an interface to the hardware layer e.g. a capacitive touch display or device . Operating system can include one or more software drivers that communicate with the hardware. For example the drivers can receive and process touch input signals generated by a touch sensitive display or device in the hardware layer. The operating system can process raw input data received from the driver s . This processed input data can then made available to touch services layer through one or more application programming interfaces APIs . These APIs can be a set of APIs that are usually included with operating systems such as for example Linux or UNIX APIs as well as APIs specific for sending and receiving data relevant to touch input.

Touch services module can receive touch inputs from operating system layer and convert one or more of these touch inputs into touch input events according to an internal touch event model. Touch services module can use different touch models for different applications for example depending on a state of the device.

The touch input events can be in a format that is easier to use in an application than raw touch input signals generated by the touch sensitive device. For example a touch input event can include a set of coordinates for each location at which a touch is currently occurring on a drafting user interface. Each touch input event can include information on one or more touches occurring simultaneously.

In some implementations gesture touch input events can also be detected by combining two or more touch input events. The gesture touch input events can contain scale and or rotation information. The rotation information can include a rotation value that is a relative delta in degrees. The scale information can also include a scaling value that is a relative delta in pixels on the display device. Other gesture events are possible.

All or some of these touch input events can be made available to developers through a touch input event API. The touch input API can be made available to developers as a Software Development Kit SDK or as part of an application e.g. as part of a browser tool kit .

Object management engine receives touch inputs from the touch services module and processes the input events for example as described above with reference to .

Sensors devices and subsystems can be coupled to peripherals interface to facilitate multiple functionalities. For example motion sensor light sensor and proximity sensor can be coupled to peripherals interface to facilitate various orientation lighting and proximity functions. For example in some implementations light sensor can be utilized to facilitate adjusting the brightness of touch screen . In some implementations motion sensor e.g. an accelerometer velocimeter or gyroscope can be utilized to detect movement of the device. Accordingly display objects and or media can be presented according to a detected orientation e.g. portrait or landscape.

Other sensors can also be connected to peripherals interface such as a temperature sensor a biometric sensor or other sensing device to facilitate related functionalities.

Location determination functionality can be facilitated through positioning system . Positioning system in various implementations can be a component internal to device or can be an external component coupled to device e.g. using a wired connection or a wireless connection . In some implementations positioning system can include a GPS receiver and a positioning engine operable to derive positioning information from received GPS satellite signals. In other implementations positioning system can include a compass e.g. a magnetic compass and an accelerometer as well as a positioning engine operable to derive positioning information based on dead reckoning techniques. In still further implementations positioning system can use wireless signals e.g. cellular signals IEEE 802.11 signals to determine location information associated with the device Hybrid positioning systems using a combination of satellite and television signals such as those provided by ROSUM CORPORATION of Mountain View Calif. can also be used. Other positioning systems are possible.

Broadcast reception functions can be facilitated through one or more radio frequency RF receiver s . An RF receiver can receive for example AM FM broadcasts or satellite broadcasts e.g. XM or Sirius radio broadcast . An RF receiver can also be a TV tuner. In some implementations RF receiver is built into wireless communication subsystems . In other implementations RF receiver is an independent subsystem coupled to device e.g. using a wired connection or a wireless connection . RF receiver can receive simulcasts. In some implementations RF receiver can include a Radio Data System RDS processor which can process broadcast content and simulcast data e.g. RDS data . In some implementations RF receiver can be digitally tuned to receive broadcasts at various frequencies. In addition RF receiver can include a scanning function which tunes up or down and pauses at a next frequency where broadcast content is available.

Camera subsystem and optical sensor e.g. a charged coupled device CCD or a complementary metal oxide semiconductor CMOS optical sensor can be utilized to facilitate camera functions such as recording photographs and video clips.

Communication functions can be facilitated through one or more communication subsystems . Communication subsystem s can include one or more wireless communication subsystems and one or more wired communication subsystems. Wireless communication subsystems can include radio frequency receivers and transmitters and or optical e.g. infrared receivers and transmitters. Wired communication system can include a port device e.g. a Universal Serial Bus USB port or some other wired port connection that can be used to establish a wired connection to other computing devices such as other communication devices network access devices a personal computer a printer a display screen or other processing devices capable of receiving and or transmitting data. The specific design and implementation of communication subsystem can depend on the communication network s or medium s over which device is intended to operate. For example device may include wireless communication subsystems designed to operate over a global system for mobile communications GSM network a GPRS network an enhanced data GSM environment EDGE network 802.x communication networks e.g. Wi Fi WiMax or G networks code division multiple access CDMA networks and a Bluetooth network. Communication subsystems may include hosting protocols such that Device may be configured as a base station for other wireless devices. As another example the communication subsystems can allow the device to synchronize with a host device using one or more protocols such as for example the TCP IP protocol HTTP protocol UDP protocol and any other known protocol.

Audio subsystem can be coupled to speaker and one or more microphones . One or more microphones can be used for example to facilitate voice enabled functions such as voice recognition voice replication digital recording and telephony functions.

I O subsystem can include touch screen controller and or other input controller s . Touch screen controller can be coupled to touch screen . Touch screen and touch screen controller can for example detect contact and movement or break thereof using any of a number of touch sensitivity technologies including but not limited to capacitive resistive infrared and surface acoustic wave technologies as well as other proximity sensor arrays or other elements for determining one or more points of contact with touch screen or proximity to touch screen .

Other input controller s can be coupled to other input control devices such as one or more buttons rocker switches thumb wheel infrared port USB port and or a pointer device such as a stylus. The one or more buttons not shown can include an up down button for volume control of speaker and or microphone .

In one implementation a pressing of the button for a first duration may disengage a lock of touch screen and a pressing of the button for a second duration that is longer than the first duration may turn power to device on or off. The user may be able to customize a functionality of one or more of the buttons. Touch screen can for example also be used to implement virtual or soft buttons and or a keyboard.

In some implementations device can present recorded audio and or video files such as MP3 AAC and MPEG files. In some implementations device can include the functionality of an MP3 player such as an iPhone 

Memory interface can be coupled to memory . Memory can include high speed random access memory and or non volatile memory such as one or more magnetic disk storage devices one or more optical storage devices and or flash memory e.g. NAND NOR . Memory can store operating system such as Darwin RTXC LINUX UNIX OS X WINDOWS or an embedded operating system such as VxWorks. Operating system may include instructions for handling basic system services and for performing hardware dependent tasks. In some implementations operating system can be a kernel e.g. UNIX kernel .

Memory may also store communication instructions to facilitate communicating with one or more additional devices one or more computers and or one or more servers. Communication instructions can also be used to select an operational mode or communication medium for use by the device based on a geographic location obtained by GPS Navigation instructions of the device. Memory may include graphical user interface instructions to facilitate graphic user interface processing sensor processing instructions to facilitate sensor related processing and functions phone instructions to facilitate phone related processes and functions electronic messaging instructions to facilitate electronic messaging related processes and functions web browsing instructions to facilitate web browsing related processes and functions media processing instructions to facilitate media processing related processes and functions GPS Navigation instructions to facilitate GPS and navigation related processes and instructions e.g. mapping a target location camera instructions to facilitate camera related processes and functions and or other software instructions to facilitate other processes and functions e.g. security processes and functions device customization processes and functions based on predetermined user preferences and other software functions. Memory may also store other software instructions not shown such as web video instructions to facilitate web video related processes and functions and or web shopping instructions to facilitate web shopping related processes and functions. In some implementations media processing instructions are divided into audio processing instructions and video processing instructions to facilitate audio processing related processes and functions and video processing related processes and functions respectively.

Each of the above identified instructions and applications can correspond to a set of instructions for performing one or more functions described above. These instructions need not be implemented as separate software programs procedures or modules. Memory can include additional instructions or fewer instructions. Furthermore various functions of device may be implemented in hardware and or in software including in one or more signal processing and or application specific integrated circuits.

Devices and can also establish communications by other means. For example wireless device can communicate with other wireless devices e.g. other devices or cell phones etc. over wireless network . Likewise devices and can establish peer to peer communications e.g. a personal area network by use of one or more communication subsystems such as a Bluetooth communication device. Other communication protocols and topologies can also be implemented.

Devices or can for example communicate with one or more services over one or more wired and or wireless networks . These services can include for example animation service object management service and touch model service . Animation service generates the animations described above when display objects are moved deleted and securely deleted. Object management service determines how to process display objects and their corresponding system objects for example as described above with reference to . Touch model service provides the touch model features described above with reference to .

Device or can also access other data and content over one or more wired and or wireless networks . For example content publishers such as news sites RSS feeds web sites blogs social networking sites developer networks etc. can be accessed by Device or . Such access can be provided by invocation of a web browsing function or application e.g. a browser in response to a user touching for example a Web object.

The features described can be implemented in digital electronic circuitry or in computer hardware firmware software or in combinations of them. The features can be implemented in a computer program product tangibly embodied in an information carrier e.g. in a machine readable storage device for execution by a programmable processor and method steps can be performed by a programmable processor executing a program of instructions to perform functions of the described implementations by operating on input data and generating output. Alternatively or addition the program instructions can be encoded on a propagated signal that is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information from transmission to suitable receiver apparatus for execution by a programmable processor.

The described features can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from and to transmit data and instructions to a data storage system at least one input device and at least one output device. A computer program is a set of instructions that can be used directly or indirectly in a computer to perform a certain activity or bring about a certain result. A computer program can be written in any form of programming language e.g. Objective C Java including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment.

Suitable processors for the execution of a program of instructions include by way of example both general and special purpose microprocessors and the sole processor or one of multiple processors or cores of any kind of computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Generally a computer will also include or be operatively coupled to communicate with one or more mass storage devices for storing data files such devices include magnetic disks such as internal hard disks and removable disks magneto optical disks and optical disks. Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non volatile memory including by way of example semiconductor memory devices such as EPROM EEPROM and flash memory devices magnetic disks such as internal hard disks and removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in ASICs application specific integrated circuits .

To provide for interaction with a user the features can be implemented on a computer having a display device such as a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer.

The features can be implemented in a computer system that includes a back end component such as a data server or that includes a middleware component such as an application server or an Internet server or that includes a front end component such as a client computer having a graphical user interface or an Internet browser or any combination of them. The components of the system can be connected by any form or medium of digital data communication such as a communication network. Examples of communication networks include e.g. a LAN a WAN and the computers and networks forming the Internet.

The computer system can include clients and servers. A client and server are generally remote from each other and typically interact through a network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

One or more features or steps of the disclosed embodiments can be implemented using an Application Programming Interface API . An API can define on or more parameters that are passed between a calling application and other software code e.g. an operating system library routine function that provides a service that provides data or that performs an operation or a computation.

The API can be implemented as one or more calls in program code that send or receive one or more parameters through a parameter list or other structure based on a call convention defined in an API specification document. A parameter can be a constant a key a data structure an object an object class a variable a data type a pointer an array a list or another call. API calls and parameters can be implemented in any programming language. The programming language can define the vocabulary and calling convention that a programmer will employ to access functions supporting the API.

In some implementations an API call can report to an application the capabilities of a device running the application such as input capability output capability processing capability power capability communications capability etc.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be made. For example elements of one or more implementations may be combined deleted modified or supplemented to form further implementations. As yet another example the logic flows depicted in the figures do not require the particular order shown or sequential order to achieve desirable results. In addition other steps may be provided or steps may be eliminated from the described flows and other components may be added to or removed from the described systems. Accordingly other implementations are within the scope of the following claims.

