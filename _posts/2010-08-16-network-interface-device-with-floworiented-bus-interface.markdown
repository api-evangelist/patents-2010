---

title: Network interface device with flow-oriented bus interface
abstract: A network interface device includes a bus interface that communicates over a bus with a host processor and memory, and a network interface that sends and receive data packets carrying data over a packet network. A protocol processor conveys the data between the network interface and the memory via the bus interface while performing protocol offload processing on the data packets in accordance with multiple different application flows. The bus interface queues the data for transmission over the bus in a plurality of queues that are respectively assigned to the different application flows, and transmits the data over the bus according to the queues.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08155135&OS=08155135&RS=08155135
owner: Broadcom Corporation
number: 08155135
owner_city: Irvine
owner_country: US
publication_date: 20100816
---
This application is a continuation of application Ser. No. 11 416 817 filed on May 3 2006 now U.S. Pat. No. 7 826 470 which is a continuation in part of U.S. application Ser. No. 11 253 509 filed on Oct. 18 2005 now abandoned which makes reference to claims priority to and claims the benefit of U.S. Provisional Patent Application Ser. No. 60 620 098 filed Oct. 19 2004 U.S. Provisional Patent Application Ser. No. 60 626 283 filed Nov. 8 2004 and U.S. Provisional Patent Application Ser. No. 60 643 335 filed Jan. 11 2005.

The present invention relates generally to data network communications and more specifically to a method and system for a network interface device with flow oriented bus interface.

In recent years the speed of networking hardware has increased by two or three orders of magnitude enabling packet networks such as Gigabit Ethernet and InfiniBand to operate at speeds in excess of about 1 Gbps. Network interface adapters for these high speed networks typically provide dedicated hardware for physical layer and medium access control MAC layer processing Layers 1 and 2 in the Open Systems Interconnect model . Some newer network interface devices are also capable of offloading upper layer protocols from the host CPU including network layer Layer 3 protocols such as the Internet Protocol IP and transport layer Layer 4 protocols such as the Transport Control Protocol TCP and User Datagram Protocol UDP as well as protocols in Layers 5 and above.

Chips having LAN on motherboard LOM and network interface card capabilities are already on the market. One such chip comprises an integrated Ethernet transceiver up to 1000 BASE T and a PCI PCI Express or PCI X bus interface to the host computer and offers the following exemplary upper layer facilities TCP offload engine TOE remote direct memory access RDMA and Internet small computer system interface iSCSI . A TOE offloads much of the computationally intensive TCP IP tasks from a host processor onto the NIC thereby freeing up host processor resources. TCP offload additionally reduces the host or CPU memory bandwidth that is required. TCP is described in Request for Comments RFC 793 published by the Internet Engineering Task Force IETF . The Microsoft Windows operating system provides an Application Programming Interface API known as TCP Chimney which is defined in the Microsoft Network Design Interface Specification NDIS versions 5.2 and 6.0.

A RDMA controller RNIC works with applications on the host to move data directly into and out of application memory without CPU intervention. RDMA runs over TCP IP in accordance with the iWARP protocol stack. RDMA uses remote direct data placement rddp capabilities with IP transport protocols in particular with SCTP to place data directly from the NIC into application buffers without intensive host processor intervention. The RDMA protocol utilizes high speed buffer to buffer transfer to avoid the penalty associated with multiple data copying. The Internet engineering task force IETF is the governing body that provides up to date information on the RDMA protocol. Features of RDMA are described in the following IETF drafts draft ieft rddp applicability draft ieff rddp arch draft ietf rddp ddp draft ietf rddp mpa draft ietf rddp problem statement draft ietf rddp rdma concerns draft ieff rddp rdmap draft ietf rddp security and draft hilland rddp verbs.

An iSCSI controller emulates SCSI block storage protocols over an IP network. Implementations of the iSCSI protocol may run over either TCP IP or over RDMA the latter of which may be referred to as iSCSI extensions over RDMA iSER . The iSCSI protocol is described in IETF RFC 3720. The RDMA consortium is the governing body that provides up to date information on the iSER protocol. Information for iSER is described in IETF draft ko iwarp iser v1. The above mentioned IETF documents are incorporated herein by reference. They are available at www.ieff.org.

These and other advantages aspects and novel features of the present invention as well as details of an illustrated embodiment thereof will be more fully understood from the following description and drawings.

A system and or method is provided for a method and system for a network interface device with flow oriented bus interface substantially as shown in and or described in connection with at least one of the figures as set forth more completely in the claims.

These and other advantages aspects and novel features of the present invention as well as details of an illustrated embodiment thereof will be more fully understood from the following description and drawings.

Certain embodiments of the invention may be found in a method and system for a method and system for a network interface device with flow oriented bus interface. Aspects of the method and system may comprise a bus interface that communicates over a bus with a host processor and memory and a network interface that sends and receive data packets carrying data over a packet network. A protocol processor conveys the data between the network interface and the memory via the bus interface while performing protocol offload processing on the data packets in accordance with multiple different application flows. The bus interface queues the data for transmission over the bus in a plurality of queues that are respectively assigned to the different application flows and transmits the data over the bus according to the queues.

Ethernet LANs in the next generation will operate at wire speeds up to 10 Gbps. As a result the LAN speed will approach the internal bus speed of the hosts that are connected to the LAN. For example the PCI Express also referred to as PCI Ex bus in the widely used 8 configuration operates at 16 Gbps meaning that the LAN speed will be more than half the bus speed. For a network interface chip to support communication at the full wire speed while also performing protocol offload functions it must therefore not only operate rapidly but also make very efficient use of the host bus. In particular the bus bandwidth that is used for conveying connection state information between the chip and host memory should be reduced as far as possible. In other words the chip should be designed for high speed low latency protocol processing while minimizing the volume of data that it sends and receives over the bus and the number of bus operations that it uses for this purpose.

Furthermore to reduce power consumption cost and data latency it is desirable that the network interface chip operates in a memory free cut through mode. Cut through mode means that the network interface chip is configured to begin processing incoming data packets as soon as the chip has received the packet header without necessarily waiting for the entire packet to arrive. Payload data from incoming packets may then be transferred directly to host memory as soon as the relevant error detection code such as checksum and or CRC has been validated with only minimal data storage on the chip. Memory free means that the network interface chip requires no dedicated external memory of its own but rather may use the host memory to store connection context payload data when fragments of upper layer protocol frames are received out of order and other information. In this configuration the chip must regularly read and write not only packet data but also connection context information over the host bus thus increasing the challenge of operating at wire speed when the wire speed is more than half the bus speed.

The embodiments of the present invention that are disclosed hereinbelow provide network interface devices that answer these needs. In these embodiments a network interface chip comprises a packet network interface for coupling to a high speed packet network and a host bus interface for coupling to one or more host processors and host memory. In addition to physical and MAC layer interface functions the network interface chip performs upper layer protocol offload functions with wire speed throughput. In contrast to devices known in the art however the speed of the chip s packet network interface and the speed at which the chip performs upper layer processing is more than half the speed of the host bus. This high ratio of processing speed to bus speed particularly in a cut through memory free mode of operation requires a number of design innovations which are described in detail hereinbelow.

In some embodiments the network interface device has two or more network ports which share the same processing pipeline. Additionally or alternatively the resources of the device may be shared by multiple hosts as well as by multiple operating systems i.e. a main operating system and one or more virtual operating systems on a given host. The network interface device is configured so that each port appears to the host as though it was a separate device with its own processing hardware and so that each operating system on each host can interact with the device independently. This resource sharing is accomplished in each case while maintaining the wire speed operation of the device.

Although the NIC is referred to as a card and is shown in the figure as a separate unit from computers . . . the key functions of the NIC are typically implemented in a single chip device. This chip may be mounted on a separate card or it may alternatively be mounted on the host motherboard in LAN on motherboard LOM configurations for example. Therefore the term NIC as used in the description that follows should be understood to refer to a network interface device in general regardless of whether or not it is mounted on its own dedicated card or on a circuit board together with other components.

The NIC comprises one or more ports for receiving incoming packets from and transmitting outgoing packets to the network . Although two such ports are shown in the figures in other embodiments the NIC may have a single port or may have more than two ports. The NIC performs TCP IP and upper layer protocol ULP offload processing of the incoming and outgoing packets as described in detail hereinbelow.

The NIC also comprises a bus interface which communicates with computers . . . via a bus . In the present embodiment bus comprises a PCI Express packet bus as is known in the art which is configured to operate at 16 Gbps in the 8 configuration. The PCI Express bus is described in detail in Revision 1.1 2005 published by the PCI SIG organization available at www.pcisig.com and incorporated herein by reference. Alternatively the bus interface may be configured to operate at different bus speeds and to communicate with buses of other types including parallel buses such as PCI X as well as packet buses. The NIC uses the bus to read and write data to and from the host memory of the host computers as well as for other host communications. Optionally the NIC may have an additional dedicated high speed memory of its own such as a double data rate DDR synchronous dynamic random access memory SDRAM chip for storing communication context and other information. In the description that follows however it is assumed that NIC is configured for memory free operation i.e. using the host memory to store packet data and context without the dedicated external memory .

Each host computer comprises a central processing unit CPU and memory as well as others components known in the art not shown . Typically computers . . . comprise servers such as a cluster of blade servers but the NIC may operate with substantially any type of host computer either alone or on a shared basis with other computers as shown in . Each computer has a master operating system OS which includes a physical driver PDR for controlling interaction with the NIC. Optionally the computer may also run one or more virtual operating systems which have virtual NIC drivers . Methods for operating system virtualization are known in the art and a number of companies such as VMWare Palo Alto Calif. and XenSource Palo Alto Calif. offer software that supports this sort of functionality.

The NIC is configured to permit the resources of the NIC including wire speed protocol offload processing to be shared among multiple host computers and or master and virtual operating systems running on the same computer. Additionally or alternatively when the NIC comprises multiple network ports as shown in the figure the processing resources of the NIC may be shared among the ports rather than providing a separate processing pipeline for each port. For example when a NIC has multiple ports each operating system may have multiple drivers one for each port. Each driver however sees the NIC and its resources as though they were dedicated to that specific driver. Alternatively an operating system may use a single driver to manage multiple ports. This configuration supports optimal use of the available resources but requires that the NIC coordinate among the potentially conflicting instructions that it receives from different drivers. The means by which the NIC permits resource sharing are described in detail hereinbelow.

Incoming packets enter a receive buffer which is shared among the network interfaces. The buffer is large enough to absorb bursts of incoming packets as may occur for example when both ports simultaneously receive 10 Gbps bursts . In an exemplary embodiment the buffer holds up to 256 KB of data although a larger or smaller buffer could be used. The buffer typically holds the packet data until TCP and upper layer processing has been completed and the corresponding Ethernet cyclic redundancy code CRC and TCP checksum have been verified.

A parser and connection lookup engine typically begins analyzing the packet header information as soon as the header enters buffer without waiting for the entire TCP frame to arrive. As noted above this is the meaning of cut through processing. Engine parses the packet headers including selected fields in one or more of the Ethernet IP TCP and UDP headers in order to identify the connection identifier CID on which the packet was received and thus determine the packet protocols and type of processing required. For TCP packets engine prepares information required for TCP offload processing by a TCP receive processor . This processor assembles TCP frames including alignment of packets received out of order from network . An upper layer protocol ULP receive processor handles header information and error detection for protocols in Layer and above. Processor is typically capable of dealing with the following RDMA iWARP iSCSI and iSER Network File System NFS a protocol suite for disk and file sharing developed by Sun Microsystems and Common Internet File System CIFS used for remote file access in Microsoft operating systems .

Additionally or alternatively the ULP receive processor may support other upper layer protocols. In addition even when an incoming packet requires only Layer 2 or TCP Layer 4 processing the ULP receive processor handles placement of the packet data in host memory . The TCP and ULP receive processors are likewise configured for cut through operation.

After the packet headers have been parsed and error detection codes such as CRC and checksums have been verified a payload extractor removes markers and CRC from the payload and sends the frame data to be written in the proper order to host memory via bus interface . Prior to placement of the payload in host memory the ULP Processor may select the destination location or locations. This may require examination of header fields within the payload and protocol specific context information. Since determination of the destinations may require fetches of control data from host memory the ULP Processor may initiate fetches on a speculative basis based upon header fields before they have been validated by the TCP checksum and protocol specific CRC. The ULP processor may notify host that the frame data are now available in memory by sending an interrupt via a host coalescing block which similarly serves other elements of NIC as shown in .

For processing an in order frame the ULP processor also notifies an acknowledgment ACK and completion processor that the upper layer frame was received in its entirety in good order and that all the preceding frames on the connection have already arrived . Processor notifies host that the data is ready for processing by the appropriate host application and prepares a suitable acknowledgment message. Processor passes the acknowledgment message to a transmit processor for transmission over network to the source of the frame. For processing and out of order frame the ULP processor may provide the same notification. This may avoid a need for the ULP processor to distinguish between frames that are out of order and those that are in order. When the completion processor receives notification of an out of order frame it may store completion related information about a work request for example a total length received or a Steering Tag STag that a remote peer has requested to be invalidated in a work request status array that may be parallel with or interleaved with a work request array.

When host computer has data to be sent over network it arranges the data in memory and then notifies transmit processor by sending an appropriate message via bus . The transmit processor generates the required header information including Ethernet IP TCP and upper layer protocol headers and error detection codes and then instructs a framer to construct the packet. The framer reads the appropriate data from host memory via bus as indicated by pointers provided by the transmit processor. The framer then frames the data in packets with the headers and error detection codes and passes the packets to port for transmission over network .

In performing the functions described above receiver and transmitter access and update context information regarding the communication connections that they are serving. High speed low latency access to the context information is one of the keys to the ability of the receiver and transmitter to process packets at wire speed. Complete context information is held in host memory or optionally stored in dedicated high speed RAM while context for connections that are currently being served is cached on chip in the NIC . A memory management unit MMU coordinates memory caching retrieval and writeback among the various processing blocks. The caching schemes used by NIC are described in detail hereinbelow. In one embodiment of the invention firmware may implement any of a plurality of known algorithms such as each STAG may point to a set of page tables similar or identical to the page table used by memory management unites MMUs for example.

The NIC comprises a dedicated management processor for control and diagnostic purposes. A dedicated filter processes incoming packet traffic at wire speed in order to identify in band management packets and pass these packets to the management processor. Typically the management processor has other communication ports such as a serial bus SMBus and or a separate LAN interface not shown for communications with a Baseboard Management Controller BMC . The filter is typically capable of handling incoming traffic at the maximal packet rate supported by network . In the present example the maximum packet rate is about 15 million packets sec assuming 64 byte packets coming in on a single port at 10 Gbps or 30 million packets sec on both ports . As a result of the dedicated high speed filtering provided by filter management processor will still be able to receive and respond to in band management traffic even under conditions of a denial of service DoS attack on system when receiver may be stalled by the traffic load. In addition ports are configured to give outgoing management packets priority over outgoing user packets.

The NIC receives operating power from an auxiliary power supply Vaux and from a main power supply Vmain . The main power supply provides power to all components of the NIC in normal operation. When the host is not in use however the NIC enters a low power mode in order to reduce power consumption and heat dissipation. Under these conditions Vmain shuts off thus powering down transmitter and receiver . Vaux supplies limited current only to certain islands that are needed to enable host to be remotely woken up when necessary. In the example shown in these islands include at least one of network ports filter management processor and bus interface . In low power mode port operates at a reduced bit rate such as 1 Gbps. Optionally both of ports may be included in islands .

While NIC is in low power mode management processor continues to run management protocols and search for wakeup frames coming in from network and wakeup instructions from bus . Upon receiving a wakeup frame the management processor asserts a wakeup signal to the power supply. The power supply activates Vmain which boots the host. The host BIOS resets the PCI bus which indicates to the management processor that the host has exited low power mode. The management processor then configures ports for full speed operation and switches on receiver and transmitter . A remote management system not shown may send wakeup frames in order to wake up NIC and an associated host computer even when host CPU is not running. The remote management system may use this method to boot computer when it is shut off.

For non offloaded connections i.e. connections for which a host processor performs the protocol processing it is desirable that processing of received traffic be distributed among multiple host processors in order to achieve high throughput. Such distribution is known in the Windows environment as Receive Side Scaling RSS . In order to increase host processor cache efficiency as well as to avoid inter processor locks the same host processor may process all traffic for a specific connection. For this purpose the RFE requests that a searcher calculate a hash on the 4 tuple for TCP traffic or on a 2 tuple source IP address destination IP address for non TCP traffic or for fragmented IP packets . The hash result is then used by a TCP receive TCP Rx processor to determine which host processor should process that packet.

Upon identifying a new TCP IP packet RFE sends a new packet message to TCP Rx processor instructing the TCP Rx processor to begin processing the TCP and IP header information in the packet. Processor reads and begins to process the appropriate header fields of packet data from buffer typically while the payload portion of the data continues to flow into the buffer from the MAC processor. The payload data remain in buffer and do not pass through TCP Rx processor at all.

TCP Rx processor comprises a dedicated reduced instruction set computer RISC engine . An exemplary RISC engine which is optimized for the sort of packet processing performed by NIC is described in the above mentioned U.S. patent application entitled High Speed Multi threaded Reduced Instruction Set Computer RISC Processor. RISC engine operates together with associated data moving and management circuits which are shown in detail in . Like RFE TCP Rx processor uses cached context for active connections based on an efficient caching scheme that is described hereinbelow. When the required context information is not in the cache a context load operation is initiated by a context manager via memory manager . The context manager waits for the context to be loaded and then instructs the RISC engine to start processing of the packet. Meanwhile processor may continue to process other packets for which context information is present.

The TCP requires that an acknowledgment ACK be returned over the network to the source address of a TCP frame after the frame has been validated. For this purpose TCP Rx processor sends an ACK message to transmitter after the entire frame has been received and the checksum has been validated. NIC may be configured to generate ACK messages in accordance with various different exemplary policies comprising 1 Send ACK after the checksum has been validated as described above and after identifying a buffer in host memory in which the data will be placed i.e. if there is no buffer available for the data behave as though the packet was not received 2 Send ACK after the conditions in 1 above are met and in addition ULP processing finishes i.e. when a DMA request to copy the packet data from buffer to host memory has been posted to bus interface 3 Send ACK after the conditions in 2 above are met and in addition confirming that the data have been placed in host memory . To guarantee that the data have been successfully written to the memory the ULP Rx processor posts a zero length read request to read from the placement address in host memory after it has first posted the DMA write request to copy the data from buffer to the host memory to bus interface . PCI bus specifications mandate that a read request cannot bypass a write. Therefore when the zero length read response completes the ULP Rx processor can be certain that the data have already been placed in host memory and not just posted to the host memory controller. Thus if the host crashes before the read operation completes the NIC behaves as though the packet was not received. It may be seen that these policies offer different trade offs between speed of acknowledgment and data security which may be appropriate for different types of applications.

The transmitter then returns the appropriate ACK packet to the source address. If the frame was corrupted or data received out of order from the network the transmitter may be instructed to return duplicate ACKs so as to cause the packet source to retransmit the data. The checksum can be computed only after the entire frame has reached buffer . To reduce latency however the TCP RX processor sends a new packet message to an upper layer protocol receive ULP Rx processor as soon as the TCP header processing is completed. ULP processing may then begin immediately. In the event that the TCP frame is found to be invalid the ULP processing results are discarded.

The ULP Rx processor comprises its own dedicated RISC engine along with data moving and management circuits as shown in detail in . Upon receiving new packet message processor reads the required ULP header data from buffer and if necessary invokes a context load operation via memory manager using a context manager shown in . In typical operation ULP Rx processor processes multiple different flows simultaneously and these flows may use different upper layer protocols with different levels of demand on the resources of the ULP Rx processor. For example some incoming packets may carry RDMA or iSCSI protocol data units PDUs which require header processing by the ULP Rx processor while others require only lower layer processing by RFE and possible TCP offload processing by TCP Rx processor . A given flow may be delayed in the pipeline of ULP Rx processor due to high resource demand or while waiting for certain data structures needed for Layer 5 processing such as the RDMA steering tag STAG and physical buffer list PBL to be loaded from host memory. In such cases the other resources of receiver are best diverted to deal with other flows. The ULP Rx processor controls the use of these resources by sending credit update messages which inform the TCP Rx processor of the ability of processing resources and buffers to handle each given flow. The TCP Rx processor allocates its resources to the flows that have a positive credit balance.

The ULP Rx processor as well as other elements of receiver transmitter and memory manager communicates with host by sending and receiving PCI messages and PCI data via bus interface . When processor determines that a given PDU has been successfully validated i.e. the header data have been processed and packet validity checks have passed it transfers the packet payload data from buffer to the appropriate location in host memory . In the case of RDMA there may still be header errors which by protocol rules may be detected once the packet is processed in order. In this regard the processing may be done in the completion processor rather than in the ULP RX processor. The ULP Rx processor may also send a completion message to acknowledgment completion processor to report that at least some of the PDUs were successfully received and processed.

In addition to the messages sent from the elements of receiver to transmitter the transmitter also conveys various messages to the receiver. Some of these messages are described in greater detail hereinbelow. Briefly the exemplary messages may comprise Synchronization messages from transmitter to ULP Rx processor with respect to objects that can be shared between transmitter and receiver for example relating to STAG invalidation for RDMA and similarly the transmitter passes send parameters to TCP Rx processor with respect to TCP connections that are established and outgoing TCP IP packets that are transmitted. The transmitter maintains timers to keep track of the times of packet transmission and sends a timer expired message to the TCP Rx processor when the timer runs out. This message alerts receiver to the need to request retransmission of a given packet or packets. The transmitter may send loopback packets to RFE rather than transmitting these packets to network . Loopback packets may be used for testing purposes as well as for communication between different hosts . . . and between different processes running on a given host. Loopback packets are also used when migrating a TCP connection from the stack of the host operating system to the offload stack on NIC . In this case all the packets pending processing on the OS stack are sent to the NIC via the loopback port. Additionally loopback may be utilized for IP fragmentation and for non aligned UL PDUs. The loopback interface may also be used to allow host software to collect non aligned PDUs and resubmit them as aligned PDUs.

For every incoming packet after a predetermined number of bytes have been received buffer sends a packet position message to a parser . Parser then reads and processes packet header data from buffer . The parser typically parses the Ethernet IP IPv4 or IPv6 and transport layer TCP or UDP headers. To process the header of each packet parser sends read requests to buffer to read the relevant data. The initial read is based on the start block indicated by packet position message . Subsequent reads may be dependent on parsing done on previous data. Parsing of the header includes functions such as parsing VLAN tags which may be nested validation of version and length fields and validation of the IP header checksum. After completing the header processing the parser sends an EOP information request to glue logic which responds after the entire packet has entered by buffer by returning the appropriate EOP descriptor information for the packet.

After the parsing the packet header parser sends a search request to MMU to find the context for the packet. The MMU is described in detail with reference to below. To summarize briefly the MMU caches the context of active connections so as to minimize the need to access context data in host memory . When parser submits search request the MMU first checks whether the connection in question is cached in NIC . If not the MMU submits a search task to searcher asking the searcher to find the connection ID of the received packet. The searcher then returns a connection update to the MMU. Search response informs the parser that the required context is not currently in cache. The parser submits a load request asking the MMU to load the context for this connection into cache. The MMU then returns a load response .

The search request uses the above mentioned 4 tuple as a search string to look for the connection ID CID . The MMU first looks for the 4 tuple in a content addressable memory CAM which contains the 4 tuples for all cached connections. If the connection is not cached the MMU consults searcher for the CID. The searcher uses a Toeplitz hash for example to look for the connection in a hash table located in host memory. When the CID is found the MMU loads the context for that connection into the context cache.

In addition for distribution of non offloaded traffic among multiple host processors RSS as described above the parser asks the searcher to calculate a hash over the 4 tuple or 2 tuple in a hash request . The searcher returns the result to the parser in a hash reply . In this case the parser sends the hash result to the TCP Rx processor which determines based on the hash result which host processor should process that packet. It should be recognized that the host processor may not be selected solely upon the a hash of the header without any other context. Accordingly in instances where multiple virtual devices are supported for example when supporting multiple guest kernels then the set of host processors may be restricted to those on which the specific guest associated with the virtual Ethernet device is enabled to run.

The MMU returns the context parameters to parser in a search response . If the connection is cached the MMU immediately returns the local CID LCID which is the number of the cache entry in the context cache after finding a match for the 4 tuple in CAM. If the connection is not cached the MMU asks the searcher to look for the CID and then loads the context into cache. After the context has been loaded the MMU returns the LCID to the parser. After receiving the response from MMU and or searcher parser sends a new packet start message to TCP Rx processor . Message includes TCP and other packet parameters context parameters received from the MMU and the hash result provided by the searcher.

In addition to the header and context related functions described above parser also performs CRC and checksum C C computations based on full packet data read from buffer . For every TCP or UDP packet the parser calculates the TCP or UDP checksum. While reading and calculating the checksum the parser simultaneously calculates the CRC of the packet payload assuming the payload to comprise an aligned framed UL PDU. Pre calculation of the CRC reduces latency in subsequent processing by ULP Rx processor . If the payload is not an aligned RDMA PDU the CRC value may simply be discarded. The C C machine is able to work on multiple interleaved packets arriving on different ports by requesting and receiving the appropriate packet data from buffer . When the C C calculations are finished and EOP information has been received from input glue parser generates a new packet end message to TCP Rx processor .

The parser uses circular queues to hold the TCP checksum and RDMA CRC results. In order to avoid prematurely overwriting the entries in these queues the TCP Rx and ULP Rx processors send serial number sync messages to the parser to indicate processing of these results.

As noted above with reference to buffer outputs packet data as required directly to TCP Rx processor and to ULP Rx processor . This data flow is shown specifically in to comprise output data to TCP Rx processor and output data to ULP Rx processor .

The TRCM may receive messages from a number of exemplary sources comprising as noted above parser sends new packet message for every incoming packet. This message is forwarded to RISC processor the ULP Rx processor sends credit update message to indicate resource availability or shortage. Credit update messages are aggregated in the connection context and registered with QM the transmitter sends timer expired message whenever a timer expires. These messages are also aggregated in the context and registered with QM the RISC engine itself sends context update messages to write back context data to cache and the QM sends the next connection in the queue. The TRCM performs arbitration based on these sources of information and chooses the next message to pass to RISC engine . The TRCM submits the required context with each input message to the RISC processor.

TRCM may keep a global usage counter indicating the number of currently queued connection. For each queue registration command sent to QM the counter is incremented and for each input request from the QM the counter is decremented. If the TRCM thus determines that the QM has no more messages waiting in its queue the TRCM may pass incoming messages directly to RISC engine without the added burden of queue registration. This feature reduces processing latency when the processing load is low. Further aspects of context and cache management functions performed by TRCM for purposes of efficient caching are described hereinbelow. The other context managers in NIC may perform similar functions.

The RISC engine performs the functions of TCP frame processing and deciding when acknowledgments should be transmitted. In addition the TCP Rx RISC engine identifies the start of each new upper layer protocol data unit ULPDU and notifies ULP RX processor by sending a ULP packet start message . A TCP Rx data mover TRDM controls data movement between TCP Rx processor and other elements of NIC . TRDM loads packet header data from buffer into internal memory of RISC engine for TCP processing. After receiving new packet end message from parser TRDM generates and sends a corresponding ULP packet end message to ULP Rx processor . The TRDM also generates serial number sync messages to the parser.

URCM likewise comprises a context cache and receives context updates from ULP Rx RISC engine . A ULP Rx data mover URDM loads ULP header data from buffer into internal memory of RISC engine and performs other functions analogous to those of TRDM . RISC engine controls data flow in earlier stages of receiver by sending credit updates as described above.

The RISC engine may be programmed in microcode to process various upper layer protocols at Layer 5 and above in the Open Systems Interface OSI scheme. As noted earlier in some embodiments these protocols include RDMA iWARP iSCSI including iSER NFS and CIFS. When incoming packets require only Layer 2 or TCP Layer 4 processing RISC engine handles placement of the packet data in host memory into application buffers when available or into global receive queues in other cases.

Upon receiving ULP packet end message from TRDM URDM informs RISC engine that the current packet has been completed. The packet end message also indicates whether the checksum and CRC results if applicable were valid. RISC engine then instructs URDM to write the packet data to host memory and sends completion message to transmitter . In response to the instruction from the RISC engine URDM issues a packet build command to a payload extractor . The URDM then reads packet payload data directly from buffer to the payload extractor which outputs PCI data packets to be written via bus interface to the host memory.

In iSCSI packets the PDU includes a data integrity field DIF following the payload data. This DIF has the form of a CRC and may contain additional data as specified in standards promulgated by the T10 Technical Committee of the International Committee on Information Technology Standards INCITS . NIC may also compute and validate the DIF CRC values so that host CPU is thus relieved of the task of DIF computation and validation. The DIF computation may be performed on the data path or alternatively it may be performed off line on data blocks in host memory as described hereinbelow with reference to A and B. DIF calculation may also be offloaded to NIC for outgoing packets in storage operations initiated by host as described hereinbelow with reference to .

The parser passes TCP packet parameters to TRCM as soon as the packet header has entered buffer and header parsing is completed without waiting for the rest of the packet to arrive. The TRCM passes the packet parameters to TCP Rx RISC engine which processes the packet header data and identifies ULP packet parameters . ULP Rx RISC engine receives the ULP packet parameters via URCM and performs the applicable upper layer processing. All these steps which depend only on the packet header may take place immediately depending on the status of the relevant queues while the remainder of the packet continues to flow into buffer from network . Thus all header processing including Ethernet IP TCP and RDMA headers may be completed even before the tail of the packet has been received.

Once the entire packet has arrived in buffer parser computes a checksum and CRC C C result as described above. The C C result is passed forward by TRDM and URDM to ULP Rx RISC engine . Upon determining that the C C result is valid RISC engine issues a read packet instruction to URDM and an extract payload instruction to payload extractor . The URDM then reads the packet data directly from buffer and the payload extractor removes markers and CRC from the payload and then selects and validates the host destination. The ACK may then be enabled and the data written to the host memory. When required such as when a Layer packet is received a TOE placement buffer is filled or an RDMA send message is a last segment that is received in order RISC engine issues a write PDU completion message to the host.

This scheme minimizes the amount of time that RDMA packets must remain in buffer and thus minimizes the processing latency and the buffer size requirements.

To initiate transmission of RDMA packets for example the application on host computer requesting the transmission submits a work request which defines the data transfer that is to take place. Based on this work request driver software on host generates a work queue element WQE in memory and writes a doorbell message via bus to a doorbell queue in transmit processor . Details of the doorbell queue are shown below in . TCM receives the request from the doorbell queue and passes it to Tx RISC engine after queuing by QM if needed . The TCM requests the context from MMU omitted from this figure for simplicity . MMU checks whether the required context information is present in its cache and if not loads it into the cache.

Upon receiving the request from TCM RISC engine sends a direct memory access DMA request via bus interface to read the WQE indicated by the doorbell message. Based on information in the WQE the RISC engine may acquire other information needed to process the WQE such as the RDMA steering tag STAG and physical buffer list PBL . Typically TCM aggregates multiple doorbell rings on the same connection and monitors the TCP transmit window of the connection in order to decide when to process the rings. Thus the TCM may invoke the RISC engine only once in order to process multiple work requests that have been posted on a given connection as long as the RISC engine has not yet started processing the first WQE and the TCP transmit window allows for transmission. RISC engine processes the WQEs using context information provided by TCM in order to generate packet header parameters and one or more pointers to the locations in memory of the RDMA data that are to be transmitted. The RISC engine then places a transmit request including the header parameters and references to payload such as DMA address of the data in a transmit command queue of framer . There is a separate queue for each of ports as well as a queue for loopback packets.

When the transmit request reaches the head of the queue framer submits a read request via bus interface to read the appropriate data from memory as indicated by the transmit request pointers. In addition the framer writes the information in the transmit request to a history queue in host memory for use in case retransmission is required as described further hereinbelow . Bus interface returns a read reply containing the requested data to which framer adds the appropriate headers in accordance with the parameters provided by transmitter RISC engine . A single work request may generate more than a single packet because the volume of data to be transmitted for example may be larger than a single packet can carry . Framer segments the payload into packets and advances the IP ID and TCP sequence number of the packets beyond the initial values provided to it by Tx processor . A header builder computes the required checksum and CRC values optionally including the DIF value in the case of SCSI blocks and the framer appends these values to the outgoing packets in the appropriate locations.

The framer places the complete packets in frame buffers which feed MAC processors or RFE in the case of loopback packets . The framer informs TCP Rx processor of transmission of a TCP packet by sending context parameter update to TRCM . Upon transmission of the packet a retransmission timeout RTO timer is set and is then reset each time another TCP packet is sent on the same connection. Timer sends timer expiration message to TCP Rx processor when the time expires. As noted earlier this timer causes processor to generate a retransmit instruction if an ACK is not received from the packet destination before timeout occurs.

When it is necessary to retransmit a packet either after transmission timeout or in response to a retransmission request which may be in the form of duplicate ACKs with the same TCP sequence number by the recipient TCM consults a history queue in host memory for the information that is needed to generate the packet or packets for retransmission. The history queue comprises in progress status associated with associated with a work request and may be a circular queue which is indexed according to the sending order of the outgoing packets. When TCP Rx processor instructs TCM that a certain packet must be retransmitted the TCM looks up the packet information header parameters and data pointers for this packet in the history queue in host memory . It then places this information in transmit command queue thus avoiding the need for RISC engine to recompute the information or for host CPU to be involved in the retransmission in any way.

The history queue also logs the interleaving order between the send queue and the incoming read request queue for each RDMA connection. TCM may then check for a given packet sequence number whether that particular packet was transmitted out of the send queue or the incoming read request queue. The history queue may record the following information for each work request the segment size based upon the maximum segment size at the time the work request s output message was originally segmented and the correlation between the message request and TCP output sequence. Specifically for each section of the output sequence taken from this work request the work request offset and the TCP sequence. Multiple TCP segments placed in sequence from the same work request may be represented by a single entry in the history queue. Restoration may account for insertion of MPA markers in the TCP output stream.

The ACK messages and completion messages from receiver are passed to CCM which queues the messages for processing by RISC engine . The CCM queues the messages and then passes them together with the appropriate context information to RISC engine for processing. Based on these inputs RISC engine writes completion queue entries CQEs to host memory by means of DMA requests and replies . When it is necessary to send an ACK packet to a remote computer over network RISC engine passes acknowledgment instructions to TCM . Transmitter RISC engine then generates the ACK packet parameters and places the ACK packet in queue for transmission as pure or piggy backed ACKs.

To receive service from NIC host processes write doorbells in the form of packets sent to a specified address on bus . The doorbell packet contains a mode flag which identifies it as either a normal mode or immediate mode doorbell. Each doorbell packet contains an address which is used by a doorbell extractor to determine the CID. When multiple RDMA connections are multiplexed tunneled over a single TCP connection as described in the above mentioned provisional application 60 626 283 the doorbell data contains a tunnel ID which the doorbell extractor translates into the CID of the TCP connection using a tunnel translation table . In the case of immediate mode doorbells the doorbell packet also contains the message payload for use by RISC engine in building instructions to framer .

Extractor places the doorbells in a queue from which a context loader reads them out. The queue entries contain the CID connection type and doorbell type transmit or receive . For each doorbell the context loader submits a context load request to MMU specifying the CID type and context regions that are required. The choice of regions to be loaded depends on the queue status of QM When the queue is almost empty the QM sends a message to the context loader causing the context loader to request both the TCP and ULP context regions. This choice reduces latency of subsequent transmit processing. On the other hand when the queue of QM is not almost full only the TCP aggregation context region is loaded. This region is used for aggregation of multiple doorbells and decision whether a sufficiently long TCP window remains to permit transmission before invoking the RISC engine. 

The MMU fulfills the load request and then responds with a load done message which also specifies the local CID LCID cache entry number. For transmit doorbells the context loader then sends a transmit doorbell message to TCM specifying the LCID CID and type of the packet to be generated. For receive doorbells a receive doorbell message is sent to URCM .

When extractor receives an immediate mode doorbell packet it writes immediate mode data from the doorbell packet directly to the internal memory of transmitter RISC engine . The extractor then disables the immediate mode channel until the corresponding doorbell message reaches TCM . Upon receiving the doorbell message the TCM instructs RISC engine to process the message that was contained in immediate mode data . Thus in contrast to the normal mode of processing that was described above with reference to in direct packet mode there is no need for repeated DMA reads via bus in order to bring the WQE and other data from host memory to RISC engine . Immediate mode thus reduces the overall latency of transmitter .

In the disclosed embodiment RISC engine has only limited memory available typically sufficient for only one or a small number of immediate mode messages. Therefore after RISC engine has processed the immediate mode doorbell it sends a release message to extractor indicating that the immediate mode channel may again be enabled.

As illustrated by the description above NIC may be required to support many connections simultaneously running a variety of different protocols. In the memory free configuration of the NIC with no dedicated high speed memory the context for all these connections is held in host memory . The context for each connection comprises multiple regions including for example TCP and ULP regions which are further divided into receive transmit and completion regions as well as aggregation regions held by the context managers and processor regions which are loaded into the RISC engines during processing .

When a connection is to be served by receiver or transmitter MMU is called upon to load the required context information into cache memory on NIC . Operation of the receiver and transmitter typically modifies the context in cache and the modified context must be written back to host memory before the cache can be overwritten with context information for other connections. Each read or write of context information from or to the host memory consumes cycles on bus . This same bus meanwhile is used by the transmitter and receiver for reading out data from host memory for insertion into packets to be transmitted onto network and for writing data from packets received over the network into the host memory. When NIC is expected to process packets at the network wire speed 10 Gbps in the present example and the wire speed is greater than half the bus speed 16 Gbps for PCI Ex 8 bus access becomes a key bottleneck in the system.

Therefore to reduce pressure on bus and thus maintain wire speed throughput NIC implements a number of novel methods for reducing the bus bandwidth required for transferring context information to and from host memory . These methods include context caching by MMU as described below with reference to in conjunction with context operations carried out by the context managers in the TCP Rx ULP Rx transmit and ACK completion processors described above.

The MMU comprises a context fetch controller CFC and a context distribution unit CDU . The CFC acts as a cache controller. It receives context requests from clients checks whether the context is already loaded and if not sends a request to the CDU to load the required context. The CFC also manages the free cache entries and initiates write back WB requests to the CDU when the number of free entries falls below a certain threshold. The CFC manages activity counters per cache entry to detect when there is no longer activity on a cache entry so that it can be taken out of the cache if needed. The CFC uses content addressable memories CAMs and to do permit quick searching of cached connections.

The CDU holds cached context information in a level 1 L1 cache memory . The cache memory is divided into multiple parts corresponding to the different regions described above TCP ULP receive transmit etc. CDU receives load and WB requests from CFC . For each request the CDU determines the corresponding address data lines in cache memory using a L1 address list which holds the translation mapping. The CDU supports different context types for different types of connections such as RDMA or TOE in order to minimize memory bandwidth. Requests to read data from or write back data to host memory are entered in request queues from which they pass to bus interface . Data received from the bus interface are entered in data queues for writing to cache memory . Upon completion of a requested cache load the CDU returns a load complete response to CFC .

When a client such as parser or one of the context managers needs context data it submits load request to CFC . The load request indicates which context region is needed. If the required context region is already present in cache memory the CFC immediately returns load response to the client indicating the LCID of the requested data. Otherwise the CFC submits load request to CDU and returns the load response after having received load complete response from the CDU. Upon receiving the load response the client asks to read the context data from the cache. After processing the packet or packets for which the context applies the client may write a context update to the cache. In this case the region that the client has updated is marked using a flag bit as dirty. 

The division of the context cache into regions thus saves considerable bandwidth in loading and writing back context data via bus . When a client requests context information that is not in the cache only the specific region or regions required by the client are loaded from the host memory. Similarly for writeback only the specific regions that are marked as dirty needed be written to the host memory.

As a further means for saving bandwidth in loading and writeback of context data some of the context parameters may be compressed. For example timer values may be stored in logarithmic form thus reducing the number of bits required at the expense of resolution in determining large timer values. Since long time spans generally do not have to be measured accurately in the operation of NIC timer compression saves context bandwidth without substantial effect on performance. Other types of compression that may be applied will be apparent to those skilled in the art.

An LCID information RAM holds other information such as type of the connection for each LCID for example RDMA TOE etc. The CFC transfers this information to the CDU to indicate the appropriate form of the context in host memory and in L1 cache memory . RAM may also hold the RSS field of the connection indicating the designated host processor for non offloaded connections as explained above. U.S. patent application Ser. No. 11 269 422 filed Nov. 8 2005 discloses a Method and System for Multi Stream Tunnel Marker Based PDU Aligned Protocol and is hereby incorporated by reference in its entirety.

The controller is fed by an arbiter which arbitrates among the different clients of the controller. The client requests are queued in input queues and in addition to the inactivate requests in queue . Context load requests may come from parser or from any of the context managers described above. Clients are blocked if there are no free entries in the cache or if the CDU is full and cannot receive additional requests. Controller issues load and writeback WB requests to CDU . Load responses and writeback responses are placed in a CDU response queue . The CDU response may also indicate that a writeback is required when the number of free or clean cache entries drops below a certain threshold. In this case the CFC controller initiates a writeback procedure which is described hereinbelow with reference to .

The multiplexers and convey load addresses and data to the appropriate cache memory regions while multiplexers and convey writeback addresses to the memory regions and writeback data from these regions. The writeback addresses and writeback data are held in respective queues and while awaiting writeback since writeback typically occurs as a background process with lower priority than context loading . Load controller submits load requests via bus interface whereupon context data are returned for loading into cache . Writeback controller similarly submits writeback requests via bus interface following which context data are written back to the host memory.

In caching schemes known in the art when the cache is full and a new cache line must be loaded from host memory the least recently used LRU cache line is identified and written back to the host memory. Only after writeback can this cache line be overwritten with the new line. This scheme necessarily results in added latency in loading the new line into cache.

In response CDU writes one of the cache lines back to host memory at a writeback step . Typically the cache line that is written back is one of the dirty lines that is not currently in use in any of the processing circuits in NIC . For example as noted above in the description of CFC activity counters measures the level of activity on each connection in the cache and then enter connections with zero activity in queue . CFC controller passes these entries to CDU for writeback. Entries may be locked in the cache by artificially incrementing the corresponding activity counter so that its value never drops to zero. The corresponding cache lines of connections in queue are not written back immediately but rather when step indicates that writeback is necessary. The entries in queue are generally assigned for writeback in queue order so that the entry that has had the longest period of inactivity is written back first. Alternatively the least recently used cache line may be selected for writeback or other inactivity criteria may be applied. Further alternatively or additionally if certain types of connections are considered to be particularly latency sensitive these types may be assigned low priority for writeback thus decreasing the likelihood that the cache lines corresponding to these latency sensitive connections will be overwritten.

As noted earlier CDU does not necessarily write back the entire cache line of the inactive connection but only those regions of the cache line that are marked as dirty. Furthermore the writeback operation may be assigned a low priority since there is no immediate need to overwrite the cache line in question. The writeback will then be performed during free cycles of memory and bus when it will not affect the latency of memory and bus operations.

After writing back the contents of the selected inactive cache line to the host memory CDU marks the line as writeback clean i.e. it resets the dirty flags in the cache line at a line marking step .

Subsequently the CDU receives data to load into the cache from bus interface at an entry loading step . If there is a free entry in cache memory the CDU writes the data to the free entry at a cache writing step . If there is no free entry however the CDU simply overwrites an inactive entry whose cache line is clean. Thus even when the cache is full new context may be loaded with zero latency without loss of the overwritten data.

As noted earlier the context managers in NIC maintain and update context information that is in use by the respective RISC engines. For example as shown in TRCM and URCM have respective caches and and receive context updates and from RISC engines and respectively. These context updates must be written back by the context managers to cache memory in MMU . Each write operation however consumes a part of the available bandwidth of the cache regardless of the number of bits of data that are actually updated up to the width of the data transfer bus .

In order to conserve cache bandwidth the context managers apply context aggregation and decision rules in determining when to write a context update to the cache. In this manner each context manager typically collects multiple updates generated in the course of processing a given connection and then aggregates them together into a single write operation to MMU . The context write may be triggered for example when a certain quantity of update data has been aggregated or upon occurrence of some event that prompts a write decision.

The context managers also use aggregation and decision rules to reduce the frequency with which they must invoke the corresponding RISC engines. For example as noted above TCM may aggregate multiple doorbells on a given connection and then invoke transmitter RISC engine only once to service all the doorbells on condition that a TCP window is available for transmission. The RISC engine will then scan the entire transmit queue for the connection and may process multiple work requests in immediate succession as described hereinbelow for example.

Thus only the CID is held in the queue for processing by the RISC engine. When NIC receives multiple work requests for the same connection only the first work request causes the CID to be registered in the queue. When TCM passes a new CID to transmitter RISC engine for processing the RISC engine checks whether there are additional work requests pending for that particular connection. In certain cases as described below the RISC engine groups together multiple work requests that belong to a single sequence of operations. Otherwise when subsequent work request do not fall into such combined sequences the RISC engine re registers the connection at the tail of the queue. As a result the queues are compact and fairness is maintained among multiple connections.

Only a small portion of the context is needed by the context manager for aggregation and decision purposes. This portion is stored in the aggregation context region of MMU . The CM loads the full context only if and when it determines that the RISC engine should be invoked.

Receiver and transmitter apply novel queue management techniques in order to perform operations in ways that enhance context caching efficiency. A number of examples are presented below 

Application flows typically involve multiple work requests in sequence such as bind send and write send operations in RDMA flows. These operations are normally queued for execution by NIC in the order in which they are submitted by the host application. In one embodiment of the invention instead of processing the work requests in strict queue order however transmitter RISC engine may look ahead down the queue and group together requests belonging to the same application flow.

For instance the send queue on the initiator side of an RDMA connection often comprises a bind request a management operation that involves context but does not cause transmission of any packets over network followed by a send request which does lead to packet transmission . Upon detecting this bind send sequence the transmitter RISC engine moves the send request ahead in the queue so that the bind and send operations are performed in immediate succession. As a result the required context remains in cache thus eliminating extra context transfers over bus and the send latency is also minimized. The transmitter may similarly process write send sequences in immediate succession. After the bind send sequence or write send is completed the initiating application waits for the packet target to respond to the request. In the meanwhile NIC processes work requests from other application queues.

Although the above mentioned bind send and write send sequences each comprise only two commands in practice the RISC engine may group multiple commands on the same connection for processing in immediate succession. For example the RISC engine may process multiple write requests followed by a send.

As another example the context managers in NIC such as TRCM and URCM may look ahead to the operations that are queued by the corresponding queue managers in order to determine which items of context data will be required when these operations come up for processing. Upon determining that the TOE or RDMA context for a given connection will be required for instance the context manager may prefetch the required context data from MMU and place the context data in its own cache such as caches and . Prefetching context data in this manner reduces the latency of operations by the RISC engines. It also helps to conserve context bandwidth since it can take advantage of any unused transfer capacity that is available. Yet another example of context bandwidth conservation involves the use of credit updates from ULP Rx processor to TCP Rx processor and . The ULP Rx processor may use these credits inter alia to inform the TCP Rx processor of the availability of buffers in host memory for the data carried by incoming packets on various connections. When the number of credits on a given connection is too small the TCP Rx processor discards incoming packets on the connection. Discarding the packets at this early stage saves processing bandwidth and avoids the need to load context information for the connection in question thus conserving context bandwidth as well.

In another embodiment of the invention the In another embodiment of the invention work requests may be processed iteratively to achieve the same result. With this implementation strategy the transmit RISC engine continues to process work requests for the QP until one of the following conditions is met 

The embodiments described above relate to methods and designs for reducing processing latency and context bandwidth demands within NIC . These methods and designs are intended inter alia to reduce the bandwidth demands placed on bus . They are complemented by the methods and designs implemented in bus interface which aim to make the most efficient use of the available bus resources. It will be understood however that the methods described above may be used together with other sorts of bus and memory access schemes. Similarly the bus access schemes described below are of more general applicability and are not limited to the types of data clients described above.

The requesters i.e. PCI Ex clients submit requests to a request interface module . The requesters include for example CDU searcher and the data movers that are associated with the various RISC engines in the transmitter and receiver. Each request has a request ID which is a local sequence number used by this specific client. The bus interface inserts this number in done indications and read completions that are returned to the client. The requester further specifies to which virtual queue each request should be added. A VQ is an ordered list of requests which are handled in order by bus interface . Typically each application flow in NIC has its own VQ thus guaranteeing that transactions for each flow will be handled in order and that bus bandwidth is divided per flow rather than per requester. For example write requests whose payload arrives from the same data port on network are added to the same VQ so that the payload will be associated with the correct request. Different requesters may submit different requests to the same VQ thus synchronizing their host writes. Read requests that depend on writes as well as read requests that should be returned to the requester in a specific order are likewise appended to the same VQ. Requests that depend on a read request i.e. they are added to the VQ behind the read request are handled when the read request has been submitted to interface logic without waiting for the read completion to arrive before handling the next request on the list. Read processor then handles the completions which may return out of order .

The requests submitted to bus interface include addresses but the requester may use a logical address instead of a physical one. A logical to physical L2P translator translates the logical addresses into physical addresses using static translation values in an on chip translation table. The translation table for each CID assumes allocation of contiguous blocks of a certain size such as 128 KB but still allows for the possibility that some allocations may not be contiguous. If the requested logical address is not present in the table the L2P translator submits its own special read request to a host memory translation table using pointers to the table that are stored on chip as well. For example if the table can reside over at most sixteen contiguous blocks of host memory then sixteen pointers are held on chip to map the table. The physical address is then returned to the requester.

The L2P translator can be used in this manner to find physical addresses by dependent read operations. For example one of requesters may specify a hash code provided by searcher in order to find the corresponding connection ID CID in a first read operation. The searcher table is then addressed by a logical address. The search entry can have two pointers 1 If there is a match on the 4 tuple the pointer indicates the physical address of the context for the CID so that a translation for context address is not required on the receive path. The translation is still required on the transmit path however since context load for transmission is initiated by the doorbell and addressed by CID which is part of the doorbell and not by the 4 tuple as on the receive side. 2 If there is no match to the 4 tuple to the searcher traverses a linked list in the hash table so that the pointer to the next item on the list is a physical address pointer rather than logical. Consequently in dependent reads although the first request may specify a logical address the data structures in memory are arranged so that subsequent requests refer to physical address pointers.

The request interface passes the request context for each request to a request context manager and enters each request in the appropriate VQ in a request flows database . A request arbiter reads the requests out of the VQs divides the requests into sub requests SRs and submits the SRs to a PCI packet builder in interface logic . Each SR is identified by a unique ID. The SR is the actual PCI Ex transaction and complies with the bus rules such as maximal read request size maximal payload size etc. Typically the arbiter defines the SRs so that the size of packets built by packet builder is as close as possible to the maximal payload size on bus thus minimizing the packet overhead.

Arbiter receives read write completion enable messages from packet builder indicating whether resources are available to process the SRs. The arbiter does not submit SRs to the packet builder unless the resources are available. Thus request processor uses backpressure to avoid asking for memory bandwidth before bottlenecks occur on bus .

The data sources place payloads to be written to the host in a write buffer via write data ports of write processor . Each requester has a unique data port for write. The sizes of the buffer allocations are adjusted to the typical payload sizes of the specific data sources. The port assignments are passed on to packet builder which uses the port assignments to determine the buffer location from which to take the payload data for each bus write. When the packet builder receives enough credits to send a write request to host it reads the payload data from write buffer and inserts the data in a packet that is transferred to PCI write port .

Completion logic receives incoming responses from PCI read port . Responses to read requests submitted by requesters comprise payload data which the completion logic places in a read queue . When a completion arrives completion logic passes a notification to read processor including the SR ID. The read processor translates the SR ID into a buffer index in a read buffer where the incoming payload should be placed. Request processor prepares the translation table for this purpose when the SR is submitted. Read buffer blocks are allocated on demand but arbiter does not issue a SR until it has ascertained that are enough free blocks to accommodate the SR in the read buffer.

When a block arrives in read buffer in order i.e. all the previous SRs and blocks have been delivered read processor transfers the block to an appropriate data sink as indicated by the corresponding request. Data accumulating in the read buffer may be delivered to the data sink even before all the requested data or even the data requested in the current SR have reached the read buffer in order to reduce latency and to empty the buffer as quickly as possible.

An address mapper in interface logic passes read and write requests initiated by host processor to host interface processor . A read write handler passes the host requests to the appropriate locations in NIC such as doorbell queue data movers general chip register file GRC and a DMA and digest engine . The DMA and digest engine is used in CRC computation offload as described below with reference to as well as for copying from host memory to host memory in order to avoid copying by the CPU in TOE operation.

The PCI packet builder receives SRs from arbiter for the next read request write request and completion request for each of the VQs. For example if there are two VQs the PCI packet builder may hold up to six pending SRs. As noted above the packet builder keeps track of credits received from host and prepares packets for transmission on bus accordingly. For each read request submitted by one of requesters packet builder takes a tag from a tag database and attaches the tag to the packet header. The tag database keeps track of released tags and consumed tags along with a mapping between each tag and the corresponding SR ID. This mapping is used by completion logic in associating each completion packet received from the host with the appropriate SR. When the number of bytes received in a given completion equals the number of bytes expected the tag is released.

Typically arbiter applies a weighted fair queuing WFQ scheme in order to choose the VQ from which to take a request at each pass through step . As noted earlier each VQ belongs to a certain application flow so that the arbitration is applied on a per flow basis as opposed to bandwidth sharing among requesters as in bus arbitration schemes known in the art. In NIC a given requester such as CDU or TRCM typically participates in multiple different flows and places its bus requests in the appropriate VQ for each flow. For example TRCM may participate in TOE flows RDMA flows and iSCSI flows and may place its requests for each flow type in a different VQ.

The weights applied in the WFQ arbitration scheme may be chosen according to the relative demand that each flow is expected to make on the bus resources. For this purpose the actual bus use and traffic mix of the different flows may be measured over the course of a test period for example one week under real operating conditions. The measurement results may be used in computing weights which are then programmed into arbiter . Thus the arbiter will allocate the bus resources in a manner that optimally matches the needs of NIC and applications running on host and thus minimizes bus latency and wasted bandwidth. The measurement results and consequently the weights are likely to vary from one host to another depending on the application environment and system configuration and the programmed weights may be updated from time to time. Alternatively weights may be determined a priori based on estimated system characteristics.

After choosing the appropriate request to be serviced arbiter submits a read SR to PCI packet builder at a SR submission step . The SR is added to the list held in VQ RAM . The arbiter need not submit all the SRs corresponding to a given request in succession but may rather interleave SRs from different requests. This sort of interleaving tends to reduce latency in servicing of the sub requests by host since some requesters typically request short DMA transactions for control information and require a small portion of the bus bandwidth but are latency sensitive since they cannot process another request until they receive the requested data from host memory. Packet builder adds each SR to a list in a sub request RAM .

A read response returns from PCI read port at a completion step . If the response gives a physical address in response to a request for resolution of a logical address as described above the physical address is written to request context RAM . The payload is delivered to read buffer at a delivery step . For this purpose free buffer blocks from a read buffer pointer list are consumed and added to the tail of a linked list in SR RAM . At step read processor reads the next SR to deliver from VQ RAM reads the location of the first block containing the response payload from SR RAM and passes the payload to the appropriate data sink . It then releases the block in list releases the SR in VQ RAM and proceeds to the next SR.

Referring back to it can be seen that although NIC has two physical ports to network the ports share the same transmitter and receiver and the same bus interface . The dual ports may be used for purposes of failure protection or they may be used to carry Ethernet traffic simultaneously. In either case sharing the protocol processing and bus interface resources between the ports saves chip area and cost by comparison with conventional schemes in which each NIC chip has a single network port.

Host operating systems such as Windows are typically built on the assumption that each port is supported by separate hardware. The operating system provides a separate driver instance for each port and assumes the ports and driver instances are independent of one another. For instance there can be no global spin lock shared between driver instances. Similarly traffic on one port must not visibly affect the other port. Thus when NIC is operated in a Windows environment or another software environment with the above characteristics the processing resources of the NIC should be shared and managed in a way that is transparent to the host operating system OS . Some of the methods that are used for this purpose are described hereinbelow 

As shown in buffer has multiple inputs one from each port and a single output to the shared receive pipeline. Each port receives a certain minimum buffer allocation while the rest of the buffer is dynamically shared between the ports on a first come first served basis. When sending pause to the transmitter different thresholds may be applied to the two ports. For example in a protection configuration the standby port may have a lower pause threshold than the active port. The pause is used to tell the switch in network to which port is coupled to stop sending packets to the NIC and buffer all incoming packets in it s the switch buffers. 

Other resources may be shared on either a dynamic or a static basis. For example cached resources such as LCID cached STAGs RISC engine threads and PCI Ex bus bandwidth are typically shared dynamically. Long term resources such as CID MR and MW are shared statically.

Some resources may still be provided separately for the different ports. Examples include post MAC packet filters clocks configurations and collection as well as reset of port statistics.

The driver software for NIC running on host uses management processor for coordination among different driver instances. There is no coordination between the driver instances at the driver level. The management processor is responsible for approving any operation that has to be synchronized between the driver instances. Before beginning a firmware download for example each driver must first request management processor approval. The management processor replies with approval if this is the first driver or with a downloaded or downloading reply if this is the second driver and firmware is already downloaded or downloading . Upon receipt of the approval the driver downloads the firmware. The management processor will send both driver instances an indication when the download is completed in order to allow the driver to continue with subsequent dependent operations. If a driver instance gets stuck in the middle of the download process the management processor will indicate that this is the situation and either driver instance may then initiate reload and reset of the NIC.

The method of is initiated when NIC management processor receives a power down command from either of the port drivers at a command input step . It is assumed arbitrarily that the port A driver issues the command. The management processor checks to determine whether port B has already been powered down at a port checking step . If so the management processor disables the clocks and switches over to auxiliary power Vaux at a full power down step . Otherwise if port B is still active the management processor keeps NIC under full power Vmain with clocks running at virtual power down step . In accordance with the power down command however port A is virtually disabled i.e. the management processor configures the port so that it will not accept PCI transactions even though the hardware resources remain available.

Wake up from the powered down state is similarly carried out on a per port basis. When one port is powered up the other port remains disabled until it gets its own wake up command. When both ports are in a cold power down state and one port discovers a wake up frame NIC waits for a PCI reset without changing the state of any port.

Diagnostic routines through each port include loopback and interrupt tests. For memory tests handlers on the RISC engines may be used to verify that parity errors do not occur in the data written in memory. This sort of parity test is preferable to active memory tests in which memory contents may be overwritten since an active memory test on one port will affect traffic on the other port. Parity testing can be done in run time on one port without affecting traffic on the other port. If a parity error is discovered it is preferable that NIC not be immediately reset since the other port may still be transferring traffic but rather that both ports fail gracefully.

A user may run a utility to update non volatile RAM NVRAM on a per port basis while the other port is transferring user and or management traffic.

Additionally or alternatively a hardware reset may be invoked in response to a report from host coalescing block to driver that a hardware error has occurred.

If the driver received a heartbeat failure at step it issues a hardware reset request to management processor at a reset request step . The management processor then checks whether Port B of NIC is active based on the Port B heart beat as described above at a second port checking step . If Port B is inactive the management processor is free to proceed with the hardware reset at a hard reset step .

If Port B is active however a hardware reset may cause an irrecoverable error in the driver instance associated with Port B which may cause the operating system to crash. To avoid this possibility management processor forces the Port B driver instance to request a hard reset at a driver forcing step . As noted above the management processor may elicit the Port B reset request by returning a heartbeat failure response to the Port B driver instance even though there is in fact no actual error that would cause such a failure . The Port B driver instance will inform the operating system of the heartbeat failure and will then submit its own request to reset NIC . Upon receiving this request the management processor performs the hardware reset at step .

Following the reset at step the management processor informs driver which in turn informs operating system that the reset is done at a reset completion step .

Alternatively if the Port A driver instance determines at step that there was no error in the last heartbeat reply from management processor then a hardware reset of NIC is unnecessary. Furthermore an unnecessary hardware reset is undesirable since it will also disturb operating system and application functions that are using Port B. Therefore when the heartbeat is found to be sound at step the Port A driver instance sends a software reset referred to herein as a ramrod through the NIC to flush out the Port A processing chain at a ramrod transmission step . In contrast to a hardware reset the ramrod does not affect the Port B processing chain.

The Port A driver instance waits for the software reset to complete successfully at a ramrod completion checking step . If the reset is completed successfully within a predetermined timeout period the driver instance informs that operating system that the reset has been completed at step . Otherwise the driver instance concludes that a hardware reset is required and instructs management processor to perform the reset at step as described above.

In contrast to the reset requests described above when one of the driver instances requests a halt management processor will invoke a hardware reset even if there is no indication of a hardware error or heartbeat failure. In response to the halt request the management processor jumps directly to step in and then proceeds as described above.

As shown in NIC may be shared among multiple hosts . . . and among both a master operation system and one or more virtual operating systems on any given host. Virtualization software that is known in the art such as VMWare and Xen mentioned above uses a hypervisor adapter layer between the master operating system and virtual operating systems running on the same platform. In conventional systems communication packets that are sent and received by the virtual driver of the virtual operating system pass through a virtual switch in the hypervisor and are actually transmitted and received over the network by the physical driver of the master operating system. The extra layer of packet processing causes performance degradation and prevents the virtual operating system from exploiting hardware acceleration of protocols such as TCP RDMA and iSCSI.

As noted in reference to NIC is able to offer protocol acceleration to virtual operating systems by interacting with two drivers physical driver PDR working in master OS and virtual driver VDR working in virtual guest OS . The physical driver handles resource allocation between virtual operating systems chip initialization and reset and other operations that are not time critical. Cut through communication takes place between NIC and VDR for TOE offload service or between the NIC and the guest user application for RDMA for example running over the virtual OS thus bypassing the hypervisor.

NIC is aware of every virtual OS running on host and has specific resources allocated to each OS since loading the corresponding virtual driver connects with the hypervisor which invokes the resource allocation by the NIC. For example the NIC has separate CIDs ring buffers and event and command queues at least one pair for each OS. These resources may be further separated by port protocol etc. Since CIDs are statically divided among the virtual operating systems the set of active CIDs might not be contiguous. To save PCI bandwidth in timer scans for example NIC may maintain a base CID for each OS and scan only the active connections in each virtual OS skipping over holes and thus refraining from reading timer context for inactive CID ranges.

The NIC performs MAC address and VLAN filtering for each virtual OS and maintains separate receive buffers to hold incoming Ethernet frames for each virtual OS. For frames with unicast MAC addresses the filters determine which virtual OS is to receive each frame by exact matching. Hash based matching is used to filter multicast addresses wherein each hash entry tells which virtual operating systems should receive the multicast packet. When packet duplication is needed it can be performed either by having NIC place multiple copies of the packet in the appropriate OS receive buffers or by the hypervisor. The former approach reduces the burden on CPU while the latter saves bandwidth on bus . Similarly simple packet transmission between virtual operating systems on the same host platform can take place either through the NIC or by the hypervisor. When the transmission involves protocols at Layer and above however it is most advantageously carried out through NIC in order to take advantage of the protocol acceleration capabilities of the NIC.

On the other hand some operations are reserved for PDR and master OS . For example PDR performs memory allocation for data structures belonging to NIC such as context searcher hash tables timers etc. as well NIC reset when required. PDR also performs translation from guest physical addresses i.e. logical addresses generated by address translation performed by virtual operating systems to actual physical addresses as described further hereinbelow.

Other functions are tied to hardware rather than to a specific OS. For example NIC statistics are maintained for each physical interface and are duplicated to all virtual operating systems. Host coalescing block performs interrupt coalescing for each operating system .

Translation from virtual to physical addresses generally involves the virtual OS but depends on the type of virtualization platform that is used. In virtualization platforms such as Xen in which the virtual OS knows that it is virtualized VDR invokes a back end driver in the hypervisor domain that performs the translation. This driver gives VDR an index to a lookup table in the hypervisor memory that maps guest physical addresses to actual physical addresses. On the other hand in platforms such as VMWare in which the virtual OS does not know that it is virtualized VDR submits the requested guest physical address to NIC . The NIC then consults the lookup table in hypervisor memory in order to determine the corresponding actual physical address before issuing DMA transactions to or from the buffer in question. In other words NIC is aware of the type of virtualization environment in which it is working and performs the extra translation with the help of the hypervisor as required. The NIC then overwrites the data structures in host memory handled by the virtual OS with the correct physical addresses.

Sharing of NIC among multiple hosts . . . such as server blades using a shared PCI Ex backplane is similar in principle to sharing among multiple operating systems. Packet communications between different hosts pass through the NIC since there is no hypervisor to perform the function directly between the hosts .

Computing the SCSI data integrity field DIF is a computation intensive task. In systems known in the art however the SCSI DIF including a CRC value and possibly other data is computed and appended to each data block by the host CPU that initiates the SCSI operation and is then validated by the host CPU of the SCSI target.

The NIC offloads the DIF computation from the host CPU. In some embodiments for SCSI target operation receiver computes and validates the DIF of each incoming SCSI data block received by the NIC and transmitter computes the DIF value for outgoing SCSI blocks transmitted by the NIC. Host CPU removes the DIF of each incoming block without further computation after the block has been placed in host memory .

In other embodiments DMA and digest engine computes and validates DIF CRC values for data blocks held in host memory . Examples of this sort of CRC offload implementation are described hereinbelow.

Thus regardless of whether the host is the SCSI initiator or target the NIC relieves the host of the burden of DIF calculation with only minimal modification to the normal operation of the SCSI protocol stack on the host.

Certain embodiments of the invention may be found in a system for a network interface device with flow oriented bus interface. Aspects of the system may comprise a network interface device comprising a bus interface a network interface and a protocol processor. The bus interface may be used to communicate with a host processor and or memory over a bus. The network interface may send and receive data packets carrying data over a packet network. The protocol processor may perform protocol offload processing on the data packets in accordance with multiple different application flows. The different application flows may comprise two or more of the following TCP IP offload processing remote direct memory access RDMA processing Internet Small Computer System Interface iSCSI processing Network File System NFS processing and or Common Internet File System CIFS processing. The protocol processor may be coupled between the bus interface and the network interface so as to convey the data between the network interface and the memory via the bus interface.

The bus interface may be adapted to queue the data for transmission over the bus in a plurality of queues that may be respectively assigned to the different application flows and may transmit the data over the bus according to the queues. The bus interface may allocate bus bandwidth for transmission of the data over the bus according to the different application flows. The bus interface may allocate the bandwidth to each of the different application flows according to a weighted queuing scheme where the weights may be determined in response to a distribution of traffic among the different application flows. The distribution of traffic may be determined based on actual use of the network interface device.

The actual use of the network interface device may be measured during a first period so as to estimate the distribution of traffic among the different application flows. The weights may be determined during a second subsequent period with respect to the distribution of the traffic during the first period. Each of the queues may comprise a sequence of transmission requests and the bus interface may process the transmission requests sequentially in queue order.

The bus interface may receive an indication of resource availability for serving each of the application flows and may apply back pressure on one or more of the queues when resources may not available to serve the application flows to which one or more of the queues may be assigned. The protocol processor may be coupled to submit requests to the bus interface for the transmission of the data over the bus and the bus interface may divide the requests into sub requests for service on the bus.

The bus interface may optimize a payload size of the sub requests with respect to a payload size constraint that may be imposed by the bus. The bus interface may divide a first and second requests belonging to different application flows into respective first and second sequences of the sub requests. The sub requests may be submitted for service on the bus while interleaving the sub requests in the first and second sequences. The bus interface may service the queues such that bus operations with respect to each of the application flows may be performed in queue order. At least one of the queues may comprise requests submitted by different requesting entities in the protocol processor.

The protocol processor may submit requests to the bus interface for the transmission of the data over the bus to and from the memory. At least some of the requests may specify logical addresses and the bus interface may convert the logical addresses to physical addresses for use in accessing the memory.

The network interface device may comprise a bus interface that may communicate over a bus with a host processor and memory such that the network interface device may write and read data to and from physical addresses in the memory. The network interface device may further comprise a network interface that may send and receive data packets carrying data over a packet network. The network interface device may also comprise a protocol processor that may be coupled between the bus interface and the network interface. The protocol processor may convey the data between the network interface and the memory via the bus interface while performing protocol offload processing on the data packets in accordance with multiple different application flows. The protocol processor may be coupled to submit requests to the bus interface for the communication of the data over the bus to and from the memory. At least some of the requests may specify logical addresses. The bus interface may convert the logical addresses to physical addresses for use in accessing the memory.

The bus interface may comprise a translation table which may contain translation parameters for use in converting the logical addresses to physical addresses. There may be a sequence of requests with respect to a logical address for which the translation table may not contain translation parameters. In those cases the bus interface may direct one or more dependent read operations to the memory so that a first request in the sequence may reference the logical address. At least one subsequent request in the sequences may refer to a physical address.

Certain embodiments may be found in a method for a network interface device with flow oriented bus interface. Aspects of the method may include coupling a processing device to communicate over a bus with a host processor and memory. Data may be processed in accordance with multiple different application flows. The data may be conveyed between the processing device and the memory by queuing the data for transmission over the bus in a plurality of queues. The queues may be assigned to different application flows and the data may be transmitted over the bus according to the queues.

The processing device may comprise a network interface device which may send and receive data packets carrying the data over a packet network while performing protocol offload processing on the data packets. Conveying the data may comprise allocating bus bandwidth for transmission of the data over the bus according to the different application flows.

The bus interface may allocate the bandwidth to each of the different application flows according to a weighted queuing scheme. The weights may be determined with respect to a distribution of traffic among the different application flows. Each of the queues may comprise a sequence of transmission requests. Conveying the data may comprise processing the transmission requests sequentially in queue order. Conveying the data may also comprise receiving requests at a bus interface for the transmission of the data over the bus and dividing the requests into sub requests at the bus interface for service on the bus. Conveying the data may further comprise servicing the queues so as to ensure that bus operations with respect to each of the application flows may be performed in queue order.

A processing device may be coupled to communicate over a bus with a host processor and the memory via a bus interface. The processing device may write and read data to and from physical addresses in memory. The processing device may also submit requests to the bus interface for transmission of the data over the bus to and from the memory and at least some of the requests may specify logical addresses. The bus interface may convert the logical addresses to physical addresses for use in accessing the memory responsively to the requests. Another embodiment of the invention may provide a machine readable storage having stored thereon a computer program having at least one code section for communicating information in a network the at least one code section being executable by a machine for causing the machine to perform steps as disclosed herein.

An embodiment of the invention may comprise a system for a network interface comprising a network interface chip. Certain aspects of the network interface chip may comprise an on chip network interface that transmits and receives data packets carrying data and an on chip bus interface that reads the data from and writes the data to physical addresses in off chip memory. The network interface chip may also comprise an on chip protocol processor communicatively coupled to the on chip network interface and the on chip bus interface so as to convey the data packets between the network interface and at least one off chip device coupled to the on chip bus interface. The conveyance of the data packets may occur while performing protocol offload processing on the data packets in accordance with a plurality of different application flows. In this regard the on chip bus interface may queue the data in a plurality of queues which are respectively assigned to the different application flows for transmission off chip.

The on chip bus interface may allocate bandwidth for a bus communicatively coupled thereto which facilitates communication of the data to the different application flows. The on chip bus interface may also allocate the bandwidth according to a weighted queuing scheme which is dependent on traffic distribution amongst the different traffic flows. The traffic distribution may be determined based on traffic utilization within the network interface chip. The weighted queuing scheme may be determined based on previously determined traffic distribution. The on chip bus interface may sequentially process data queued in the plurality of queues. The on chip bus interface may apply back pressure to at least one of the plurality of queues whenever there are insufficient resources available to serve application flows to which the queues are assigned.

The on chip protocol processor may submit requests to the on chip bus interface for transmission of data off chip via the on chip bus interface. The on chip bus interface may apportion the submitted requests based on allowable payload for a bus that is communicatively coupled to the on chip bus interface. The on chip bus interface may apportion first and second submitted requests for different application flows into respective first and second sequences of apportioned requests and submit the apportioned requests for service while the first and second sequences of apportioned requests are interleaved.

Another embodiment of the invention may provide a machine readable storage having stored thereon a computer program having at least one code section for communicating information in a network the at least one code section being executable by a machine for causing the machine to perform steps as disclosed herein.

Although certain processor designs and processing techniques are described hereinabove in the specific context of the NIC and certain types of networks and communication protocols the principles of these designs and techniques may likewise be implemented in other processing environments and in connection with other protocols.

Accordingly the present invention may be realized in hardware software or a combination of hardware and software. The present invention may be realized in a centralized fashion in at least one computer system or in a distributed fashion where different elements are spread across several interconnected computer systems. Any kind of computer system or other apparatus adapted for carrying out the methods described herein is suited. A typical combination of hardware and software may be a general purpose computer system with a computer program that when being loaded and executed controls the computer system such that it carries out the methods described herein.

Various aspects of the present invention may also be embedded in a computer program product which comprises all the features enabling the implementation of the methods described herein and which when loaded in a computer system is able to carry out these methods. Computer program in the present context means any expression in any language code or notation of a set of instructions intended to cause a system having an information processing capability to perform a particular function either directly or after either or both of the following a conversion to another language code or notation b reproduction in a different material form.

While the present invention has been described with reference to certain embodiments it will be understood by those skilled in the art that various changes may be made and equivalents may be substituted without departing from the scope of the present invention. In addition many modifications may be made to adapt a particular situation or material to the teachings of the present invention without departing from its scope. Therefore it is intended that the present invention not be limited to the particular embodiment disclosed but that the present invention will include all embodiments falling within the scope of the appended claims.

