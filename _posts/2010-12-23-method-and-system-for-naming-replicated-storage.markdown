---

title: Method and system for naming replicated storage
abstract: Method and system for uniquely identifying a replicated copy of a storage volume is provided. A unique identifier is created by a storage system managing the replicated copy. The unique identifier includes a time stamp of when the identifier is being created, a system clock of the storage system and a unique address for an adapter that is used by the storage system.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08380955&OS=08380955&RS=08380955
owner: Netapp, Inc.
number: 08380955
owner_city: Sunnyvale
owner_country: US
publication_date: 20101223
---
Various forms of storage systems are used today. These forms include direct attached storage DAS network attached storage NAS systems storage area networks SANs and others. Network storage systems are commonly used for a variety of purposes such as providing multiple users with access to shared data backing up data and others.

A storage system typically includes at least one computing system executing a storage operating system for storing and retrieving data on behalf of one or more client processing systems clients . The storage operating system stores and manages shared data containers in a set of mass storage devices such as magnetic or optical disks or tapes.

Information stored at storage devices may be frequently replicated. Managing and uniquely naming the replicated copies can be challenging as storage systems span across networks and data centers. Conventional systems typically use a limited number of identifiers for identifying replicated copies. The identifiers are recycled when a replicated copy is deleted. This is an undesirable limitation since the numbers of replicated copies continue to grow in today s data centers. Continuous efforts are being made to efficiently manage replicated storage copies.

In one embodiment a method and system for uniquely identifying a replicated copy of a storage volume is provided. A unique identifier is generated by a storage system managing the replicated copy. The unique identifier includes a time stamp of when the unique identifier is being created a system clock of the storage system and a unique address for an adapter that is used by the storage system.

In another embodiment a machine implemented method for translating an identifier of a replicated copy of a storage volume is provided. A unique identifier for the storage volume is first generated. The unique identifier of the storage volume includes a time stamp based on when the unique identifier is being generated an original identifier that identifies the storage volume of the replicated copy a system clock of a storage system managing the storage volume and a unique address of an adapter used by the storage system.

Thereafter a first identifier for the replicated copy is derived from the unique identifier for the storage volume by replacing the original identifier of the storage volume with an original logical identifier for the replicated copy. A second identifier for the replicated copy is also derived from the unique identifier for the storage volume by replacing the original identifier of the storage volume with an original physical identifier of the replicated copy and replacing the time stamp of the unique storage volume identifier with an original time stamp of the replicated copy.

In yet another embodiment a machine implemented method for translating an identifier of a replicated copy of a storage volume is provided. A unique identifier for the storage volume having a time stamp value based on when the unique identifier is being generated an original identifier that identifies the storage volume of the replicated copy a system clock of a storage system managing the storage volume and a unique address of an adapter used by the storage system is provided.

A first identifier for the replicated copy is derived from the unique identifier for the storage volume by replacing the original identifier of the storage volume with an original logical identifier for the replicated copy. The replicated copy is also identified with a second identifier derived from the unique identifier of the storage volume by replacing the original identifier of the storage volume with an original physical identifier of the replicated copy and replacing the time stamp of the unique storage volume identifier with an original time stamp of the replicated copy.

In another embodiment a machine implemented method for translating an identifier of a replicated copy of a storage volume is provided. A first identifier for the replicated copy is generated having an original logical identifier of the replicated copy a time stamp based on when a unique identifier for the storage volume is generated a system clock for the storage system accessing the storage volume and a unique address of an adapter used by the storage system. An original physical identifier of the replicated copy and an original time stamp of the replicated copy is used for identifying the replicated copy with a second identifier.

In yet another embodiment a machine implemented method for uniquely identifying a replicated copy of a storage volume is provided. The method includes providing a first component of a unique identifier having a time stamp value indicating a time when the unique identifier is being generated using a system clock value generated by a storage system that manages the replicated copy for identifying a second component of the unique identifier and generating a third component of the unique identifier having a unique address of a network adapter used by the storage system.

In another embodiment a system for translating an identifier of a replicated copy of a storage volume is provided. The system includes a first storage system identifying the replicated copy using an original logical identifier of the replicated copy and an original physical identifier of the replicated copy.

The system also includes a second storage system translating the identifier of the replicated copy to generate a first identifier for the replicated copy having the original logical identifier of the replicated copy a time stamp based on when a unique identifier for the storage volume is generated a system clock for the second storage system and a unique address of an adapter used by the second storage system and using the original physical identifier of the replicated copy and the original time stamp of the replicated copy for generating a second identifier for the replicated copy.

This brief summary has been provided so that the nature of this disclosure may be understood quickly. A more complete understanding of the disclosure can be obtained by reference to the following detailed description of the various embodiments thereof in connection with the attached drawings.

As a preliminary note the terms component module system and the like as used herein are intended to refer to a computer related entity either software executing general purpose processor hardware firmware and a combination thereof. For example a component may be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer.

By way of illustration both an application running on a server and the server can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers. Also these components can execute from various computer readable media having various data structures stored thereon. The components may communicate via local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems via the signal .

Computer executable components can be stored for example on computer readable media including but not limited to an ASIC application specific integrated circuit CD compact disc DVD digital video disk ROM read only memory floppy disk hard disk EEPROM electrically erasable programmable read only memory memory stick or any other storage device in accordance with the claimed subject matter.

In one embodiment a method and system for uniquely identifying a replicated copy of a storage volume is provided. A unique identifier is created by a storage system managing the replicated copy. The unique identifier includes a time stamp of when the unique identifier is being generated a system clock of the storage system and a unique address for an adapter that is used by the storage system.

In another embodiment a machine implemented method for renaming a replicated copy of a storage volume is provided. A unique identifier for the storage volume is first generated. The unique identifier of the storage volume includes a time stamp based on when the unique identifier is being generated an original identifier that identifies the storage volume of the replicated copy a system clock of a storage system managing the storage volume and a unique address of an adapter used by the storage system.

Thereafter a first identifier for the replicated copy is derived from the unique identifier for the storage volume by replacing the original identifier for the storage volume with an original logical identifier for the replicated copy. A second identifier for the replicated copy is also derived from the unique identifier of the storage volume by replacing the original identifier of the storage volume with an original physical identifier of the replicated copy and replacing the time stamp of the unique storage volume identifier with an original time stamp of the replicated copy.

Storage environment may include a plurality of storage systems each coupled to a storage subsystem . Each storage subsystem may include multiple mass storage devices may also be referred to as storage device or storage devices that may be used to store a plurality of data containers. The mass storage devices may be for example conventional magnetic disks optical disks such as CD ROM or DVD based storage magneto optical MO storage flash memory storage device or any other type of non volatile storage devices suitable for storing data. The storage devices in each storage subsystem can be organized into one or more redundant array of inexpensive disks RAID groups in which case the corresponding storage system accesses the storage subsystem using an appropriate RAID protocol.

Each storage system may operate as a network attached storage NAS based file server a block based storage server such as used in a storage area network SAN or a combination thereof or a node in a clustered environment described below with respect to or any other type of storage server. Note that certain storage systems from NetApp Inc. the assignee of the present application are capable of providing clients with both file level data access and block level data access.

Storage environment may also include a plurality of client systems . . may also be referred to as client or clients a management console and at least one network communicably connecting the clients storage systems and management console . Clients may be general purpose computers connected to the storage systems via the computer network such as a packet switched network.

Processors executing instructions in storage systems for example operating system and clients communicate according to well known protocols such as the NFS protocol or the CIFS protocol to make data stored on disk appear to users and or application programs as though the data were stored locally on the client systems . CIFS means the standard Common Internet File System Protocol an access protocol that client systems typically use to request file access services from storage systems over a network. NFS means Network File System a standard protocol that allows a user to access storage over a network.

The operating system can present or export data stored on disks as a volume to each client . Each volume may be configured to store data files scripts word processing documents executable programs and the like.

The term volume as used herein is a logical data set which is an abstraction of physical storage combining one or more physical mass storage devices e.g. disks or parts thereof into a single logical storage object and which is managed as a single administrative unit such as a single file system. A volume is typically defined from a larger group of available storage such as an aggregate .

The term aggregate as used herein is a logical aggregation of physical storage i.e. a logical container for a pool of storage combining one or more physical mass storage devices e.g. disks or parts thereof into a single logical storage object which includes or provides storage for one or more other logical data sets at a higher level of abstraction e.g. volumes .

From the perspective of one of the client systems . . each volume can appear to be a single disk drive. However each volume can represent the storage space in one disk an aggregate of some or all of the storage space in multiple disks a RAID group or any other suitable set of storage space.

The storage volumes used herein may also be striped . An example of a striped volume is described in detail below with respect to . Briefly a striped volume is a volume that includes a plurality of addressable stripes. When data is stored to a stripe it may be physically stored across multiple disks. Striped volumes are commonly used in a cluster based storage environment that is described below in detail with respect to .

Referring back to in a typical mode of operation one of the client systems transmits one or more input output commands such as an NFS or CIFS request over the computer network to one of the storage systems . The operating system receives the request issues one or more I O commands to storage device to read or write the data on behalf of the client system and issues an NFS or CIFS response containing the requested data over the network to the respective client system .

The management console shown in system executes storage management application may also be referred to as management application may be for example a conventional PC workstation or the like. In another embodiment management application may also be executed by storage system .

The management application may be a module with executable instructions typically used by a storage network administrator to manage a pool of storage devices . Management application enables the administrator to perform various operations such as monitoring and allocating storage space in the storage pool creating and deleting volumes directories and others.

Communication between the storage management application clients and operating system may be accomplished using any of the various conventional communication protocols and or application programming interfaces APIs the details of which are not germane to the technique being introduced here. This communication can be implemented through the network or it can be via a direct link not shown between the management console and one or more of the storage systems .

One or more other storage related applications may also be operatively coupled to the network residing and executing in one or more other computer systems . Examples of such other applications include data backup software snapshot management software and others. It is noteworthy that these applications may also be executed by storage system or any other computing system.

Information stored at storage volumes that are presented by the operating system is often replicated for disaster recovery redundancy and other reasons. There are different techniques that may be used to replicate information stored at storage volumes. One such technique is by taking snapshots without derogation to any trademark rights of NetApp Inc. . A snapshot is a persistent point in time image of a file system that enables quick recovery of data after data has been corrupted lost or altered. Snapshots can be created by copying the data at each predetermined point in time to form a consistent image.

Typically a snapshot is initiated by an application for example SnapManager provided by Netapp Inc. that sends a snapshot request to operating system . The snapshot request may be sent via an interface not shown for example SnapDrive provided by Netapp Inc. or directly.

Based on the snapshot request the operating system obtains information regarding one or more storage volumes and then replicates the affected volumes. Various snapshots may be taken at different times and then stored and managed by operating system . The details of creating the snapshots are not described in detail since they are not germane to the inventive embodiments.

One challenge in managing snapshots is the mechanism by which snapshots are identified. In conventional systems a certain number of identifiers for example 1 to 255 are used by the operating system to identify a snapshot. The operating system first uses all the 255 identifiers. Once all the 255 identifiers have been used operating system reuses identifiers after a snapshot using a particular identifier is deleted. For example a snapshot maybe identified by using identifier 1. When the snapshot identified by identifier 1 is deleted then operating system can re use identifier 1 for identifying another snapshot.

This approach of recycling snapshot identifiers has shortcomings. For example operating system can only uniquely identify snapshots. Another shortcoming is evident from the following example a first request to rename a snapshot identified by identifier 5 is sent by a client to operating system . While the first request is in transit operating system gets another request to delete the snapshot identified by identifier 5. The operating system deletes the snapshot with identifier 5 and reuses identifier 5 to name another snapshot.

Thereafter the original request is received and operating system renames a different snapshot i.e. the new snapshot identified by identifier 5. This is an undesirable situation because not only one can rename a wrong snapshot one may also delete a wrong snapshot by mistake.

The embodiments disclosed herein provide a mechanism for uniquely and efficiently identifying snapshots without having to recycle identifiers.

In block S an instance UUID may also be referred to as instance identifier is assigned for the requested snapshot. The instance UUID identifies the snapshot and its physical data layout. If any two snapshots have the same instance UUID then they are a different instance of the same snapshot i.e. a same data block appears at the same exact physical location for example a physical volume block number used for managing the physical storage space.

In block S a version unique universal identifier UUID is assigned for the requested snapshot. The version UUID may also be also referred to as version identifier is used to identify a snapshot and its logical data layout. If any two snapshots have the same version UUID then their contents may be logically equivalent.

The version UUID may be provided by the request of block S for example if the request is for replicating a source snapshot. The version UUID used for replicating the source snapshot is the same as the version UUID of the source snapshot. In this case operating system uses the version UUID that is included with the request.

If no version UUID is provided with the request of block S then the instance UUID of block and the version UUID of block Sare the same.

In block S the snapshot request is processed and a snapshot is generated using the UUIDs described above in blocks Sand S

Component is a system clock of the storage system that generates the snapshot. The system clock information is also available to the operating system .

Component is a unique address of an adapter that is used by the storage system to communicate with other devices. In one embodiment the unique address may be a Media Access Control MAC address of a network adapter used by the storage system for network communication. The MAC address is typically a globally unique number that may be assigned by a network adapter manufacturer. The MAC address is available to the operating system during storage system initialization.

As explained above the version UUID for a non striped storage volume is the same as the instance UUID when a new snapshot is generated. After a snapshot is generated if the operating system replicates the snapshot then the replicated snapshot may have the same version UUID as the source snapshot and a different instance UUID.

The instance UUID may include components . Component also includes a time stamp of when the instance UUID is created while components and are similar to and . Component is also generated by operating system because it is aware of its own system clock and can generate a timestamp based on when the instance UUID is being created.

The following example shows how the version UUID and instance UUID are generated when information is stored across a striped volume. Assume that a striped volume X has data stored across three volumes volume A volume B and volume C. One of the volumes among the three for example volume A may be designated by operating system to operate as a master volume.

When a snapshot of the striped volume is taken first the version UUID is generated for master volume A. The instance UUID is also generated for the master volume. In one embodiment the version UUID and instance UUID may be the same for the master volume A.

After the master volume A is identified the snapshot UUID for volumes B and C are generated by operating system . The version UUID for volumes B and C is the same as the version UUID of the master volume A while the instance UUID for volumes B and C are different from that of master volume A.

The UUID scheme described above has numerous advantages. For example one is not limited to a certain number of snapshots for example 255. Furthermore because identifiers don t have to be recycled it reduces the chances of renaming or deleting a wrong snapshot because each snapshot is uniquely identified.

Another advantage is that one can universally search for snapshots using the UUID scheme. This makes is easier to manage the snapshots. This is especially useful in a distributed architecture for example a cluster environment where snapshots are stored and managed by multiple nodes in a network environment.

In one embodiment a method and system is provided for translating a snapshot identifier or renaming a snapshot that was generated before the UUID scheme of was available. A process for translating the snapshot identifier is shown in . The translation operation maybe a part of an overall upgrade process that starts in block S for upgrading a storage volume. The upgrade process may take place when a newer version of operating system is installed or for any other reason. The upgrade process may be requested by an administrator using management console or initiated by any other means.

In block S a UUID is first generated for a storage volume that was replicated by the snapshot. An example of the UUID for the storage volume is shown as in that is described below. Briefly the UUID includes a timestamp of when the UUID is being created for the storage volume a system clock of the storage system that renames the snapshot a unique adapter address and a storage volume identifier.

In block S a version UUID shown as in and may also be referred to as the first identifier with respect to process for the snapshot is generated using an original logical identifier that was used to identify the original snapshot. The various components of the version UUID are also described below in detail with respect to .

In block S an instance UUID shown as in and may also be referred to as the first identifier with respect to process is generated based on a time stamp of the original snapshot and an original physical snapshot identifier. The details of the instance UUID are also described below in detail with respect to .

The storage volume UUID may include components . Component is a timestamp showing a time when the UUID for the storage volume is created. This component may be generated by operating system that has access to system level information including time and clock information. Component is the system clock similar to and component is the adapter identifier similar to .

Component includes an original volume identifier that identifies the storage volume prior to the upgrade process. The volume identifier is used by operating system to identify the storage volume.

The version UUID includes components . Component includes the same timestamp value as component . Components and are also similar to components and having the same clock and adapter identifier.

Component is created by replacing the original volume identifier with a logical snapshot identifier shown as in when the snapshot was previously created. This logical snapshot identifier may be based on an assigned value of 1 255 as described above. The logical snapshot identifier may be used by a cluster based system. For a non cluster based system a physical snapshot identifier having a value of 1 255 may be used to create component because a non cluster based system may not use a logical snapshot identifier and instead uses the physical snapshot identifier.

The instance UUID includes components . Component is derived by replacing component with the original timestamp of the snapshot. This information may be obtained by the operating system from field of snapshot table or data structure as shown in and described below in detail. Component and are the same as and . Component is based on the original physical snapshot identifier. The physical identifier is assigned by operating system when the snapshot was created and may be from 1 255.

The embodiments described above allow one to rename previously taken snapshots or translate snapshot identifiers to the UUID format. The translation retains information from the original identifiers while uniquely identifying the snapshots using the UUID format. This allows a storage administrator to name new snapshots and the old snapshots under the same UUID scheme. This allows a storage administrator to better manage the snapshots using the UUID scheme without losing snapshot identifier information that was prevalent prior to the UUID scheme.

The process flow diagrams described above may be used and implemented in a cluster based storage system that is now described below in detail.

Snapshots and may be stored within cluster across multiple volumes including striped volumes. The new snapshots may be named using the process flow of FIG. B C or upgraded based on the process of FIG. D E that is also described above.

Storage environment may also include a plurality of client systems . . may also be referred to as client s management console and at least one computer network similar to network communicably connecting the client systems . . and the clustered storage system .

The clustered storage system includes a plurality of nodes . . may also be referred to as a cluster switching fabric and a plurality of mass storage devices such as disks . . may also be referred to as disks similar to storage . Each of the plurality of nodes . . in the clustered storage system provides the functionality of a storage server.

Each of the plurality of nodes . . may be configured to include an N module a D module and an M host each of which can be implemented as a separate software module. Specifically node . includes an N module . a D module . and an M host . node . includes an N module . a O module . and an M host . and node . includes an N module . a D module . and an M host ..

The N modules . . may also be referred to as include functionality that enables the respective nodes . . to connect to one or more of the client systems . . over the computer network while the D modules . . may also be referred to as connect to one or more of the disks . .. The M hosts . . may also be referred to as provide management functions for the clustered storage server system .

A switched virtualization layer including a plurality of virtual interfaces VIFs may also be referred to a logical interfaces LIFs is provided between the respective N modules . . and the client systems . . allowing the disks . . associated with the nodes . . to be presented to the client systems . . as a single shared storage pool.

In one embodiment the clustered storage system can be organized into any suitable number of virtual servers also referred to as vservers in which each vserver represents a single storage system namespace with separate network access. Each vserver is associated with one or more VIFs and can span one or more physical nodes each of which can hold one or more VIFs and storage associated with one or more vservers. Client systems can access the data on a vserver from any node of the clustered system.

Each of the nodes . . is defined as a computer adapted to provide application services to one or more of the client systems . .. In this context a vserver is an instance of an application service provided to a client system. The nodes . . are interconnected by the switching fabric which for example may be embodied as a Gigabit Ethernet switch. Although depicts an equal number i.e. 3 of the N modules . . the D modules . . and the M Hosts . . any other suitable number of N modules D modules and M Hosts may be provided. There may also be different numbers of N modules D modules and or M Hosts within the clustered storage server system . For example in alternative embodiments the clustered storage server system may include a plurality of N modules and a plurality of D modules interconnected in a configuration that does not reflect a one to one correspondence between the N modules and D modules.

The clustered storage server system can include the NETAPP DATA ONTAP storage operating system available from NetApp Inc. that implements the WAFL storage system or any other suitable storage operating system.

The client systems . . of may be implemented as general purpose computers configured to interact with the respective nodes . . in accordance with a client server model of information delivery. Each client system . . may request the services of one of the respective nodes . . . and that node may return the results of the services requested by the client system by exchanging packets over the computer network which may be wire based optical fiber wireless or any other suitable combination thereof. The client systems . . may issue packets according to file based access protocols such as the NFS protocol or the CIFS protocol when accessing information in the form of files and directories.

In a typical mode of operation one of the client systems . . transmits an NFS or CIFS request for data to one of the nodes . . within the clustered storage server system and the VIF associated with the respective node receives the client request. It is noted that each VIF within the clustered system is a network endpoint having an associated IP address and that each VIF can migrate from N module to N module.

The following provides a description of a storage operating system that may be used in storage environment and according to one embodiment.

An example of operating system is the DATA ONTAP Registered trademark of NetApp Inc. operating system available from NetApp Inc. that implements a Write Anywhere File Layout WAFL Registered trademark of NetApp Inc. file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such where the term ONTAP is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.

Operating system maintains various data structures that are used for implementing the process steps of . For example snapshot data volume information system clock data and unique adapter address .

Snapshot data is maintained by operating system for storing snapshot identifiers as described above. Snapshot data includes version and instance UUIDs.

Volume information is used by operating system for renaming a snapshot as described above with respect to . The volume identifier that is replaced by a snapshot identifier may be included in volume information . Volume information may include other data for example volume size volume attributes permissions storage path and others.

Operating system may obtain a system clock when it is naming or renaming a snapshot. Such information is typically stored by computing systems and accessible to operating system .

The unique adapter address is stored by operating system and is used for uniquely identifying the snapshots as described above. This information is typically obtained by operating system when a computing system is powered up.

In one example operating system may include several modules or layers executed by one or both of N Module and D Module . These layers include a file system manager that keeps track of a directory structure hierarchy of the data stored in storage devices and manages read write operations i.e. executes read write operations on disks in response to client requests. File system may maintain the snapshot data and volume information .

Operating system may also include a protocol layer and an associated network access layer to allow node . to communicate over a network with other systems such as clients and storage management application . Protocol layer may implement one or more of various higher level network protocols such as NFS CIFS Hypertext Transfer Protocol HTTP TCP IP and others as described below.

Network access layer may include one or more drivers which implement one or more lower level protocols to communicate over the network such as Ethernet. Interactions between clients and mass storage devices are illustrated schematically as a path which illustrates the flow of data through operating system .

The operating system may also include a storage access layer and an associated storage driver layer to allow D module to communicate with a storage device. The storage access layer may implement a higher level disk storage protocol such as RAID while the storage driver layer may implement a lower level storage device access protocol such as FC or SCSI.

It should be noted that the software path through the operating system layers described above needed to perform data storage access for a client request received at node . may alternatively be implemented in hardware. That is in an alternate embodiment of the disclosure the storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an ASIC. This type of hardware implementation increases the performance of the file service provided by node . in response to a file system request issued by client .

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node . implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this disclosure can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and a disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present invention may be utilized with any suitable file system including a write in place file system.

Snapshot data structure may include a plurality of fields . Field ID may be used to identify a snapshot using the recycled identifiers 1 255. This field is used to populate components and for the version UUID and the instance UUID respectively as described above with respect to . Operating system obtains the value from Field and uses it to generate the version and or instance UUIDs. Field NAME may be used to store the name of a previously created snapshot.

Field Creation Time may be used to store the time when the snapshot was taken. This field is used to generate component of the instance UUID as described above.

Field ATTR may be used to store attribute information regarding a snapshot. For example field may include a snapshot version indicator indicating a snapshot when a change in status for the data container was discovered.

Snapshots for a stripe may involve multiple volumes. As described above one of the volumes from among the multiple volumes is designated as a master volume. The version UUID for the master volume and the other volumes is the same. The instance identifier of the different volumes is different.

The cluster access adapter comprises a plurality of ports adapted to couple node . to other nodes of cluster . The cluster access adapter may have a unique identifier for example a MAC address that is obtained by operating system and stored as data structure . This information is used to populate and components of the UUIDs that have been described above in detail.

In the illustrative embodiment Ethernet may be used as the clustering protocol and interconnect media although it will be apparent to those skilled in the art that other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternate embodiments where the N modules and D modules are implemented on separate storage systems or computers the cluster access adapter is utilized by the N D module for communicating with other N D modules in the cluster .

Each node . is illustratively embodied as a dual processor storage system executing a storage operating system that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks on disks .. However it will be apparent to those of ordinary skill in the art that the node . may alternatively comprise a single or more than two processor systems. Illustratively one processor A executes the functions of the N module . on the node while the other processor B executes the functions of the D module ..

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing programmable instructions and data structures. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the programmable instructions and manipulate the data structures. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the invention described herein.

The storage operating system portions of which is typically resident in memory and executed by the processing elements functionally organizes the node . by inter alia invoking storage operations in support of the storage service implemented by the node.

The network adapter comprises a plurality of ports adapted to couple the node . to one or more clients . . over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Illustratively the computer network may be embodied as an Ethernet network or a Fibre Channel FC network.

The storage adapter cooperates with the storage operating system executing on the node . to access information requested by the clients. The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to storage devices over an I O interconnect arrangement such as a conventional high performance FC link topology.

The processing system includes one or more processors and memory coupled to a bus system . The bus system shown in is an abstraction that represents any one or more separate physical buses and or point to point connections connected by appropriate bridges adapters and or controllers. The bus system therefore may include for example a system bus a Peripheral Component Interconnect PCI bus a HyperTransport or industry standard architecture ISA bus a small computer system interface SCSI bus a universal serial bus USB or an Institute of Electrical and Electronics Engineers IEEE standard 1394 bus sometimes referred to as Firewire .

The processors are the central processing units CPUs of the processing system and thus control its overall operation. In certain embodiments the processors accomplish this by executing executable instructions stored in memory . A processor may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices.

Memory represents any form of random access memory RAM read only memory ROM flash memory or the like or a combination of such devices. Memory includes the main memory of the processing system . Instructions may be used to implement the techniques introduced above and may reside in and executed by processors from memory .

Also connected to the processors through the bus system are one or more internal mass storage devices and a network adapter . Internal mass storage devices may be or may include any conventional medium for storing large volumes of data in a non volatile manner such as one or more magnetic or optical based disks.

The network adapter provides the processing system with the ability to communicate with remote devices e.g. storage servers over a network and may be for example an Ethernet adapter a Fibre Channel adapter or the like. The processing system also includes one or more input output I O devices coupled to the bus system . The I O devices may include for example a display device a keyboard a mouse etc.

The system and techniques described above are applicable and useful in the upcoming cloud computing environment. Cloud computing means computing capability that provides an abstraction between the computing resource and its underlying technical architecture e.g. servers storage networks enabling convenient on demand network access to a shared pool of configurable computing resources that can be rapidly provisioned and released with minimal management effort or service provider interaction. The term cloud is intended to refer to the Internet and cloud computing allows shared resources for example software and information to be available on demand like a public utility.

Typical cloud computing providers deliver common business applications online which are accessed from another web service or software like a web browser while the software and data are stored remotely on servers. The cloud computing architecture uses a layered approach for providing application services. A first layer is an application layer that is executed at client computers. In this example the application allows a client to access storage via a cloud.

After the application layer is a cloud platform and cloud infrastructure followed by a server layer that includes hardware and computer software designed for cloud specific services. The storage systems described above can be a part of the server layer for providing storage services. Details regarding these layers are not germane to the inventive embodiments.

Thus a method and apparatus for naming and renaming snapshots have been described. Note that references throughout this specification to one embodiment or an embodiment mean that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Therefore it is emphasized and should be appreciated that two or more references to an embodiment or one embodiment or an alternative embodiment in various portions of this specification are not necessarily all referring to the same embodiment. Furthermore the particular features structures or characteristics being referred to may be combined as suitable in one or more embodiments of the invention as will be recognized by those of ordinary skill in the art.

While the present disclosure is described above with respect to what is currently considered its preferred embodiments it is to be understood that the disclosure is not limited to that described above. To the contrary the disclosure is intended to cover various modifications and equivalent arrangements within the spirit and scope of the appended claims.

