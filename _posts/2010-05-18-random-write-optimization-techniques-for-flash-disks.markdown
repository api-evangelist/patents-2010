---

title: Random write optimization techniques for flash disks
abstract: Disclosed is a method for managing logical block write requests for a flash drive. The method includes receiving a logical block write request from a file system; assigning a category to the logical block; and generating at least three writes from the logical block write request, a first write writes the logical block to an Erasure Unit (EU) according to the category assigned to each logical block, a second write inserts a Block Mapping Table (BMT) update entry to a BMT update log, and a third write commits the BMT update entry to an on-disk BMT, wherein the first and second writes are performed synchronously and the third write is performed asynchronously and in a batched fashion.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08880784&OS=08880784&RS=08880784
owner: Rether Networks Inc.
number: 08880784
owner_city: Stony Brook
owner_country: US
publication_date: 20100518
---
This application claims priority under 35 U.S.C. 119 e to an application entitled Log structured Flash Storage Manager which was filed in the United States Patent and Trademark Office on Dec. 2 2009 and assigned Ser. No. 61 265 926 Jan. 19 2010 the content of which is hereby incorporated by reference.

The recent commoditization of Universal Serial Bus USB based flash disks mainly used in digital cameras mobile music video players and cell phones has many pundits and technologists predicting that flash memory based disks will become the mass storage of choice on mainstream laptop computers in two to three years. In fact some of the ultra mobile Personal Computers PCs already use flash disks as the only mass storage device. Given the superior performance characteristics and enormous economies of scale behind the flash disk technology it appears inevitable that flash disks will replace magnetic disks as the main persistent storage technology at least in some classes of computers.

Compared to magnetic disks flash disks consume less power occupy less space and are more reliable because they do not include any moving parts. Moreover flash disks offer superior latency and throughput because they work similar to a Random Access Memory RAM chip and do not incur any head positioning overhead. However existing flash disk technology has two major drawbacks that render it largely a niche technology at this point.

Second flash disk performance is better than a magnetic disk when the input workload consists of sequential reads random reads or sequential writes. Under a random write workload flash disks performance is comparable to that of magnetic disk at best and in some cases actually worse. The flash disks random write performance problem is rooted in the way flash memory cells are modified and thus cannot be easily addressed.

A flash memory chip is typically organized into a set of Erasure Units EUs typically 256 Kbytes each of which is the basic unit of erasure and in turn consists of a set of 512 byte sectors which correspond to the basic units of read and write. After an EU is erased subsequent writes to any of its sectors can proceed without triggering an erasure if their target addresses are disjoint. That is after a sector is written to and before it can be written to a second time the sector must be erased first. Because of this peculiar property of flash memory random writes to a storage area mapped to an EU may trigger repeated copying of the storage area to a free EU and erasing of the original EU holding the storage area resulting in significant performance overhead.

Flash disks are typically produced with a Flash Translation Layer FTL which is implemented in firmware. The FTL maps logical disk sectors which are exposed to the software to physical disk sectors and performs various optimizations such as wear leveling which equalizes the physical write frequency of the EUs. This logical to physical map requires 64 million entries in order to keep track of individual 512 byte sectors on a 32 GB flash disk. To reduce this map s memory requirement flash disks increase the mapping granularity sometimes to the level of an EU. As a result of this coarser mapping granularity two temporally separate writes to the same mapping unit e.g. an EU will trigger a copy and erasure operation if the target address of the second write is not greater than that of the first write because a flash disk cannot always tell whether a disk sector in an EU has already been previously written to. That is if an Nth sector of a mapping unit is written to any attempt to write to any sector whose sector number is less than or equal to N will require an erasure even if the target sector itself has not been written to at all. Consequently coarser mapping granularity further aggravates flash disks random write performance problem.

An aspect of the present invention is to address at least the above mentioned problems and or disadvantages and to provide at least the advantages described below.

Accordingly an object of the present invention is to provide a Log structured Flash Storage Manager LFSM apparatus and method that effectively solves at least the above described problems.

According to the present invention there is provided a method for managing logical block write requests for a flash drive including receiving a logical block write request from a file system assigning a category to the logical block and generating at least three writes from the logical block write request a first write writes the logical block to an Erasure Unit EU according to the category assigned to each logical block a second write inserts a Block Mapping Table BMT update entry to a BMT update log and a third write commits the BMT update entry to an on disk BMT wherein the first and second writes are performed synchronously and the third write is performed asynchronously and in a batched fashion.

Hereinafter embodiments of the present invention will be described in detail with reference to the accompanying drawings. In the following description of the present invention a detailed description of known technologies incorporated herein will be omitted when it may obscure the subject matter of the present invention.

To address the random write performance problem of the prior art flash disks a Log structured Flash Storage Manager LFSM of the present invention converts all random writes into sequential writes to a set of unified logs by introducing an additional level of indirection above the Flash Translation Layer FTL . Because all prior art flash disks have good sequential write performance LFSM effectively solves the random write performance problems for these prior art flash disks in a uniform way without requiring any modifications to the prior art flash disk hardware implementations. With this novel log structured storage organization the LFSM overcomes two additional major challenges.

First the LFSM still faces random writes because the LFSM maintains a separate map for the level of indirection or translation the LFSM introduces. Writes to this map are random. The LFSM minimizes the performance overhead of these random writes by using a technique referred to as Batching Updates with Sequential Commit BUSC .

Second to minimize the amount of copying required whenever the LFSM reclaims an EU the LFSM allocates EUs to logical blocks in such a way that the logical blocks assigned to the same EU have a similar life time and each EU contains a stabilized utilization ratio which means it is less likely that the utilization ratio will change in the future.

LFSM is a storage manager that is positioned between a file system and a flash disk s native driver. LFSM can be considered an auxiliary driver specifically designed to optimize the random write performance of existing flash disks in a disk independent way. A property shared by all prior art flash disks is good sustained throughput for sequential writes between 30 60 MB sec. The LFSM converts random writes into sequential writes so as to eliminate random writes from the workload of the flash disk. To perform such conversion the LFSM implements a linear disk address space exposed to the file system using multiple logs and turns every incoming logical write into a physical write to the end of one of these logs which is mapped to different active EUs. Because writes to each log are sequential within an EU their performance is the same as sequential write performance.

As shown in LFSM is positioned between file system database and flash drive . Accordingly there are two address spaces in this design the file system and or user applications see a linear sequence of logical blocks exposed by LFSM and the native flash disk driver exposes a linear sequence of physical blocks to LFSM . The main data structures of the system are the BMT the BMT update log and the data region. The main components of LFSM are the synchronous logging module the BMT update commit manager and the garbage collection module . Flash drive includes BMT region .

The LFSM consists of two threads. A main thread is responsible for synchronous flash disk logging and another thread a background thread is responsible for asynchronous Block Mapping Table BMT commit and garbage collection. The main function of the logging module is to handle the conversion of a random write to a sequential write. While receiving a random logical write request the logging module converts the random write request to a sequential write address in one of the three different temperature logs e.g. a hot log a warm log and a cold log or categories based on an amount of activity associated with the logical block. Although three logs are used as an example herein any number of logs greater than two can be implemented. In addition the use of the word temperature does not imply heat but is used to describe the amount of activity associated with a particular block. The three different temperature logs store data blocks of different life time which is defined as the time interval between two consecutive writes to a given block. As will be discussed further below logging data blocks with a similar life time to the same EU increases the garbage collection performance.

A Log structured File System LFS was one of the earliest works on organizing the entire file system as a log in order to mitigate the disk Input Output I O bottleneck problem of flash disks. The LFS maintains a single log of segments and uses a product of segment age and segment utilization ratio as a metric to determine the order in which segments are reclaimed. In contrast the LFSM advocates multiple logs each of which is designed to hold data blocks with a distinct estimated life time range. The LFSM maintains a fixed sized Least Recently Used LRU hot list Hlist or hot list to move the least recently used log EU to the Least Valid Page LVP heap LVP heap . The LFSM sorts the EUs in LVP heap according to their utilization ratio which is defined as the percentage of an EU that contains live data blocks and the root of the LVP heap has the EU with the minimum utilization ratio. The LFSM chooses to reclaim the EU corresponding to the LVP heap s root because it is stable and has the lowest utilization ratio instead of reclaiming EUs only according to their utilization ratio e.g. smallest first as in the case of LFS.

As LFSM converts a Logical Block Address LBA to a Physical Block Address PBA LFSM needs to look up the BMT to perform this LBA to PBA conversion. The BMT is stored on disk. To mitigate the performance penalty due to disk I O operations associated with BMT look up LFSM incorporates a BMT cache that utilizes an interval based data structure to cache the most recently used BMT records in the memory. A BMT update log is a circular log to record the pending modified BMT entries i.e. the BMT entries that have been modified and not yet committed to flash disk . As will be discussed in more detail below LFSM also uses the BMT update log to reconstruct the pending BMT entries after a system crash.

The BMT commit manager along with the on disk BMT manager manages the process of committing pending BMT records to an on disk BMT region . To ensure that the BMT commit is a sequential process LFSM brings in an EU worth of BMT entries commits pending updates to those BMT entries and writes the modified BMT entries back to the same EU. After this the BMT commit manager also retires the corresponding pending records in the BMT update log . The order in which BMT EUs are brought in is determined based on the following considerations a the effective number of pending BMT updates committed and thus the space in per page BMT update queues freed and b the extent to which the global frontier in the BMT update log is moved and thus the extent to which old entries in the BMT update log are freed. When the free space in the BMT update log runs low there is a need to focus exclusively on consideration b to free up enough space in the BMT update log to continue LFSM s normal operations. Consideration a is referred to as BMT popularity commit and consideration b is referred to as BMT critical commit.

To reclaim unused space on the logs LFSM performs garbage collection in the background through the use of the garbage collection unit . The performance cost of reclaiming an EU is primarily linked to the copying out of the live physical blocks stored in the EU and is thus proportional to the number of such blocks at the time of reclamation. To minimize the performance overhead associated with garbage collection the LFSM garbage collection process selects for garbage collection the least utilized EU i.e. an EU whose number of live blocks is the lowest .

Upon receipt of a write request associated with a LBA LFSM performs a BMT query to identify the temperature of the LBA. To accelerate the BMT look up procedure LFSM utilizes the BMT cache . Based on the LBA s temperature LFSM logs the write request s payload to the corresponding EU and updates the LBA s associated BMT entry with its new PBA. To prevent BMT corruption due to a crash LFSM puts this modified BMT record in an in memory BMT update request queue logs this modified BMT record to the BMT update log and finally returns a write success hardware interrupt.

As mentioned above the main thread is responsible for handling the synchronous flash disk logging. The synchronous flash disk logging is accomplished through the use of a process I O request function e.g. process io request . The process I O request function is an I O handler function registered to the kernel e.g. a Linux kernel by a blk queue make request Application Programming Interface API with an I O queue i.e. disk queue . While receiving the I O request the read write handlers are called appropriately. The write I O handler e.g. i write bio is defined in an I O write file e.g. io write.c . The details of the I O handler are described as follows 

Step 1. Get bio container and resolve the conflict I O a bio container is retrieved and I O conflicts are resolved. The bio container is the basic data structure to hold the I O request which provides the information of the original bio e.g. bio container.org bio and a newly constructed sequential bio e.g. bio container.bio . Any conflicts of the write to the same physical address are properly handled by placing the requests in a wait queue.

Step 3. Handle the unaligned I O request The I O request unit i.e. the page size PAGE SIZE of LFSM is 4 kilobytes KB . When the I O request is not aligned to the 4 KB LFSM unit the missing data is read out from the disk and copied to the sequential bio which is referred to as a read modify write operation. The handling of the unaligned I O requests checks the head and tail address of the bio for the unaligned I O request and performs read modify write to construct the buffer of the sequential bio. If the I O is aligned the buffer of the sequential bio is simply copied from the original bio.

Step 4. Get the destination sector The target disk sector is obtained through a function which generates the next physical block number e.g. get dest pbno . In this function the temperature of the data is determined and the sequential destination address is returned.

Step 5. Issue the write request The write I O request for the sequential bio is issued to the hardware driver through a finish write function e.g. finish write . Also the BMT update entry has to be logged to the update log synchronously. This is to ensure safe recovery even if the system crashes while some BMT updates are not committed to disk.

The read I O handler is similar to the write I O handler and is well defined in an I O read e.g. io read.c .

As also introduced above the background thread is responsible for the asynchronous BMT commit and the garbage collection. The entry point for the background thread is handled by a background thread function e.g. lfsm bg thread . The garbage collection is handled by a garbage collection function e.g. gc collect valid blocks and the BMT update commit is handled by a BMT commit manager function e.g. BMT commit manager .

The logging module of LFSM converts a random logical LBA to a sequential PBA belonging to one of the active logs depending on the temperature of the LBA. The temperature logging idea eases the garbage collection overhead. LFSM categorizes write data into multiple temperature levels e.g. three temperature levels hot warm and cold . The cold data is expected to have the longest life time warm data has a medium life time and hot data has the shortest life time. Initially all LBAs are cold. After a LBA is written once it becomes warm. If a warm LBA is written once again it becomes hot and continues to stay hot as it is written even more times. The temperature level of a LBA drops one level whenever it is copied out during a garbage collection operation as live data. A cold LBA continues to stay cold as it is overwritten even more times.

All EUs are categorized into three different groups in LFSM . The three groups are free recently used and ones whose utilization ratios stabilize. LFSM links all free EUs in a linked list e.g. HListGC.free list . Active EUs are picked from the free list. When one active EU becomes full the full EU is moved to a garbage collection heap called LVP Heap e.g. HListGC.LVP Heap expecting that this EU would be stable by default. If an LBA within an EU which is in LVP Heap is invalidated over written it implies that the EU utilization is not yet stabilized. This EU is moved to a hot list e.g. HListGC.hot list . The EUs in the hot list might have different utilization ratios as time passes. If the hot list is full the least recently used EU in the hot list will be moved to the LVP Heap. The EUs in the garbage collection heap are considered to have stabilized utilization ratios.

The BMT module maps an LBA to a PBA. As show in the BMT module is divided into three subsystems the on disk BMT manager the BMT cache and the BMT update log . LFSM manages the on disk BMT as an array indexed by LBA and stored on disk. illustrates an example where one BMT EU can store up to 64K BMT records. On disk BMT look up can be simply served by a disk read I O with an LBA offset.

When a write request with a target LBA triggers a BMT update this BMT update is inserted into an in memory queue associated with the disk block that contains the LBA s BMT entry. This queue is referred to as a per page BMT update queue which also doubles as the BMT cache. The BMT lookup process is defined in a BMT lookup function e.g. bmt lookup . The BMT lookup function queries the BMT cache using a per page queue BMT cache lookup function e.g. PPQ BMT cache lookup and if the target BMT record is not in the BMT cache a read to on disk BMT function e.g. read small disk io temp is called to obtain the target BMT entry from the on disk BMT and a per page queue BMT cache insert function e.g. PPQ BMT cache insert nonpending is called to insert the BMT entries read from disk into the BMT cache .

The BMT cache mitigates the disk I O performance overhead associated with the BMT look up procedure. The BMT cache is arranged in a per BMT page structure as shown in . The data in the BMT cache can be categorized as pending BMT entries and non pending BMT entries. The pending BMT entries represent the BMT entries that have yet to be committed to the on disk BMT while the non pending BMT entries have been committed to the on disk BMT . When the logging module logs a new pending BMT record to the BMT update log LFSM also updates the BMT cache for the same entry. This was previously referred to as the synchronous write steps. During the BMT lookup processes if the access to the BMT cache is a miss the on disk BMT manager performs a BMT query by fetching one sector worth of BMT entries that contain the target BMT entry. All of the BMT records in this sector will be added to the BMT cache . When the BMT cache is full LFSM ejects non pending BMT entries in the least recently used BMT EU. Although the interval based BMT cache saves memory space by aggregating the adjacent BMT entries the interval based BMT cache also introduces additional complexity to merge and split the BMT entries. While inserting a BMT entry into the BMT cache the BMT entry must be merged with the adjacent entries if they have contiguous PBAs. While ejecting or updating the BMT entry one BMT entry may need to be split apart into different intervals.

Although LFSM has converted random LBA writes into consecutive PBA writes the BMT commit manager has to randomly write BMT entries to the LBA. LFSM solves this problem by using the BUSC scheme to synchronously log the BMT update and asynchronously commit multiple updates to the BMT in a batched fashion. Because of the existence of the on disk BMT update log even if the system crashes the BMT updates that have not been flushed to the on disk BMT can be correctly reconstructed at recovery time. The BMT commit manager asynchronously commits the BMT pending records through aggregated and sequential writes to reduce the performance overhead of the random writes to the BMT .

Using BUSC to update the BMT means that each logical block write operation triggers three related write operations. The first write operation writes a new version of the logical block to an EU according to the logical block s temperature the second write operation logs the associated BMT update to the BMT update log and the third write operation actually updates the corresponding on disk BMT entry. The first two writes are done synchronously and the third write is done asynchronously and in a batched fashion.

The BMT update commit manager ensures that uncommitted BMT updates can be correctly recovered when the system crashes and thus makes it possible to commit pending BMT updates in an efficient manner without compromising the integrity of the BMT .

The on disk BMT update log is a circular sequence of EUs with two pointers i.e. a tail pointer and a head pointer . The logging module writes the pending BMT records to the BMT update log and moves the head pointer to the next free write sector. After the BMT commit manager commits a pending BMT update to the on disk BMT it will release the corresponding BMT record in the BMT update log by freeing the space it occupies. The space in the BMT update log is reclaimed on an EU by EU basis. If all of the BMT records in the EU pointed to by the tail pointer EU are released the tail pointer is moved toward the next adjacent EU. The size of the BMT update log defines the maximum number of pending BMT update records in LFSM . When the BMT update log is full which means the head pointer and tail pointer have overlapped due to the circular nature of the logging process the incoming write remains pending until the BMT commit manager can move ahead the tail pointer EU. The BMT update log entry is designed as an aggregated BMT entry e.g. A BMT E structure. The advantage of using an interval based representation for BMT update log entries is because if LFSM receives a write request that spans more than one sector only one BMT update log entry is needed by properly setting the run length field e.g. run length .

When a machine crashes LFSM can recover uncommitted data. LFSM scans through the BMT update log and reconstructs the pending BMT entries according to the sequence number in the BMT update log entries. To facilitate the identification of not yet committed BMT updates LFSM includes the following information in the BMT update log entry associated with each logical block write operation 1 LBA PBA and run length 2 a unique sequence number assigned to each write request and 3 a commit point. The commit point refers to the sequence number of the youngest logical block write request of all the BMT updates before which have already been committed to disk. That is the commit point indicates where the backward traversal of the BMT update log can stop during the crash recovery process. With this information LFSM reconstructs pending BMT updates by first identifying the latest or youngest BMT log entry whose sequence number is N then obtaining its associated commit point whose sequence number is N and finally reading in all the BMT update log entries between N and N to insert them into their corresponding per page BMT update queues.

The logging of the BMT updates creates a space overhead problem. Because the minimum unit for reading and writing a flash disk is a 512 byte sector each BMT update log entry requires a 512 byte sector even though in actuality it may only require 22 bytes. Thus the space overhead associated with BMT logging is about 12.5 512 bytes for every 4 KB page which is too high to be acceptable. LFSM sitting above to the firmware level cannot utilize the out of band area of each block. To minimize the performance overhead LFSM preserves 10 Mb of disk space dedicated for the BMT update log on a 64 GB disk. The BMT update log disk space can store up to 20K BMT update log entries. A BMT update log function e.g. BMT update log init is a BMT update log commit function e.g. bmt update log.c handles the configuration of the BMT update log .

With the above design LFSM successfully services each logical block write operation using one sequential write to the BMT update log and another sequential write to the active EU and thus greatly improves the random write performance of modern flash disks. However the BMT update log introduces an additional disk write penalty. One way to solve the additional disk write penalty is to put the BMT update log and active EUs onto different disks so as to perform write operations to them in parallel.

The BMT cache is used to improve the performance of the BMT look up process. The BMT cache is embedded in the per page BMT update queues and consists of non pending entries clean and pending entries dirty that form a sorted linked list. In order to save memory space consecutive BMT entries in the same per page queue with consecutive Logical Block Number LBN or lbno and Physical Block Number PBN or pbno can be merged together as an aggregated BMT entry i.e. A BMT E . For example BMTLBN 100 PBN 200 and BMTLBN 101 PBN 201 can be merged as BMTLBN 100 PBN 200 runlength 2. During normal operations a BMT look up into per page queues requires O n complexity and an insert of an entry into per page queues also requires O n complexity where n is the average length of each per page queue.

A threshold e.g. PPQ CACHE T HIGH is set for a maximum number of non pending entries that can exist in the BMT cache . The background thread is responsible for detecting when the total number of non pending BMT cache entries i.e. BMT.total non pending items exceeds the threshold and for removing a proper number of entries from the BMT cache when it does. A corresponding control algorithm for the total pending entry count can be handled by the BMT commit manager .

In Table 1 sector t lbno is the logical block number associated with an BMT update entry sector t pbno is the physical block number associated with the entry int run length is the length of the entry in number of consecutive blocks struct list head ppq abmte is a pointer to the next BMT entry in the same per page queue and unsigned short pending defines whether a BMT entry is a pending entry or a non pending entry.

The purpose of a per page queue BMT update function e.g. PPQ BMT update is to insert one single aggregated BMT entry into a per page queue BMT cache e.g. ppq.bmt cache . To insert an aggregated BMT entry into the BMT cache by traversing from the head of the BMT cache list the following factors need to be taken into consideration whether the new entry needs to be merged to existing entries triggers a split of existing entries or is an independent entry. The non pending count and pending count need to be adjusted for the BMT cache as well.

It is not sufficient to only evict the smallest number of entries so as to keep the total number of non pending entries in the BMT cache less than the PPQ CACHE T HIGH because the cache may become full in a short period of time and triggers an additional round of eviction. To address this problem another threshold i.e. PPQ CACHE T LOW is used. When a cache full condition arises a number of entries greater than or equal to PPQ CACHE T HIGH PPQ CACHE T LOW are evicted in order to keep the total pending entries less than PPQ CACHE T LOW. PPQ CACHE T HIGH and PPQ CACHE T LOW are set by default to 10K and 8K but can be modified as system needs require and resources allow.

The BMT Commit Manager BCM commits in memory pending BMT updates to the on disk BMT . This process is also referred to as BMT commit or simply commit. BMT commit is always performed in the background thread in a BMT commit manager function e.g. BMT commit manager . Batching Updates with Sequential Commit BUSC is used in the BCM . Specifically BUSC batches updates to basic units of an on disk data structure and commits these updates to each of these units in an order that is tailored to the need of the data structure in question.

Commit and BMT update logging are highly inter dependent. At any given point in time the pending BMT updates that are not yet committed should exist securely in the BMT update log as well as per page BMT update queues. Even if the system crashes before the pending updates are committed to the disk this backup information from the BMT update log can be used to recover these pending BMT updates. Crash recovery will be described in greater detail below.

Since BCM implementation is interleaved with that of BMT update logging BMT update logging will now be described. BMT update log is treated as a circular log of EUs where new writing operations are against its head EU. Once the head EU reaches the end of the update log it wraps around. After committing the pending BMT entries in the tail EU of the BMT update log LFSM moves the tail pointer ahead to reclaim the old tail EU. After every commit the sectors in the update log holding the committed updates are freed i.e. can be re used. The design goal of BMT update logging is to keep as much free space as possible available in the update log minimize the number of commits and to make sure that the tail EU of the BMT update log is not overrun by the head EU. Based on the aforementioned constraints there are two types of commit deployed by the BCM popularity based and critical.

For each BMT EU LFSM maintains a data structure that records all the pending updates to BMT entries in that BMT EU. This data structure is referred to as a dependency list is implemented as an array of per page update queues and is released after updates to the corresponding BMT EU are omitted. Similarly every BMT update log EU maintains information about the set of BMT EUs that have pending updates residing on the BMT update log EU. These dependency lists are contained in a per page BMT update log queue array. This list is populated during a BMT update logging operation e.g. BMT update log and is shrinked when pending updates to a BMT EU are committed.

Popularity of a BMT EU is defined as the number of pending BMT updates whose target BMT entries fall into that BMT EU. The BMT EU that has the maximum number of pending updates is called the most popular BMT EU. An array used to maintain this popularity information is the BMT EU update log per page queue array e.g. bmt eu2 ul popularity . The BCM starts operating after at least 25 of the BMT update log is full. At this point popularity based commit happens i.e. pending updates of the most popular BMT EU will be committed so as to free a maximum number of sectors from the BMT update log. The desired EU is brought into memory from the on disk BMT using a read BMT page function e.g. read bmt page its content is modified by committing this BMT EU s pending updates using a per page queue BMT commit build page buffer function e.g. PPQ BMT commit build page buffer and written back using a BMT write page function e.g. write bmt page .

Though popularity based commit frees as many sectors as possible popularity based commit will not guarantee the advancement of the tail pointer of the BMT update log . Without advancing the tail pointer of the BMT update log no space from the BMT update log can be freed and reclaimed. Critical commit occurs when the free space in the BMT update log is below a certain threshold. In this situation the BCM commits all the pending BMT update entries in the tail EU of the BMT update log and thus directly frees the tail EU of BMT update log . Consequently this would move the tail pointer of BMT update log by at least one EU so that the head pointer of the update log can advance for at least one more EU.

After a commit of a BMT EU a check is made to determine if the tail pointer of the BMT update log can be moved further. This is beneficial in the cases where the popularity based commit alone might be good enough to move the tail pointer.

The purpose of crash recovery is to reconstruct the system status after a crash when a loss of memory data occurs. To this end the most important information of LFSM stored in the memory is the pending BMT entries in the BMT cache . Thus the crash recovery module is responsible for reconstructing these BMT entries from the BMT update log after a system crash.

LFSM detects a crash by examining the signature sector during system initialization e.g. generate freemap frontier . A signature sector equal to the LFSM load e.g. LFSM LOAD means LFSM was previously loaded and was not unloaded successfully. Thus a BMT crash recovery function e.g. BMT crash recovery is called to perform the crash recovery.

BMT crash recovery is a main function for the LFSM crash recovery module. BMT crash recovery reads out all of the data in the BMT update log whose address is equal to the BMT update log start position e.g. BMT update log start . The pending BMT entries are obtained by parsing the data of BMT update log . Finally a per page queue BMT cache insertion function e.g. PPQ BMT cache insert one pending is called to insert the discovered pending BMT entries to the BMT cache . LFSM completes the reconstructing procedure by calling an update on disk BMT function e.g. update ondisk BMT to commit all of the BMT entries in the BMT cache to the on disk BMT .

Generally crash recovery reconstructs the pending BMT entries from the BMT update log and commits them to the on disk BMT . Because the BMT update log is a circular buffer which guarantees that no data will be overwritten and that all of the pending BMT entries are recorded in the BMT update log the pending BMT update entries can be successfully reconstructed from a crash.

When the LFSM driver is properly unloaded and re loaded LFSM can construct all of its data structures at the initialization time. Also through a signature sector the LFSM driver can determines whether the connected flash disk is a fresh disk or a previously used disk before building up the head and tail pointers for the BMT update log and other data structures. Generally the signature sector has the fields char signature .

The signature is a predefined ASCII value which determines if the connected disk is a fresh disk or a used disk and also helps to identify if crash recovery needs to be performed. When the LFSM driver is loaded the signature field is assigned as LFSM loaded e.g. LFSM LOAD and will be assigned to LFSM unloaded e.g. LFSM UNLOAD after the LFSM is successfully unloaded or if the LFSM is a fresh one and never been initialized before.

A successful unload signature value e.g. signature successful unload is also a predefined ASCII value which is used to decide whether the recovery algorithm needs to be performed. If the driver is properly unloaded there is no need to perform recovery else recovery should be performed. As described above in order to convert random writes to sequential writes LFSM requires three I O operations for each write request. This produces a race condition in which multiple write requests target at the same LBA and the most recent one is scheduled after other write requests. To solve this problem LFSM uses an active list to ensure that any possible conflicting I O requests are processed according to their incoming order.

All of the processing of the I O requests are stored in an LFSM active list e.g. lfsm dev struct.datalog active list . Each I O request in the active list is described by a bio container e.g. bio container data structure. The insertion of the active list is handled by a get bio container function e.g. get bio container . Before processing a new incoming I O request the get bio container function checks if the I O request conflicts with any items in the active list by traversing the active list e.g. datalog active list . If the I O request does not conflict with any request in the active list a new bio container is initialized and added to the active list and processing is started. If an I O request S does conflict with some request R in the active list the bio container of S will be appended to the wait list e.g. wait list of the bio container associated with R and the thread handling S will be added in an I O queue e.g. io queue and its status will be changed to pending.

After an I O request in the active list is finished the entries in its wait list are examined to identify those that are ready to be put into the active list which is done in the move from active to free function e.g. move from active to free . The thread of a conflicting I O request in the wait list will be woken up if all of the conflicts are removed that is the number of conflicts with the active list is reduced to 0.

In the processing of a write request the LFSM always logs the writes sequentially. This sequential nature is with respect to one EU i.e. inside one EU the writes are kept sequential . Though many logical blocks get overwritten over time they cannot immediately be over written since immediate over writing would break the sequential write property of LFSM . Instead sequentially logging is continued using the free blocks and marking the old over written blocks as invalid. Thus over time the number of invalid blocks increases and proportionally the number of free blocks decreases. Hence to clean up the invalid blocks and make them re usable free Garbage Collection GC is performed. The goal of GC is to reclaim invalid physical blocks as quickly as possible while reducing the performance overhead associated with reclamation to the minimum. GC in LFSM is always done in the background thread in a collect valid blocks function e.g. gc collect valid blks .

Garbage collection according to the present invention is EU based. In other words valid blocks of one EU are completely moved to another EU and this EU is then erased and moved to the free pool. The same process is then performed on another EU. A threshold ratio i.e. the number of free EUs to the total number of EUs is used to trigger the GC. Generally a good ratio is 20 and is represented by a garbage collection threshold e.g. GC THRESHOLD FACTOR although other ratios are acceptable. When the threshold ratio is reached GC starts in the background. Due to various reasons like scheduling heavy I O in a main thread etc. there might be a case where the background GC might not be able to process EUs fast enough to provide enough EUs in the free pool and hence the main thread might not find any free EU to process its write requests. In this scenario the main thread yields to the background thread i.e. to perform GC and waits until the main thread finds at least one free EU in the free pool. This is referred to as critical garbage collection.

The information regarding HList and LVP heap are stored in a Hlist garbage collection structure e.g. HlistGC and the information regarding the utilization ratio and temperature of every EU are stored in its respective EU property structure e.g. EUProperty .

Since the garbage collection thread and the main thread writes run concurrently there might be a possibility of conflicts i.e. both targeting the same LBN. For example during a GC write it might be determined that the LBN being targeted is already in the Active List . This means that the particular EU having this LBN is in the process of being invalidated and will soon be moved to HList and should not be garbage collected. Hence the garbage collection of this EU should be aborted.

As explained earlier the main goal of GC is to reclaim invalidated EUs as quickly as possible while minimizing its associated performance overhead. To accomplish this EUs are selected from the LVP heap one after another until it is determined that garbage collection of these EUs would provide one EU worth free space. If it is determined that EUs in the LVP heap are insufficient to satisfy these constraints a EU is selected from Hlist and GC proceeds on this list of EUs one EU at a time starting from the selected EU.

The information regarding the LBN of all the blocks in the EU is kept in a sector referred to as a metadata sector. This sector resides in the last block e.g. 8 sectors of the EU. The metadata sector is read to determine which LBNs are present and also how many of those LBNs are still valid using an EU bitmap. After making this determination bio containers are allocated to copy these valid blocks. If conflicts with the main thread write are detected the GC of that EU is stopped and GC would proceed with the next EU. Then the entire EU is read and after having read the content of the EU into memory the EU is moved to Free List from the LVP Heap Hlist depending on the present location of the EU. Then a new PBN is assigned to these blocks based on their new location. The read content is copied before it is written to the allocated containers. The writes are executed one block after another. After this process is completed the containers are released and the data structures are promptly freed. This process is repeated for all the EUs in the list.

There are three main data structures in LFSM the BMT the payload EUs that hold the user data and the BMT update log. Because these three data structures can be spread anywhere on the disk the BMT needs to describe the payload EUs the BMT map to describe the BMT and the update log map to describe the BMT update log. The BMT update log is organized as a linked list of EUs e.g. the last sector of an EU points to the EU following it etc. Therefore only the physical address of the first EU of the BMT update log needs to be remembered in order to locate the BMT update log. Every time the first EU of the BMT update log changes this EU s physical address is logged to a special data structure called the dedicated map.

The BMT map is modified whenever the background BMT update commit thread brings in a BMT EU commits all the EU s pending updates to the EU and writes the resulting EU to a free EU on disk. At this point the background thread should log a BMT map update to the BMT update log and retire the associated BMT update entries in the BMT update log. To retire BMT map update entries in the BMT update log LFSM checkpoints the entire BMT map which is sufficiently small to be memory resident to the dedicated map from time to time.

The dedicated map region of 2 N EUs where the first two EUs reside in a fixed disk location and the other N EUs are from the standard EU pool thus participating in wear leveling and are organized as a linked list. The only responsibility of the first two EUs is to maintain a pointer to the remaining N EUs. The checkpoints of the BMT map and the address of the first EU of the update log are both stored in these N EUs with proper timestamps. Note that the first two EUs in the dedicated map do not participate in wear leveling and are the only data structure that is in a fixed place.

During recovery LFSM traverses through the first two EUs in the dedicated map because they are in a fixed location to locate the most recent pointer to the remaining N EUs in the dedicated map. Then LFSM traverses through these N EUs to find the base address of the first EU of the BMT update log and the latest checkpoint of the BMT map. Then by scanning the BMT update log LFSM can reconstruct the most up to date version of the BMT map before the crash and eventually the BMT. By scanning the BMT update log again LFSM can reconstruct the pending BMT update entries in the per page BMT update queues in memory.

While the invention has been shown and described with reference to certain embodiments thereof it will be understood by those skilled in the art that various changes in form and details may be made therein without departing from the spirit and scope of the invention as defined by the appended claims and their equivalents.

