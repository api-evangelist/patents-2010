---

title: Remote restarting client logical partition on a target virtual input/output server using hibernation data in a cluster aware data processing system
abstract: Hibernation and remote restore functions of a client logical partition (LPAR) that exists within a data processing system having cluster-aware Virtual Input/Output (I/O) Servers (VIOSes) is performed via receipt of commands via a virtual control panel (VCP) through an underlying hypervisor. The client hibernation data file is stored in a shared repository by a source/original VIOS assigned to the client. The hypervisor receives a remote restart command and assigns a target/remote client LPAR and a target VIOS. The source I/O adapters and target I/O adapters are locked and the target VIOS gathers adapter configuration information from the source VIOS and configures the target adapters to be able to perform the I/O functionality provided by the source adapters to the client LPAR. The target VIOS then retrieves the client's hibernation data file, and the client LPAR is restored at the remote LPAR with the target VIOS providing the client's I/O functionality.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08959323&OS=08959323&RS=08959323
owner: International Business Machines Corporation
number: 08959323
owner_city: Armonk
owner_country: US
publication_date: 20101027
---
The present invention relates in general to clustered data processing systems and in particular to management and utilization of shared storage within a clustered data processing system. Still more particularly the present invention relates to an improved method and system for interfacing a Virtual Input Output I O Servers VIOS with a Virtual Control Panel VCP to facilitate partition hibernation and remote restarting.

Large scale distributed data processing systems are known in the art. As cloud computing becomes more and more ubiquitous in the computer world methods for providing enhanced functionality and greater up time are required to continue to adequately serve commercial needs.

Today storage virtualization and management is a separate entity than server virtualization and management. Different clients logical partitions LPARs associated with different servers access the same SAN storage. A client s LPARs on one server may not know if the SAN disk that it is trying to access is being used by some another client s LPAR belonging to some other server. This can cause data integrity issues and may potentially cause data corruption and client partition crashes. This problem is exacerbated for memory sharing and remote hibernation solutions. With client hibernation in distributed environments the hibernation file is stored to the client s assigned storage which is only accessible from that client LPAR and the client can then be restored on the same client LPAR from which the client was hibernated. When a client LPAR crashes or is not available on the local device terminal at which the user is located restoring the client from its hibernation state is often not possible and the client has to be restarted rebooted reinstalled utilizing the client s OS image. When the client has to be completely re installed at a new location within the distributed system in order to access the client applications this re installation process can be an immensely time consuming and arduous task for a client or a systems administrator and also requires an amount of foresight that is not always possible particularly when the original terminal is unavailable due to hardware failure .

Disclosed are a cluster aware data processing system for interfacing a Virtual Input Output I O System VIOS and a virtual control panel VCP for enabling system hibernation and remote restarting of system hibernation data. During an initial creation of a client s hibernation data file or image at a first computing electronic complex CEC I O adapter configuration information is included within the hibernation data file that is stored by the VIOS in the client s assigned logical unit LU within a shared storage repository. The hypervisor on a different second CEC receives a remote restart command from an input entered into the VCP and determines a source VIOS configuration from the selected hibernation data file and the hypervisor identifies a target VIOS for performing a remote restart of the client utilizing the stored hibernation data file. The hypervisor assigns configurations of specific virtual I O adapters of the source VIOS retrieved from the stored hibernation data file or from the VIOS database to newly generated virtual I O adapters of the target VIOSes. The hypervisor then gathers adapter information of the target I O adapters to determine if the set of the target adapters has the correct configurations and security settings and is capable of performing the I O functionality provided by the source adapters. In response to the set of target adapters of the target VIOS not being capable of performing all of the functionality provided by the source adapters additional target adapters are created with the required configurations. The remote restore of the client on the remote client LPAR using the target VIOS is then initialized using the hibernation data file. Once restored at the second CEC the client is able to perform all of its I O operations with the target VIOS providing access to the client s assigned LU.

The above summary contains simplifications generalizations and omissions of detail and is not intended as a comprehensive description of the claimed subject matter but rather is intended to provide a brief overview of some of the functionality associated therewith. Other systems methods functionality features and advantages of the claimed subject matter will be or will become apparent to one with skill in the art upon examination of the following figures and detailed written description.

The above as well as additional objectives features and advantages of the present invention will become apparent in the following detailed written description.

The illustrative embodiments provide a method data processing system and computer program product that enables interfacing between a Virtual Control Panel VCP and a Virtual Input Output I O Servers VIOS to facilitate partition hibernation and remote restarting operations in a VIOS cluster environment. The method is performed within a clustered data processing system DPS environment architecture in which one or more cluster aware virtual input output server VIOS enable efficient secure access for a client logical partition LPAR to a single shared network storage resource of the cluster. The client LPAR and VIOS are located on a computing electronic complex CEC which is a computing node within the cluster environment.

In one embodiment hibernation and remote restore functions of a client logical partition LPAR that exists within a data processing system having cluster aware Virtual Input Output I O Servers VIOSes is performed via receipt of commands via a virtual control panel VCP through an underlying hypervisor. The Client hibernation data file is stored in a shared repository by a source original VIOS assigned to the client. The hypervisor receives a remote restart command and assigns a target remote client LPAR and a target VIOS. The source I O adapters and target I O adapters are locked and the target VIOS gathers adapter configuration information from the source VIOS to determine how to configure the target adapters to perform the I O functionality provided by the source adapters to the client LPAR. The target adapters are properly configured e.g. with required security permissions to access the client s storage files including the hibernation data file and the remote restart of the client LPAR is activated with the target VIOS assigned to perform all I O operations of the client LPAR.

In the following detailed description of exemplary embodiments of the invention specific exemplary embodiments in which the invention may be practiced are described in sufficient detail to enable those skilled in the art to practice the invention and it is to be understood that other embodiments may be utilized and that logical architectural programmatic mechanical electrical and other changes may be made without departing from the spirit or scope of the present invention. The following detailed description is therefore not to be taken in a limiting sense and the scope of the present invention is defined by the appended claims and equivalents thereof.

Within the descriptions of the different views of the figures similar elements are provided similar names and reference numerals as those of the previous figure s . The specific numerals assigned to the elements are provided solely to aid in the description and are not meant to imply any limitations structural or functional or otherwise on the described embodiment.

It is understood that the use of specific component device and or parameter names such as those of the executing utility logic firmware described herein are for example only and not meant to imply any limitations on the invention. The invention may thus be implemented with different nomenclature terminology utilized to describe the components devices parameters herein without limitation. References to any specific protocol or proprietary name in describing one or more elements features or concepts of the embodiments are provided solely as examples of one implementation and such references do not limit the extension of the invention to embodiments in which different element feature or concept names are utilized. Thus each term utilized herein is to be given its broadest interpretation given the context in which that terms is utilized. For example as utilized herein the term cluster aware refers to the operational state of each VIOS within the cluster where the VIOSes contain information about which other VIOSes are connected within the cluster the configuration of the different CECs within the DPS supported by the cluster information about which client LPARs are supported by each VIOS and other state and operating information and data related to performing VIO operations using the physical I O devices of the DPS and those of the distributed storage repository storage repository . Cluster awareness is supported by both a shared networked VIOS database and locally maintained copies of VIOS cluster data within each VIOS.

As further described below implementation of the functional features of the invention is provided within processing devices structures and involves use of a combination of hardware firmware as well as several software level constructs e.g. program code . The presented figures illustrate both hardware components and software components within example data processing architecture having a specific number of processing nodes e.g. computing electronic complexes . The illustrative and described embodiments assume that the system architecture may be scaled to a much larger number of processing nodes.

In the following descriptions headings or section labels are provided to separate functional descriptions of portions of the invention provided in specific sections. These headings are provided to enable better flow in the presentation of the illustrative embodiments and are not meant to imply any limitation on the invention or with respect to any of the general functions described within a particular section. Material presented in any one section may be applicable to a next section and vice versa. The following sequence of headings and subheadings are presented within the specification 

With specific reference now to there is depicted a block diagram of an example cluster aware CA distributed data processing system DPS architecture within which the functional aspects of the described embodiments may advantageously be implemented. For simplicity cluster aware distributed DPS architecture shall be referred to herein simply as DPS . DPS comprises a plurality of computing nodes each referred to herein as a computing electronic complex CEC of which CECs A and B are illustrated. The number of CECs within DPS may vary ranging from a single CEC in a smaller system extending up to hundreds or thousands of CECs in larger scaled systems. For simplicity the embodiments shall be described from the perspective of a single CEC CEC A or two CECs CECs A B . Each CEC A B comprises at least one and in most instances a plurality of Virtual Input Output Server also referred to herein as a VIO Server or VIOS with functionality as described below. The actual number of VIOSes within each CEC of DPS is a design feature and may vary. As shown each VIOS has a universally unique identifier UUID associated with the particular VIOS. Thus no two VIOSes within the entire DPS has a same UUID and each new VIOS added to the DPS is provided with a new UUID. While presented with some sense of a sequence of allocation within in one or more embodiments the UUIDs of VIOSes within a same CEC are not necessarily sequential to or associated with each other or to the CEC and a system wide UUID allocation scheme may be implemented that results in a non sequential allocation across VIOSes within multiple CECs . Also supported within each CEC A B are client logical partitions interchangeably referred to as client LPARs or clients of which a first two clients clientA and clientB are illustrated. As described below with reference to client LPARs are logical partitions of a virtualized or operating system partitioned computing system. The actual number of clients within each CEC may vary and could range from a single client to hundreds or thousands of clients without limitation. For efficiency in presenting the inventive concepts herein only two clients are presented within each CEC of the various illustrative and described embodiments.

DPS also comprises a distributed storage facility accessible to each of the CECs and the components within the CECs . Within the described embodiments the distributed storage facility will be referred to as distributed storage repository and the distributed storage repository enables several of the client level functional features provided by the embodiments described herein. Distributed storage repository provides a single view of storage that is utilized by each CEC and for each client of each CEC within a cluster aware distributed system. Distributed storage repository comprises local physical storage and network storage both of which comprise multiple physical storage units e.g. disks solid state drives etc. . The physical disks making up distributed storage repository may be distributed across a storage network e.g. a SAN . Additionally distributed storage repository provides a depository within which is stored and maintained the software utility instruction code OS images client images data system node and client level and or other functional information utilized in maintaining the client level system management and storage level operations features of DPS . In addition to distributed storage repository DPS also comprises a VIOS database DB which may also be a distributed storage facility comprising physical disks across a storage network. VIOS DB or DB is a repository that stores and provides access to various cluster configuration data and other functional components modules and data structures that enable the various cluster aware functionality described herein. In one embodiment portions of distributed storage repository may be allocated to provide storage pools for a cluster. Each VIOS of the cluster maintains a local view of the DB and updates the cluster level information data data structures within DB as such information data is created or updated.

Communication between each VIOS of each CEC as well as with the VIOSes of at least one other CEC is generally supported via a plurality of inter CEC interconnects illustrated as bi directional dashed lines connecting pairs of VIOSes . The arrows indicated two way data exchange or communication between components. In addition to the inter CEC interconnects each VIOS is also connected to distributed storage repository via VIOS to Store or CEC to Store interconnects which are also illustrated as full lined bi directional arrows. Also each VIOS is connected to DB via VIOS to DB interconnects presented as dashed and dotted lines. With the exception of the inter CEC connectors running from a first VIOS e.g. VIOS of a first CEC to a second VIOS e.g. VIOS on the same CEC the various interconnects represent a network level connectivity between the VIOS nodes of the cluster and the DB and the distributed storage repository . As utilized herein references to one or more nodes are assumed to refer specifically to a VIOS within the cluster. DPS also comprises a management console on which a management tool not shown executes.

Turning now to there is illustrated another view of DPS illustrating the network based connection of the CECs to the distributed storage repository and DB . illustrates in greater detail the network connectivity of VIOSes and CECs to each other and to Distributed storage repository . With this view CEC A Node A A and CEC B Node B B comprise similar constructs as presented in . Each CEC within DPS connects to distributed storage repository via one or more networks and or I O interconnect switch fabric generally illustrated as interconnect network fabric . The descriptions and illustrations assume that at least some of the CECs of DPS and distributed storage repository are located remotely from each other including being located in different countries for example such that no direct physical connectivity exists between the respective devices. For simplicity the embodiments are described as having primary interconnect network comprising a private wide area network WAN or a public WAN such as the Internet although other network types e.g. a local area network are possible and supported.

As depicted in one or more embodiments each CEC is also connected to one or more neighbor CECs in order to provide efficient fail over and or mobility support and other functions as described hereinafter. As utilized herein the term neighbor refers to a connected second CEC with which a first CEC is able to communicate and references to a neighbor CEC is not limited to a second CEC in geographic proximity to the first CEC. CEC A A and CEC B B are illustrated connected to each other via some connecting medium which may include a different network such as a local area network or some type of direct interconnect e.g. a fiber channel connection when physically close to each other. The connection between neighbor CECs A and B is illustrated as a direct line connection or a secondary network connection between CECs A and B. However it is appreciated that the connections are not necessarily direct and may actually be routed through the same general interconnect network as with the other CEC connections to distributed storage repository . In one or more alternate embodiments the connections between CECs may be via a different network e.g. network such as a local area network LAN .

As depicted each CEC comprises one or more network interfaces and one or more I O adapters to enable the CEC and thus the other components i.e. client partitions of the CEC to engage in network level communication as illustrated by . As illustrated within within an example virtual I O architecture each VIOS emulates virtual client I O adapters to enable communication by specifically assigned client LPARs with distributed storage repository and or VIOS DB and or other clients within the same CEC or on a different CEC. The VIOSes emulate these virtual I O adapters and communicates with distributed storage repository by connecting with corresponding virtual sever I O adapters SVA at distributed storage repository . In various embodiments these pairings of virtual client I O adapters with specific SVAs are unique for each client LPAR to enable each client LPAR to have secure access to the specific storage location assigned to that client LAPR . Internal CEC communication between VIOS and client LPARs are illustrated with solid connecting lines which are routed through the virtualization management component while VIOS to server communication is provided by dashed lines which connect via the network interconnect fabric . The VIOSes within each CEC are thus able to support client level access to distributed storage and enable the exchange of system level and client level information with distributed storage repository . Each client LPAR has a unique client identifier UCID . Also each VIOS has a specific DRC identifying the network location or address of the VIOS or resources within the VIOS . Additionally each VIOS has a universally unique identifier UUID which is associated with that particular VIOS configuration. Also shown by is the connection of the management console which is utilized to perform the setup and or initialization of the backup and restore operations described herein for the individual VIOSes and or for the OS cluster as a whole in various embodiments. Included within management console and as utilized in the described embodiments is management tool which has access to and or a copy of VIOS UUID Table .

In addition each VIOS also comprises the functional components modules and data to enable the VIOSes within DPS to be aware of the other VIOSes anywhere within the cluster DPS . From this perspective the VIOSes are referred to herein as cluster aware and their interconnected structure within DPS thus enables DPS to also be interchangeably referred to as cluster aware DPS . As a part of being cluster aware each VIOS also connects to DB via network and communicates cluster level data with DB to support the cluster management functions described herein.

Also illustrated by is an initial view of the component make up of an example distributed storage repository and an initial listing of some components of DB . To support the virtual I O operations with the VIOSes and the associated virtual client I O adapters distributed storage repository comprises communication infrastructure . Communication infrastructure comprises network interface s and a plurality of server I O adapters utilized for cluster level communication and enabling access to data code software utility stored on distributed storage repository to complete I O operations thereto. Specifically these server I O adapters are also presented as virtual sever I O adapters see which are paired with respective virtual I O adapters via emulation of physical I O adapters that are assigned to specific clients of CECs .

As shown distributed data store generally comprises general storage space the available local and network storage capacity that may be divided into storage pools providing assigned client storage which may be divided into respective storage pools for a group of clients unassigned spare storage and backup redundant CEC VIOS client configuration data storage . In one embodiment the assigned client storage is allocated as storage pools and several of the features related to the sharing of a storage resource providing secure access to the shared storage and enabling cluster level control of the storage among the VIOSes within a cluster are supported with the use of storage pools. When implemented within a VIOS cluster storage pools provide a method of logically organizing one or more physical volumes for use by the clients supported by the VIOSes making up the VIOS cluster. illustrates an example configuration of a storage pool utilized within a cluster aware DPS . Specifically provides details on how these physical volumes are used within the storage pool. As shown storage pool within the cluster contains one or more Disk Groups . Disks Groups provide administrators the ability to provide access policies to a given subset of physical volumes within the storage pool . Once a disk group has been defined administrators can further categorize the subset into Storage Tiers based on disk characteristics. Once a Disk Group and Storage Tier have been defined administrators carve Logical Units LU to be exported to client partitions .

With the capability of virtual pooling provided herein an administrator allocates storage for a pool and deploys multiple VIOSes from that single storage pool. With this implementation the storage area network SAN administration functions is decoupled from the system administration functions and the system administrator can service customers specifically clients of customers or add an additional VIOS if a VIOS is needed to provide data storage service for customers. The storage pool may also be accessible across the cluster allowing the administrator to manage VIOS work loads by moving the workload to different hardware when necessary. With the cluster aware VIOS implementation of storage pools additional functionality is provided to enable the VIOSes to control access to various storage pools such that each client customer data information is secure from access by other clients customers. One such functionality is the allocation to each client LPAR of individual virtual I O VIO adapters having unique adapter identifiers AdapterID as presented in the descriptions of the embodiments herein.

Returning now to located within backup redundancy data storage of distributed storage repository DSR are one or more redundant LUs . Specific functionality of these redundant LUs and the method by which the redundant LUs are created as well as how the redundant LUs are utilized is provided or described in greater detail in Section D of the present disclosure.

As illustrated DSR further comprises a plurality of software firmware and or software utility components including DSR configuration utility DSR configuration data e.g. inodes for basic file system access metadata authentication and other processes and DSR management utility .

To support the cluster awareness features of the DPS and in accordance with the illustrative embodiment DPS also comprises VIOS database DB in which is stored various data structures generated during set up and or subsequent processing of the VIOS cluster connected processing components e.g. VIOSes and management tool . VIOS DB comprises a plurality of software or firmware components and or and data data modules or data structures several of which are presented in for illustration. Among these components are cluster management CM utility VIO AdapterID data structure cluster configuration data Client identifying ID data active nodes list and I O redundancy data among others. Also included is a copy of VIOS backup file in the illustrative embodiment. These various components support the various clustering functionality and cluster aware I O operations of the one or more VIOSes as described herein. Additional features of DB and distributed storage repository as well as the specific components or sub components that enable the various clustering functionality are presented within the description of the remaining figures and throughout the description of the various presented embodiments.

The various data structures illustrated by the figures and or described herein are created maintained and or updated and or deleted by one or more operations of one or more of the processing components modules described herein. In one embodiment the initial set up of the storage pools VIOS DB and corresponding data structures is activated by execution of a management tool to roll out the installation and activation of a plurality of cluster aware operating systems by and or on one or more VIOSes . Once the infrastructure has been established however maintenance of the infrastructure including expanding the number of nodes where required is performed by the VIOSes in communication with DB and the management tool .

Also associated with DPS and communicatively coupled to distributed storage repository and DB and VIOSes is management console which may be utilized by an administrator of DPS or of distributed storage repository or DB to access DB or distributed storage repository and configure resources and functionality of DB and of distributed storage repository for access usage by the VIOSes and clients of the connected CECs within the cluster. As shown in and described throughout the specification management tool is implemented within management console . However it is appreciated that resources of any node within DPS may be selected elected to perform the functions of management tool and the selected node would then be utilized to activate initiate assist with and or perform one or more of the below described cluster creation monitoring and management functions including migration functions utilizing the availability of the resources provided by the CA OS the DB and distributed storage repository .

In an alternate embodiment management tool is an executable module that is executed within a client partition at one of the CECs within DPS . In one embodiment the management tool controls some of the operations of the cluster and enables each node within the cluster to maintain current updated information regarding the cluster including providing notification of any changes made to one or more of the nodes within the cluster. In one embodiment management tool registers with a single VIOS and is thus able to retrieve receive cluster level data from VIOS including FFDC data of the entire cluster. In one implementation the management tool the VIOS with which the management tool registers is a primary node of the cluster. In the embodiments detailed herein management tool can support the completion of a migration operation of a client from a first CEC to a second CEC where a redundant logical unit is provisioned as the primary LU for the migrated client at the second CEC as defined in greater details in Section D below.

With reference now to there is presented a third view of an example DPS emphasizing a processing system architecture i.e. architecture of the individual CECs and specifically CEC A A . CEC A A CEC A serves as the example CEC that is described in greater detail in and throughout the specification. CEC A is presented as a server that comprises hardware components and software firmware OS components that are logically partition to create a plurality of virtualized machine partitions which are assigned as client logical partitions LPARs and virtual I O servers VIOSes . Hardware components of example CEC A comprises one or more processors A P one or more memories A M and local storage . The processors A P are interconnected with one or a plurality of memories A M and with local storage via a bus interconnect switch or an interconnect fabric not specifically shown . The specific internal connectivity of components which may be distributed across a large scale interconnect fabric is not germane to the described embodiments and no further detail is presented regarding the particular type of interconnectivity between the system hardware components.

Also included within hardware components are one or more physical network interfaces by which CEC A A connects to an external network such as network among others. Additionally hardware components comprise a plurality of I O adapters A E which provides the I O interface for CEC A A. I O adapters A E are physical adapters that enable CEC A to support I O operations via an I O interface with both locally connected and remotely networked connected I O devices including SF storage . Examples of I O adapters include Peripheral Component Interface PCI PCI X or PCI Express Adapter and Small Computer System Interconnect SCSI adapters among others. CEC is logically partitioned such that different I O adapters are virtualized and the virtual I O adapters may then be uniquely assigned to different logical partitions.

Logically located above the hardware level is a virtualization management component provided as a Power Hypervisor PHYP trademark of IBM Corporation as one embodiment. While illustrated and described throughout the various embodiments as PHYP it is fully appreciated that other types of virtualization management components may be utilized and are equally applicable to the implementation of the various embodiments. PHYP has an associated service processor coupled thereto within CEC . Service processor may be used to provide various services for one or more logical partitions. PHYP is also coupled to hardware management controller HMC which exists outside of the physical CEC . Operations of the different logical partitions may be controlled through HMC which is a separate data processing system from which a system administrator may perform various functions such as reallocation of resources to different logical partitions.

CEC A A further comprises a plurality of user level logical partitions LPARs of which a first two are shown represented as individual client LPARs A B within CEC A. According to the various illustrative embodiments CEC A supports multiple clients and other functional operating OS partitions that are created within a virtualized environment. Each LPAR e.g. client LPAR A receives an allocation of specific virtualized hardware and OS resources including virtualized CPU A Memory A OS A local firmware and local storage LStore . Each client LPAR includes a respective host operating system that controls low level access to hardware layer of CEC A and or to virtualized I O functions and or services provided through VIOSes . In one embodiment the operating system s may be implemented using OS 400 which is designed to interface with a partition management firmware such as PHYP and is available from International Business Machines Corporation. It is appreciated that other types of operating systems such as Advanced Interactive Executive AIX operating system a trademark of IBM Corporation Microsoft Windows a trademark of Microsoft Corp or GNU Linux registered trademarks of the Free Software Foundation and The Linux Mark Institute for example may be utilized depending on a particular implementation and OS 400 is used only as an example.

Additionally according to the illustrative embodiment CEC A also comprises one or more VIOSes of which two VIOS A and B are illustrated. In one embodiment each VIOS is configured within one of the memories A M and comprises virtualized versions of hardware components including CPU memory local storage and I O adapters among others. According to one embodiment each VIOS is implemented as a logical partition LPAR that owns specific network and disk I O adapters. Each VIOS also represents a single purpose dedicated LPAR. The VIOS facilitates the sharing of physical I O resources between client logical partitions. Each VIOS allows other OS LPARs which may be referred to as VIO Clients or as Clients to utilize the physical resources of the VIOS via a pair of virtual adapters. Thus VIOS provides virtual small computer system interface SCSI target and shared network adapter capability to client LPARs within CEC . As provided herein VIOS supports Virtual real memory and Virtual shared storage functionality with access to Distributed storage repository as well as clustering functionality.

Within CEC A VIOSes and client LPARs utilize an internal virtual network to communicate. This communication is implemented by API calls to the memory of the PHYP . The VIOS then bridges the virtual network to the physical I O adapter to allow the client LPARs to communicate externally. The client LPARs are thus able to be connected and inter operate fully in a VLAN environment.

Those of ordinary skill in the art will appreciate that the hardware firmware software utility and software components and basic configuration thereof depicted in B and may vary. The illustrative components of DPS and specifically those within CEC A are not intended to be exhaustive but rather are representative to highlight some of the components that are utilized to implement certain of the described embodiments. For example different configurations of data processing systems CECs devices may be provided containing other devices components which may be used in addition to or in place of the hardware depicted and may be differently configured. The depicted example is not meant to imply architectural or other limitations with respect to the presently described embodiments and or the general invention. The CEC depicted in the various figures may be for example an IBM eServer pSeries system a product of International Business Machines Corporation in Armonk N.Y. running the Advanced Interactive Executive AIX operating system or LINUX operating system.

Certain of the features associated with the implementation of a cluster aware VIOS e.g. VIOS of B and are introduced above with reference to the description of the previous figures and particularly . Descriptions of the specific functionality of the VIOS will continue to be provided with reference to the illustrations of B and . As presented by each VIOS is a virtual machine instance that emulates hardware in a virtualized environment. The VIOS is tasked with emulating SCSI storage devices and the VIOS provides client LPARs with access to distributed storage repository in cooperation with the PHYP . Configuration of the VIOS is performed through the hardware management tools of HMC . SCSI storage devices support a set of commands that allow SCSI initiators the ability to control access to storage . Database programs for example may manage access to distributed storage repository through a set of SCSI commands commonly referred to as persistent reserve. Other types of reserves are also supported by VIOS and the collective group of such commands is referred to herein as reserve commands.

As provided herein each VIOS allows sharing of physical I O resources between client LPARs including sharing of virtual Small Computer Systems Interface SCSI and virtual networking. These I O resources may be presented as internal or external SCSI or SCSI with RAID adapters or via Fibre Channel adapters to distributed storage repository . The client LPAR however uses the virtual SCSI device drivers. In one embodiment the VIOS also provides disk virtualization for the client LPAR by creating a corresponding file on distributed storage repository for each virtual disk. The VIOS allows more efficient utilization of physical resources through sharing between client LPARs and supports a single machine e.g. CEC to run multiple operating system OS images concurrently and isolated from each other.

In one or more embodiments the VIOS operating system s is an enhanced OS that includes cluster aware functionality and is thus referred to as a cluster aware OS CA OS . One embodiment for example utilizes cluster aware AIX CAA as the operating system. According to one embodiment cluster awareness enables multiple independent physical systems to be operated and managed as a single system. As provided within VIOS of CEC A VIOS comprises cluster aware CA OS kernel or simply CA OS as well as LPAR function code for performing OS kernel related functions for the VIOS LPARs . When executed within two or more nodes of DPS CA OS enables various clustering functions such as forming a cluster adding members to a cluster and removing members from a cluster as described in greater detail below. CA OS manages the VIOS LPARs and enables the VIOSes within a cluster to be cluster aware. CA OS comprises several functional modules. In the described embodiments CA OS comprises cluster management CM utility which supports the configuration of the VIOS to enable cluster awareness and cluster level functionality such as redundant virtual I O. Each of these additional software components of CA OS may be a functional module within CM utility in one embodiment and each module is thus described as such throughout the remainder of this specification. In one embodiment CM utility may be a separate utility that is locally installed or downloaded from DB for example as an enhancement to an existing OS within a CEC or VIOS when initially configured for operation within the VIOS cluster. CM utility is then executed when configuring the individual VIOS to create or join a cluster and or become a cluster aware node within the VIOS cluster. With this implementation structure CM utility enables the OS to support the various cluster awareness and other cluster level features and functionality. In an alternate embodiment CA OS includes all the clustering features and functionality and established the various features when the CEC VIOS joins the cluster and or during configuration of VIOS to become cluster aware.

In one implementation functional components of CM utility are encoded on local device storage of a corresponding VIOS such that the VIOS becomes automatically configured as a part of the VIOS cluster when the VIOS is initially activated. On initial set up of the VIOS VIOS API kernel extensions and virtual adapters are configured within VIOS to enable communication with the other VIOSes the VIOS DB and with the distributed storage repository . During this initial setup of the VIOS the VIOS executes a registration module of CM utility to register VIOS with the cluster. The registration module enables VIOS to retrieve download or have forwarded from DB on successful registration with the cluster any additional CM software components and or cluster level information and or data required to establish full cluster awareness when the VIOS has completed installation and is activated within the CEC . Thus in one embodiment in addition to the locally stored CA OS components and software modules of CM utility other functional components of CM utility may be downloaded from DB when CEC is powered on or when one or more VIOSes are enabled on CEC . Once the VIOS has completed its setup one or more client LPARs that are activated within CEC may be assigned to VIOS and VIOS subsequently performs the various I O operations initiated by the client as initiator or directed to the client as target . Updates to the local VIOS data may periodically be made as changes are made within the VIOS cluster and or as one or more new client LPARs are added to the CEC requiring VIOS support. In one embodiment CM utility may also enable retrieval and presentation of a comprehensive view of the resources of the entire cluster.

It is appreciated that while various functional aspects of the clustering operations are described as separate components modules and or utility and associated data constructs the entire grouping of different components utility data may be provided by a single executable utility application such as CA OS or CM utility . Thus in one embodiment CA OS executes within VIOS and generates a plurality of functional components within VIOS and within DB . Several of these functional components are introduced within and and others are described throughout the various embodiments provided herein. For simplicity in the descriptions which follow references to CM utility and CA OS will be assumed to be referring to the same general component i.e. CM utility being a subcomponent of CA OS and the terms may be utilized interchangeably throughout the specification.

As further presented by the illustrative embodiments e.g. VIOS includes one or more additional functional modules components such as VIO adapter s interface virtual I O drivers utility which provides I O functionality to VIOS and enables VIOS to route data traffic to and from data structures and storage within distributed storage repository and or DB . Virtual I O adapter s and CM utility also enable the VIOS to provide each client LPAR with access to the full range of storage accessible within distributed storage repository and other cluster supported functionalities as described herein.

In the illustrative embodiment each client LPAR communicates with VIOS via PHYP . VIOS and client LPAR A B are logically coupled to PHYP which enables supports communication between both virtualized structures. Each component forwards information to PHYP and PHYP then routes data between the different components in physical memory A M . In one embodiment a virtualized interface of I O adapters is also linked to PHYP such that I O operations can be communicated between the different logical partitions and one or more local and or remote I O devices. As with local I O routing data traffic coming in and or out of I O adapter interface or network interface from a remote I O device is passed to the specific VIOS via PHYP .

With the above introduced system configuration of B and A a first VIOS through a communication channel established via PHYP grants access to another VIOS through one or more virtual adapters. VIOS includes the functionality to query PHYP for the identity of the Client LPAR on the CEC where the VIOS is currently running.

With the cluster aware VIOS infrastructure different VIOSes associated with different CECs access the distributed storage repository and cluster level information is shared communicated across the VIOS cluster via VIOS DB while each client I O process is being performed. In this manner the VIOS associated with a first client on a first CEC is aware of which SAN disk resources are being accessed by a second client on a second CEC or on the same CEC . With this awareness factored into the I O exchange with the distributed storage repository the VIOS associated with the first client can avoid accessing the same storage resource that is concurrently being utilized by the second client thus preventing data integrity issues which could potentially cause data corruption and client partition crashes.

In one embodiment VIOS functionality is enhanced to enable assigning of client identifiers ID and unique virtual I O adapter IDs in a secure manner while enabling storage pooling within virtual storage within distributed storage repository . According to the described implementation the different clientID vioAdapterID pairings are unique throughout the cluster so that no two clients throughout the entire cluster can share a same virtual adapter and no two vioAdapterIDs are the same within a single client. is a flow chart illustrating the method by which a VIOS on a CEC with DPS enables cluster level communication between a client LPAR and distributed storage repository according to one embodiment. The process begins at block at which the VIOS queries PHYP for the identity of the client LPAR . At block the VIOS creates a unique identifier ID for the client i.e. a ClientID . The VIOS then stores the unique ClientID in ClientID data structure within DB block . The DB and by extension the ClientID data structure are accessible to each VIOS partition in the cooperating cluster DPS . At block the VIOS also generates an identifier for each virtual IT nexus virtual I O AdapterID that is utilized for each virtual adapter assigned to the client LPAR . In one embodiment a client LPAR can have multiple virtual adapters assigned thereto. These vio AdapterIDs are stored in the AdapterID data structure block and are associated with their corresponding clientIDs block . The method illustrated by ends at termination block with each clientID having been associated with the corresponding one or more vio AdapterIDs with DB .

As described herein a cluster is a set of one or more networked VIOS partitions where each VIOS within the cluster has access to a common set of physical volumes. The physical volume resides within the VIOS cluster and is utilized to provide block storage. Implementation of the cluster awareness with the VIOSes of the cluster enables the VIOSes to provide cluster storage services to virtual clients client LPARs . The VIOS software stack provides the following advanced capabilities among others Storage Aggregation and Provisioning Thin Provisioning Virtual Client Cloning Virtual Client Snapshot Virtual Client Migration Distributed Storage Repository Virtual Client Mirroring and Server Management Infrastructure integration. More generally the VIOS protocol allows distributed storage to be viewed as centralized structured storage with a namespace location transparency serialization and fine grain security. The VIOS protocol provides storage pooling distributed storage and consistent storage virtualization interfaces and capabilities across heterogeneous SAN and network accessible storage NAS . In order to provide block storage services utilizing the distributed repository each VIOS configures virtual devices to be exported to virtual clients. Once each virtual device is successfully configured and mapped to a virtual host VHOST adapter the clients may begin utilizing the devices as needed. In one embodiment the virtualization is performed utilizing POWER virtual machine VM virtualization technology which allows the device configuration process to occur seamlessly because the physical block storage is always accessible from the OS partition.

According to one or more embodiments the algorithms functional software modules provided by CM utility also account for the VIOS moving from a first CEC referred to herein as the source CEC to a second CEC referred to herein as the destination CEC. One of the roles played by the VIOS in enable performance of a mobility operation within the cluster aware DPS is to describe the storage that is in use on the source CEC to the VIOS on the destination CEC. The description provide by the first VIOS includes a key into the adapter table for the source adapter. The key is utilized to find the client and unique AdapterID information based on the data base relationship e.g. the association of data structures e.g. tables within the database . The unique AdapterID is passed to the kernel extension which verifies storage access. The PHYP signals the termination of the mobility operation and as part of that completion the row within the VIOS table is updated with the new CEC relative identifier. Thus while the move of the particular LPAR is completed the unique AdapterID assigned to that OS partition is not changed within the database distributed storage repository . The CEC relative identifier allows the VIOS to be discovered while the unique AdapterID allows secure implementation of storage pool access rights. This scheme allows flexibility in the management tools implementation for pool security allowing for convenience of use by the system administrator.

In one or more embodiments conditions may require a client to be migrated to a different CEC and or to be handled by a different VIOS while an existing VIOS continued to handle I O operations in the interim during the migration . Examples of scenarios that my trigger these client moves transfers include but are not limited to a fabric connectivity losses and enable the I O operations to proceed via redundant VIOS connectivity within the VIOS cluster b hardware issues and or c manually triggered transfer of the client to other machines servers by a system administrator. The below described embodiments thus apply to communication loss that is a physical fabric loss as well as situations in which the first VIOS A itself fails or has an internal error condition that prevents the first VIOS A from being able to provide fulfill the I O operations to of the client LPAR . In one scenario the loss may be software related. In another embodiment a third type of fabric loss which is a loss of VIOS fabric connection to with VIOS DB may also trigger move of a client. When a move of a client is performed consideration has to be given to the virtual adapter pairings that enable the client to access the specific client assigned LU and general distributed storage. These pairings can be transferred as a part of the migration. However according to one embodiment presented herein migration of a client can be completed more efficiently utilizing features related to AMS as described below.

As introduced above with advanced VIOS implementation a distributed storage configuration across clustered VIOS partitions is supported by distributed storage repository . In other words distributed storage repository is treated as a one big storage pool with chunks of physical storage logical units or LUs allocated to each client LPAR. In one or more embodiments each VIOS within the cluster DPS can be considered a node in the cluster. Each VIOS is able to communicate with other VIOSes utilizing an established communication protocol. If two or more client LPARs belonging to different CECs share storage on the SAN distributed storage repository implementation of the VIOS communication protocol makes it possible to query each VIOS about the current usage of the shared storage device and disseminate this information to other VIOSes within the cluster. This shared communication enables each VIOS to know whether or not the SAN storage device distributed storage repository that the VIOS is trying to access is currently being used by some other LPAR and thus prevent data corruption and possible VIOS and or client crashes.

With the above described configurations of a DPS configured with distributed storage repository DB and CECs having VIOSes that are clustered and or cluster aware through use of DB additional embodiments are provided to enable efficient storage virtualization and management utilizing the VIOSes described above. Implementation of these additional embodiments may involve additional functional components utilities of the CA OS and or specifically CM utility . According to one or more embodiments the CM utility also enables active memory sharing of a same storage device within the distributed storage repository by one or more VIOSes . Within the distributed storage repository all the storage devices are virtualized into a large storage pool where chunks of storage units logical units LUs can be carved out and assigned as paging devices for each client. Each client is able to utilize an assigned logical unit LU as a paging file thereby facilitating sharing of the storage device and reducing wastage. PHYP provides an interface between a client LPAR and a VIOS and performs various storage I O operations such as moving or pulling data for one or more VIOSes accessing the LUs. A same logical unit LU may be used accessed by one or more client LPARs owned by the same client via one or more VIOSes of one or more CECs . For security purposes however a client is unable to access a LU belonging to another client and a single LU is not shared between different clients.

In one implementation certain functional components of CM utility are encoded on local device storage accessible to corresponding VIOS . VIOS is able to immediately register with the cluster and retrieve download or have forwarded from DB on successful registration with the cluster the necessary CM software information and or data the VIOS utilizes to become cluster aware when the VIOS is initially activated within the CEC . In addition to the locally stored software components of CM utility other functional components of CM utility may be downloaded from DB when CEC is powered on or when one or more VIOSes and or one or more new client LPARs are enabled on CEC . CM utility may comprise firmware and or specially stored OS code on the CEC that allows for cluster specific boot up and or setup of a VIOS within the cluster.

CM utility provides code program instructions that are executed on one or more virtual processor resources of one or more VIOSes within CEC to provide specific functions. Among the functionality provided by CM utility when executed and which are described in greater details herein are the following non exclusive list 1 providing by a first VIOS a virtual memory space of a distributed storage repository wherein the virtual memory space is actively shared by two or more client logical partitions LPARs 2 the first VIOS receiving a paging file request from an application running on a first client LPAR of the two or more client LPARs wherein the paging file request indicates a minimum required capacity and wherein the minimum required capacity is a minimum amount of paging file storage required by the application 3 determining if an existing logical unit LU of a plurality of LUs within the distributed storage repository that has an available amount of storage equal to or greater than the minimum required capacity and is not currently being utilized by a VIOS 4 in response to determining that the existing LU has an available amount of storage that is at least equal to the minimum required capacity and is not currently being utilized assigning the existing LU to the first LPAR as a shared paging file and 5 the first VIOS autonomously directing all subsequent paging file requests of the application to available storage within the existing LU.

In a traditional Active Memory Sharing AMS enabled system each client requires a dedicated storage device physical disk . This requires a large amount of expensive physical storage devices each of which may only be minimally used by the assigned client. Across a large AMS system this equates to a significant amount of wasted storage both in terms of financial cost to a provider of the AMS enabled system and in terms of utilized disk space within the AMS enabled system.

For example an AMS system can have only 25 GB of physical memory that is used to support clients each requiring 10 GB memory. The AMS implementation recognizes that for various reasons such as the clients only using a small portion of their assigned memory or not all clients are online at the same time the full allotment of 50 GB is not needed to support these five clients. AMS functionality allows for over committing memory such that each client s provided memory may be only 5 GB in size even though the clients have requested 10 GB. Whenever a client requests data not currently in memory data stored in the other 5 GB a hypervisor pages out what is not in use from the data in memory to persistent storage and pages in the reference data into memory.

The AMS functionality described herein over commits memory at the LPAR level. Each client LPAR is allocated a Logical Unit LU of a determined size. The size of the allocated LU may be less than the amount that is actually available to the client and is based on the memory required by software e.g. Applications Operating Systems of the client LPAR.

Since a single disk can be divided into small chunks LUs of a smaller size e.g a 20 GB disk divided into smaller 10 GB LUs when a client is offline the available memory previously allocated to that client that is not in use can be given by the PHYP to clients that are online thus reducing page in and page outs. If a client is allowed up to 10 GB but only requires 5 GB at a specific time the PHYP may only provide a 5 GB LU to the client. The LUs also allow thin provisioning which further reduces wasted memory. The thin provisioning feature allows for efficient use of storage by only allocating physical storage for the logical unit when the actual storage is needed. Additionally the PHYP may dynamically increase or decrease the size of a LU as needed by a client. Since all LPARs share storage from the same pool the distributed storage repository is much more efficiently utilized as clients are only provided with the amount of storage they require at a given time not what is assigned or paid for i.e. memory storage is not over committed while still providing each requesting application OS with persistent paging storage.

An application e.g. Application or OS of a client LPAR submits a paging file request to a VIOS e.g. VIOS or a utility executing in the VIOS such as CM Utility . The paging file request indicates a minimum required capacity required for paging file operations. The minimum required capacity may be specified by the requesting application or OS or may be determined by a managing VIOS. The application does not need to know any other details about the physical storage. This greatly reduces the number of configuration steps by a system administrator when compared with traditional AMS systems and also reduces the chance of user error.

Once the paging file request is received the VIOS automatically partitions a new LU from the distributed storage repository and assigns the new LU to the requesting application OS of the client LPAR. Once allocated a logical unit LU functions as a normal paging file for input output data for use by the requesting application OS via VIOS . VIOS may then autonomously direct all subsequent paging file I O data of the requesting application OS to the new LU . Each new LU may be thin provisioned by the VIOS from distributed storage repository to be equal to the same size as the indicated minimum required capacity required for paging file operations. For example a logical unit of 100 MB can be created by a VIOS but a client LPAR may only require 10 MB of physical storage at a given time. Thus only 10 MB of physical storage would actually be allocated to the client LPAR. Clients LPARs may be assigned a certain amount of memory but are only provided with the amount of memory that will be effectively utilized by that client LPAR at any a given moment. In an alternative embodiment a predetermined amount of storage that is more than what is anticipated to be utilized by the requesting application OS such as an additional two percent storage may be allocated to account for differences in media such as different disk cluster sizes for a source storage for example. After being created the new LU is assigned to the requesting client LPARs via VIOSes and the LU operates like real memory . Each LU in the distributed storage repository is visible to all VIOSes . This also provides for easy migration of LUs from one CEC to another. While the LUs all share the same distributed storage repository storage pool each LU is only available to one client. To ensure data is kept confidential no two different clients are able to access or share a same LU .

In one embodiment instead of creating a new LU a VIOS managing the requesting application OS may also determine if memory of an existing LU within the distributed storage repository that is not currently utilized by a VIOS has an available amount of storage at least equal to the minimum required capacity of a received client request for memory allocation that would be suitable for use by the requesting application OS. When an existing LU meets this criterion the VIOS assigns the existing LU or a portion of the existing LU to the requesting application OS. In response to none of the existing LUs having sufficient available space that is suitable for use by the requesting application OS the managing VIOS may partition a new LU from available memory in the shared distributed storage repository . Alternatively in one embodiment the managing VIOS may select an existing LU and increase the storage capacity of the selected existing LU by an amount equal to the difference between minimum required capacity and the available amount of storage of the existing LU . Thus the selected existing LU is resized thin provisioned to accommodate the storage requirements of the new paging file request as well as the needs of each existing LPAR that is currently utilizing the selected existing LU . The managing VIOS may then assign the existing LU to the requesting LPAR as a shared paging file and autonomously direct all subsequent paging file I O data of the requesting application OS to the existing LU .

An application that requires more storage than is currently allocated in a LU is automatically allocated the additional storage required. Conversely if storage needs should be reduced for an application OS the managing VIOS may autonomously shrink the size of a LU. As the number of clients increases additional LUs in the storage network i.e. distributed storage repository may be allocated as additional paging storage devices for access by one or more VIOSes . In one embodiment each LU may be subdivided into sub partitions . Sub partitions are separate slices for storing system hibernation data and paging file data within a same LU . Sub partitions may be locked to a particular VIOS or may be accessible by only one or more applications within a VIOS . A sub partition and may be allocated as paging file storage for active memory sharing or may be restricted only for use as a hibernation partition for a VIOS .

Turning now to the flowcharts illustrate various methods by which LUs may be assigned as paging files. In there is depicted a high level logical flowchart of the process for creating a new LU as an active memory sharing paging file according to one embodiment. After initiator block a VIOS receives a paging file request from an application OS of a client LPAR block . The paging file request indicates a minimum required capacity which is a minimum amount of paging file storage required by the application. Upon receiving the paging file request the VIOS reads the minimum required capacity from the paging file request block . The VIOS server then thin provisions a new LU from distributed storage repository equal to the minimum required capacity block . The VIOS server then assigns the provisioned LU to the client LPAR block . All subsequent paging file I O data of the requesting application OS are then autonomously directed and or routed the hypervisor to the new LU block . The process terminates at block .

When there is not an existing LU within a plurality of existing LUs in the distributed data repository that has an available amount of storage at least equal to the minimum required capacity the VIOS selects an existing LU that is not currently utilized from the plurality of existing LUs block . The VIOS then increases the storage capacity of the selected existing LU by an amount equal to the difference between the minimum required capacity and the available amount of storage of the existing LU block . The VIOS assigns the selected existing LU as a shared paging file to the requesting application OS block . All subsequent paging file I O data of the requesting application OS are then autonomously directed and or routed to the selected existing LU block . The process terminates at block .

Although the methods illustrated in may be described with reference to components and functionality illustrated by and described in reference to it should be understood that this is merely for convenience and that alternative components and or configurations thereof can be employed when implementing the various methods. Certain portions of the methods may be completed by CM utility executing on one or more virtual processors CPU A within CEC or or on processing resources of distributed storage repository . The executed processes then control specific operations of or on CECs client LPARs VIOSes or distributed storage repository . For simplicity in describing the methods all method processes are described from the perspective of VIOS node .

Returning now to as previously disclosed multiple VIOSes may also share the same LU . An LU provisioned as a paging file provides temporary storage to each VIOS sharing the LU . Since VIOSes are cluster aware the VIOSes may query each other with regards to the device usage to prevent client crashes or data corruption. For example in response to a second VIOS querying a LU currently utilized for processing a client I O operation by a first VIOS first VIOS may respond to the second VIOS by providing specific information about the current status of the LU that is currently being utilized by the first VIOS . This information may include specific portions partitions or sectors of data which are currently utilized or allocated to LU . This information is then utilized by the second VIOS in order to prevent any portion of the LU from being overwritten or deleted while the LU is being accessed and or utilized by the first VIOS . Thus critical hibernation data or paging data in a LU that is currently being utilized cannot be accidentally overwritten accessed or deleted by other VIOSes that are not servicing the same client as the first VIOS

Additionally individual sub partitions of a shared LU may be locked by a managing VIOS so that another application does not overwrite or delete data being utilized by a first application. These sub partitions in a shared LU may or may not be accessed or checked in checked out via page in and page out requests by each of the VIOSes sharing the shared LU . A firmware e.g. FMWRE may manage the memory in AMS in a secure manner and send read or write requests to the VIOSes using a Virtual Asynchronous Services Interface VASI for both AMS and partition hibernation. The VIOS is able to appropriately route the I O data in and out of the LU while providing protection against currently utilized data from being deleted or overwritten inadvertently. The VIOS may also verify the identity and access permissions of a client before unlocking paging file or hibernation data.

In another embodiment a logical redundancy partition of a LU may also be created generally illustrated within block . The logical redundancy partition is a second LU that is a dynamic copy of a first LU. For both AMS and hibernation applications and also for Remote Access Service RAS applications a redundant configuration allows a first LU to be accessible from one or more paging VIOSes. Each change in a first LU is autonomously dynamically echoed in real time to a logical redundancy LU redundant LU . If a portion of data in a first LU is locked by a first VIOS a second VIOS may instead redirect a read request to the logical redundancy LU since the second LU is a dynamic up to date copy of the first LU and therefore contains the same data . In this embodiment a second VIOS can simultaneously access the same data being utilized by the first VIOS without having to wait for the first VIOS to unlock the first logical unit. In one or more embodiments the logical redundancy LU may be restricted to read only access with the exception of the periodic synchronization redundancy operations in order to prevent changes to the redundant LU redundant copy of data being overwritten unless until those changes occur in the primary first LU. In one or more embodiments access to the first LU and the second LU may be controlled within the VIOS cluster by each VIOS tracking the lock status of a LU that the VIOS is attempting to access before proceeding with such access. Thus for example a first VIOS that has a client request directed at the first LU assigned to that client checks with a LU status table within distributed storage repository or in an alternate embodiment within VIOS DB or some other shared storage. The first VIOS locks the LU when the LU is not currently locked. When a second VIOS has an I O request that targets the same first LU the second VIOS checks the LU status table and receives notification that the first LU is currently locked. However if the request is a read request the VIOS may retrieve the location of the second redundant LU complete a security verification process e.g. to ensure the I O adapter information matches the original I O adapter information and once verification is confirmed the second VIOS can then access the redundant LU and read the data from the redundant LU. This embodiment can be enabled when the LU is a shared LU that contains data for a plurality of different clients.

Additionally this functionality may be extended to enable seamless migration of a client LPAR to another CEC. A VIOS may receive a request to migrate a first client LPAR having an allocated LU and a logical redundancy LU that is an exact copy of the allocated LU including the I O adapters and other settings except the write to access setting in one embodiment of a first CEC to a second CEC that is different from the first CEC. Responsive to receiving the migration request the logical redundancy LU is allocated as a primary LU to a second VIOS within the second CEC. The second VIOS may then start restart a second client LPAR for the client within the second CEC. I O operations of the second client LPAR are then performed managed by the second VIOS at the new primary LU of the second VIOS the former logical redundancy LU of the first allocated LU .

With reference now to there is depicted a high level logical flowchart of an exemplary method for providing information to a second VIOS that is querying data currently utilized by a first VIOS of a first CEC according to one embodiment. The dashed blocks illustrate a redundancy operation that can be optionally implemented in one or more embodiments. After initiator block the redundant copy of the first LU is generated block . Following a second VIOS queries a first LU that is currently utilized by a first VIOS block . A determination is then made if the second VIOS is attempting to overwrite or delete data in the first LU that is currently utilized by the first VIOS block . If the second VIOS is not attempting to overwrite or delete data in the first LU that is currently utilized by the first VIOS the process continues to block . In the redundancy embodiments as the first VIOS makes modifications changes to the first LU those changes are dynamically reflected within the redundant LU such that both the first LU and the redundant LU have synchronized data.

When the second VIOS is attempting to overwrite or delete data in the paging file that is currently utilized by the first VIOS the request of the second VIOS is rejected block . At block information of the first VIOS that is currently utilized is provided to the second VIOS by the first VIOS. A read request is then received at a second VIOS for the first LU currently utilized by the first VIOS block . Responsive to receiving the read request for the first LU the second VIOS autonomously redirects the read request to the logical redundancy LU for the first LU block . A determination is then made if a request to migrate the first client LPAR to a second CEC that is different from the first CEC block . When the determination is made that a migration request has not been received the process terminates at block .

In response to a determination being made that a migration request has been received the logical redundancy LU is allocated to a new client LPAR on the second VIOS located within the second CEC as a primary LU block . A new I O adapter paring is established with the virtual I O adapters of the second VIOS that is assigned to the new client LPAR and the server I O adapter linked to the redundant LU. The write to access permission of the redundant LU is modified to allow both read and write access by one or more VIOSes. The second VIOS then starts restarts a second client LPAR within the second CEC block . Finally at block subsequent I O operations of the second client LPAR are performed by the second VIOS at the primary LU formerly logical redundancy LU . The process then terminates at block .

Although the method illustrated in may be described with reference to components and functionality illustrated by and described in reference to it should be understood that this is merely for convenience and that alternative components and or configurations thereof can be employed when implementing the various methods. Certain portions of the methods may be completed by CM utility executing on one or more virtual processors CPU A within CEC or or on processing resources of distributed storage repository . The executed processes then control specific operations of or on CECs client LPARs VIOSes or distributed storage repository . For simplicity in describing the methods all method processes are described from the perspective of VIOS node .

With reference now to there is depicted a block diagram illustrating the storing and restarting of hibernation data of a client LPAR from a LU. The partition hibernation feature described herein provides the capability to suspend a running client LPAR with its OS and applications and virtual device information to persistent storage. VIOS and LUs used for AMS may also be used to support partition hibernation and remote restarting of hibernation data. The hibernation data may comprise any data executing on or stored within the client LPAR . In one or more embodiments this hibernation data can include for example one or more operating systems e.g. OS one or more applications Application running on the one or more operating systems OSes application and system data associated with a client LPAR that is stored in volatile or non volatile memory e.g. MEM an external storage connected to the CEC data stored on LU or storage within the cloud storage distributed storage repository or any combination therein. The hibernation data creates a hibernation image from an assigned hibernation utility CM Utility executing on the CEC . The hibernation image may be stored by a VIOS on a dedicated LU or in a sub partition of a LU . LU may be allocated to serve as a dedicated storage for hibernation data or in another embodiment LU may operate as a paging file while having a sub partition allocated and accessible for system hibernation data by only one client or one or more VIOS allocated to the client. The sub partition may also be thin provisioned within the allocated LU .

In one or more embodiments a hibernation request may be received from a management console e.g. VCP and or submitted to a VIOS or CM Utility by an application e.g. Application or an OS of a client LPAR . The hibernation request is triggered by VCP and indicates a minimum required capacity required for storing the hibernation data of the client LPAR. The hibernation request may be generated by the managing VIOS autonomously or in response to stimuli such as inactivity of the client LPAR for a predetermined amount of time. Once the hibernation request is received the managing VIOS autonomously determines if an existing Logical Unit LU on the distributed storage repository has an available amount of storage at least equal to the minimum required capacity. When an existing LU meets this criterion the VIOS allocates a sub partition of the existing LU to the Client LPAR for storing the hibernation data. In response to an existing LU not being suitable for use by the requesting application OS the VIOS automatically partitions a new LU from the distributed storage repository and assigns the new LU to the client LPAR . In an alternative embodiment a separate LU may be allocated and or thin provisioned for each hibernation request. When the hibernation data is stored within a sub partition of an AMS partition the AMS partition may be resized in order to wholly contain the hibernation data.

After the LU has been allocated VIOS or a utility executing in VIOS such as CM Utility can begin copying data from the currently executing state of a managed LPAR to the allocated LU . The hibernation data is written to the LU by a managing source VIOS and the writing of data is managed by PHYP . An identification of the location contents and size or any combination thereof for the hibernation data is stored within a table of VIOS Dbase . Once all of the data in the currently executing state of the LPAR has been copied to the allocated LU managing source VIOS may suspend and or shut down the client LPAR . In the described embodiments stored hibernation data and hibernation images may be restarted by the client in any location.

In there is depicted a high level logical flowchart of an exemplary method for hibernating a currently executing state of a client LPAR according to one embodiment. After initiator block PHYP receives a hibernation request for a currently executing state of a client LPAR controlled by the source VIOS block . The PHYP then determines the source VIOS for the client LPAR block and triggers the VIOS to determine a storage device e.g. a LU or a sub partition of a LU in the distributed storage repository for storing the hibernation data block . The source VIOS writes the hibernation data for the client LPAR onto the storage device block . The process then terminates at block .

With reference now again to the remote restart functionality and interfacing features described herein provide the capability to resume hibernation data at a later time and or by a different VIOS than the source VIOS that carried out the hibernation request. The remote restart also supports several new commands for remotely restarting a hibernated client partition by a target VIOS on a different CEC from the original CEC and source VIOS . Virtual Control Panel VCP also provides therein a user interface that receives and issues commands and requests for performing hibernation migration and restarting remote restarting operations for a LU . In one embodiment VCP is provided on within HMC . In another embodiment VCP may be any management console and or an Integrated Virtualization Manager IVM . PHYP provides an interface for facilitating communication between a paging VIOS and a VCP . In one embodiment VIOS may also perform the role of a migration manager for hibernation data. The source VIOS e.g. VIOS C is the last VIOS to manage a client LPAR that has been hibernated . The target VIOS e.g. VIOS A is the next VIOS to manage the client LPAR hibernation data image once the client LPAR is remotely restarted. To enable hibernation migration and restarting remote restarting features remote restart remote restart stop and remote restart stop collect commands described below in greater detail are invoked by VCP also referred to herein as an orchestrator and coordinator using a migmgr command for partition migration to discover the storage associated with the migrated client partition and to read in the hibernation data and resume a client LPAR .

Before restarting a hibernation image a requesting VIOS determines the location of the saved hibernation data by reading a database entry that is stored within VIOS Dbase corresponding to the hibernation image for the source client LPAR . A utility executing in VIOS may then determine if the requesting LPAR is the same VIOS LPAR that was previously hibernated or is a second LPAR belonging to the same client as the hibernation data. This ensures that sensitive client information of one client is not accessed by another client. In response to the requesting LPAR being the same LPAR that was previously hibernated or belonging to the same client a VIOS assigned to the requesting LPAR restores the hibernation state stored in the allocated LU to the requesting LPAR . Thus the LPAR can be resumed brought out of hibernation by one or more of the same VIOS or a different VIOS 

In one embodiment PHYP receives remote restart commands invoked by the orchestrator. Responsive to receiving a remote restart command PHYP may lock one or more source adapters of the source VIOS and one or more target adapters of the target VIOS. This locking prohibits the source and target VIOSes from being reallocated to another client while the restart operation is being executed. The remote restart command is part of a migmgr command within the partition migration infrastructure of a CEC . The migmgr command has sub commands for different tasks such as obtaining VSCSI and VFC adapter information on the source VIOS and identifying the potential target VIOSes that are capable of serving the desired devices. In response to a remote restart command being successfully completed and a hibernation data or hibernation data being restarted for a client LPAR for by the target VIOS PHYP initializes the restored hibernation data on the target VIOS for the client LPAR .

The remote restart command which may be performed by a h2vios rr start command further supports leveraging most of the partition migration infrastructure to remotely recover a client partition using a different VIOS on a different CEC when the source VIOS or the source CEC itself are rendered unavailable. The remote restart command is invoked by the orchestrator before the start of a remote restart operation and is also analogous to the lock source adapter and lock target adapter commands for partition migration. The remote restart command may also include operators for providing additional information to the hypervisor for a remote restart operation. A remote restart command entered to a Resource Monitoring Control RMC interface would be in the format of 

The s flag identifies a virtual adapter device. The device identified by the drc name value is locked from being modified by another orchestrator or an admin operation. The i flag is used to pass the operation token ID which may also be analogous to a stream ID used to tie migration tasks together for partition mobility. The d flag specifies a detail level of the remote restart command. The value passed for the operation token id is a set value that is the same for each subsequent migration command that is invoked as part of the remote restart operation. The m flag inputs the capability bit of the 64 bit capability value. The capability bit identifies one or more capabilities supported by VCP such as migration virtual real memory VRM or logical unit LU support N Port ID Virtualization NPIV provisioning of entire dedicated logical ports to client LPAR s rather than individual LUNs and remote restart. The remote restart command may also identify or trigger an identification call for determining a source VIOS and a target VIOS for the remote restart command. It should be noted that the ordering of the flags is inconsequential.

In one or more embodiments the remote restart command may also include a get adapter command issued by PHYP or VCP on the source VIOS for gathering a plurality of adapter information for one or more source adapters in use by the source VIOS and saving the gathered information for use by PHYP . The gathering may also include determining if the combination of one or more target adapters of the target VIOS are capable of performing the functionality provided by the one or more source adapters of a source VIOS. When a combination of one or more target adapters of the target VIOS are not capable of performing the functionality provided by the one or more source adapters of a source VIOS PHYP or the target VIOS may create one or more additional target adapters for the target VIOS for perform the functionality provided by the one or more source adapters. Additionally the remote restart command may further include a set adapter call on the target VIOS for setting one or more target adapters once the adapter instance are created on the target VIOS. Thus the remote restart request provides instructions to the PHYP for identifying a hibernation image and locking the hibernation image from further modifications until a remote restart operation has completed.

The remote restart stop command which may be performed by a h2vios rr stop command is invoked by the orchestrator to indicate to the target VIOS that a remote restart operation identified by the operation token id is finished. The remote restart stop command may also include operators for providing additional information to the hypervisor for a remote restart operation. A typical RMC interface for the remote restart stop command would be in the format of 

The i flag is used to pass the operation token ID which may also be analogous to a stream ID used to tie migration tasks together for partition mobility. The value passed for the operation token id is a set value that is the same for each subsequent migration command that is invoked as part of the remote restart operation. The d flag specifies a detail level of the remote restart stop command. The m flag inputs the capability bit of the 64 bit capability value. The capability bit identifies one or more capabilities supported by VCP such as migration virtual real memory VRM or logical unit LU support N Port ID Virtualization NPIV provisioning of entire dedicated logical ports to client LPAR s rather than individual LUNs and remote restart. The flag to input the termination state is T . The termination states that can be passed to the VIOS are RESTART SUCCEEDED defined as 0 or RESTART FAILED defined as 1 . It should be noted that the ordering of the flags is inconsequential.

The VIOS may unlock the virtual adapters when the remote restart stop command is received. If the termination state is RESTART SUCCEEDED there is no further action taken by the VIOS. However if the termination state is RESTART FAILED the VIOS will perform any cleanup work required by the remote restart command that still remains such as removal of the virtual adapters that were created on the VIOS.

If the VIOS does not receive the remote restart stop command within some time out period implemented by the VIOS from when the remote restart command is received the VIOS may unlock the locked virtual adapters. However the virtual adapters will not be removed.

The remote restart stop collect command which may be performed by a h2vios rr stop collect command is invoked by the orchestrator to indicate to the source VIOS that a remote restart operation identified by the operation token id is finished. The remote restart stop collect command further indicates to the source VIOS that the gathering performed in the remote restart command is finished. Unlike the remote restart stop command the remote restart stop collect command does not include a termination state.

The remote restart stop collect command may also include operators for providing additional information to the hypervisor for a remote restart operation. A typical RMC interface for the remote restart stop collect command would be in the format of 

The i flag is used to pass the operation token ID which may also be analogous to a stream ID used to tie migration tasks together for partition mobility and must be the same value as the one passed to the remote restart command or in a get adapter command of the remote restart command. The value passed for the operation token id is a set value that is the same for each subsequent migration command that is invoked as part of the remote restart operation. The d flag specifies a detail level of the remote restart stop collect command. The m flag inputs the capability bit of the 64 bit capability value. The capability bit identifies one or more capabilities supported by VCP such as migration virtual real memory VRM or logical unit LU support N Port ID Virtualization NPIV provisioning of entire dedicated logical ports to client LPAR s rather than individual LUNs and remote restart. It should be noted that the ordering of the flags is inconsequential.

As illustrated in Client LPAR has allocated therein a provisioned LU as a system hibernation file for backing up system hibernation data. Once a system hibernation process has begun the system hibernation data of Client LPAR is copied and stored within LU . Upon the storage of system hibernation data to LU completing Client LPAR may be suspended and or shut down by source VIOS . In the example of Client LPAR is owned by a same client as Client LPAR . The system hibernation data stored in LU may be restored by a target VIOS at any later time to either client LPAR or client LPAR

In there is a high level logical flowchart of an exemplary method for remote restarting a hibernated partition according to one embodiment. At block the PHYP receives a remote restart command from the orchestrator. The PHYP then determines the source VIOS that created the hibernation data and the target VIOS that will remotely restart the hibernation data and determines the location of the saved hibernation data by reading a database entry that is stored within VIOS Dbase corresponding to the hibernation image for the source client LPAR block . At block the adapters of the source VIOS and the target VIOS are both locked. The PHYP then gathers information on the one or more source adapters used by the source VIOS block . Using this information a determination is made by the PHYP if the target adapters of the target VIOS are capable of collectively performing the functionality provided by the source adapters of the source VIOS block .

In response to the target adapters of the target VIOS not being capable of collectively performing the functionality provided by the source adapters of the source VIOS the PHYP creates one or more additional target adapters for the target VIOS such that the one or more additional target adapters are capable of collectively performing the functionality provided by the one or more source adapters block . In response to the target adapters of the target VIOS being capable of collectively performing the functionality provided by the source adapters of the source VIOS the process moves to block . At block the remote restart of the client LPAR is initialized on the target VIOS using the hibernation data. The process then terminates at block .

In there is depicted a high level logical flowchart of a method for identifying and locking hibernation data and providing the hibernation data to a target VIOS according to one embodiment. The method provided in may be a sub routine of a remote restart command. After initiator block PHYP receives an identification of hibernation data corresponding to the remote restart request block . The hibernation data is a plurality of data that was previously hibernated by a source VIOS and is identified for remote restart operation on a target VIOS. PHYP then locks the hibernation data from further modifications to ensure the hibernation data is not modified while the remote restart command is being executed block . At block PHYP provides the hibernation data to the target VIOS. The process terminates at block .

In there is depicted a high level logical flowchart of a method provided by a remote restart stop command for identifying to a target VIOS that a remote restart operation is finished according to one embodiment. After initiator block the VCP invokes the remote restart stop command block . A determination is then made if the remote restart operation has finished block . The process loops until the remote restart command is finished. Once a determination is made that remote restart command is finished the PHYP terminates the remote restart command block . A termination state of the remote restart stop command is then read in order to determine if the remote restart operation has failed or has successfully completed block .

In response to determining that the remote restart operation has failed PHYP instructs the target VIOS to perform a cleanup of the remote restart command block and the process continues to block . When the remote restart operation has successfully completed the process continues directly to block . At block PHYP unlocks source adapters of the source VIOS and target adapters of the target VIOS. The process then terminates at block .

In there is depicted a high level logical flowchart of a method provided by a remote restart stop collect command for identifying to a source VIOS that gathering operation has finished according to one embodiment. After initiator block PHYP receives a remote restart stop collect command from a VCP block . A determination is then made if the gathering of a plurality of adapter information for one or more source adapters in use by the source VIOS has completed block . The process loops until the gathering is completed. Once a determination is made that the gathering is completed the VCP invokes the remote restart stop collect command to the source VIOS block . The process then terminates at block .

Thus according to one or more of the described embodiments a method data processing system and computer program product enables interfacing between a Virtual Control Panel VCP and a Virtual Input Output I O Servers VIOS to facilitate partition hibernation and remote restarting operations in a VIOS cluster environment. During an initial creation of a client s hibernation data file or image at a first computing electronic complex CEC I O adapter configuration information is included within the hibernation data file that is stored by the VIOS in the client s assigned logical unit LU within a shared storage repository. The hypervisor on a different second CEC receives a remote restart command from an input entered into the VCP and determines a source VIOS configuration from the selected hibernation data file and the hypervisor identifies a target VIOS for performing a remote restart of the client utilizing the stored hibernation data file. The hypervisor assigns configurations of specific virtual I O adapters of the source VIOS retrieved from the stored hibernation data file or from the VIOS database to newly generated virtual I O adapters of the target VIOSes. The hypervisor then gathers adapter information of the target I O adapters to determine if the set of the target adapters has the correct configurations and security settings and is capable of performing the I O functionality provided by the source adapters. In response to the set of target adapters of the target VIOS not being capable of performing all of the functionality provided by the source adapters additional target adapters are created with the required configurations. The remote restore of the client on the remote client LPAR using the target VIOS is then initialized using the hibernation data file. Once restored at the second CEC the client is able to perform all of its I O operations with the target VIOS providing access to the client s assigned LU.

In one embodiment a method provides receiving at a source VIOS of the at least one VIOS a hibernation request for a client LPAR the source VIOS determining a storage device within a distributed storage repository for storing a generated hibernation data file of the client LPAR the source VIOS securely writing the hibernation data file from the client LPAR to the storage device initializing via the VCP a remote restart of the client LPAR at a target LPAR managed by a second the target VIOS using the hibernation data stored within the storage device. The remote restart comprises receiving at the second target VIOS a remote restart command from the VCP wherein the VCP is communicatively coupled to a hypervisor which enables communication interfacing with the second VIOS the target VIOS identifying the source VIOS and the location of the generated hibernation data file of the client LPAR and the target VIOS emulating one or more I O adapters to assign to a target client LPAR in which the client LPAR will be restored wherein the emulating of the one or more I O adapters is based on received configuration information of the source I O adapters assigned to the client LAPR by the source VIOS.

In one or more embodiments the method further comprises locking one or more source adapters of the source VIOS and one or more target adapters of the target VIOS gathering a plurality of adapter configuration information for one or more source adapters utilized by the source VIOS to handle I O operations of the client LPAR determining if a current configuration of one or more target adapters of the target VIOS are capable of performing all I O functionality provided by the one or more source adapters and in response to current configuration of one or more target adapters not being capable of performing the functionality provided by the one or more source adapters dynamically creating one or more additional target adapters for the target VIOS wherein the one or more additional target adapters are configured using the adapter configuration information of the one or more source adapters to enable the one more target adapters to perform all required I O functionality provided by the one or more source adapters for the client LPAR and assigning to the target client LPAR the one or more target adapters that exhibit a configuration required to enable the I O operations of the client LPAR using the target VIOS.

In one or more embodiments the remote restart command provides instructions for identifying the hibernation data and locking the hibernation data from further modifications while the hibernation data is being restarted. Also the VCP invokes a remote restart stop command where the remote restart command indicates to the target VIOS that a remote restart operation is finished the remote restart stop command further performing the functions of determining if the remote restart command has finished in response to determining that the remote restart command has finished terminating the remote restart command determining if the remote restart command has failed in response to determining the remote restart command has failed performing a cleanup of the remote restart command and unlocking all adapters of the source VIOS and the target VIOS.

In one embodiment the VCP invokes a remote restart stop collect command to the source VIOS wherein the remote restart stop collect command indicates to the source VIOS that the gathering of the plurality of adapter information for one or more source adapters in use by the source VIOS is finished. In one or more embodiments the VCP is associated with one or more of a hardware management console HMC and an Integrated Virtualization Manager IVM . Also in one or more embodiments the target VIOS is on a different computing electronic complex CEC than the source VIOS.

The flowcharts and block diagrams in the various figures presented and described herein illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowcharts or block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It should also be noted that in some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

In the flow charts above one or more of the methods are embodied in a computer readable medium containing computer readable code such that a series of steps are performed when the computer readable code is executed by a processing unit on a computing device. In some implementations certain processes of the methods are combined performed simultaneously or in a different order or perhaps omitted without deviating from the spirit and scope of the invention. Thus while the method processes are described and illustrated in a particular sequence use of a specific sequence of processes is not meant to imply any limitations on the invention. Changes may be made with regards to the sequence of processes without departing from the spirit or scope of the present invention. Use of a particular sequence is therefore not to be taken in a limiting sense and the scope of the present invention extends to the appended claims and equivalents thereof.

As will be appreciated by one skilled in the art aspects of the present invention may be embodied as a system method or computer program product. Accordingly aspects of the present invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium s having computer readable program code embodied thereon.

Any combination of one or more computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this document a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable R.F etc. or any suitable combination of the foregoing. Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Aspects of the present invention are described below with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks. The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

As will be further appreciated the processes in embodiments of the present invention may be implemented using any combination of software firmware or hardware. As a preparatory step to practicing the invention in software the programming code whether software or firmware will typically be stored in one or more machine readable storage mediums such as fixed hard drives diskettes optical disks magnetic tape semiconductor memories such as ROMs PROMs etc. thereby making an article of manufacture in accordance with the invention. The article of manufacture containing the programming code is used by either executing the code directly from the storage device by copying the code from the storage device into another storage device such as a hard disk RAM etc. or by transmitting the code for remote execution using transmission type media such as digital and analog communication links The methods of the invention may be practiced by combining one or more machine readable storage devices containing the code according to the present invention with appropriate processing hardware to execute the code contained therein. An apparatus for practicing the invention could be one or more processing devices and storage systems containing or having network access to program s coded in accordance with the invention.

Thus it is important that while an illustrative embodiment of the present invention is described in the context of a fully functional computer server system with installed or executed software those skilled in the art will appreciate that the software aspects of an illustrative embodiment of the present invention are capable of being distributed as a program product in a variety of forms and that an illustrative embodiment of the present invention applies equally regardless of the particular type of media used to actually carry out the distribution.

While the invention has been described with reference to exemplary embodiments it will be understood by those skilled in the art that various changes may be made and equivalents may be substituted for elements thereof without departing from the scope of the invention. In addition many modifications may be made to adapt a particular system device or component thereof to the teachings of the invention without departing from the essential scope thereof. Therefore it is intended that the invention not be limited to the particular embodiments disclosed for carrying out this invention but that the invention will include all embodiments falling within the scope of the appended claims. Moreover the use of the terms first second etc. do not denote any order or importance but rather the terms first second etc. are used to distinguish one element from another.

The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

The corresponding structures materials acts and equivalents of all means or step plus function elements in the claims below are intended to include any structure material or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present invention has been presented for purposes of illustration and description but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention. The embodiment was chosen and described in order to best explain the principles of the invention and the practical application and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.

