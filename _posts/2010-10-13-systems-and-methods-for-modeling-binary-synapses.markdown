---

title: Systems and methods for modeling binary synapses
abstract: Methods and system for modeling the behavior of binary synapses are provided. In one aspect, a method of modeling synaptic behavior includes receiving an analog input signal and transforming the analog input signal into an N-bit codeword, wherein each bit of the N-bit codeword is represented by an electronic pulse. The method includes loading the N-bit codeword into a circular shift register and sending each bit of the N-bit codeword through one of N switches. Each switch applies a corresponding weight to the bit to produce a weighted bit. A signal corresponding to a summation of the weighted bits is output and represents a synaptic transfer function characterization of a binary synapse.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09342780&OS=09342780&RS=09342780
owner: Hewlett Packard Enterprise Development LP
number: 09342780
owner_city: Houston
owner_country: US
publication_date: 20101013
---
This application is a U.S. National Stage Application of and claims priority to International Patent Application No. PCT US2010 052490 filed on Oct. 13 2010 and entitled SYSTEMS AND METHODS FOR MODELING BINARY SYNAPSES which claims priority to U.S. Provisional Patent Application No. 61 369 603 filed Jul. 30 2010 entitled SYSTEMS AND METHODS FOR MODELING BINARY SYNAPSES. 

This invention was made with government support under Contract No. HR0011 09 3 0001 awarded by Defense Advanced Research Projects Agency. The government has certain rights in the invention.

Early in the history of computing computer scientists became interested in biological computing structures including the human brain. Although sequential instruction processing engines have technologically evolved with extreme rapidity during the past 50 years many seemingly straightforward problems cannot be effectively addressed by even the largest and highest speed distributed computer systems and networks. One trivial example is the interpretation of photographs and video images. A human can often in a fraction of a second glance at a photograph and accurately interpret objects interrelationships between objects and the spatial organization of objects represented by the two dimensional photograph while such interpretation of photographic images is currently beyond the ability of the largest computer systems running the cleverest algorithms.

Extensive research efforts have been expended in investigating the structure and function of the human brain. Many of the fundamental computational entities in such biological systems have been identified and characterized physiologically at microscale dimensions as well as at the molecular level. For example the neuron a type of cell responsible for signal processing and signal transmission within the human brain is relatively well understood and well characterized although much yet remains to be learned. This understanding of neuron function has inspired a number of fields in computer science including neural network and perception network subfields of artificial intelligence. Many successful software implementations of neural networks have been developed to address a variety of different applications including pattern recognition diagnosis of the causes of complex phenomena various types of signal processing and signal denoising and other applications. However the human brain is massively parallel from a structural standpoint and while such parallelism can be simulated by software implementations and neural networks the simulations are generally processor cycle bound because the simulations necessarily run on one or a relatively small number of sequential instruction processing engines rather than making use of physical parallelism within the computing system. Thus neural networks may provide tolerance to noise learning capabilities and other desirable characteristics but do not currently provide the extremely fast and high bandwidth computing capabilities of massively parallel biological computational structures.

In order to achieve the extremely fast and high bandwidth computing capabilities of biological computational structures in physical manufactured devices computational tasks are typically carried out on massively parallel and interconnected networks of computational nodes. Many different approaches for implementing physical neural networks have been proposed but implementations have so far have fallen fall short of the speed parallelism and computational capacity of even relatively simple biological structures. In addition design and manufacture of massively parallel hardware is fraught with any number of different practical problems including reliable manufacture of large numbers of dynamical connections size and power constraints heat dissipation reliability flexibility including programmability and many other such considerations. However unlike many theoretical problems for which it is unclear whether or not solutions can be found the fact that computational biological structures including the human brain exist and perform spectacular feats of computation on a regular basis would suggest that the goal of designing and constructing computational devices with similar computational capacities and efficiencies is quite possible.

Computer scientists hardware designers researchers focused on artificial intelligence biological intelligence and a wide variety of different fields within computer science and information sciences continue to seek advancements in physical hardware devices suitable for implementing the types of massively parallel distributed dynamical processing that occurs within the human brain and other computational biological structures.

This disclosure is directed to systems and methods that model Instar and Outstar synaptic behavior. Populations of synapses are modeled as dynamical two terminal devices that connect pre synaptic neurons called source neuron or source to post synaptic neurons called a sink neuron or a sink. The source emits an analog source signal that is modified by a synaptic transfer function and delivered to the sink. The synaptic transfer function depends on at least one continuous state variable that evolves over time as a function of the signals sent by the source and the state of the sink. The combination of a synaptic transfer function and state evolution equation models the behavior of a population of synapses. Systems and methods of the present invention are based on an Instar and Outstar model of synaptic behavior with two terminal switches. In particular the binary two terminal switches can be implemented using memristive nanodevices because such devices can be densely packed using crossbar arrays and posses a long time constant or memory.

Neurons are a type of cell found in the brains of animals. Neurons are thought to be one of if not the fundamental biological computational entity. It is estimated that the human brain contains on the order of 100 billion 10 neurons and on the order of 100 trillion 10 interconnections between neurons. The massive number of interconnections between neurons in the human brain is thought to be directly correlated with the massively parallel nature of biological computing.

Each neuron is a single cell. shows a generalized and stylized illustration of a neuron. The neuron includes a cell body containing the cell nucleus and various organelles including mitochondria a number of branching dendrites such as dendrite emanating from the cell body and generally one very long axon that terminates in many branching extensions . In general the dendrites provide an enlarged neuron surface area for receiving signals from other neurons while the axon serves to transmit signals from the neuron to other neurons. The terminal branches of the axon interface with the dendrites and less frequently with the cell bodies of other neurons. A single neuron may receive as many as 100 000 different signal inputs. Similarly a neuron may transmit signals to tens hundreds or even thousands of downstream neurons. Neurons vary tremendously within a given individual with respect to the number of and degree of branching of dendrites and terminal axon extensions as well as with respect to volume and length. For example axons range in length from significantly less than one millimeter to over one meter. This flexibility in axon length and connectivity allow for hierarchical cascades of signal paths and extremely complex connection based organizations of signaling paths and cascades within the brain.

As mentioned above input signals received by a given neuron are generated by output signals of other neurons connected to the given neuron by synapse junctions between the other neurons terminal axon branches and the given neuron s dendrites. These synapses or connections between neurons have dynamically adjusted connection strengths or weights. The adjustment of the connection strengths or weights is thought to significantly contribute to both learning and memory and represents a significant portion of parallel computation within the brain.

Neuron functionalities are derived from and depend on complex electrochemical gradients and ion channels. is an abstract representation of a neuron cell showing the different types of electrochemical gradients and channels in the neuron s outer membrane that control and respond to electrochemical gradients and signals and that are used to trigger neuron output signal firing. In the neuron is represented as a spherical membrane enclosed cell the contents of which are separated from the external environment by a double walled hydrophobic membrane that includes various types of channels such as channel . The various types of channels provide for controlled chemical communication between the interior of the neuron and the external environment.

The channels primarily responsible for neuron characteristics are highly selective ion channels that allow for transport of particular inorganic ions from the external environment into the neuron and or from the interior of the neuron to the external environment. Particularly important inorganic ions include sodium Na potassium K calcium Ca and chlorine Cl ions. The ion channels are generally not continuously open but are selectively opened and closed in response to various types of stimuli. Voltage gated channels open and close depending on the voltage or electrical field across the neuron membrane. Other channels are selectively opened and closed by mechanical stress and still other types of channels open and close in response to binding and release of ligands generally small molecule organic compounds including neurotransmitters. Ion channel behavior and responses may additionally be controlled and modified by the addition and deletion of certain functional groups to and from ion channel proteins carried out by various enzymes including kinases and phosphatases that are in turn controlled by various types of chemical signal cascades.

In general in a resting or non firing state the neuron interior has a relatively low concentration of sodium ions a correspondingly low concentration of chlorine ions and a relatively high concentration of potassium ions with respect to the concentrations of these ions in the external environment . In the resting state there is a significant 40 50 mV electrochemical gradient across the neuron membrane with the interior of the membrane electrically negative with respect to the exterior environment. The electrochemical gradient is primarily generated by an active Na K pumping channel which uses chemical energy in the form of adenosine triphosphate to continuously exchange three sodium ions expelled from the interior of the neuron to the external environment for every two potassium ions imported from the external environment into the interior of the neuron. The neuron also contains passive K leak channels that allow potassium ions to leak back to the external environment from the interior of the neuron. This allows the potassium ions to reach equilibrium with respect to the ion concentration gradient and the electrical gradient.

Neuron firing or spiking is triggered by a local depolarization of the neuron membrane. In other words collapse of the normally negative electrochemical gradient across a membrane results in triggering of an output signal. A wave like global depolarization of the neuron membrane that represents neuron firing is facilitated by voltage gated sodium channels that allow sodium ions to enter the interior of the neuron down the electrochemical gradient previously established by the Na K pump channel . Neuron firing represents a short pulse of activity following which the neuron returns to a pre firing like state in which the normal negative electrochemical gradient across the neuron membrane is reestablished. Voltage gated potassium channels open in response to membrane depolarization V to allow an efflux of potassium ions down the chemical potassium ion gradient in order to facilitate reestablishment of an electrochemical gradient across the neuron membrane following firing. The voltage gated potassium channels opened by local depolarization of the neuron membrane Y are unstable in the open state and relatively quickly move to an inactivated state to allow the negative membrane potential to be reestablished both by operation of the voltage gated potassium channel and the Na K channel pump .

Neuron membrane depolarization begins at a small local region of the neuron cell membrane and sweeps in a wave like fashion across the neuron cell including down the axon to the axon terminal branches. Depolarization at the axon terminal branches triggers voltage gated neurotransmitter release by exocytosis . Release of neurotransmitters by axon terminal branches into synaptic regions between the axon terminal branches of the firing neuron referred to as the pre synaptic neuron and dendrites of the signal receiving neurons each referred to as a post synaptic neuron results in binding of the released neurotransmitter by receptors on dendrites of post synaptic neurons that results in transmission of the signal from the pre synaptic neuron to the post synaptic neurons. In the post synaptic neurons binding of transmitters T to neurotransmitter gated ion channels and results in excitatory input signals and inhibitory input signals respectively. Neurotransmitter gated ion channels that import sodium ions into the neuron contribute to local depolarization of the neuron membrane adjacent to the synapse region and thus provide an excitatory signal. By contrast neurotransmitter activated chlorine ion channels result in import of negatively charged chlorine ions into the neuron cell resulting in restoring or strengthening the normal resting negative voltage gradient across the membrane and thus inhibit localized membrane depolarization and provide an inhibitory signal. Neurotransmitter release is also facilitated by voltage gated calcium ion channels that allow calcium influx into the neuron.

A Ca activated potassium channel serves to decrease the depolarizability of the membrane following a high frequency of membrane depolarization and signal firing that results in build up of calcium ions within the neuron. A neuron that has been continuously stimulated for a prolonged period therefore becomes less responsive to the stimulus. Early potassium ion channels serve to reduce neuron firing levels at stimulation levels close to the threshold stimulation required for neuron firing. This prevents an all or nothing type of neuron response about the threshold stimulation region instead providing a range of frequencies of neuron firings that correspond to a range of simulations of the neuron. The amplitude of neuron firing is generally constant with output signal strength reflecting in the frequency of neuron firing.

Another interesting feature of the neuron is long term potentiation. When a pre synaptic neuron fires at a time when the membrane of a post synaptic neuron is strongly depolarized the post synaptic neuron may become more responsive to subsequent signals from the pre synaptic neuron. In other words when pre synaptic and post synaptic neuron firings occur close in time the strength or weighting of the interconnection may increase.

In summary neurons serve as somewhat leaky input signal integrators combined with a thresholding function and an output signal generation function. A neuron fires with increasing frequency as excitatory stimulation of the neuron increases although over time the neuron response to constant high stimulus decreases. Synapses or junctions between neurons may be strengthened or weakened by correlations in pre synaptic and post synaptic neuron firings. In addition and synapse strength and neuron stimulation both decay over time without reinforcing stimulus. Neurons provide a fundamental computational unit for massively parallel neuronal networks within biological organisms as a result of the extremely high density of connections between neurons supported by the highly branched dendrites and axon terminus branches as well as by the length of axons.

Instar and Outstar synaptic models are now described with reference to a single source neuron and single sink neuron. Note however that for the sake of convenience the single source and sink neurons used to describe Instar and Outstar synaptic models actually represent at least one source neuron and at least one sink neuron.

An electronic implementation of an Instar synapse using conventional electronic components can be difficult to implement because of the time scale at which learning takes place i.e. seconds or longer . A typical analog implementation of the differential equation 2 requires a capacitor for storing the state w along with various operational amplifiers and multipliers. Although circuit design of a conventional electronic implementation of the Instar synapse may reduce the area required for such a circuit unavoidable leakage across at the capacitor which introduces a decay term into equation 2 which perturbs the desired behavior and is difficult to mitigate. At the network level such decay appears as memory loss or drift.

Typical neural network models use differential equations to describe average behavior of neuron populations even though the majority of a neuron s communications are performed using discrete pulses. Systems and methods implement a stochastic approximation of Instar and Outstar synapses using synchronous spiking neurons in order to implement the signals x and y and binary switches that implement the synaptic weight w assembled as a population of components. Systems implement the weights with binary devices and neurons communicate with discrete electrical pulses rather than analog voltages. In particular the binary synapses can be implemented with memristive nanoscale devices. Systems of the present invention use less surface area and require less power than either software or conventional electronic implementations and may enable the construction of neuromorphic integrated circuits that approach biological scale density and power.

Returning to the bits of the N bit codeword x x x . . . x are parallel loaded into a circular shift register . The weight w is implemented with N configurable switches where each configurable switch applies a weight wto a bit xof the N bit codeword. The weight applied by each switch is characterized as a binary variable. An auxiliary value tilde over w is defined as the average of the N binary variables was follows 

Returning to after the completion of the N cycles a synaptic state update of the weights also called learning is performed. The analog sink signal y is replaced with a stochastic binary value denoted by tilde over y that pulses at a rate proportional to the value of the sink signal y. For example a y value of 0.25 corresponds to a stochastic tilde over y that randomly pulses i.e. assumes the bit value 1 on average 25 of the time. In other words tilde over y equals 1 with probability y and tilde over y equals 0 with probability 1 y . Independent stochastic binary variables . . . each take on the bit value 1 with fixed probability where is an application specific design parameter with a selected value that is typically smaller than 0.001 The independent stochastic binary variables define N auxiliary variables y y y . . . ydefined as Each of the N auxiliary variables y y y . . . y determines the state evolution of the weight of a corresponding switch as described below. In other words at the end of N cycles N auxiliary variables y y y . . . yare generated and used to change or update the weights w w w . . . w respectively associated with each configurable switch. The N cycles and updating the weights of the switches is called a major cycle. 

Next evaluation of the transfer function is described and is shown to approximate Equation 1 . Subsequently the circuit operations that implement synaptic weight update are described and it is shown that the circuit stochastically approximates the time evolution of equation 2 . Finally a circuit implementation for the schematic representation shown in is described.

The schematic representation of is implemented sequentially. At the beginning of a major cycle a source signal x is input. As described above each major cycle is performed N 1 times. The first N cycles carry out evaluation of the transfer function to produce the output signal out. At the conclusion of the first N cycles an approximation of the desired output signal out is generated . The final N 1 th cycle performs a synaptic weight update.

The method shown in can be shown to approximate the Instar transfer function as follows. First note that in the for loop of blocks the value of the output signal can be represented as follows 

Synaptic state update or learning as referenced above in block of is performed only during the final sub cycle of a major cycle. shows a control flow diagram of a method for synaptic state evolution. The method of describes a learning law for the state evolution of each binary weight variable wfrom its current state w to its next state w of the subsequent major cycle. The for loop of block repeats blocks for each cycle. In block when yequals the binary value 1 the method proceeds to block otherwise the method proceeds to block . In block the binary value assigned to the weight of the next major cycle w is binary value of the bit x. In block the binary value assigned to weight of the next major cycle w is the binary value of the weight from the previous major cycle w. In block when i is less than N the method proceeds to block otherwise the method proceeds to block . In block the index i is incremented. In block the method returns to the method described above with reference to .

An implementation of the learning law described in is described in the subsequent subsection IV. Note that when y 1 Consider the evaluating all possible scenarios of the learning law as presented in Table 1 

Conventional analog and digital circuitry can be used to implement Instar and Outstar learning models with memristive devices or memristors used as two terminal switches. For example the Instar model can be implemented at the nanoscale or microscale using a crossbar array with memristor crossbar junctions. shows an isometric view of a crossbar array . The crossbar array is composed of a first layer of approximately parallel nanowires that are overlain by a second layer of approximately parallel nanowires . The nanowires of the second layer are approximately perpendicular in orientation to the nanowires of the first layer although the orientation angle between the layers may vary. The two layers of nanowires form a lattice or crossbar each nanowire of the second layer overlying all of the nanowires of the first layer and coming into close contact with each nanowire of the first layer at nanowire intersections that represent the closest contact between two nanowires. Although individual nanowires in are shown with rectangular cross sections nanowires can also have square circular elliptical or more complex cross sections. The nanowires may also have many different widths or diameters and aspect ratios or eccentricities. The term crossbar may refer to crossbars having at least one layer of nanowires sub microscale wires microscale wires or wires with larger dimensions.

The layers of a crossbar array can be fabricated by mechanical nanoimprinting techniques. Alternatively nanowires can be chemically synthesized and can be deposited as layers of approximately parallel nanowires in at least one processing step including Langmuir Blodgett processes. Other alternative techniques for fabricating wires may also be employed. Thus a two layer crossbar comprising first and second layers as shown in can be manufactured by any of numerous relatively straightforward processes. Many different types of conductive and semi conductive nanowires can be chemically synthesized from metallic and semiconductor substances from combinations of these types of substances and from other types of substances. A nanowire crossbar may be connected to microscale address wire leads or other electronic leads through a variety of different methods in order to incorporate the nanowires into electrical circuits.

At each nanowire intersection an electronic component such as a nanoscale memristor or analogous electronic component can be fabricated to interconnect two overlapping nanowires to form a switch. For example as shown in memristors located at each of the crossbar intersections form switches.

Memristors can be used as resistive analog memories that are capable of storing state information with very little decay for long periods of time such as days weeks months and possibly years. In certain examples a single bit can be stored in each memristor using the high resistance state of the memristor to represent a logic 0 bit value and the low resistance state to represent a logic 1 bit value. show the memristive characteristics of cross junctions that can be fabricated by currently available techniques. illustrates a single crossbar junction. The crossbar junction comprises memristive material at the junction between a first input nanowire and a second output nanowire . shows a circuit symbol that represents the memristor and lines and that represent the nanowires and respectively. shows a plot of current I versus voltage V for a typical memristor located at a nanoscale crossbar junction. Solid curve shows a plot of the current of the memristor in a low resistance state and dashed nonlinear curve represents the memristor in a high resistance state. Low voltages within the voltage interval have negligible affect on the resistive state of the memristor while larger voltages can change the memristor s resistance state. A positive voltage exceeding the positive on threshold causes the memristor to switch into the low resistance state while a negative voltage less than the negative off threshold causes the memristor to switch into the high resistive state .

The pulses sent by x and g y are approximately aligned in time. Each pulse has an amplitude V which is below the switching threshold of the memristor. In other words if a single x pulse arrives at a memristor without a corresponding aligned g y neither pulse alters the resistance state of the memristor. But when x and g y pulses are aligned in time the net voltage drop across the memristor exceeds the switching threshold of the memristor potentially changing the resistance state as described above with reference to . In summary with reference to there are three cases to consider for changing the weight wof a switch 

1. y 0. In this case g y does not generate any pulses so the resistance state of the memristor does not change. In other words wis not changed.

2. y 1 and x 0. In this case g y generates positive and negative pulses and as shown in while x generates only the negative pulse that aligns with the positive g y pulse . The net voltage drop across the memristor is negative and exceeds the off threshold required to drive the device into the low conductance state . In other words wequals logic bit value 0. 

3. y 1 and x 1. Again g y generates positive and negative pulses and while x generates only the positive pulse that aligns with the negative pulse . The net voltage drop across the memristor is positive and exceeds the on threshold and drives the memristor into the closed high conductance state . In other words wequals logic bit value 1. 

In certain systems all of the circuitry except for the memristive switches can be shared by a large number of synapses. Since neurons typically have a high fan in and high fan out the cost of the analog and digital circuitry can be amortized over a very large number of implemented synapses. shows an example of four source neurons that share the same circuitry to communicate with four sink neurons. For example the circuitry used by source neuron to send source signals to sink neuron can be used by source neuron to send source signals to sink neurons respectively.

The Outstar learning law has the same transfer function as the Instar model and a state evolution equation that interchanges the roles of the singles x and y as described above with reference to . shows an example schematic representation of an Outstar model . The Outstar model interleaves computation of the transfer function out and the time evolution of the synaptic weight w. Analog sink signal y is a continuous input transformed into a discrete N bit codeword represented by y y y . . . y using thermometer encoding as described above with reference to . An auxiliary continuous variable z is defined as the average of the bit values of the N bit codeword as follows 

The Instar learning model can be used to learn analog patterns. When the sink single y is fixed at 1 in the state evolution equation 2 the steady state solution becomes Consider for example the visualization example shown in . shows a two dimensional array of input source signals x fed through Instar synapses to a single sink neuron . The Instar synapses are represented by directional arrows such as directional arrow . The two dimensional array corresponds to a gray photograph where each input source signal xcorresponds to a gray scale value associated with a pixel of the gray scale photograph. For example source signal xequal to 0 corresponds to a black pixel xequal to 1 corresponds to a white pixel and intermediate values correspond to a gray scale value. As shown in each source signal passes through a synapse with an associated weight w.

The foregoing description for purposes of explanation used specific nomenclature to provide a thorough understanding of the invention. However it will be apparent to one skilled in the art that the specific details are not required in order to practice the invention. The foregoing descriptions of specific examples of the present invention are presented for purposes of illustration and description. They are not intended to be exhaustive of or to limit the invention to the precise forms disclosed. Obviously many modifications and variations are possible in view of the above teachings. The examples are shown and described in order to best explain the principles of the invention and its practical applications to thereby enable others skilled in the art to best utilize the invention and various examples with various modifications as are suited to the particular use contemplated. It is intended that the scope of the invention be defined by the following claims and their equivalents 

