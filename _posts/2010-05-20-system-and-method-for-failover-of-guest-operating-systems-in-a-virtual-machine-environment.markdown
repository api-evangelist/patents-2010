---

title: System and method for failover of guest operating systems in a virtual machine environment
abstract: A system and method provides for failover of guest operating systems in a virtual machine environment. During initialization of a computer executing a virtual machine operating system, a first guest operating system allocates a first memory region within a first domain and notifies a second guest operating system operating in a second domain of the allocated first memory region. Similarly, the second guest operating system allocates a second region of memory within the second domain and notifies the first operating system of the allocated second memory region. In the event of a software failure affecting one of the guest operating systems, the surviving guest operating system assumes the identity of the failed operating system and utilizes data stored within the shared memory region to replay to storage devices to render them consistent.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08255735&OS=08255735&RS=08255735
owner: NetApp, Inc.
number: 08255735
owner_city: Sunnyvale
owner_country: US
publication_date: 20100520
---
The present application is a continuation of U.S. patent application Ser. No. 11 742 209 filed on Apr. 30 2007 entitled SYSTEM AND METHOD FOR FAILOVER OF GUEST OPERATING SYSTEMS IN A VIRTUAL MACHINE ENVIRONMENT by Garth Goodson et al now issued as U.S. Pat. No. 7 809 796 on Oct. 5 2010.

The present invention relates to virtual machine systems and more particularly to failover of guest operating systems executing in a virtual machine environment.

A storage system typically comprises one or more storage devices into which information may be entered and from which information may be obtained as desired. The storage system includes a storage operating system that functionally organizes the system by inter alia invoking storage operations in support of a storage service implemented by the system. The storage system may be implemented in accordance with a variety of storage architectures including but not limited to a network attached storage environment a storage area network and a disk assembly directly attached to a client or host computer. The storage devices are typically disk drives organized as a disk array wherein the term disk commonly describes a self contained rotating magnetic media storage device. The term disk in this context is synonymous with hard disk drive HDD or direct access storage device DASD .

The storage operating system of the storage system may implement a high level module such as a file system to logically organize the information stored on volumes as a hierarchical structure of data containers such as files and logical units. For example each on disk file may be implemented as set of data structures i.e. disk blocks configured to store information such as the actual data for the file. These data blocks are organized within a volume block number vbn space that is maintained by the file system. The file system may also assign each data block in the file a corresponding file offset or file block number fbn . The file system typically assigns sequences of fbns on a per file basis whereas vbns are assigned over a larger volume address space. The file system organizes the data blocks within the vbn space as a logical volume each logical volume may be although is not necessarily associated with its own file system.

A known type of file system is a write anywhere file system that does not overwrite data on disks. If a data block is retrieved read from disk into a memory of the storage system and dirtied i.e. updated or modified with new data the data block is thereafter stored written to a new location on disk to optimize write performance. A write anywhere file system may initially assume an optimal layout such that the data is substantially contiguously arranged on disks. The optimal disk layout results in efficient access operations particularly for sequential read operations directed to the disks. An example of a write anywhere file system that is configured to operate on a storage system is the Write Anywhere File Layout WAFL file system available from Network Appliance Inc. Sunnyvale Calif.

The storage system may be further configured to operate according to a client server model of information delivery to thereby allow many clients to access data containers stored on the system. In this model the client may comprise an application such as a database application executing on a computer that connects to the storage system over a computer network such as a point to point link shared local area network LAN wide area network WAN or virtual private network VPN implemented over a public network such as the Internet. Each client may request the services of the storage system by issuing file based and block based protocol messages in the form of packets to the system over the network.

Storage systems may be interconnected as a cluster with the capability that if one of the cluster storage systems fails the surviving storage system assumes the identity and continues to service data access requests directed to the failed storage system. Typically such systems include a cluster interconnect adapted to provide a dedicated hardware monitoring system between the storage systems. An example of such a clustered system is described in U.S. Pat. No. 7 039 828 entitled SYSTEM AND METHOD FOR CLUSTERED FAILOVER WITHOUT NETWORK SUPPORT by John A. Scott issued May 2 2006.

Typical clustered failover systems require access to dedicated cluster interconnect hardware mechanisms for effectuating certain operations such as mirroring of the contents of memory. In such systems when a write operation is received at a first cluster storage system or member the contents of the write operation are both stored in a memory of the first cluster member and transported via a cluster interconnect to a second cluster member for storage in that member s memory. Thus in the event of a failure of one of the members the surviving cluster member may replay operations stored within its memory to bring the disks or other storage devices originally associated with the failed cluster member to a consistent state.

A virtual machine environment illustratively includes a computer such as a storage system executing a virtual machine operating system along with one or more guest operating systems to essentially implement virtual machines on the storage system. Each guest operating system may comprise a conventional operating system such as Microsoft Windows or Linux etc. or may comprise a specialized operating system such as a storage operating system. Furthermore it may be desirable to execute a plurality of guest operating systems or a plurality of instantiations of a guest operating system within a virtual machine environment configured in a clustered arrangement. Thus a cluster may comprise of identical guest operating systems or may comprise of a heterogeneous pairing of guest operating systems e.g. a Microsoft Windows based guest operating system paired with a Linux based guest operating system. Such a clustered configuration typically includes the capability that if one of the guest operating systems instantiations fails the surviving instantiation of the guest operating system assumes the identity of the failed instantiation of the operating system thereby to enable continued processing of requests directed to the failed operating system instantiation . However when operating in a virtual machine environment the virtual machine operating system typically controls the hardware of the storage system thereby preventing use of dedicated cluster interconnect hardware by the guest operating systems. As a result the guest operating systems are unable to effectuate certain operations such as mirroring of the contents of their virtual machine memories. Accordingly in response to a failure of one of the guest operating systems the surviving guest operating system does not have a copy of received write data from the failed guest operating system and thus cannot replay such write requests to render storage devices consistent. As such the utility of the clustered arrangement is reduced.

The present invention overcomes the disadvantages of the prior art by providing a system and method for failover of guest operating systems in a virtual machine environment configured in a clustered arrangement. In accordance with the present invention the guest operating systems establish shared memory regions within domains of the operating systems. In response to an error condition resulting in a failure of a guest operating system the surviving guest operating system may access data stored in the failed guest operating system s shared memory region.

Illustratively during initialization of a computer executing a virtual machine operating system a first guest operating system allocates a first memory region within a first domain and notifies a second guest operating system operating in a second domain of the allocated first memory region. Similarly the second guest operating system executing allocates a second region of memory within the second domain and notifies the first operating system of the allocated second memory region. Notifications may occur via for example interprocess communication or via remote procedure calls RPCs between the two guest operating systems.

The first and second guest operating systems then utilize the allocated memory regions as shared memory regions accessible by each operating system for storage of data such as for example write data received from a client prior to storage on persistent storage devices. In the event of a software failure affecting one of the guest operating systems the surviving guest operating system assumes the identity of the failed guest operating system and utilizes data stored within the shared memory region to replay to the storage devices to render them consistent. The surviving guest operating system may then continue servicing data access requests directed to the failed guest operating system.

In accordance with an embodiment of the present invention a first and second guest operating systems utilize allocated memory regions as shared memory regions accessible by each guest operating system for storage of data such as for example write data received from a client prior to storage on persistent storage devices such as disks. In the event of a software failure affecting one of the guest operating systems the surviving guest operating system assumes the identity of the failed guest operating system and utilizes data stored within the shared memory region to replay to the disks to render them consistent. The surviving guest operating system may then continue servicing data access requests directed to the failed guest operating system.

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data structures associated with the present invention. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures.

The network adapter comprises a plurality of ports adapted to couple the storage system to one or more clients over a network embodied as point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the storage system to the network . Illustratively the computer network may be further embodied as an Ethernet network or a Fibre Channel FC network. Each client may communicate with the storage system over network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The clients may be general purpose computers configured to interact with the system in accordance with a client server model of information delivery. That is each client may request the services of the storage system and the system may return the results of the services requested by the client by exchanging packets over the network . The client may issue packets including file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of files and directories. Alternatively the client may issue packets including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of blocks.

The storage adapter cooperates with the virtual machine operating system executing on the storage system to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on storage devices such as disks . The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC link topology.

Illustratively disposed over the hypervisor module is a plurality of domains for example Domain etc. As used herein a hypervisor is a virtualization platform that permits a plurality of guest operating systems to execute on a computer at the same time. Each domain is representative of a virtual machine within which a guest operating system executes. In the illustrative embodiment of the Xen operating system Domain provides administrator functionality. Domain illustratively executes a Linux based kernel along with one or more administrative modules such as a heartbeat monitor . The heartbeat monitor illustratively monitors the status of various guest operating systems such as storage operating systems A B organized in a cluster configuration. In response to detecting a failure in a guest operating system the heartbeat monitor will alert the surviving guest operating system to ensure that appropriate failover operations occur. The heartbeat monitor may alert the surviving guest operating system using e.g. a remote procedure call to the guest operating system. The heartbeat monitor may monitor the status of the guest operating systems by e.g. monitoring a routine signal sent by each operating system. Should the guest operating system fail to send the signal the heartbeat monitor may assume an error condition has occurred. Domain may also include for example a plurality of software drivers adapted to interface with various hardware components including for example the network adapters storage adapters etc. The drivers illustratively provide an interface for I O operations issued by the guest operating systems.

In the example shown in Domain and Domain each execute a storage operating system arranged in a cluster configuration. In accordance with the principles of the present invention Domain includes a shared memory region A while Domain includes a shared memory region B. Since the memory regions are shared storage operating system A has access to shared memory region B while storage operating system B has access to shared memory region A. Thus in the event of e.g. storage operating system A suffering an error condition surviving guest operating system storage operating system B may access shared memory region A to replay stored write operations to ensure consistency among storage devices associated with the failed guest operating system.

In accordance with an illustrative embodiment of the present invention the guest operating systems are implemented as storage operating systems. However it should be noted that the principles of the present invention may be utilized with other types of guest operating systems organized in a cluster arrangement. As such the description of storage operating systems being utilized as the guest operating systems should be taken as exemplary only. The storage operating system illustratively implements a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by storage devices such as disks. The file system logically organizes the information as a hierarchical structure of named data containers such as directories and files on the disks. Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the file system to further logically organize information as a hierarchical structure of named data containers such as blocks on the disks that are exported as named logical unit numbers luns .

In the illustrative embodiment the storage operating system is preferably the NetApp Data ONTAP operating system available from Network Appliance Inc. Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such where the term Data ONTAP is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.

In addition the storage operating system includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on the disks of the storage system . To that end the storage server includes a file system module in cooperating relation with a RAID system module and a disk driver system module . The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with I O operations while the disk driver system implements a disk access protocol such as e.g. the SCSI protocol.

The storage operating system also includes a failover monitor that implements failover functionality in accordance with embodiments of the present invention. To that end the failover monitor may send routine heartbeat signals to the heartbeat monitor to alert the monitor that the storage i.e. guest operating system is functioning normally. Conversely upon detecting a failure of a guest operating system the heartbeat monitor alerts the failover monitor of the surviving guest operating system of the failure. The failover monitor may then effectuate failover operations including e.g. assumption of the identity of the failed guest operating system etc.

The file system implements a virtualization system of the storage guest operating system through the interaction with one or more virtualization modules illustratively embodied as e.g. a virtual disk vdisk module not shown and a SCSI target module . The vdisk module enables access by administrative interfaces such as a user interface of a management framework see in response to a user system administrator issuing commands to the guest operating system . The SCSI target module is generally disposed between the FC and iSCSI drivers and the file system to provide a translation layer of the virtualization system between the block lun space and the file system space where luns are represented as blocks.

The file system is illustratively a message based system that provides logical volume management capabilities for use in access to the information stored on the storage devices such as disks. That is in addition to providing file system semantics the file system provides functions normally associated with a volume manager. These functions include i aggregation of the disks ii aggregation of storage bandwidth of the disks and iii reliability guarantees such as mirroring and or parity RAID . The file system illustratively implements the WAFL file system hereinafter generally the write anywhere file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . The file system uses files to store meta data describing the layout of its file system these meta data files include among others an inode file. A file handle i.e. an identifier that includes an inode number is used to retrieve an inode from disk.

Broadly stated all inodes of the write anywhere file system are organized into the inode file. A file system fs info block specifies the layout of information in the file system and includes an inode of a file that includes all other inodes of the file system. Each logical volume file system has an fsinfo block that is preferably stored at a fixed location within e.g. a RAID group. The inode of the inode file may directly reference point to data blocks of the inode file or may reference indirect blocks of the inode file that in turn reference data blocks of the inode file. Within each data block of the inode file are embedded inodes each of which may reference indirect blocks that in turn reference data blocks of a file.

Operationally a request from the client is forwarded as a packet over the network and onto storage system where it is received at the network adapter . An appropriate network driver of the virtual machine operating system processes the packet and forwards it to the appropriate guest operating system . A network driver of layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the write anywhere file system . Here the file system generates operations to load retrieve the requested data from disk if it is not resident in core i.e. in memory . If the information is not in memory the file system indexes into the inode file using the inode number to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system the logical vbn is mapped to a disk identifier and disk block number disk dbn and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the request the guest operating system returns a reply to the driver which forwards the reply over the network adapter to the client over the network .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the node may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC .

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a guest operating system implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows NT or as a general purpose is operating system with configurable functionality which is configured for storage applications as described herein. It should be noted that while this description is written in terms of a write any where file system the teachings of the present invention may be utilized with any suitable file system including a write in place file system.

In the illustrative embodiment the storage server is embodied as D module of the guest operating system to service one or more disks . In addition the multi protocol engine is embodied as N module to i perform protocol termination with respect to a client issuing incoming data access request packets over the network as well as ii redirect those data access requests to any storage server of the virtual machine operating system. Moreover the N module and D module cooperate to provide a highly scalable distributed storage system architecture. To that end each module includes a cluster fabric CF interface module adapted to implement communication among the modules including D module to D module communication.

The protocol layers e.g. the NFS CIFS layers and the iSCSI FC layers of the N module function as protocol servers that translate file based and block based data access requests from clients into CF protocol messages used for communication with the D module . That is the N module servers convert the incoming data access requests into file system primitive operations commands that are embedded within CF messages by the CF interface module for transmission to the D module of the cluster . Notably the CF interface modules cooperate to provide a single file system image across all D modules . Thus any network port of an N module that receives a client request can access any data container within the single file system image located on any D module .

Further to the illustrative embodiment the N module and D module are implemented as separately scheduled processes of guest operating system however in an alternate embodiment the modules may be implemented as pieces of code within a single operating system process. Communication between an N module and D module is thus illustratively effected through the use of message passing between the modules. A known message passing mechanism provided by the guest operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF application programming interface API . Examples of such an agnostic protocol are the SpinFS and SpinNP protocols available from Network Appliance Inc. The SpinFS protocol is described in U.S. Pat. No. 6 671 773 entitled METHOD AND SYSTEM FOR RESPONDING TO FILE SYSTEM REQUESTS by Kazar et al. issued on Dec. 30 2003 the contents of which are hereby incorporated by reference.

The CF interface module implements the CF protocol for communicating file system commands among the modules of the virtual machine operating system . Communication is illustratively effected by the D module exposing the CF API to which an N module or another D module issues calls. To that end the CF interface module is organized as a CF encoder and CF decoder. The CF encoder of e.g. CF interface on N module encapsulates a CF message as i a local procedure call LPC when communicating a file system command to a D module residing on the same domain or ii a remote procedure call RPC when communicating the command to a D module residing on a different domain of the virtual machine operating system . In either case the CF decoder of CF interface on D module de encapsulates the CF message and processes the file system command.

The VLDB is a database process that tracks the locations of various storage components e.g. virtual flexible volumes aggregates etc. among various storage operating systems . Flexible volumes and aggregates are further described in U.S. Pat. No. 7 409 494 entitled EXTENSION OF WRITE ANYWHERE FILE SYSTEM LAYOUT by John K. Edwards et al. issued on Aug. 5 2008 the contents of which is are hereby incorporated by reference. Examples of such VLDB entries include a VLDB volume entry and a VLDB aggregate entry .

The VLDB illustratively implements a RPC interface e.g. a Sun RPC interface which allows the N module to query the VLDB . When encountering contents of a data container handle the N module sends an RPC to the VLDB process. In response the VLDB returns to the N module the appropriate mapping information including an ID of the D module that owns the data container. The N module caches the information in its configuration table and uses the D module ID to forward the incoming request to the appropriate data container. All functions and interactions between the N module and D module are coordinated on a virtual machine operating system wide basis through the collection of management processes and the RDB library user mode applications .

To that end the management processes have interfaces to are closely coupled to RDB . The RDB comprises a library that provides a persistent object store storing of objects for the management data processed by the management processes. Notably the RDB replicates and synchronizes the management data object store access across all entities organized either the virtual machine operating system to thereby ensure that the RDB database image is identical on all of the domains. At system startup each guest operating system records the status state of its interfaces and IP addresses those IP addresses it owns into the RDB database.

The present invention provides a system and method for failover of guest operating systems in a virtual machine environment configured in a clustered arrangement. In accordance with the present invention the guest operating systems establish shared memory regions within domains of the operating systems. In response to an error condition resulting in a failure of a guest operating system the surviving guest operating system may access data stored in the failed guest operating system s shared memory region.

Illustratively during initialization of a computer e.g. storage system executing a virtual machine operating system a first guest operating system A allocates a first memory region A within a first domain and notifies a second guest operating system B operating in a second domain of the allocated first memory region. Similarly the second guest operating system allocates a second region of memory B within the second domain and notifies the first operating system of the allocated second memory region. Notifications may occur via for example interprocess communication or via RPC between the two guest operating systems.

The first and second guest operating systems then utilize the allocated memory regions as shared memory regions A B accessible by each guest operating system for storage of data such as for example write data received from a client prior to storage on persistent storage devices such as disks . In the event of a software failure affecting one of the guest operating systems the surviving guest operating system assumes the identity of the failed guest operating system and utilizes data stored within the shared memory region to replay to the disks to render them consistent. The surviving guest operating system may then continue servicing data access requests directed to the failed guest operating system.

Concurrently the second guest operating system B allocates a memory region B in the second domain in step and notifies the first guest operating system of the allocated memory in step . Once the first and second guest operating systems have allocated memory and notified to their partners the guest operating systems begin utilizing the allocated memory regions as shared memory regions . By shared memory region it is meant generally that data that may be required by the other guest operating system is stored within the memory region similar to the use of NVRAM in conventional clustering systems. Thus for example when new write data is received by one of the guest operating systems the write data is stored in the appropriate shared memory region until such time as the data can be persistently stored on disk or other storage devices . By storing the write data within a shared memory region the partner guest operating system may access the data in the event of a software failure of the guest operating system as described further below. The procedure completes in step .

In the illustrative embodiment the partner guest operating system only acquires read access to its partner s shared memory region. However in alternative embodiments the guest operating systems may grant read write access to their partner. As such the granting of read only access to a partner guest operating system should be taken as exemplary only.

In accordance with an alternative embodiment of the present invention a third guest operating system operating in a third domain may allocate a memory region for use by two other guest operating systems. is a flowchart detailing the steps of a procedure for allocating memory regions in accordance with an illustrative embodiment of the present invention. The procedure begins in step and continues to step where the storage system initializes due to e.g. a power cycle. During the initialization a third guest operating system executing within a third domain allocates a memory region within the third domain. The third guest operating system then notifies the first and second guest operating systems of the allocated memory region in step . In response to the notification the first guest operating system maps the allocated memory region from the third domain into the address space of the first domain in step . Similarly in step the second guest operating system maps the allocated memory region from the third domain into the address space of the second domain. Then in step the first and second guest operating systems utilize the allocated memory within the third domain as a shared memory region. The procedure completes in step .

The second guest operating system B then replays the stored write operations from the shared memory region A to render the storage devices consistent in step . Thus a write operation stored within the shared memory region A may be read by the surviving guest operating system B and written to the storage devices such as disks previously controlled by the failed guest operating system A. This replaying of data renders such disks in a consistent state thereby enabling the surviving guest operating system B to continue to service data access requests directed to the failed the guest operating system A. The procedure completes in step . Upon completion of procedure the surviving guest operating system B may continue to serve data access requests directed to the failed guest operating system A.

The foregoing description has been directed to particular embodiments of this invention. It will be apparent however that other variations and modifications may be made to the described embodiments with the attainment of some or all of their advantages. Additionally the procedures processes and or modules described herein may be implemented in hardware software embodied as a computer readable medium having program instructions firmware or a combination thereof. Furthermore it should be noted that while the present invention has been written in terms of two member clusters the principles of the present invention may be utilized in n way clusters. As such the description of two member clusters should be taken as exemplary only. Therefore it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the invention.

