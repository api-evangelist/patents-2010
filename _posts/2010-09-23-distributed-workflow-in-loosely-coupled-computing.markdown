---

title: Distributed workflow in loosely coupled computing
abstract: A method that can be used in a distributed workflow system that uses loosely coupled computation of stateless nodes to bring computation tasks to the compute nodes is disclosed. The method can be employed in a computing system, such as cloud computing system, that can generate a computing task separable into work units and performed by a set of distributed and decentralized workers. In one example, the method arranges the work units into a directed acyclic graph representing execution priorities between the work units. The plurality of distributed and decentralized workers query the directed acyclic graph for work units ready for execution based upon the directed acyclic graph. In one example, the method is included in a computer readable storage medium as a software program.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09262228&OS=09262228&RS=09262228
owner: Microsoft Technology Licensing, LLC
number: 09262228
owner_city: Redmond
owner_country: US
publication_date: 20100923
---
A distributed computer system includes multiple autonomous computers that communicate with each other through a computer network. The computers interact with each other in order to achieve a common goal and a problem is divided into many tasks which are distributed across the nodes of the computer system. Often the distributed computer system is able to concurrently process several computations and to run parallel computer applications on its nodes. Cloud computing is a form of a distributed computer system.

Cloud computing is a model for enabling convenient on demand network access to a shared pool of configurable computing resources e.g. networks servers storage applications and services that can be rapidly provisioned and released with minimal management effort or service provider interaction. Cloud computing is a model for enabling convenient on demand network access to a shared pool of configurable computing resources e.g. networks servers storage applications and services that can be rapidly provisioned and released with minimal management effort or service provider interaction. Details of the cloud are abstracted from the users or clients who can have little if any expertise in or control over the technology infrastructure in the cloud that supports them. Cloud computing describes a supplement consumption and delivery model for IT services based on the Internet and it typically involves over the Internet provision of dynamically scalable and often virtualized resources.

Cloud computing includes the potential to provide capabilities that can be rapidly and elastically provisioned in some cases automatically to scale out and rapidly released to quickly scale in. Work in the cloud is performed with typical workflow engines. Typical workflow engines however can have a limited scaling capability a limited capacity for complexity and a reliance on a small set of hardware thus providing susceptibility to failure.

This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

The present disclosure is directed to a method that can be used in a distributed workflow system that uses loosely coupled computation of stateless nodes to bring computation tasks to the compute nodes. The method can be employed in a computing system such as cloud computing system that can generate a computing task separable into work units and performed by a set of distributed and decentralized workers. In one example the method arranges the work units into a directed acyclic graph representing execution priorities between the work units. The plurality of distributed and decentralized workers query the directed acyclic graph for work units ready for execution based upon the directed acyclic graph. In one example the method is included in a computer readable storage medium as a software program.

In the following Detailed Description reference is made to the accompanying drawings which form a part hereof and in which is shown by way of illustration specific embodiments in which the invention may be practiced. It is to be understood that other embodiments may be utilized and structural or logical changes may be made without departing from the scope of the present invention. The following detailed description therefore is not to be taken in a limiting sense and the scope of the present invention is defined by the appended claims. It is to be understood that features of the various embodiments described herein may be combined with each other unless specifically noted otherwise.

Computing device can also have additional features or functionality. For example computing device may also include additional storage removable and or non removable including but not limited to magnetic or optical disks or solid state memory or flash storage devices such as removable storage and non removable storage . Computer storage media includes volatile and nonvolatile removable and non removable media implemented in any suitable method or technology for storage of information such as computer readable instructions data structures program modules or other data. Memory removable storage and non removable storage are all examples of computer storage media. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile discs DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices universal serial bus USB flash drive flash memory card or other flash storage devices or any other medium that can be used to store the desired information and that can be accessed by computing device . Any such computer storage media may be part of computing device .

Computing device includes one or more communication connections that allow computing device to communicate with other computers applications . An example communication connection can be an Ethernet interface. In some examples the computing device can also have one or more additional processors or specialized processors not shown to perform processing functions offloaded from the processor . Computing device may also include input device s such as keyboard pointing device e.g. mouse pen voice input device touch input device etc. Computing device may also include output device s such as a display speakers printer or the like.

The computing device can be configured to run an operating system software program and one or more software applications which make up a system platform. In one example the computing device includes a software component referred to as a managed or runtime environment. The managed environment can be included as part of the operating system or can be included later as a software download. The managed environment typically includes pre coded solutions to common programming problems to aid software developers to create software programs such as applications to run in the managed environment.

A computer application configured to execute on the computing device includes at least one process or task which is an executing program. Each process provides the resources to execute the program. One or more threads run in the context of the process. A thread is the basic unit to which an operating system allocates time in the processor . The thread is the entity within a process that can be scheduled for execution. Threads of a process can share its virtual address space and system resources. Each thread can include exception handlers a scheduling priority thread local storage a unique thread identifier and a thread context or thread state until the thread is scheduled. A thread context includes the thread s set of machine registers the kernel stack a thread environmental block and a user stack in the in the address space of the process corresponding with the thread. In parallel applications threads can be concurrently executed on the processor .

A multiple core processor can be implemented as the processor in the computing device to concurrently execute threads within the computing device . One example includes multiple cores implemented on a single die. Other examples are contemplated where the processor exists on separate chips or other configurations. In some architectures the processors can exist on separate machines such as in a cloud a cluster a grid or other forms of distributed computing. Further each physical core can capable of efficiently and concurrently executing multiple threads of a concurrent process. Such physical cores are often referred to as Simultaneous Multi Threading or often simply SMT cores and the concurrently executed threads on each physical core share hardware resources included within the single physical core. Each physical core capable of multithreading can present the operating system with as many logical cores as concurrently executing threads it supports. The systems and methods described below however are not limited to a particular architecture.

In one example a cloud architecture includes an infrastructure an application platform sometimes referred to as Platform as a Service or PaaS storage and applications which permits the client to access systems and information with out having to purchase or maintain the underlying software and hardware used to perform the services of the back end . Most cloud computing infrastructures consist of services delivered through common centers and built on servers. The application platform allows applications to be hosted and run at one or more typically remote datacenters. In one example the datacenters can themselves include forms of distributed computing such as computing clusters and storage. The application platform can also provide a cloud operating system that serves as a runtime for the applications and provides a set of services that allows development management and hosting of applications off premises. Services and applications built using the platform or for the platform can run on top of the operating system. An example operating system for the cloud system is available under the trade designation Windows Azure available from Microsoft Inc. of Redmond Wash. Another example of a operating system for other forms of distributed computing is available under the trade designation Windows HPC Server also available from Microsoft Inc.

Generally the operating system can include three components including compute storage and host. Compute provides a computation environment and storage provides scalable storage such as tables queue and so on for large scale needs. The host environment can pool individual systems into a network for managing resources load balancing other services for the applications and the like without using the hosted applications to explicitly perform those functions.

Some distributed computing systems such as cloud computing systems as well as others encourage the use of loosely coupled computation using stateless nodes. Typical workflow engines that are used to manage or orchestrate tasks by responding to change and handling failure retry however are typically implemented as stateful programs executing on a single node or a centralized collection of at least one node but sometimes a few strongly coupled nodes which can be referred to as head node.

Some typical workflow engines are able to support high availability through replication and shared storage but this centralization of workflow control in the head node places upper bounds on how far a workflow can scale how complex the workflow can be and how reliable it can be executed. For example a head node can often provide bottlenecks. Centralized systems often suffer from performance bottlenecks because a single node tracks the state of the distributed system such as the job scheduler and node health monitoring. In addition if the head node fails the system can become headless and may not function. Centralized systems can assume nodes are available for performing work. If a node fails an entire distributed computation may be lost such as message passing interface MPI programs. Cloud computing as well as other forms of distributed are able to accommodate programs that can continue even if nodes crash or disappear. Performance can improve as new nodes come online. Despite these issues centralization of workflow control remains the focus of development in these distributed computing systems and there is much inertia to build centralized work schedulers that can scale and address complex workflow systems. For example reliability is addressed through maintaining a head node structure but introducing redundancy in hardware.

A work unit is a unit of work that can be scheduled for execution on a worker. The work unit can represent a managed assembly object reference with associated parameters or a child process to spawn. The work unit also specifies security information such as an identifier representing the user that creates the work unit machine recommendations and a retry policy in case of failure. A directed acyclic graph representing dependencies between work units is referred to as a work unit context. This graph expresses allowable parallelism between unrelated work units.

A directed acyclic graph often abbreviated as DAG is a directed graph with no directed cycles. That is it is formed by a collection of vertices and directed edges each edge connecting a vertex to another such that there is no way to start at any selected vertex in the graph and follow a sequence of edges that eventually loops back to the selected vertex. The vertices of the directed acyclic graph represent work units in which information and work flows in a consistent direction through the job. Thus the directed acyclic graph represents at least a partial ordering of the work units. In some examples a distributed acyclic graph can have many valid orderings of execution. For example a first vertex can depend on execution of two or more other vertices. The order of the execution of the two or more other vertices can be irrelevant as long as they are completed before the first vertex begins execution.

Coupling refers to the degree of direct knowledge that one class has of another such as one class s knowledge of other class itself. The dependent class s dependency is to a contract specified by the interface a defined list of methods properties or both that implementing classes provide. Classes that implement the interface can satisfy the dependency of a dependent class without having to change the class. A new class implementing an interface can be written to replace a current dependency in at least some situations without a change to the dependent class i.e. the new and old classes can be easily interchanged. For example in a service oriented architecture services comprise unassociated loosely coupled units of functionality that have no calls to each other embedded in them. Each service implements an action such as filling out an online application for an account or viewing an online bank statement or placing an online booking or airline ticket order. Instead of services embedding calls to each other in their source code they use defined protocols that describe how services pass and parse messages using description metadata.

The work balancer application implemented on the system generally includes three stages i.e. submission code scheduling code and querying code . The submission code of the work balancer application is included on at least some of the services . The submission code is configured to apply interfaces such as application programming interfaces when called so the work units and work unit context can be provided to the queue . The scheduling code is largely included and performed at the work queue and it is called to create the directed acyclic graph in a reliable form of storage such as the table referred to above in the description of . The work queue often does not include a first in first out method of handling storage but instead arranges the work units according to the work unit context which can be based on priorities such as execution dependencies or other priorities. The querying code is included and performed on at least some of the distributed workers and it is called to steal work units from the work queue and then to execute the work units.

In one example a work stealing algorithm is performed along with the scheduling code . The work balancer application is configured to allow idle workers to query the work queue for eligible work units to steal an eligible work unit for execution. Because the workers and work queue are loosely coupled to each other typically no other system other than a specific worker such as worker and the work queue is aware of the work unit on the worker . The work unit can include a retry policy that dictates what happens if execution fails or is canceled. When a worker steals a work unit from the work queue scheduling code can flag the work unit as taken in the work queue . The worker executes the work unit until it is executed cancelled or has failed. If the work unit is executed the flag and the work unit can be removed from the work queue . The retry policy can dictate to the worker what to do if the execution has canceled or failed. If after a selected amount of time the worker has not finished executing the work unit the work queue can remove the flag and assume execution has failed such as from a defective worker or has been canceled. The retry policy can also dictate to the work queue what to do if the execution has canceled or failed such as to make the work unit eligible to be stolen or otherwise.

The directed acyclic graph also can be altered as work units within the graph invoke the workflow engine. For example a first work unit of a workflow engine can depend on a second work unit and a second work unit. After the second and third work units are complete the first work unit is stolen and executed. The work flow engine looks at the results generated by the second and third work units and determines whether they are acceptable. If they are acceptable the work flow engine reports success back to the work balancer application and the first work unit completes successful. If the results are unacceptable however the work flow engine can create one or more new work units that perform tasks such as creating a trouble ticket for the system administrator or re scheduling the second and third work units to re run against updated data.

As illustrated the work balancer application does not operate in a centralized or tightly coupled form. Instead the work balancer application is distributed on many devices in the distributed computing environment so if one device fails the work balancer application can continue executing. In one example the work balancer application can be included as part of the operating system or in another example the work balancer can be configured as a layer on top of the operating system and below the services applications.

Although specific embodiments have been illustrated and described herein it will be appreciated by those of ordinary skill in the art that a variety of alternate and or equivalent implementations may be substituted for the specific embodiments shown and described without departing from the scope of the present invention. This application is intended to cover any adaptations or variations of the specific embodiments discussed herein. Therefore it is intended that this invention be limited only by the claims and the equivalents thereof.

