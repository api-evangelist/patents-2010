---

title: Virtual drafting tools
abstract: Techniques for using virtual tools are disclosed. In one aspect, a user interface is presented. A first touch input including touch inputs at two or more locations is received, and a virtual tool corresponding to the relative positions of the two or more locations is identified. A second touch input interacting with the virtual tool is received, and a graphical object corresponding to the identified virtual tool and the second touch input is presented. In another aspect, an input activating a drafting mode of a device is received, and a drafting user interface is presented. A second touch input including touch inputs at two or more locations is received, and a third touch input is received. A graphical object corresponding to the third touch input and a virtual drafting tool corresponding to the second touch input is generated and presented.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08487889&OS=08487889&RS=08487889
owner: Apple Inc.
number: 08487889
owner_city: Cupertino
owner_country: US
publication_date: 20100115
---
Computer aided design CAD programs and other drafting applications allow users to create two dimensional and three dimensional graphical objects on a virtual drafting area. However conventional CAD user interfaces often require users to act in non intuitive ways. For example if a user wants to draw a straight line the user typically moves a cursor from the drafting area to a tool menu or bar selects a tool for drawing a straight line then returns the cursor to the drafting area and specifies the start and end points for the line. These multiple steps are time consuming and can result in inefficient drafting.

Techniques and systems supporting the use of virtual tools in a drafting application are disclosed. These techniques can be used to match user input defining a tool to a virtual tool and process user input using the tool.

In one aspect a user interface is presented on a display of a device. A first touch input including touch inputs at two or more locations is received and a virtual tool corresponding to the relative positions of the two or more locations is identified. A second touch input interacting with the virtual tool is received and a graphical object corresponding to the identified virtual tool and the second touch input is presented in the user interface.

In another aspect an input activating a drafting mode of a device is received and a drafting user interface is presented. A second touch input including touch inputs at two or more locations is received and a third touch input is received. A graphical object corresponding to the third touch input and a virtual drafting tool corresponding to the second touch input is generated and presented in the drafting user interface.

Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. Users can intuitively interact with a drafting application. Users can indicate what drafting tools are desired by the position of their fingers or other input devices without needing to interact with menus or other user interface elements in the drafting application. Users can easily indicate whether graphics objects should be draft objects without needing to interact with menus in the drafting application. Users can easily indicate the appropriate thickness of lines making up graphics objects without needing to interact with menus in the drafting application.

In some implementations device includes touch sensitive display . Touch sensitive display can implement liquid crystal display LCD technology light emitting polymer display LPD technology or some other display technology. Touch sensitive display can be sensitive to haptic and or tactile contact with a user. In some implementations touch sensitive display is also sensitive to touch inputs received in proximity to but not actually touching display . In addition device can include a touch sensitive surface e.g. a trackpad or touchpad .

In some implementations touch sensitive display can include a multi touch sensitive display. A multi touch sensitive display can for example process multiple simultaneous points of input including processing data related to the pressure degree and or position of each point of input. Such processing facilitates gestures and interactions with multiple fingers chording and other interactions. Other touch sensitive display technologies can also be used e.g. a display in which contact is made using a stylus or other pointing device.

A user can interact with device using various touch inputs e.g. when a user touches touch sensitive display . Gesture inputs can also be derived from multiple touch inputs e.g. where a user moves his or her finger or other input tool across touch sensitive display . An example gesture input is a swipe input where a user swipes his or her finger or other input tool across touch sensitive display . In some implementations device can detect inputs that are received in direct contact with display or that are received within a particular vertical distance of display e.g. within one or two inches of display . Users can simultaneously provide input at multiple locations on display . For example inputs simultaneously touching at two or more locations can be received.

In some implementations device can display one or more graphical user interfaces on touch sensitive display for providing the user access to various system objects and for conveying information to the user. In some implementations the graphical user interface can include one or more display objects e.g. display objects and . In the example shown display objects and are graphic representations of system objects. Some examples of system objects include device functions applications windows files alerts events or other identifiable system objects. In some implementations the display objects can be configured by a user e.g. a user may specify which display objects are displayed and or may download additional applications or other software that provides other functionalities and corresponding display objects.

In some implementations device can implement various device functionalities. As part of one or more of these functionalities device presents graphical user interfaces on touch sensitive display of device and also responds to touch input received from a user for example through touch sensitive display . For example a user can invoke various functionality by launching one or more applications on the device. A user can invoke functionality for example by touching one of the display objects in menu bar of the device. A user can alternatively invoke particular functionality in other ways including for example using one of user selectable menus included in the user interface.

Once an application has been selected one or more windows or pages corresponding to the application can be displayed on display of device . A user can navigate through the windows or pages by touching appropriate places on display . For example window corresponds to a drafting or drawing application. When the drafting application is launched on the device the device enters a drafting mode. In the drafting mode the user can use virtual tools to indicate that shapes lines and other graphical objects should be displayed in application view window as will be described in more detail below.

Device enters a drafting mode for example when a user launches a drafting application causing drafting user interface to be displayed on device . In drafting mode device maps touch inputs to stored data defining virtual tools and interactions with of virtual tools. For example the user can provide a touch input to device to indicate a particular virtual tool that the user wants to use. The touch input can include one or more simultaneous touch inputs at different locations. The user can then use a second touch input to indicate that device should create and display a graphical object corresponding to the virtual tool and the second touch input.

Device can continue to detect the first touch input while the second touch input is received. Alternatively the effect of the first touch input can persist for example for a predetermined amount of time after the first touch input ends. Device can associate the second touch input with a virtual tool corresponding to the first touch input when the second input is received within the predetermined amount of time. In some implementations the effect of the first touch input persists or does not persist according to the type of tool. For example some tools can be defined on the device as not persisting while others can be defined as persisting.

When the touch input persists for a predetermined period of time device can differentiate between input adjusting the tool e.g. repositioning orienting or scaling the tool and input using the tool according to the location where the input is received. For example input within the boundaries of the tool can be treated as input modifying the tool while input outside the tool can be treated as input using the tool.

A graphical object can be for example a shape a line or an object e.g. a three dimensional object . A virtual tool can be a virtual representation of a tool that can be used by a user when drafting. Virtual tools can but need not be associated with a visual representation that can be displayed on device .

Virtual tools can correspond to physical tools that are commonly used by drafters. Some examples of virtual tools of this type include a ruler a t square a protractor a compass and various stencils that can be traced. Virtual tools can also correspond to user defined tools. User defined tools are tools defined by a user to have particular properties and need not correspond to physical drafting tools. In some implementations selectable representations of tools for example user defined tools are displayed in the drafting user interface to allow a user to edit data for the tool.

While the examples described below describe using a single tool at a time in some implementations the device allows a user to interact with multiple tools simultaneously.

In the example illustrated in the user simultaneously touches the user interface in two places and . Device compares the relative positions of the input locations e.g. along x and y axes to stored data for virtual tools and selects a virtual ruler as best matching user touch inputs and .

A device can identify a ruler tool as the tool best matching user touch inputs when the user simultaneously provides touch inputs at two locations on the drafting user interface. For example if the x and y axes of the two touch inputs are within a threshold distance of each other e.g. parallel or slightly off parallel the ruler tool can be identified. Alternatively the ruler tool can be identified as running directly between the two touch inputs specified by a user e.g. to allow the user to use the ruler at a desired angle .

For illustrative purposes the right edge of tool is shown as immediately adjacent to user touch inputs and . However in some implementations the overall distances of the edges of tool from the user touch inputs can be set according to user preferences. In addition the user can specify whether the left edge or the right edge of tool is closer to touch inputs and for example to accommodate both right handed and left handed users.

In some implementations other aspects of the visual appearance of tool can also be determined according to user preferences. For example the user can specify one or more of the preferred units e.g. metric units English units or both displayed on tool the color of tool the transparency of tool the width of tool and the material of tool wood metal plastic etc. . In some implementations the user can use a particular touch input to indicate that one or more of the details of the visual appearance of tool should be changed. For example the user can tap the fingers used to input the touch input at locations and to indicate that a different unit set should be displayed on tool . For example for each tap the units can alternate between English and metric in a cyclic manner with a corresponding change in tic spacing shown in the visual representation of the virtual ruler.

In some implementations the user can use pinching gestures to adjust the overall length of tool by moving his or her fingers further apart resulting in a longer ruler or closer together resulting in a shorter ruler on the drafting user interface. In some implementations tool can be detached or floating on the drafting use interface to allow the user to use one or more fingers to drag or rotate tool in the drafting user interface.

In some implementations the distance between the user s fingers and the drafting user interface e.g. in the z direction can govern visual details of the resulting graphical object displayed in the drafting user interface. For example in some implementations the thickness of the line s of the graphical object can be determined by the distance of user fingers the further away the fingers the lighter the line s . The correlation between thickness of line and distance can be continuous or can be discrete e.g. according to discrete levels. As another example the line s are draft line s if the user s fingers are not touching the drafting user interface and are final lines if the user s fingers are touching the drafting user interface. Draft lines can differ in appearance from final lines. For example draft lines can be lighter or dashed lines while final lines can be solid or heavier lines. Draft lines can also differ in function. For example draft lines can extend beyond their intersection with other lines in the user interface. Users can then manually snap other lines being drawn to intersections with the draft lines. A user can also provide input to selectively hide and or delete unused draft lines once a drawing is complete. In some implementations a user can cause a draft line to become a final line for example by using a gesture input to trace the draft line or selecting the draft line or a portion of the draft line and providing input indicating that the draft line should be made final.

In other implementations the pressure of the fingers on the drafting user interface determines the thickness of the lines. Light pressure results in light lines or dashed lines and hard pressure results in thick or solid lines.

Device determines the appropriate angle indicated by x and y positions of touch input relative to the base of the virtual protractor and draws an angle corresponding to the touch inputs and . illustrates graphical object resulting from the touch inputs and of .

The user can also interact with the protractor in other ways for example tracing part of a curve along either the inside or outside edge of the protractor.

Other definitions of the virtual compass can also be used. For example a user can provide simultaneous touch input at locations and and then provide a gesture input e.g. rotating down and to the left indicating that a virtual compass is desired.

The stencil illustrated in is just one possible virtual stencil. Other virtual stencils corresponding to other shapes for example squares rectangles pentagons and other types of triangles can also be used by device .

Users can alternatively use other touch input to indicate that a stencil tool is desired. For example a user can use gesture input to draw the shape of the stencil he or she is interested in and device can identify a stencil best matching the drawn shape.

In various implementations the drafting user interface can additionally include one or more features present in conventional drafting user interfaces. For example the drafting user interface can include options to allow users to select color or shading of their objects and can provide representations of pre defined objects that a user can select and drag into a drawing. A pre defined object can be a graphical object the user previously created and saved. These pre defined objects are then generated and presented alongside objects the user created using virtual tools.

The system presents a user interface . The user interface can be for example a drafting user interface. Example drafting user interfaces are described above with reference to .

The system receives first touch input including touch inputs at two or more locations . The first touch input can be one or more touch input events received from a touch services module executing on the device. The touch services module is described in more detail below with reference to .

The first touch input can be a single touch input event corresponding to touch inputs received at multiple locations on the drafting user interface e.g. simultaneously. Alternatively the first touch input can be made up of multiple touch input events. The system can determine that the multiple touch input events correspond to a single touch input for example according to the time delay between the touch inputs or whether one touch input event still persists when another is touch input event is received.

The system identifies a virtual tool corresponding to the relative positions of the two or more touch input locations . The system can make this identification by comparing the relative positions of the locations of the first touch input to stored data defining virtual tools. The system can make this determination for example using conventional comparison techniques. For example in some implementations the system can compare one or more of the number of points the distance made between the points and the positions of the points.

The data can include data for pre defined virtual tools e.g. tools defined by the system and can alternatively or additionally include data for user defined virtual tools. The data for a virtual tool can include for example the number of locations of touch input and the relative positions of the locations. The relative positions of the locations can be represented for example by the relative angles between the positions of input or the absolute angles between the positions of input and an origin point. The relative positions can also be represented by relationships between the distance between the locations. For example for a square stencil tool the distance between any two points that are parallel to an x or y axis is equal to the distance between any other two points that are parallel to an x or y axis. Other relationships for example based on a scaling factor that governs the difference in distance between pairs of points in a two dimensional coordinate frame can also be used.

In some implementations if the system identifies multiple virtual tools that correspond to the user s input the system can ask the user to select between the identified tools.

Once the appropriate virtual tool is identified the system can also determine an appropriate size for the tool. The size of the tool can be determined from the distance between the touch input locations used to identify the tool and or the size of the drafting user interface. For example the length of a ruler virtual tool can be equal to the distance between the two locations of touch input used to request the tool.

The system optionally displays a visual representation of the identified virtual tool . The visual representation can be defined by data stored by the system. For example the data can specify that the visual representation of a virtual ruler tool is an image of a ruler e.g. a rectangle with specific proportions that has tics marked on it . The visual appearance of the visual representation can be governed by one or more user preferences for example as described above with reference to .

The system receives a second touch input interacting with the virtual tool for example from a touch services module executing on the device.

The system presents a graphical object corresponding to the identified virtual tool and second touch input for example as illustrated in .

The system can generate the graphical object according to the identified virtual tool and the touch input specified in the second touch input event. Each virtual tool can be associated with data defining the functionality of the tool. This data can specify how touch input should be processed to generate the appropriate virtual object. The system can retrieve this stored data and processes the touch input according to the functionality of the virtual tool. The functionality can be represented for example as one or more instruction sets that are executed by the system.

For example the functionality data for a ruler can include one or more program instructions that cause the system to generate a straight line that is parallel to the ruler and immediately adjacent to the edge of the ruler on which the second touch input was received. The instructions can further specify that the start and end points of the line are defined by the start and end points of for example a dragging gesture initiating with the second touch input.

In some implementations the system can also determine an appropriate thickness of the line s of the graphical object and or whether the line s are draft lines or final lines. This determination can be made based on the distance between the user s fingers and the drafting user interface for example as described above with reference to .

In some implementations the system further receives touch input corresponding to a modification of the first touch input and modifies the presented graphical object in response to the new input for example as illustrated in . The system can modify the presented graphical object by generating a new graphical object according to the modified user touch input and then replacing the previous graphical object with the new graphical object.

The system receives a first input activating a drafting mode of a device . For example the first input can be input indicating that a user has requested execution of a drafting application on the device. Other inputs can also be received for example a user can select a drafting option within a drafting application.

The system presents a drafting user interface in response to the first input for example as described above with reference to .

The system receives a second touch input including inputs at two or more locations and receives a third touch input. The system generates a graphical object corresponding to the third touch input event and a virtual drafting tool corresponding to a relationship between the two or more locations of the second touch input . The relationship between the two or more locations can be a physical relationship for example the relative locations of the inputs as described above with reference to . The system presents the graphical object in the drafting user interface for example as described above with reference to .

Software architecture can include operating system touch services module and drafting application . This architecture can conceptually operate on top of a hardware layer not shown .

Operating system provides an interface to the hardware layer e.g. a capacitive touch display or device . Operating system can include one or more software drivers that communicates with the hardware. For example the drivers can receive and process touch input signals generated by a touch sensitive display or device in the hardware layer. The operating system can process raw input data received from the driver s . This processed input data can then made available to touch services layer through one or more application programming interfaces APIs . These APIs can be a set of APIs that are usually included with operating systems such as for example Linux or UNIX APIs as well as APIs specific for sending and receiving data relevant to touch input.

Touch services module can receive touch inputs from operating system layer and convert one or more of these touch inputs into touch input events according to an internal touch event model. Touch services module can use different touch models for different applications. For example a drafting application will be interested in events that correspond to input requesting a virtual tool and input interacting with a virtual tool and the touch model can be adjusted or selected accordingly to reflect the expected inputs.

The touch input events can be in a format e.g. attributes that are easier to use in an application than raw touch input signals generated by the touch sensitive device. For example a touch input event can include a set of coordinates for each location at which a touch is currently occurring on a drafting user interface. Each touch input event can include information on one or more touches occurring simultaneously.

In some implementations gesture touch input events can also be detected by combining two or more touch input events. The gesture touch input events can contain scale and or rotation information. The rotation information can include a rotation value that is a relative delta in degrees. The scale information can also include a scaling value that is a relative delta in pixels on the display device. Other gesture events are possible.

All or some of these touch input events can be made available to developers through a touch input event API. The touch input API can be made available to developers as a Software Development Kit SDK or as part of an application e.g. as part of a browser tool kit .

Drafting application can be a drafting application executing on a device. Drafting application can include virtual tool definition manager input manager virtual tool selection engine graphical object generator and user preference engine . These components can be communicatively coupled to one or more of each other. Though the components identified above are described as being separate or distinct two or more of the components may be combined in a single process or routine. The functional description provided herein including separation of responsibility for distinct functions is by way of example. Other groupings or other divisions of functional responsibilities can be made as necessary or in accordance with design preferences.

Virtual tool definition manager manages the data for each defined virtual tool. The data can be represented for example as a data object data structure or according to other data representation conventions. The data can include the relative location data functionality data and visual representation data for each tool. This data is described in more detail above with reference to . The data can also include other data for other attributes of the virtual tool.

Input manager receives touch input events from touch services layer for example through a function call having a defined call convention based on the touch API. Input manager can pull data from touch services layer e.g. by querying touch services layer for data or can receive data that is pushed from touch services layer e.g. as the touch input events are generated by the touch services layer . Input manager processes these events as they are received and provides the event data to virtual tool selection engine or graphical object generator as appropriate.

Virtual tool selection engine receives touch input events from input manager that correspond to a user s request for a virtual tool. Virtual tool selection engine compares the relative positions of input specified in the touch input event to the virtual tool data maintained by virtual tool definition manager and selects the best matching tool for example as described above with reference to .

Graphical object generator receives an identified virtual tool and a touch input event corresponding to an interaction with the virtual tool and generates an appropriate graphical object for display for example as described above with reference to . The actual drawing of the graphical object can also be a service accessible through a function call defined by a graphics processing service API.

User preference engine manages one or more preferences input by a user for example preferences specifying attributes of the visual appearance of virtual tools.

Sensors devices and subsystems can be coupled to peripherals interface to facilitate multiple functionalities. For example motion sensor light sensor and proximity sensor can be coupled to peripherals interface to facilitate various orientation lighting and proximity functions. For example in some implementations light sensor can be utilized to facilitate adjusting the brightness of touch screen . In some implementations motion sensor can be utilized to detect movement of the device. Accordingly display objects and or media can be presented according to a detected orientation e.g. portrait or landscape.

Other sensors can also be connected to peripherals interface such as a temperature sensor a biometric sensor a gyroscope or other sensing device to facilitate related functionalities.

For example device can receive positioning information from positioning system . Positioning system in various implementations can be a component internal to device or can be an external component coupled to device e.g. using a wired connection or a wireless connection . In some implementations positioning system can include a GPS receiver and a positioning engine operable to derive positioning information from received GPS satellite signals. In other implementations positioning system can include a compass e.g. a magnetic compass and an accelerometer as well as a positioning engine operable to derive positioning information based on dead reckoning techniques. In still further implementations positioning system can use wireless signals e.g. cellular signals IEEE 802.11 signals to determine location information associated with the device such as those provided by SKYHOOK WIRELESS of Boston Mass. Hybrid positioning systems using a combination of satellite and television signals such as those provided by ROSUM CORPORATION of Mountain View Calif. can also be used. Other positioning systems are possible.

Broadcast reception functions can be facilitated through one or more radio frequency RF receiver s . An RF receiver can receive for example AM FM broadcasts or satellite broadcasts e.g. XM or Sirius radio broadcast . An RF receiver can also be a TV tuner. In some implementations RF receiver is built into wireless communication subsystems . In other implementations RF receiver is an independent subsystem coupled to device e.g. using a wired connection or a wireless connection . RF receiver can receive simulcasts. In some implementations RF receiver can include a Radio Data System RDS processor which can process broadcast content and simulcast data e.g. RDS data . In some implementations RF receiver can be digitally tuned to receive broadcasts at various frequencies. In addition RF receiver can include a scanning function which tunes up or down and pauses at a next frequency where broadcast content is available.

Camera subsystem and optical sensor e.g. a charged coupled device CCD or a complementary metal oxide semiconductor CMOS optical sensor can be utilized to facilitate camera functions such as recording photographs and video clips.

Communication functions can be facilitated through one or more communication subsystems . Communication subsystem s can include one or more wireless communication subsystems and one or more wired communication subsystems. Wireless communication subsystems can include radio frequency receivers and transmitters and or optical e.g. infrared receivers and transmitters. Wired communication system can include a port device e.g. a Universal Serial Bus USB port or some other wired port connection that can be used to establish a wired connection to other computing devices such as other communication devices network access devices a personal computer a printer a display screen or other processing devices capable of receiving and or transmitting data. The specific design and implementation of communication subsystem can depend on the communication network s or medium s over which device is intended to operate. For example device may include wireless communication subsystems designed to operate over a global system for mobile communications GSM network a GPRS network an enhanced data GSM environment EDGE network 802.x communication networks e.g. Wi Fi WiMax or 3G networks code division multiple access CDMA networks and a Bluetooth network. Communication subsystems may include hosting protocols such that device may be configured as a base station for other wireless devices. As another example the communication subsystems can allow the device to synchronize with a host device using one or more protocols such as for example the TCP IP protocol HTTP protocol UDP protocol and any other known protocol.

Audio subsystem can be coupled to speaker and one or more microphones . One or more microphones can be used for example to facilitate voice enabled functions such as voice recognition voice replication digital recording and telephony functions.

I O subsystem can include touch screen controller and or other input controller s . Touch screen controller can be coupled to touch screen . Touch screen and touch screen controller can for example detect contact and movement or break thereof using any of a number of touch sensitivity technologies including but not limited to capacitive resistive infrared and surface acoustic wave technologies as well as other proximity sensor arrays or other elements for determining one or more points of contact with touch screen or proximity to touch screen .

Other input controller s can be coupled to other input control devices such as one or more buttons rocker switches thumb wheel infrared port USB port and or a pointer device such as a stylus. The one or more buttons not shown can include an up down button for volume control of speaker and or microphone .

In one implementation a pressing of the button for a first duration may disengage a lock of touch screen and a pressing of the button for a second duration that is longer than the first duration may turn power to device on or off. The user may be able to customize a functionality of one or more of the buttons. Touch screen can for example also be used to implement virtual or soft buttons and or a keyboard.

In some implementations device can present recorded audio and or video files such as MP3 AAC and MPEG files. In some implementations device can include the functionality of an MP3 player such as an iPhone .

Memory interface can be coupled to memory . Memory can include high speed random access memory and or non volatile memory such as one or more magnetic disk storage devices one or more optical storage devices and or flash memory e.g. NAND NOR . Memory can store operating system such as Darwin RTXC LINUX UNIX OS X WINDOWS or an embedded operating system such as VxWorks. Operating system may include instructions for handling basic system services and for performing hardware dependent tasks. In some implementations operating system can be a kernel e.g. UNIX kernel .

Memory may also store communication instructions to facilitate communicating with one or more additional devices one or more computers and or one or more servers. Communication instructions can also be used to select an operational mode or communication medium for use by the device based on a geographic location obtained by the GPS Navigation instructions of the device. Memory may include graphical user interface instructions to facilitate graphic user interface processing sensor processing instructions to facilitate sensor related processing and functions e.g. the touch services layer described above with reference to phone instructions to facilitate phone related processes and functions electronic messaging instructions to facilitate electronic messaging related processes and functions web browsing instructions to facilitate web browsing related processes and functions media processing instructions to facilitate media processing related processes and functions GPS Navigation instructions to facilitate GPS and navigation related processes and instructions e.g. mapping a target location camera instructions to facilitate camera related processes and functions and or other software instructions to facilitate other processes and functions e.g. drafting functions. Memory may also store other software instructions not shown such as web video instructions to facilitate web video related processes and functions and or web shopping instructions to facilitate web shopping related processes and functions. In some implementations media processing instructions are divided into audio processing instructions and video processing instructions to facilitate audio processing related processes and functions and video processing related processes and functions respectively.

Each of the above identified instructions and applications can correspond to a set of instructions for performing one or more functions described above. These instructions need not be implemented as separate software programs procedures or modules. Memory can include additional instructions or fewer instructions. Furthermore various functions of device may be implemented in hardware and or in software including in one or more signal processing and or application specific integrated circuits.

Devices and can also establish communications by other means. For example wireless device can communicate with other wireless devices e.g. other devices or cell phones etc. over wireless network . Likewise devices and can establish peer to peer communications e.g. a personal area network by use of one or more communication subsystems such as a Bluetooth communication device. Other communication protocols and topologies can also be implemented.

Devices or can for example communicate with one or more services over one or more wired and or wireless networks . These services can include for example virtual tool services and drafting services . Virtual tool services provide virtual tool definitions and data to a user device and may also generate graphical objects corresponding to user input for example as described above. Drafting service provides drafting application functionality to a user for example as described above.

Device or can also access other data and content over one or more wired and or wireless networks . For example content publishers such as news sites RSS feeds web sites blogs social networking sites developer networks etc. can be accessed by device or . Such access can be provided by invocation of a web browsing function or application e.g. a browser in response to a user touching for example a Web object.

The features described can be implemented in digital electronic circuitry or in computer hardware firmware software or in combinations of them. The features can be implemented in a computer program product tangibly embodied in an information carrier e.g. in a machine readable storage device for execution by a programmable processor and method steps can be performed by a programmable processor executing a program of instructions to perform functions of the described implementations by operating on input data and generating output. Alternatively or in addition the program instructions can be encoded on a propagated signal that is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a programmable processor.

The described features can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from and to transmit data and instructions to a data storage system at least one input device and at least one output device. A computer program is a set of instructions that can be used directly or indirectly in a computer to perform a certain activity or bring about a certain result. A computer program can be written in any form of programming language e.g. Objective C Java including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment.

Suitable processors for the execution of a program of instructions include by way of example both general and special purpose microprocessors and the sole processor or one of multiple processors or cores of any kind of computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Generally a computer will also include or be operatively coupled to communicate with one or more mass storage devices for storing data files such devices include magnetic disks such as internal hard disks and removable disks magneto optical disks and optical disks. Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non volatile memory including by way of example semiconductor memory devices such as EPROM EEPROM and flash memory devices magnetic disks such as internal hard disks and removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in ASICs application specific integrated circuits .

To provide for interaction with a user the features can be implemented on a computer having a display device such as a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer.

The features can be implemented in a computer system that includes a back end component such as a data server or that includes a middleware component such as an application server or an Internet server or that includes a front end component such as a client computer having a graphical user interface or an Internet browser or any combination of them. The components of the system can be connected by any form or medium of digital data communication such as a communication network. Examples of communication networks include e.g. a LAN a WAN and the computers and networks forming the Internet.

The computer system can include clients and servers. A client and server are generally remote from each other and typically interact through a network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

One or more features or steps of the disclosed embodiments can be implemented using an Application Programming Interface API . An API can define on or more parameters that are passed between a calling application and other software code e.g. an operating system library routine function that provides a service that provides data or that performs an operation or a computation.

The API can be implemented as one or more calls in program code that send or receive one or more parameters through a parameter list or other structure based on a call convention defined in an API specification document. A parameter can be a constant a key a data structure an object an object class a variable a data type a pointer an array a list or another call. API calls and parameters can be implemented in any programming language. The programming language can define the vocabulary and calling convention that a programmer will employ to access functions supporting the API.

In some implementations an API call can report to an application the capabilities of a device running the application such as input capability output capability processing capability power capability communications capability etc.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be made. For example elements of one or more implementations may be combined deleted modified or supplemented to form further implementations. As yet another example the logic flows depicted in the figures do not require the particular order shown or sequential order to achieve desirable results. In addition other steps may be provided or steps may be eliminated from the described flows and other components may be added to or removed from the described systems. Accordingly other implementations are within the scope of the following claims.

