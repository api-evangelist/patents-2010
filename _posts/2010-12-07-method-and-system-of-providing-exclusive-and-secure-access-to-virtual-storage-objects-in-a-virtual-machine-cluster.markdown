---

title: Method and system of providing exclusive and secure access to virtual storage objects in a virtual machine cluster
abstract: A system, method, and medium for implementing I/O fencing in a virtual machine cluster sharing virtual storage objects. A volume manager driver receives access requests from virtual machines directed to a virtual storage object such as a volume. The volume manager driver then translates the access request to point to a storage device underlying the volume. The access request includes keys and/or other group reservation data required to implement an I/O fencing method so as to prevent access to shared data by malfunctioning or non-responsive virtual machines.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08495323&OS=08495323&RS=08495323
owner: Symantec Corporation
number: 08495323
owner_city: Mountain View CA
owner_country: unknown
publication_date: 20101207
---
The present invention relates generally to virtual machine clusters and in particular to a method and system for sharing virtual storage objects among applications in a virtual machine cluster.

Virtual machines allow organizations to make efficient use of their available computing resources. Virtual machines are often grouped into distributed clusters to maintain high availability HA and flexibility. One of the goals of a HA system is to minimize the impact of the failure of individual components on system availability. An example of such a failure is a loss of communications between some of the virtual machine nodes of a distributed cluster. A way to prevent data corruption following a failure of one or more nodes is to implement Input Output I O fencing.

I O fencing is the process of isolating shared storage devices from nodes that are no longer operating as a part of the cluster to protect the data on the shared storage devices from becoming corrupted. The cluster isolates a node when it is malfunctioning to ensure that I O operations can no longer be performed by that isolated note on the shared storage devices. When multiple nodes have access to data on shared storage devices the integrity of the data depends on the nodes communicating with each other such that each is aware when the other accesses data on the shared storage devices. This communication occurs through connections between the nodes. If the connections between nodes are lost or if one of the nodes is hung malfunctions or fails each node could be unaware of the other s activities with respect to the data on the shared storage device. This condition is known as split brain and can lead to data corruption. To prevent the split brain condition I O fencing can be utilized to isolate the non cooperating node and control its ability to access the shared storage device. I O fencing allows the integrity of the data to be maintained.

One method used for implementing I O fencing of physical storage devices is based on the small computer system interface version three persistent group reservation SCSI 3 PGR standard. The SCSI 3 PGR standard is described in further detail in SCSI 3 Primary Commands published by the American National Standards Institute Inc. the contents of which are hereby incorporated by reference. SCSI 3 PGR based mechanisms can be used to provide I O fencing capabilities for shared storage devices. In SCSI 3 PGR based fencing a persistent reservation is placed on a shared storage device. This reservation grants access to a specified set of nodes while at the same time denying access to other nodes.

SCSI 3 PGR allows a node to make a physical storage device registration that is persistent across power failures and bus resets. Also group reservations are permitted allowing all nodes within a single group to have concurrent access to the physical storage device while restricting access to nodes not in the group. The SCSI 3 PGR standard is based on the storage reading and preemption of reservation keys on a reserved area or private region of a physical storage device. To comply with the standard each node stores certain node specific information on a portion of the physical storage device. Also group reservation information from a group of nodes may also be stored in a portion of the physical storage device. This information may then be used to determine which nodes may access the storage device.

For a node to be registered the node s registration key may be written in the node s area on the reserved portion of the shared physical device. A group reservation for all registered nodes may also be placed in a separate reserved portion of the shared physical device. In some cases the reservation key of one node may be preempted by other nodes. The SCSI 3 PGR standard allow for preemption to ensure that only one group of nodes has access to a shared storage device in the case of a split brain scenario.

The SCSI 3 PGR standard is based on a physical hardware implementation and does not apply to virtual storage devices. Other I O fencing standards are also limited to physical storage devices. Organizations use virtual storage devices to make their storage infrastructure more manageable and flexible. Relationships are established between physical storage devices e.g. disk drives tape drives and virtual storage devices e.g. volumes virtual disks virtual logical units . Using virtual storage devices provides system wide features e.g. naming sizing and management better suited to the entire virtual machine network than those features dictated by the physical characteristics of the actual storage devices.

Therefore what is needed in the art is a method and system for implementing I O fencing for virtual storage objects. It would be advantageous to implement a method that would allow virtual machines to use industry standard I O fencing methodology to access virtual storage devices. This would allow virtual machines to use standard I O fencing application programming interface API calls and would not require making any significant changes to the software implemented on the virtual machines or to the underlying storage device hardware.

In view of the above improved methods and mechanisms for implementing I O fencing for virtual machine clusters and virtual storage objects are desired.

Various embodiments of methods and mechanisms for sharing virtual storage objects in a virtual machine cluster are contemplated. In one embodiment a volume manager driver may reserve a first private region in a virtual storage object such as a volume. The volume may be logically mapped to a set of one or more storage devices. The first private region may be used to store metadata required for I O fencing. The metadata may include keys group reservation data and other data. One or more virtual machines of a virtual machine cluster may store metadata in the private region. The first private region may be partitioned into separate regions for different virtual machines. The first private region may also be partitioned into separate regions for storing keys and for storing group reservation data. Other methods of partitioning the first private region are possible and are contemplated.

The volume manager driver may also reserve a second private region in a storage device. In one embodiment the second private region may reside on a storage device of the set mapped from the volume. In another embodiment the second private region may reside on a storage device separate from the set mapped from the volume. The volume manager driver may map the first private region in the volume to the second private region in the storage device. Then a virtual machine or an application running on the virtual machine may send an access request to the volume manager driver. The application may need access to data stored on the volume. The access request may be addressed to the first private region on the volume.

The volume manager driver may translate the access request so that the request addresses the second private region. Keys group reservation data and or other data may be sent as part of the access request. Then the access may be accepted or rejecting according to the metadata stored in the second private region and the I O fencing methodology being followed. If the virtual machine requesting access has a key stored in the second private region then the access may be granted. The access request may be a preemption request the preemption request if granted may result in the removal of keys from one or more virtual machines from the second private region. The access request may be handled according to an I O fencing standard such as the SCSI 3 PGR standard. The access may also be handled according to a different I O fencing standard or protocol. The preceding steps may be repeated for one or more additional volumes.

These and other features and advantages will become apparent to those of ordinary skill in the art in view of the following detailed descriptions of the approaches presented herein.

In the following description numerous specific details are set forth to provide a thorough understanding of the methods and mechanisms presented herein. However one having ordinary skill in the art should recognize that the various embodiments may be practiced without these specific details. In some instances well known structures components signals computer program instructions and techniques have not been shown in detail to avoid obscuring the approaches described herein. It will be appreciated that for simplicity and clarity of illustration elements shown in the figures have not necessarily been drawn to scale. For example the dimensions of some of the elements may be exaggerated relative to other elements.

Referring to a generalized block diagram of one embodiment of a virtual machine cluster architecture is shown. Generally speaking a cluster such as cluster is a group of linked nodes. The nodes such as master node and node are typically connected to one another through fast local area networks LANs which are not shown to simplify the illustration. Cluster is representative of any number of virtual machine clusters which may be connected to applications server . Master node and node are representative of any number of nodes which may be part of cluster . Each node may be a single computer or a multi processor system.

Virtual machine runs on master node and virtual machine is representative of any number of virtual machines which may execute on master node . Master node may send control messages to other nodes of the cluster i.e. node and master node may make sure all of the nodes of the cluster are synchronized and have a consistent view of the cluster s data stored in shared storage devices. Virtual machine runs on node and virtual machine is representative of any number of virtual machines which may execute on node . Virtual machines and may each execute one or more software applications.

Applications server may be connected to master node and node through any of a variety of direct or network connections. Applications server may host one or more software applications associated with virtual machines and including hypervisor . Hypervisor may be a virtualization layer or module configured to mask low level hardware operations from one or more guest operating systems executing on virtual machines and . Hypervisor may allow multiple operating systems to execute on a single server i.e. applications server . Other software applications hosted by applications server may include volume manager and volume manager driver . Alternatively volume manager driver may be running on a node such as master node . Applications server is representative of any number of applications servers or other types of servers which may be connected to network . In other embodiments applications server may be a media server master server host server file server data server and or other type of server.

Applications server is connected to network . Network may comprise a variety of network connections including combinations of local area networks LANs such as Ethernet networks Fibre Channel FC networks token ring networks and wireless local area networks WLANs based on the Institute of Electrical and Electronics Engineers IEEE 802.11 standards Wi Fi and wide area networks WANs such as the Internet cellular data networks and other data communication networks such as a virtual private network VPN implemented over a public network e.g. the Internet . Other network connections and architectures are possible and contemplated.

Storage devices and are representative of any number of backup storage devices and may comprise any of a variety of types of storage media such as a hard disk drive disk volume server blade optical drive flash drive tape drive tape volume robotic tape library or other storage medium. In some embodiments storage devices and may be SCSI 3 PGR compliant storage devices. Storage devices and may be referred to as a storage cluster.

In one embodiment storage devices and may be accessible to applications server and virtual machines and over network using an internet small computer system interface iSCSI compliant protocol. The iSCSI standard facilitates data transfers over many types of networks by carrying SCSI commands over the networks. The protocol allows virtual machines and to send SCSI commands to remote SCSI storage devices as if the remote SCSI storage devices were directly connected to virtual machines and . The iSCSI standard provides virtual machines with the illusion of locally attached storage devices.

Volume manager may run on applications server and volume manager may enable physical storage devices attached to the virtual machine cluster network to be managed as logical devices. Specifically volume manager may create and manage one or more volumes mapped from storage devices and and present the volumes to virtual machines and as virtual storage objects. Volume manager may abstract the actual physical interface and underlying physical devices from virtual machines and and applications server .

Volume manager driver may also run on applications server and virtual machines and may access storage devices and through volume manager driver . Volume manager driver may translate addresses received from virtual machines and to point to the actual storage device underneath the volume. Virtual machines and may reference or access portions of volume address space and volume manager driver may translate the address space of a volume into the address space of the underlying storage devices.

Volume manager driver may also implement I O fencing to manage and restrict access to the volumes associated with application server and virtual machines and . One common technique of fencing utilizes SCSI 3 PGR based mechanisms for restricting access to shared storage devices. Although the SCSI 3 PGR standard is only one example of an I O fencing standard the term SCSI 3 PGR may be used interchangeably with I O fencing method throughout this specification. Other I O fencing methods include SAN fabric fencing Shoot the other node in the head STONITH and reserve release.

Volume manager driver may allow fine grain access to the volumes mapped to storage devices and . One virtual machine may have write access to a volume a second virtual machine may have read access to the volume and a third virtual machine may have no access to the volume. Volume manager driver may create a private region in the volume for the storage of keys placeholders group reservation data and or other metadata associated with the virtual machines. Volume manager driver may also create a private region in storage device or storage device . Alternatively volume manager driver may create a private region in a storage object separate from storage devices and . The private regions may be partitioned into separate regions for different virtual machines. The private regions may also be partitioned into separate regions for storing keys and for storing group reservation data. Other methods of partitioning the private regions are possible and are contemplated.

Volume manager driver may map the private region of the volume to the private region in the actual physical storage device. When volume manager driver receives an access request from a virtual machine addressed to the private region of the volume volume manager driver may translate the access request to address the private region in the physical storage device. The above described steps may be repeated for multiple volumes.

The access request may include a key from the virtual machine making the request. The key may be a 64 bit key as defined by the SCSI 3 PGR standard. Alternatively other sizes of keys may be used. Also the access request may include group reservation data corresponding to the group to which the virtual machine making the request belongs. The private region of the physical storage device may contain a set of keys and or group reservation data for all of the virtual machines that have access to the storage device.

Volume manager driver may store one or more extra sets of keys on different storage devices. The extra sets of keys may allow for keys to be recovered if there is a failure of the storage device containing the primary set of keys. It may be necessary for volume manager to migrate a volume from a first set of one or more storage devices to a second set of one or more storage devices. Volume manager may migrate a volume from a first set to a second set of storage devices for a variety of reasons. The reasons may include a failure malfunction or corruption of data at one of the storage devices underlying the volume. Migrating the volume may involve copying one of the extra sets of keys to a private region of the second set of storage devices. If the storage device storing the primary set of keys fails or malfunctions the primary set of keys may be lost or inaccessible. In this case volume manager driver may retrieve a set of keys from one of the locations where the extra sets of keys are stored. Volume manger driver may create a private region in the second set of storage devices and then volume manager driver may copy the extra set of keys to the newly created private region in the second set of storage devices. The set of keys may include keys from one or more nodes group registration data for one or more groups of nodes and other data associated with an I O fencing method. One or more of the functions described above as being performed by volume manager driver may be performed by volume manager and vice versa.

Quorum device may also be connected to network and quorum device may play a role in determining which group of virtual machines will maintain access to a shared storage device during a split brain scenario. To resolve a split brain scenario each virtual machine with a key stored in the private region of the shared storage device may be given a vote to determine which group is preempted. Quorum device may be given a certain number of votes to ensure there is not a tie or split decision when the virtual machines vote on which group to preempt. The group that receives the most votes referred to as establishing a quorum is allowed to continue to access the shared storage device in question while the remaining group s access to the shared storage device is restricted.

Applications server master node and node of may comprise various hardware and software components. The hardware components may include one or more processors memory devices and input output I O devices connected together via a bus architecture. The software components may include an operating system or a portion of an operating system stored in a memory device. The operating system may be any of various types of operating systems such as Microsoft Windows Linux Solaris or others. The operating system may be operable to provide various services to the user and may support the execution of various programs such as database applications software agents or any of a variety of other applications.

In other embodiments the number and type of application servers hypervisors volume managers volume manager drivers nodes virtual machines networks and storage devices is not limited to those shown in . Any number and combination of application servers and nodes may be interconnected in network architectures via various combinations of modem banks direct LAN connections wireless connections WAN links etc. Also at various times one or more virtual machines may operate offline. In addition during operation individual virtual machine connection types may change as mobile users travel from place to place connecting disconnecting and reconnecting to applications server .

Referring now to an illustration of another embodiment of a virtual machine cluster architecture is shown. Applications server may host one or more software applications including volume manager volume manager driver and hypervisor . Applications server may connect to network and master node and node may connect to applications server through network . Network may be any of the types of networks previously mentioned. Master node hosts virtual machine and node hosts virtual machine . Cluster includes master node and node and cluster is representative of any number of clusters which may connect to applications server through network .

Storage device is also connected to network and storage device is representative of any number of storage devices which may be connected to network . Storage device may also include a quorum device. Alternatively a separate quorum device not shown may be connected to network .

Turning now to one embodiment of a volume logically mapped to a set of storage devices is shown. As referred to herein a volume corresponds to one or more portions of one or more physical storage devices. A volume may also be referred to as a virtual storage device or as a virtual storage object. A volume is a system wide entity in that its definition does not bind it to any specific application virtual machine node or server. A virtual machine may send and receive I O commands to from a volume as if it were a physical storage device.

As part of an I O fencing implementation a volume manager driver may create private region in volume . Volume may be mapped to storage device address space which corresponds to storage devices and . Private region may be created in storage device address space and private region may be mapped to private region . Private region may be stored in storage device . Storage devices and are representative of any number of storage devices which may be mapped to volume . In other embodiments volume may be mapped to a portion of one storage device and other volumes may be mapped to other portions of the same storage device.

Referring now to another embodiment of a volume logically mapped to a set of storage devices is shown. Volume includes private region and volume is mapped to storage device address space . Storage device address space is mapped to storage devices and . Private region corresponds to storage device and private region may be mapped to private region . As shown in private region is created on a separate storage device storage device from the set of storage devices storage devices and organized to form storage device address space corresponding to volume .

Turning now to one embodiment of a method for implementing I O fencing in a virtual machine cluster with shared virtual storage objects is shown. For purposes of discussion the steps in this embodiment are shown in sequential order. It should be noted that in various embodiments of the method described below one or more of the elements described may be performed concurrently in a different order than shown or may be omitted entirely. Other additional elements may also be performed as desired.

Method starts in block and then a first private region may be reserved in a volume in block . Next a second private region may be reserved in a storage device in block . The storage device may be the physical storage entity pointed to by the volume. Then the first private region may be mapped to the second private region block . After block an access request may be received from a virtual machine and the access request may be addressed to the first private region block . Next the access request may be translated so that it addresses the second private region block . The access request may be translated by a volume manager driver. Then the access request may be processed in accordance with an I O fencing technique block . The I O fencing technique may be based on the SCSI 3 PGR standard or it may be based on a different I O fencing standard. After block method may end in block .

It is noted that the above described embodiments may comprise software. In such an embodiment program instructions and or a database both of which may be referred to as instructions that represent the described systems and or methods may be stored on a computer readable storage medium. Generally speaking a computer readable storage medium may include any storage media accessible by a computer during use to provide instructions and or data to the computer. For example a computer readable storage medium may include storage media such as magnetic or optical media e.g. disk fixed or removable tape CD ROM DVD ROM CD R CD RW DVD R DVD RW or Blu Ray. Storage media may further include volatile or non volatile memory media such as RAM e.g. synchronous dynamic RAM SDRAM double data rate DDR DDR2 DDR3 etc. SDRAM low power DDR LPDDR2 etc. SDRAM Rambus DRAM RDRAM static RAM SRAM ROM non volatile memory e.g. Flash memory accessible via a peripheral interface such as the USB interface etc. Storage media may include micro electro mechanical systems MEMS as well as storage media accessible via a communication medium such as a network and or a wireless link.

In various embodiments one or more portions of the methods and mechanisms described herein may form part of a cloud computing environment. In such embodiments resources may be provided over the Internet as services according to one or more various models. Such models may include Infrastructure as a Service IaaS Platform as a Service PaaS and Software as a Service SaaS . In IaaS computer infrastructure is delivered as a service. In such a case the computing equipment is generally owned and operated by the service provider. In the PaaS model software tools and underlying equipment used by developers to develop software solutions may be provided as a service and hosted by the service provider. SaaS typically includes a service provider licensing software as a service on demand. The service provider may host the software or may deploy the software to a customer for a given period of time. Numerous combinations of the above models are possible and are contemplated.

Although several embodiments of approaches have been shown and described it will be apparent to those of ordinary skill in the art that a number of changes modifications or alterations to the approaches as described may be made. Changes modifications and alterations should therefore be seen as within the scope of the methods and mechanisms described herein. It should also be emphasized that the above described embodiments are only non limiting examples of implementations.

