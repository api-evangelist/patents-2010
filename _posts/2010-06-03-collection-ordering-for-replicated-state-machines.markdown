---

title: Collection ordering for replicated state machines
abstract: A replicated state machine with N replica servers may be configured to tolerate a count of F faults. A first operation (of a first ordering type) executes when a first quorum of correctly functioning replicas is available. A second operation (also of the first operation type) executes when a second quorum of correctly functioning replicas is available. A third operation (of a second ordering type) executes when a third quorum of correctly functioning replicas are available. The operations are executed by the replicated state machine such that: (1) the replicated state machine does not guarantee operational ordering between the first operation and the second operation; (2) the replicated state machine guarantees ordering between the first operation and the third operation; and (3) the replicated state machine guarantees ordering between the second operation and the third operation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08135987&OS=08135987&RS=08135987
owner: Microsoft Corporation
number: 08135987
owner_city: Redmond
owner_country: US
publication_date: 20100603
---
Network services or server systems cloud services etc. provide computing services to clients. For availability and performance often such a service may be constructed with multiple servers or machines. The various machines may cooperate to maintain a state that is consistent with respect to the clients or applications that access the service. To this end servers may each maintain a copy or replica of the state of the service and updates by one server are applied or performed by the other servers. At some level each machine is considered equivalent. That is if each machine is running the same service software and maintaining the same state each will provide the same output for the same input. This well known type of system is often referred to as a replicated state machine RSM . With an RSM the functionality of a single server is duplicated among a set of some N replicas.

The replication subsystem may be used as follows. From the perspective of an application developer the developer writes an ordinary but deterministic server and client where the client sends messages such as operation requests to the server via a network and the server performs operations and sends reply messages. The replication subsystem operates as local components on the servers and clients i.e. each server replication component is collocated with a server and each client replication component is collocated with a client . The components have respective application programming interfaces APIs that the servers and clients use to access the replication subsystem. When running as an RSM there are multiple instances of the server each sending and receiving updates via the replication subsystem in particular by a local replication component of the replication subsystem . A server s local server replication component may maintain a shadow copy of the application state of its server . The replication subsystem components cooperate to provide an RSM while preserving the semantic of multiple clients accessing a single server. When a client accesses the RSM the client s client replication component will communicate with various of the server replication components on behalf of the client . When a server performs an operation that affects the state of the RSM the server s server replication component will coordinate with other server replication components to replicate the operation. As used herein depending on the context the term replication subsystem may refer to a client replication component a server replication component or both.

To provide fault tolerance the replication subsystem may implement a consensus protocol. When a client submits an operation to the server system the replica machines first communicate according to a consensus protocol via the server replication components to establish the order in which the operation will execute relative to other operations received by the server system or RSM. Then according to this consensus the replicated servers each separately execute the operation and the server replication components send the corresponding results to the client machine of the requesting client specifically to the client replication component on the client machine which then provides the result to the client . It has been proven that if certain conditions hold the RSM as implemented by the replication subsystem may experience faults and yet produce results that are identical to those of a single correctly functioning server. In particular the ordering of all operations from various clients is well defined. That is to say some level of fault tolerance may be guaranteed by the replication subsystem under some specific conditions.

RSM faults have been divided into two categories stopping faults and Byzantine faults. By selecting a particular consensus protocol an RSM can be configured to deal with a particular class of fault. A stopping fault occurs when a replica exits the RSM by loss of connectivity machine failure software failure and so on. A consensus protocol able to handle such a fault is stopping fault tolerant SFT . A Byzantine fault is a fault that occurs when a replica has failed in a way that renders its behavior incorrect. For example a replica that has experienced Byzantine failure may produce random outputs may continue to communicate via the consensus protocol in erroneous ways may generate random messages may stop may act correctly and so on. If the consensus protocol is not designed for Byzantine fault tolerance Byzantine faults may cause replicas to become corrupt and clients may in turn become corrupt or fail. However if a properly functioning Byzantine fault tolerant BFT consensus protocol is used the state of the replicas that have not experienced Byzantine failure will remain consistent and correct meaning they will advance in the way a single correct machine would advance and the responses that all clients see will be correct. An example of an SFT consensus protocol was described in a paper titled The SMART Way to Migrate Replicated Stateful Services Jacob R. Lorch Atul Adya William J. Bolosky Ronnie Chaiken John R. Douceur and Jon Howell in the Proceedings of EuroSys 2006 . An example of a BFT consensus protocol was described in a paper titled Practical Byzantine Fault Tolerance Miguel Castro and Barbara Liskov in the Proceedings of the Third Symposium on Operating Systems Design and Implementation OSDI 99 .

In practice a client replication component may obtain messages from all replicas but if implementing a BFT consensus protocol will know how to handle corrupt replies and will give the client a single machine consistent view. For example a BFT client replication component may resolve conflicting replies from replicas by following a majority of equivalent replies. In sum if a replica server enters a Byzantine failure state and erroneous or nonsensical messages are received by a client machine the client application above the BFT replication subsystem will see only sane and consistent messages.

As mentioned an RSM may be guaranteed to tolerate faults under certain conditions e.g. limited failure and in practice this may involve the replication subsystem implementing a consensus protocol. The consensus protocol conditionally guarantees operation ordering meaning that if a first client submits operation and a second client submits operation and assuming that the fault tolerance conditions are true either operation will be applied to the RSM before operation or operation will be applied to the RSM before operation. In either case there is a strict order in which the operations are applied. This type of ordering property is sometimes referred to as classic ordering strict ordering or strong ordering. It has been proven that an RSM configured with an SFT consensus protocol can guarantee ordering and tolerate stopping faults only under the condition that the count of replicas N is greater than or equal to 2F 1 where F is the count of faults. It has also been proven that an RSM configured to tolerate Byzantine faults can guarantee ordering only under the condition that N is greater than or equal to 3F 1.

While SFT and BFT strong ordering guarantees are useful a previous BFT consensus protocol added unordered or so called weak operations. That is this previous consensus protocol provided two types of operation strong and weak operations. A strong operation when submitted by a client is a classic BFT well ordered operation as described in the preceding paragraph meaning that when a client submits a strong operation if enough replica machines are available the operation completes strict ordering is guaranteed relative to other strict operations and the client gets an answer to that effect. If not enough machines are available the operation fails and is not performed by the RSM. A weak operation was devised that may guarantee replication i.e. the operation will be applied to the RSM but does not guarantee ordering with respect to other weak operations or with respect to strong operations. Weak operations are more tolerant of faults. That is a weak operation needs fewer machines than a strong operation needs to form a sufficient consensus. In practice however perhaps due to lack of any ordering guarantees weak operations have limited practical application. Furthermore while the strict ordering guarantees of classic BFT and SFT consensus protocols have been established by highly complex and rigorous mathematical proofs it is possible that such guarantees have not been conclusively proven for the strong and weak approach.

Embodiments described herein relate to fault tolerant consensus protocols and implementation thereof.

The following summary is included only to introduce some concepts discussed in the Detailed Description below. This summary is not comprehensive and is not intended to delineate the scope of the claimed subject matter which is set forth by the claims presented at the end.

A replicated state machine with N replica servers may be configured to tolerate a count of F faults. A first operation of a first ordering type executes when a first quorum of correctly functioning replicas is available. A second operation also of the first operation type executes when a second quorum of correctly functioning replicas is available. A third operation of a second ordering type executes when a third quorum of correctly functioning replicas is available. The operations are executed by the replicated state machine such that 1 the replicated state machine does not guarantee operational ordering between the first operation and the second operation 2 the replicated state machine guarantees ordering between the first operation and the third operation and 3 the replicated state machine guarantees ordering between the second operation and the third operation.

Many of the attendant features will be explained below with reference to the following detailed description considered in connection with the accompanying drawings.

Embodiments described herein may provide BFT or SFT consensus protocols that provide operations of varying ordering strictness i.e. different ordering types may be implemented . From weakest to strongest ordering guarantees these operations or ordering types will be referred to as collection including CI operations classic strict ordering SO operations and collection completing CC operations. Collection including operations are not ordered with respect to other collection including operations are not ordered with respect to strict ordering operations but are ordered with respect to collection completing operations. Strict ordering operations are ordered with respect to each other and with respect to collection completing operations but are not ordered with respect to collection including operations. Collection completing operations are ordered with respect to each other are ordered with respect to strong ordering operations and are ordered with respect to collection including operations. Moreover as will be described these operations may be implemented on top of existing classic BFT and SFT consensus protocols and therefore may have the same well proven fault tolerant guarantees.

Fundamentally consensus protocols work by employing a principle known as quorum overlap. A quorum is a set of replicas that agree to perform a given operation. Any two quorums performing different respective operations for the RSM must have at least a minimum count of replicas in common the count depending on the type of fault to be tolerated. For the case of stopping faults this minimum overlap is 1 replica and for the case of Byzantine faults this minimum is F 1 replicas. An easy and common way to achieve the necessary overlap property is by defining quorums based on size. A classic SFT consensus protocol uses a quorum size of ceiling N 1 2 which ensures that any two quorums overlap by at least 1 replica for N 2F 1 this quorum size is equal to F 1. A BFT consensus protocol for basic ordering uses a quorum size of ceiling N F 1 2 which ensures that any two quorums overlap by at least F 1 replicas for N 3F 1 this quorum size is equal to 2F 1. These quorums are for strict ordering operations.

Under some conditions it may not be possible to form a strict ordering quorum. shows two data centers . The data centers house each house replicas of an RSM of N 6. If the connection between the two data centers and is lost the system or RSM will become partitioned causing a condition in which there are insufficient replicas in each data center to perform a consensus for a strict ordering operation. That is there will be no overlap between the set of three replicas of data center A and the set of three replicas of data center B . Even if each data center can still communicate with clients that are nearby the server system or RSM might not be able to proceed.

For such a scenario but without limitation thereto two classes of operation can be implemented collection including and collection completing. Collection including operations can proceed even when a strict ordering quorum is not available because such operations do not require ordering with respect to each other. For example this type of operation might be acceptable for placing a bid in a second price auction. Every collection completing operation e.g. closing an auction is guaranteed to be ordered with respect to every collection including operation that has been previously performed.

To facilitate these ordering types or classes of operation two new types of quorum may be implemented using a consensus protocol collection including quorums and collection completing quorums. A collection including operation can execute whenever a collection including quorum of correctly functioning replicas is available and a collection completing operation can execute whenever a collection completing quorum of correctly functioning replicas is available. The size of a collection including quorum will be termed Qand the size of a collection completing quorum will be termed Q. The quorums are established such that every collection including quorum satisfies the quorum overlap property with respect to every collection completing quorum even though two collection including quorums might not satisfy the quorum overlap property with respect to each other. For classic SFT consensus this overlap is at least 1 replica and for BFT consensus this overlap is at least F 1 replicas.

As mentioned above one way to achieve the necessary overlap property is by defining quorums based on size. The quorum sizes mentioned earlier may be used. For either form of consensus the collection completing quorum size Qcan be no greater than N F because the protocol must be able to continue to work when F replicas are faulty. To ensure the appropriate overlap collection including quorums in a classic SFT consensus protocol are of size F 1 and collection including quorums in a BFT consensus protocol are of size 2F 1. It will readily be seen that these sizes are the same as those of strict ordering quorums when the count of replicas is set to its traditional value N 2F 1 for stopping faults N 3F 1 for Byzantine faults . Note that benefits of consensus protocols with collection including and collection completing operations may increase as N increases.

The example of is an RSM in which N 6 and F 1. Consider a scenario in which this system is configured to tolerate stopping faults. The designation of F 1 means that the system must tolerate up to 1 fault but in fact this system can tolerate up to 2 faults in normal operation because the strict ordering quorum size is 4. In other words the system can execute strict ordering operations even when two replicas are faulty. If connection is severed each data center is left with 3 server replicas . Neither data center partition has sufficient server replicas to perform strict ordering operations or collection completing operations. However because the collection including quorum size is 2 each data center has enough server replicas to continue independently executing collection including operations and in fact each data center can tolerate a single stopping fault and still guarantee the ordering properties of the collection including operations. When the network connection is restored any set of 5 non faulty server replicas can perform a collection completing operation because the collection completing quorum size is 5. Note that when there is no partition the system is able to perform collection completing operations while tolerating F 1 faulty replica.

Now consider a scenario in which the RSM system of is configured to tolerate Byzantine faults. The designation of F 1 means that the system must tolerate up to 1 fault but in fact this system can tolerate up to 1 Byzantine fault and 1 stopping fault in normal operation because the strict ordering quorum size is 4. In other words the system can execute strict ordering operations even when one server replica is Byzantine faulty and one server replica has stopped. If the network connection is disabled each data center is left with three replicas. Because the collection including quorum size is three each data center has enough servers to continue independently executing collection including operations. When the network connection is restored any set of 5 non faulty server replicas can perform a collection completing operation which will be ordered relative to collection including operations received during the partition because the collection completing quorum size is 5. When there is no partition the RSM system is able to perform collection completing operations while tolerating F 1 faulty replica.

In some applications there is no need for collection including or collection completing operations as it may be better to disallow service than to perform operations incorrectly. For instance in a banking application it would be undesirable for two data centers to both perform a withdrawal of 1000 from an account that contains only 1000 because the account will be overdrawn. However in some applications it may be acceptable to allow some forms of service to continue. For instance in a second price auction it is acceptable for two disconnected data centers to both accept bids on a single item a collection including operation . On the other hand it would not be acceptable for the two data centers to both declare winners of the auction a collection completing operation .

The consensus subsystem may implement a known consensus protocol either BFT or SFT modified to provide a parameterized quorum size. Prior consensus protocols use hard coded quorum sizes. Any known consensus protocol may be trivially revised to allow an incoming operation to specify a quorum size. That is a server consensus subsystem may implement an API or protocol in which quorum size may be specified externally by the entity invoking the consensus subsystem . The consensus subsystem server consensus component receives the quorum size and implements the requested operation by requiring a quorum of the specified size. In sum the consensus subsystem implements an RSM collection ordering consensus protocol either BFT or SFT . In either case the server consensus components are designed to allow the accordance subsystem to specify the size of a quorum. As the semantics of the consensus protocol are not reduced from those of prior proven consensus protocols the proven properties thereof hold true.

The accordance subsystem allows a client accordance component to know that a given count M of replicas have agreed to accept its request and therefore depending on the value of M the corresponding client application can be informed that a given type of requested operation has been accepted. In one embodiment the accordance subsystem implements a collection including operation as follows with reference to . On client machine a process starts a client application issuing a completion including operation to its client accordance component which sends the requested collection including operation to all replicas possible and waits for acknowledgements from at least M of them as appropriate for the fault type . When a server accordance component receives a request it stores the operation in a set of received requests the weak state data and sends an acknowledgement to the requesting client machine s accordance component .

When collection including operations are submitted e.g. auction bids various of the replicas have received and acknowledged the operations and stored the operations in their weak state data sets. When a client performs a collection completing operation e.g. a close auction operation at least N F of the replicas each initiate a strict ordering operation using the underlying consensus subsystem with a quorum size of N F. This set of at least N F strict ordering operations ensures that the replicas are in sync regarding the pending collection including operations as described in the following. The collection completing operation is held while the N F strict ordering operations are executed and the replicas thereby exchange the collection including operations in their weak state data sets. When a sufficient number of replicas have applied the outstanding collection including operations e.g. 5 out of 6 the collection completing operation is then performed e.g. the auction is closed . In other words when a collection completing operation starts N F strong operations are performed each strong operation being an operation that carries out some of the collection including operations. Consider the following example.

Client submits a bid of 30 client a bid of 50 and client a bid of 20 and there are 6 replicas R R . Client s bid is accepted by replicas R and R client s bid is accepted by replicas R R and R and client s bid is accepted by replicas R and R. Because each client operation was accepted by at least F 1 replicas each client is assured that its bid will be included in the result of the auction. A collection completing operation issues and there are 6 replicas R R . Replica R receives the collection completing operation finds nothing in its weak state data and reports there is nothing to add. At this point the RSM s logical state has an indication that R submitted its collection including operations and that the collective RSM wide set of collection including operations is empty. Then replica R performs the auction closing operation. Replica R has two bids 30 and 50 and sends the operation to the other replicas which perform the operation. At this point the RSM has a logical state that it has heard from R and R for example a heardfrom list may include R and R and the collective bid list includes 30 and 50. Replica R has a bid of 50 and the replicas perform the bid. The heardfrom list now includes R R and R and the bidlist set is unchanged because adding an existing member to a set has no effect on the set . Replica R was in a same state as R and when it is finished the heardfrom list includes R R R and R and the bidlist is unchanged. Assuming that R fails R has a bid of 20 which R R R and R apply. Now the RSM logical state indicates that R R R R and R have been heard from. Having a sufficient quorum the replicas all perform the collection completing operation closing the auction . Even though R failed the auction completed. None of the collection including operations were lost because every collection including operation was performed by a collection including quorum value of 2 . Even though 1 replica was lost it is guaranteed that its collection including operations were not lost because each was received by at least one other replica.

To explain further the overlap property of the underlying consensus subsystem provides the fault tolerance for the various operation types. In the example a bid was not lost because only 1 machine failed and the choice of the numbers for a collection completing quorum size a collection including quorum size and an amount of overlap between them were set to make sure that operations were not lost. In the case of stopping failures an overlap of 1 is enough. For Byzantine failures the overlap must be F 1. Note that if there are no more than F failures if there is always an overlap of least F 1 there is at least one correct replica machine in the overlap set. While the base implementation of a consensus subsystem may start with a known consensus protocol various quorum sizes as discussed above may maintain the quorum overlap property according to the type of operation. The collection including quorum size may be sufficient for ordering properties of collection including operations but may be too small to maintain sufficient overlap for other types of operations. The collection completing quorum size should be large enough to allow sufficient quorum overlap between a quorum having a collection including size and a quorum having a collection completing size.

Regarding the accordance subsystem other embodiments to perform the accordance forming process are possible. For example the client might not send requests to all replicas initially instead the client might contact only M replicas and then contact more if fewer than M reply in a timely fashion. As another example the client might send a request to a single replica making that single replica responsible for relaying the request to other replicas and if the client does not receive M replies in a timely fashion it resends its request to another replica.

Embodiments and features discussed above can be realized in the form of information stored in the storage . The stored information can be in the form of machine executable instructions e.g. compiled executable binary code source code bytecode or any other information that can be used to enable or configure computing devices to perform the various embodiments discussed above. This is also deemed to include at least volatile memory such as RAM and or virtual memory storing information such as CPU instructions during execution of a program carrying out an embodiment as well as non volatile media storing information that allows a program or executable to be loaded and executed. The embodiments and features can be performed on any type of computing device including portable devices workstations servers mobile wireless devices and so on.

