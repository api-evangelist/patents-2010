---

title: Apparatus, system, and method for power reduction in a storage device
abstract: An apparatus, system, and method are disclosed for managing power consumption in a data storage device. An audit module monitors a power consumption rate of the data storage device relative to a power consumption target. A throttle module adjusts execution of one or more operations on the data storage device in response to the power consumption rate of the data storage device failing to satisfy the power consumption target. A verification module verifies whether the power consumption rate of the data storage device satisfies the power consumption target in response to adjusting the execution of the one or more operations.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08429436&OS=08429436&RS=08429436
owner: Fusion-io, Inc.
number: 08429436
owner_city: Salt Lake City
owner_country: US
publication_date: 20100909
---
This application claims priority to U.S. Provisional Patent Application No. 61 240 991 entitled APPARATUS SYSTEM AND METHOD FOR POWER REDUCTION IN A SOLID STATE STORAGE DEVICE and filed on Sep. 9 2009 for Lance L. Smith et al. to U.S. Provisional Patent Application No. 61 245 622 entitled APPARATUS SYSTEM AND METHOD FOR POWER REDUCTION IN A SOLID STATE STORAGE DEVICE and filed on Sep. 24 2009 for Lance L. Smith et al. and to U.S. Provisional Patent Application No. 61 368 564 entitled APPARATUS SYSTEM AND METHOD FOR WRITING DATA TO STORAGE MEDIA IN A SINGLE ATOMIC OPERATION and filed on Jul. 28 2010 for David Flynn et al. each of which are incorporated herein by reference.

This invention relates to reducing power usage in solid state storage devices and more particularly relates to ensuring that power consumption does not exceed specified parameters.

Power usage matters a great deal in computing devices and in a number of different contexts. In one context most devices have specified power use thresholds that should not be crossed or the device may be damaged. Certain systems may be physically configured to use multiple devices but be unable to supply power to those devices under all conditions without doing damage.

For example many computing systems such as servers laptops desktops etc provide a Peripheral Component Interface PCI or PCI express PCI e that complies with a PCI specification. The PCI specification prescribes a minimum amount of power that the system must provide. Many systems offer a number of PCI slots for various components but are designed with the assumption that no user will fully occupy the slots with devices that all consume the maximum amount of power permitted under the PCI specification. If a user were to insert multiple devices into the PCI slots that consume maximum power the system may not function properly either the devices may not receive the power they need or the system may provide more power than it is designed to supply risking serious damage to the devices and or the system as a whole.

In other situations reducing power can be a valuable way to improve battery life batteries generally have not kept pace with the increasing power requirements of computing systems such as laptops mobile phones and others. In certain situations a user may want to reduce the performance of the device in order to save power. In addition certain components may provide functions that are very important to the user at a particular time while others are not. For example a good wireless connection may matter more to a user on the Internet than performance of storage on the device.

In short users may need certain components to operate at less than their full potential in order to save power. The reasons the power savings are needed may vary as described above.

From the foregoing discussion it should be apparent that a need exists for an apparatus system and method that manages power consumption for a storage device. Beneficially such an apparatus system and method may also manage a thermal state for a storage device.

The present invention has been developed in response to the present state of the art and in particular in response to the problems and needs in the art that have not yet been fully solved by currently available power management techniques. Accordingly the present invention has been developed to provide an apparatus system and method for managing power consumption that overcome many or all of the above discussed shortcomings in the art.

A method of the present invention is presented for managing power consumption in a data storage device. In one embodiment the method includes monitoring a power consumption rate of a data storage device relative to a power consumption target. The method in another embodiment includes adjusting execution of one or more operations on the data storage device in response to the power consumption rate of the data storage device failing to satisfy the power consumption target. The method in a further embodiment includes verifying whether the power consumption rate of the data storage device satisfies the power consumption target in response to adjusting the execution of the one or more operations.

The method in one embodiment includes readjusting execution of one or more subsequent operations on the data storage device in response to verifying that the power consumption rate fails to satisfy the power consumption target in response to adjusting the execution of the one or more operations. The power consumption target in one embodiment includes a power consumption ceiling defining an allowed upper bound for the power consumption rate of the data storage device. In one embodiment the power consumption fails to satisfy the power consumption target when the power consumption rate approaches the power consumption ceiling.

In a further embodiment the method includes monitoring a temperature for the data storage device relative to a thermal ceiling. In another embodiment the method includes adjusting execution of the one or more operations on the data storage device in response to the temperature approaching the thermal ceiling. In one embodiment the method includes verifying whether the temperature is moving away from the thermal ceiling in response to adjusting the execution of the one or more operations. The method in another embodiment includes combining the adjustment of execution of the one or more operations in response to the power consumption rate with the adjustment of execution of the one or more operations in response to the temperature into a single adjustment.

Adjusting execution of the one or more operations in one embodiment includes reducing a frequency with which the one or more operations execute on the data storage device. Reducing the frequency with which the one or more operations execute in a further embodiment includes setting a timer value specifying a frequency with which hardware of the data storage device polls to verify that an operation has completed. In one embodiment the data storage device is a nonvolatile solid state storage device and the timer value is a tPROG time specifying a period of time between submitting a program command to a memory area of the data storage device and submitting a status command to the same memory area of the data storage device to verify that execution of the program command has completed. In another embodiment reducing the frequency with which the one or more operations execute includes setting a timer value specifying a frequency with which the one or more operations are submitted to the data storage device for execution.

The power consumption target in one embodiment includes a number of quanta that define an amount of energy to be used during a period of time. The one or more operations on the data storage device in a further embodiment are assigned quanta based on an amount of energy used to perform an operation. Adjusting the execution of the one or more operations in one embodiment includes scheduling the one or more operations based on the assigned quanta. Each operation of the one or more operations in one embodiment is divided into time periods and each time period of the operation is assigned one or more quanta based on an amount of energy used to perform the operation during the time period.

Adjusting execution of the one or more operations in one embodiment includes selecting operations from one or more queues of operations to be executed. Selecting the operations in one embodiment is performed such that an amount of energy used to execute the selected operations satisfies the power consumption target.

A method of the present invention is also presented for managing a thermal state of a data storage device. In one embodiment the method includes monitoring a temperature for the data storage device relative to a thermal ceiling. In another embodiment the method includes adjusting execution of the one or more operations on the data storage device in response to the temperature approaching the thermal ceiling. In a further embodiment the method includes verifying whether the temperature is moving away from the thermal ceiling in response to adjusting the execution of the one or more operations.

In another embodiment the method includes readjusting execution of one or more subsequent operations on the data storage device. Readjusting execution of subsequent operations in one embodiment is in response to verifying that the temperature fails to move away from the thermal ceiling in response to adjusting the execution of the one or more operations.

The method in a further embodiment includes monitoring a power consumption rate of the data storage device relative to a power consumption target. The method in one embodiment includes adjusting execution of one or more operations on the data storage device in response to the power consumption rate of the data storage device failing to satisfy the power consumption target. In one embodiment the method includes verifying whether the power consumption rate of the data storage device satisfies the power consumption target in response to adjusting the execution of the one or more operations. The method in another embodiment includes combining the adjustment of execution of the one or more operations in response to the temperature with the adjustment of execution of the one or more operations in response to the power consumption rate into a single adjustment.

Another method of the present invention is also presented for managing power consumption in a data storage device. In one embodiment the method includes receiving a set of concurrent operations for execution on the data storage device the concurrent operations each having an associated operation type. The method in another embodiment includes determining a type for at least two operations in the set of concurrent operations. In a further embodiment the method includes scheduling two or more concurrent operations from the set of concurrent operations based on the type of each operation. Scheduling the concurrent operations in one embodiment is done such that a power consumption rate for the data storage device does not exceed a power consumption ceiling during concurrent execution of the two or more concurrent operations.

Scheduling the two or more concurrent operations in one embodiment includes scheduling a predefined number of operations of a predefined type for concurrent execution on the data storage device. In one embodiment scheduling the two or more concurrent operations further includes delaying for at least a predefined duration execution of one or more concurrent operations of a predefined type in response to concurrently executing the predefined number of operations.

The predefined duration in one embodiment is less than an execution time for the predefined type of operation so that execution of the one or more operations overlaps execution of the predefined number of operations. In a further embodiment no more than the predefined number of operations of the predefined type of operation begin execution on the data storage device at a time. The predefined type of operation in one embodiment includes an erase operation type on the data storage device. Execution of an operation of the erase operation type in certain embodiments consumes more power during the predefined duration than after the predefined duration.

A system of the present invention is also presented to manage power consumption in a plurality of data storage devices. The system may be embodied in various embodiments by a plurality of data storage devices one or more audit modules one or more throttle modules and one or more verification modules.

The plurality of data storage devices in one embodiment receives electric power from a power source. The one or more audit modules in one embodiment are configured to monitor a power consumption rate of at least one of the plurality of data storage devices relative to a power consumption target. In a further embodiment the one or more throttle modules are configured to adjust execution of operations on the plurality of data storage devices in response to the power consumption rate of the at least one of the plurality of data storage devices failing to satisfy the power consumption target. The one or more verification modules in one embodiment are configured to verify whether the power consumption rate of the at least one of the plurality of data storage devices satisfies the power consumption target in response to adjusting the execution of the operations.

In a further embodiment the one or more throttle modules adjust execution of the operations substantially identically for each of the plurality of data storage devices in response to the power consumption rate of a single one of the plurality of data storage devices failing to satisfy the power consumption target. In another embodiment the power consumption target is a system power consumption target for the plurality of data storage devices and the one or more throttle modules adjust execution of the operations on each of the plurality of data storage devices dynamically so that a total power consumption rate of the plurality of data storage devices satisfies the system power consumption target.

The one or more audit modules in one embodiment are further configured to monitor temperatures of the plurality of data storage devices relative to a thermal ceiling. The one or more throttle modules in one embodiment are further configured to adjust execution of the operations on a data storage device from the plurality of data storage devices in response to the temperature of the data storage device from the plurality of data storage devices approaching the thermal ceiling. The one or more verification modules in one embodiment are further configured to verify whether the temperature of the data storage device from the plurality of data storage devices is moving away from the thermal ceiling in response to adjusting the execution of the operations.

The one or more throttle modules in a further embodiment adjust execution of the operations substantially identically for each of the plurality of data storage devices in response to the power consumption rate of the at least one of the plurality of data storage devices failing to satisfy the power consumption target. In another embodiment the one or more throttle modules adjust execution of the one or more operations independently for each individual data storage device of the plurality of data storage devices in response to the temperature of the corresponding individual data storage device approaching the thermal ceiling.

A system of the present invention is also presented to manage thermal states for a plurality of data storage devices. The system may be embodied in various embodiments by a plurality of data storage devices one or more audit modules one or more throttle modules and one or more verification modules.

The one or more audit modules in one embodiment are configured to monitor a temperature of at least one of the plurality of data storage devices relative to a thermal ceiling. The one or more throttle modules in one embodiment are configured to adjust execution of operations on the plurality of data storage devices in response to the temperature of the at least one of the plurality of data storage devices approaching the thermal ceiling. The one or more verification modules in one embodiment are configured to verify whether the temperature of the at least one of the plurality of data storage devices is moving away from the thermal ceiling in response to adjusting the execution of the operations.

The one or more audit modules in one embodiment are further configured to monitor a power consumption rate of one or more of the plurality of data storage devices relative to a power consumption target. In another embodiment the one or more throttle modules are further configured to adjust execution of the operations on the plurality of data storage devices in response to the power consumption rate of the one or more of the plurality of data storage devices failing to satisfy the power consumption target. The one or more verification modules in one embodiment are further configured to verify whether the power consumption rate of the one or more of the plurality of data storage devices satisfies the power consumption target in response to adjusting the execution of the operations.

Reference throughout this specification to features advantages or similar language does not imply that all of the features and advantages that may be realized with the present invention should be or are in any single embodiment of the invention. Rather language referring to the features and advantages is understood to mean that a specific feature advantage or characteristic described in connection with an embodiment is included in at least one embodiment of the present invention. Thus discussion of the features and advantages and similar language throughout this specification may but do not necessarily refer to the same embodiment.

Furthermore the described features advantages and characteristics of the invention may be combined in any suitable manner in one or more embodiments. One skilled in the relevant art will recognize that the invention may be practiced without one or more of the specific features or advantages of a particular embodiment. In other instances additional features and advantages may be recognized in certain embodiments that may not be present in all embodiments of the invention.

These features and advantages of the present invention will become more fully apparent from the following description and appended claims or may be learned by the practice of the invention as set forth hereinafter.

Reference throughout this specification to features advantages or similar language does not imply that all of the features and advantages that may be realized with the present invention should be or are in any single embodiment of the invention. Rather language referring to the features and advantages is understood to mean that a specific feature advantage or characteristic described in connection with an embodiment is included in at least one embodiment of the present invention. Thus discussion of the features and advantages and similar language throughout this specification may but do not necessarily refer to the same embodiment.

Furthermore the described features advantages and characteristics of the invention may be combined in any suitable manner in one or more embodiments. One skilled in the relevant art will recognize that the invention may be practiced without one or more of the specific features or advantages of a particular embodiment. In other instances additional features and advantages may be recognized in certain embodiments that may not be present in all embodiments of the invention. These features and advantages of the present invention will become more fully apparent from the following description and appended claims or may be learned by the practice of the invention as set forth hereinafter.

Many of the functional units described in this specification have been labeled as modules in order to more particularly emphasize their implementation independence. For example a module may be implemented as a hardware circuit comprising custom VLSI circuits or gate arrays off the shelf semiconductors such as logic chips transistors or other discrete components. A module may also be implemented in programmable hardware devices such as field programmable gate arrays programmable array logic programmable logic devices or the like.

Modules may also be implemented in software for execution by various types of processors. An identified module of executable code may for instance comprise one or more physical or logical blocks of computer instructions which may for instance be organized as an object procedure or function. Nevertheless the executables of an identified module need not be physically located together but may comprise disparate instructions stored in different locations which when joined logically together comprise the module and achieve the stated purpose for the module.

Indeed a module of executable code may be a single instruction or many instructions and may even be distributed over several different code segments among different programs and across several memory devices. Similarly operational data may be identified and illustrated herein within modules and may be embodied in any suitable form and organized within any suitable type of data structure. The operational data may be collected as a single data set or may be distributed over different locations including over different storage devices and may exist at least partially merely as electronic signals on a system or network. Where a module or portions of a module are implemented in software the software portions are stored on one or more computer readable media.

Reference throughout this specification to one embodiment an embodiment or similar language means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Thus appearances of the phrases in one embodiment in an embodiment and similar language throughout this specification may but do not necessarily all refer to the same embodiment.

Reference to a computer readable medium may take any form capable of storing machine readable instructions on a digital processing apparatus. A computer readable medium may be embodied by a compact disk digital video disk a magnetic tape a Bernoulli drive a magnetic disk a punch card flash memory integrated circuits or other digital processing apparatus memory device.

Furthermore the described features structures or characteristics of the invention may be combined in any suitable manner in one or more embodiments. In the following description numerous specific details are provided such as examples of programming software modules user selections network transactions database queries database structures hardware modules hardware circuits hardware chips etc. to provide a thorough understanding of embodiments of the invention. One skilled in the relevant art will recognize however that the invention may be practiced without one or more of the specific details or with other methods components materials and so forth. In other instances well known structures materials or operations are not shown or described in detail to avoid obscuring aspects of the invention.

The schematic flow chart diagrams included herein are generally set forth as logical flow chart diagrams. As such the depicted order and labeled steps are indicative of one embodiment of the presented method. Other steps and methods may be conceived that are equivalent in function logic or effect to one or more steps or portions thereof of the illustrated method. Additionally the format and symbols employed are provided to explain the logical steps of the method and are understood not to limit the scope of the method. Although various arrow types and line types may be employed in the flow chart diagrams they are understood not to limit the scope of the corresponding method. Indeed some arrows or other connectors may be used to indicate only the logical flow of the method. For instance an arrow may indicate a waiting or monitoring period of unspecified duration between enumerated steps of the depicted method. Additionally the order in which a particular method occurs may or may not strictly adhere to the order of the corresponding steps shown.

The computing system stores data in the storage device and communicates data with the storage device via a communications connection. The storage device may be internal to the computing system or external to the computing system . The communications connection may be a bus a network or other manner of connection allowing the transfer of data between the computing system and the storage device . In one embodiment the storage device is connected to the computing system by a PCI connection such as PCI express PCI e . The storage device may be a card that plugs into a PCI e connection on the computing system .

The power supply in the depicted embodiment provides electrical power to the computing system and through it the storage device and other components of the computing system . The power supply may receive power from a standard electrical connection and transform the power received into one or more voltages that can be used by the computing system . The power supply may be a battery or other power source used in computing systems . The power supply may be internal or external to the computing system . Those of skill in the art in view of this disclosure will appreciate that there are a variety of different configurations and types of power supplies .

The storage device in the depicted embodiment receives electric power from the power supply that provides the storage device with power to perform data storage operations such as reads writes erases etc. The storage device in the depicted embodiment receives the power from the power supply over a power connection with the computing system . In certain embodiments a power connection and the communications connection discussed above are part of the same physical connection between the computing system and the storage device . For example the storage device may receive power over PCI PCI e serial advanced technology attachment serial ATA or SATA parallel ATA PATA small computer system interface SCSI IEE 1394 FireWire Fiber Channel universal serial bus USB PCIe AS or another connection with the computing system . In one embodiment the computing system distributes power from the power supply through one or more slots ports or the like of a motherboard.

In other embodiments the storage device may connect to the power supply directly independently of a computing system . For example the power supply may be a power converter often called a power brick a power supply unit PSU for the storage device or the like. Those of skill in the art will appreciate in view of this disclosure that there are various ways by which a storage device may receive power and the variety of devices that can act as the power supply for the storage device .

The storage device provides nonvolatile storage for the computing system . shows the storage device as a nonvolatile solid state storage device comprising a solid state storage controller a write data pipeline a read data pipeline and nonvolatile solid state storage media . The storage device may contain additional components that are not shown in order to provide a simpler view of the storage device .

The solid state storage media stores data such that the data is retained even when the storage device is not powered. Examples of solid state storage media include flash memory nano random access memory NRAM magneto resistive RAM MRAM dynamic RAM DRAM phase change RAM PRAM Racetrack memory Memristor memory and nanocrystal wire based memory silicon oxide based sub 10 nanometer process memory graphene memory Silicon Oxide Nitride Oxide Silicon SONOS Resistive random access memory RRAM programmable metallization cell PMC conductive bridging RAM CBRAM and the like. While in the depicted embodiment the storage device includes solid state storage media in other embodiments the storage device may include magnetic media such as hard disks tape and the like optical media or other nonvolatile data storage media. The storage device also includes a storage controller that coordinates the storage and retrieval of data in the solid state storage media . The storage controller may use one or more indexes to locate and retrieve data and perform other operations on data stored in the storage device . For example the storage controller may include a groomer for performing data grooming operations such as garbage collection.

As shown the storage device in certain embodiments implements a write data pipeline and a read data pipeline an example of which is described in greater detail below. The write data pipeline may perform certain operations on data as the data is transferred from the computing system into the solid state storage media . These operations may include for example error correction code ECC generation encryption compression and others. The read data pipeline may perform similar and potentially inverse operations on data that is being read out of solid state storage media and sent to the computing system .

In one embodiment the computing system includes one or more other components in addition to the storage device such as additional storage devices graphics processors network cards and the like that also receive power from the power supply . Those of skill in the art in view of this disclosure will appreciate the different types of components that may be in a computing system . The components may be internal or external to the computing system . In one embodiment some of the components may be PCI or PCI e cards that connect to the computing system and receive power through the computing system .

The system in the depicted embodiment also includes a power management apparatus . In certain embodiments the power management apparatus may be implemented as part of the storage controller . In further embodiments the power management apparatus may be for instance implemented as part of a software driver of the computing system implemented in firmware for the storage device or the like. In other embodiments the power management apparatus may be implemented partially in a software driver and partially in the storage controller or the like.

As noted above the computing system may be configured to accept more components such as storage devices than the computing system can provide power for under all conditions. The computing system may not be able to provide sufficient power to a storage device operating at or near full performance or providing sufficient power may damage the computing system and or the power supply especially if other components are also drawing power.

Other situations may arise in which it is important to tailor the performance of the storage device or other computer components based on power considerations. In certain embodiments the power management apparatus manages a variety of states that correspond to various power conditions. One possible state is a power disruption state where the available power is reduced to an amount that can be provided by a secondary power supply or the like. In this state the power management apparatus may perform power disruption or power failure management.

The power management apparatus in one embodiment monitors a power consumption rate of the storage device relative to a power consumption target. In a further embodiment the power management apparatus adjusts execution of operations on the storage device based on the power consumption rate. Adjusting execution of operations such as throttling or slowing execution of operations in one embodiment reduces the power consumption rate of the storage device . In one embodiment the power management apparatus verifies whether the power consumption rate of the storage device satisfies the power consumption target in response to adjusting execution of operations on the storage device . In another embodiment the power management apparatus manages and schedules execution of operations on the storage device to stay within the power consumption target.

The power management apparatus in one embodiment references operations parameters such as the power consumption target that specify performance limitations on the storage device and or on other components. The performance limitations are related to power consumption of the storage device or of other components. The parameters may be designed coded hard corded and or programmed directly into the power management apparatus or may be sent to the power management apparatus by the computing system or by another power management apparatus . In one embodiment the system includes a plurality of power management apparatuses for different components. In certain embodiments the different power management apparatuses share information with each other in other embodiments each power management apparatus acts independently. One embodiment of a system with a plurality of power management apparatuses is described below with regard to .

In certain embodiments the power management apparatus may dynamically adjust the power allocation for the storage device . The computing system may dedicate more or less power to the storage device at a particular time for example in the event that another component is a graphics card there may be considerably less power available to the storage device during graphics intensive operations. Similarly more power may be available to the storage device when the computing system is not performing intensive graphics operations. The computing system may thus dynamically change the allocation of power and send appropriate parameters to the power management apparatus . In certain embodiments the computing system includes software firmware hardware or some combination thereof that is specifically tailored to determine how much power is available in the computing system .

The computing system in one embodiment may send a power consumption target and or other power management information to the power management apparatus over a communications channel such as a PCI e bus a data network or the like. In one embodiment the power management apparatus supports a protocol an application programming interface API or the like for setting a power consumption target. The power consumption target in one embodiment is user configurable or selectable through an interface on the computing system an interface of the storage device or the like.

In other embodiments the power management apparatus sets a power management plan or parameters without receiving parameters from the computing system . The power management apparatus may dynamically adjust the power allocation to different elements within the storage device . For example in one embodiment the storage device is a solid state storage device with multiple dual in line memory modules DIMMs . The power management apparatus may have or set a particular power limit such as 25 watts which may also be referred to as a power budget a power consumption target or a power consumption ceiling for the entire storage device and may dynamically allocate power to remain within the constraints of the power budget. Thus one DIMM performing a number of energy intensive operations may be allocated 20 watts while another DIMM is allocated 5 watts until the energy intensive operations are complete.

The power consumption target as used herein is the amount of power that a power supply system is configured to make available and or provide to the storage device and or other components supplied by the power supply system. In certain embodiments the power consumption target is a power consumption ceiling a maximum allowed upper bound for the power consumption rate of the storage device . In other embodiments while the power consumption target is a maximum amount the power supply may permit uses of power over the maximum amount so long as the average power use falls below a power consumption target. In certain embodiments the power consumption target is a single power value. In other embodiments the power consumption target includes a baseline power but allows for bursts and provides power restrictions on bursting. For example these restrictions might include magnitude duration rise time fall time and the like. In a further embodiment the power consumption target is a range of allowable power consumption rates or the like. The power consumption target in one embodiment is defined as a value or range of values having a standard power metric unit such as the watt W .

A power consumption rate of the storage device or of another component in various embodiments may satisfy a power consumption target by falling away from the power consumption target by being outside of a predefined range around the power consumption target by being inside a predefined range around the power consumption target and or by having another predefined relationship with the power consumption target. Conversely a power consumption rate in various embodiments may fail to satisfy a power consumption target by approaching the power consumption target by exceeding the power consumption target by being outside of a predefined range around the power consumption target by being inside a predefined range around the power consumption target and or by having another predefined relationship with the power consumption target. One of skill in the art in view of this disclosure will recognize many possible types of power consumption targets and corresponding ways that a power consumption rate can satisfy or fail to satisfy the different types of power consumption targets.

The computing system may dynamically change the power consumption target for the storage device in response to changes in the power supply . The power consumption target may also be dynamically allocated in response to changes in workload as discussed in greater detail below. For example if the power supply is connected to a source such as an electrical outlet the computing system may specify one power consumption target. If the power supply is disconnected from the source such that the power supply is operating on battery power the computing system may decrease the power consumption target for the storage device . In one embodiment the computing system may dynamically change the power consumption target for the storage device as the battery life decreases. In certain embodiments with multiple components the power consumption targets for the components are not adjusted downward equally as battery life decreases for example the power consumption target for a component providing an active wireless connection may remain stable or be reduced at a slower rate than that of the storage device or the like.

In one embodiment the power management apparatus provides thermal management for the storage device . The power management apparatus in one embodiment monitors a temperature for the storage device relative to a thermal ceiling or other thermal threshold. The power management apparatus in one embodiment adjusts execution of operations on the storage device in response to the temperature of the storage device approaching the thermal ceiling. In one embodiment the power management apparatus combines thermal adjustments to execution of the operations with power adjustments to execution of the operations into a single combined adjustment. In a further embodiment the power management apparatus verifies that the temperature of the storage device is moving away from the thermal ceiling in response to adjusting execution of the operations.

In one embodiment at least one solid state controller is field programmable gate array FPGA and controller functions are programmed into the FPGA. In a particular embodiment the FPGA is a Xilinx FPGA. In another embodiment the solid state storage controller comprises components specifically designed as a solid state storage controller such as an application specific integrated circuit ASIC or custom logic solution. Each solid state storage controller typically includes a write data pipeline and a read data pipeline which are describe further in relation to . In another embodiment at least one solid state storage controller is made up of a combination FPGA ASIC and custom logic components.

The solid state storage media is an array of non volatile solid state storage elements arranged in banks and accessed in parallel through a bi directional storage input output I O bus . The storage I O bus in one embodiment is capable of unidirectional communication at any one time. For example when data is being written to the solid state storage media data cannot be read from the solid state storage media . In another embodiment data can flow both directions simultaneously. However bi directional as used herein with respect to a data bus refers to a data pathway that can have data flowing in only one direction at a time but when data flowing one direction on the bi directional data bus is stopped data can flow in the opposite direction on the bi directional data bus.

A solid state storage element e.g. SSS . is typically configured as a chip a package of one or more dies or a die on a circuit board. As depicted a solid state storage element e.g. operates independently or semi independently of other solid state storage elements e.g. even if these several elements are packaged together in a chip package a stack of chip packages or some other package element. As depicted a row of solid state storage elements is designated as a bank . As depicted there may be n banks and m solid state storage elements per bank in an array of n m solid state storage elements in a solid state storage media . Of course different embodiments may include different values for n and m. In one embodiment a solid state storage media includes twenty solid state storage elements per bank with eight banks . In one embodiment the solid state storage media includes twenty four solid state storage elements per bank with eight banks . In addition to the n m storage elements one or more additional columns P may also be addressed and operated in parallel with other solid state storage elements for one or more rows. The added P columns in one embodiment store parity data for the portions of an ECC chunk i.e. an ECC codeword that span m storage elements for a particular bank. In one embodiment each solid state storage element is comprised of single level cell SLC devices. In another embodiment each solid state storage element is comprised of multi level cell MLC devices.

In one embodiment solid state storage elements that share a common storage I O bus e.g. are packaged together. In one embodiment a solid state storage element may have one or more dies per chip with one or more chips stacked vertically and each die may be accessed independently. In another embodiment a solid state storage element e.g. SSS . may have one or more virtual dies per die and one or more dies per chip and one or more chips stacked vertically and each virtual die may be accessed independently. In another embodiment a solid state storage element SSS . may have one or more virtual dies per die and one or more dies per chip with some or all of the one or more dies stacked vertically and each virtual die may be accessed independently.

In one embodiment two dies are stacked vertically with four stacks per group to form eight storage elements e.g. SSS . SSS . each in a separate bank . In another embodiment 24 storage elements e.g. SSS . SSS . form a logical bank so that each of the eight logical banks has 24 storage elements e.g. SSS . SSS . . Data is sent to the solid state storage media over the storage I O bus to all storage elements of a particular group of storage elements SSS . SSS . . The storage control bus is used to select a particular bank e.g. Bank so that the data received over the storage I O bus connected to all banks is written just to the selected bank

In a one embodiment the storage I O bus is comprised of one or more independent I O buses IIOBa m comprising wherein the solid state storage elements within each column share one of the independent I O buses that accesses each solid state storage element in parallel so that all banks are accessed simultaneously. For example one channel of the storage I O bus may access a first solid state storage element of each bank simultaneously. A second channel of the storage I O bus may access a second solid state storage element of each bank simultaneously. Each row of solid state storage element is accessed simultaneously. In one embodiment where solid state storage elements are multi level physically stacked all physical levels of the solid state storage elements are accessed simultaneously. As used herein simultaneously also includes near simultaneous access where devices are accessed at slightly different intervals to avoid switching noise. Simultaneously is used in this context to be distinguished from a sequential or serial access wherein commands and or data are sent individually one after the other.

Typically banks are independently selected using the storage control bus . In one embodiment a bank is selected using a chip enable or chip select. Where both chip select and chip enable are available the storage control bus may select one level of a multi level solid state storage element . In other embodiments other commands are used by the storage control bus to individually select one level of a multi level solid state storage element . Solid state storage elements may also be selected through a combination of control and of address information transmitted on storage I O bus and the storage control bus .

In one embodiment each solid state storage element is partitioned into erase blocks and each erase block is partitioned into pages. An erase block on a solid state storage element may be called a physical erase block or PEB. A typical page is 2000 bytes 2 kB . In one example a solid state storage element e.g. SSS . includes two registers and can program two pages so that a two register solid state storage element has a capacity of 4 kB. A bank of 20 solid state storage elements would then have an 80 kB capacity of pages accessed with the same address going out the channels of the storage I O bus .

This group of pages in a bank of solid state storage elements of 80 kB may be called a logical page or virtual page. Similarly an erase block of each storage element of a bank may be grouped to form a logical erase block or a virtual erase block. In one embodiment an erase block of pages within a solid state storage element is erased when an erase command is received within a solid state storage element . Whereas the size and number of erase blocks pages planes or other logical and physical divisions within a solid state storage element are expected to change over time with advancements in technology it is to be expected that many embodiments consistent with new configurations are possible and are consistent with the general description herein.

Typically when a packet is written to a particular location within a solid state storage element wherein the packet is intended to be written to a location within a particular page which is specific to a particular physical erase block of a particular storage element of a particular bank a physical address is sent on the storage I O bus and followed by the packet. The physical address contains enough information for the solid state storage element to direct the packet to the designated location within the page. Since all storage elements in a column of storage elements e.g. SSS . SSS N. are accessed simultaneously by the appropriate bus within the storage I O bus to reach the proper page and to avoid writing the data packet to similarly addressed pages in the column of storage elements SSS . SSS N. the bank that includes the solid state storage element SSS . with the correct page where the data packet is to be written is simultaneously selected by the storage control bus .

Similarly satisfying a read command on the storage I O bus requires a simultaneous signal on the storage control bus to select a single bank and the appropriate page within that bank . In one embodiment a read command reads an entire page and because there are multiple solid state storage elements in parallel in a bank an entire logical page is read with a read command. However the read command may be broken into subcommands as will be explained below with respect to bank interleave. A logical page may also be accessed in a write operation.

An erase block erase command may be sent out to erase an erase block over the storage I O bus with a particular erase block address to erase a particular erase block. Typically an erase block erase command may be sent over the parallel paths of the storage I O bus to erase a logical erase block each with a particular erase block address to erase a particular erase block. Simultaneously a particular bank e.g. Bank is selected over the storage control bus to prevent erasure of similarly addressed erase blocks in all of the banks Banks N . Alternatively no particular bank e.g. Bank is selected over the storage control bus to enable erasure of similarly addressed erase blocks in all of the banks Banks N simultaneously. Other commands may also be sent to a particular location using a combination of the storage I O bus and the storage control bus . One of skill in the art will recognize other ways to select a particular storage location using the bi directional storage I O bus and the storage control bus .

In one embodiment packets are written sequentially to the solid state storage media . For example packets are streamed to the storage write buffers of a bank of storage elements and when the buffers are full the packets are programmed to a designated logical page. Packets then refill the storage write buffers and when full the packets are written to the next logical page. The next logical page may be in the same bank or another bank e.g. . This process continues logical page after logical page typically until a logical erase block is filled. In another embodiment the streaming may continue across logical erase block boundaries with the process continuing logical erase block after logical erase block.

In a read modify write operation data packets associated with requested data are located and read in a read operation. Data segments of the modified requested data that have been modified are not written to the location from which they are read. Instead the modified data segments are again converted to data packets and then written sequentially to the next available location in the logical page currently being written. The index entries for the respective data packets are modified to point to the packets that contain the modified data segments. The entry or entries in the index for data packets associated with the same requested data that have not been modified will include pointers to original location of the unmodified data packets. Thus if the original requested data is maintained for example to maintain a previous version of the requested data the original requested data will have pointers in the index to all data packets as originally written. The new requested data will have pointers in the index to some of the original data packets and pointers to the modified data packets in the logical page that is currently being written.

In a copy operation the index includes an entry for the original requested data mapped to a number of packets stored in the solid state storage media . When a copy is made a new copy of the requested data is created and a new entry is created in the index mapping the new copy of the requested data to the original packets. The new copy of the requested data is also written to the solid state storage media with its location mapped to the new entry in the index. The new copy of the requested data packets may be used to identify the packets within the original requested data that are referenced in case changes have been made in the original requested data that have not been propagated to the copy of the requested data and the index is lost or corrupted.

Beneficially sequentially writing packets facilitates a more even use of the solid state storage media and allows the solid storage device controller to monitor storage hot spots and level usage of the various logical pages in the solid state storage media . Sequentially writing packets also facilitates a powerful efficient garbage collection system which is described in detail below. One of skill in the art will recognize other benefits of sequential storage of data packets.

In various embodiments the solid state storage device controller also includes a data bus a local bus a buffer controller buffers N a master controller a direct memory access DMA controller a memory controller a dynamic memory array a static random memory array a management controller a management bus a bridge to a system bus and miscellaneous logic which are described below. In other embodiments the system bus is coupled to one or more network interface cards NICs some of which may include remote DMA RDMA controllers one or more central processing unit CPU one or more external memory controllers and associated external memory arrays one or more storage controllers peer controllers and application specific processors which are described below. The components connected to the system bus may be located in the computing system or may be other devices.

Typically the solid state storage controller s communicate data to the solid state storage media over a storage I O bus . In a typical embodiment where the solid state storage is arranged in banks and each bank includes multiple storage elements accessed in parallel the storage I O bus is an array of busses one for each column of storage elements spanning the banks . As used herein the term storage I O bus may refer to one storage I O bus or an array of data independent busses . In one embodiment each storage I O bus accessing a column of storage elements e.g. may include a logical to physical mapping for storage divisions e.g. erase blocks accessed in a column of storage elements . This mapping or bad block remapping allows a logical address mapped to a physical address of a storage division to be remapped to a different storage division if the first storage division fails partially fails is inaccessible or has some other problem.

Data may also be communicated to the solid state storage controller s from a requesting device through the system bus bridge local bus buffer s and finally over a data bus . The data bus typically is connected to one or more buffers controlled with a buffer controller . The buffer controller typically controls transfer of data from the local bus to the buffers and through the data bus to the pipeline input buffer and output buffer . The buffer controller typically controls how data arriving from a requesting device can be temporarily stored in a buffer and then transferred onto a data bus or vice versa to account for different clock domains to prevent data collisions etc. The buffer controller typically works in conjunction with the master controller to coordinate data flow. As data arrives the data will arrive on the system bus be transferred to the local bus through a bridge .

Typically the data is transferred from the local bus to one or more data buffers as directed by the master controller and the buffer controller . The data then flows out of the buffer s to the data bus through a solid state controller and on to the solid state storage media such as NAND flash or other storage media. In one embodiment data and associated out of band metadata metadata arriving with the data is communicated using one or more data channels comprising one or more solid state storage controllers and associated solid state storage media while at least one channel solid state storage controller solid state storage media is dedicated to in band metadata such as index information and other metadata generated internally to the solid state storage device .

The local bus is typically a bidirectional bus or set of busses that allows for communication of data and commands between devices internal to the solid state storage device controller and between devices internal to the solid state storage device and devices connected to the system bus . The bridge facilitates communication between the local bus and system bus . One of skill in the art will recognize other embodiments such as ring structures or switched star configurations and functions of buses and bridges .

The system bus is typically a bus of a computing system or other device in which the solid state storage device is installed or connected. In one embodiment the system bus may be a PCI e bus a Serial Advanced Technology Attachment serial ATA bus parallel ATA or the like. In another embodiment the system bus is an external bus such as small computer system interface SCSI FireWire Fiber Channel USB PCIe AS or the like. The solid state storage device may be packaged to fit internally to a device or as an externally connected device.

The solid state storage device controller includes a master controller that controls higher level functions within the solid state storage device . The master controller in various embodiments controls data flow by interpreting object requests and other requests directs creation of indexes to map object identifiers associated with data to physical locations of associated data coordinating DMA requests etc. Many of the functions described herein are controlled wholly or in part by the master controller .

In one embodiment the master controller uses embedded controller s . In another embodiment the master controller uses local memory such as a dynamic memory array dynamic random access memory DRAM a static memory array static random access memory SRAM etc. In one embodiment the local memory is controlled using the master controller . In another embodiment the master controller accesses the local memory via a memory controller . In another embodiment the master controller runs a Linux server and may support various common server interfaces such as the World Wide Web hyper text markup language HTML etc. In another embodiment the master controller uses a nano processor. The master controller may be constructed using programmable or standard logic or any combination of controller types listed above. One skilled in the art will recognize many embodiments for the master controller .

In one embodiment where the storage device solid state storage device controller manages multiple data storage devices solid state storage media the master controller divides the work load among internal controllers such as the solid state storage controllers . For example the master controller may divide an object to be written to the data storage devices e.g. solid state storage media so that a portion of the object is stored on each of the attached data storage devices. This feature is a performance enhancement allowing quicker storage and access to an object. In one embodiment the master controller is implemented using an FPGA. In another embodiment the firmware within the master controller may be updated through the management bus the system bus over a network connected to a NIC or other device connected to the system bus .

In one embodiment the master controller which manages objects emulates block storage such that a computing system or other device connected to the storage device solid state storage device views the storage device solid state storage device as a block storage device and sends data to specific physical addresses in the storage device solid state storage device . The master controller then divides up the blocks and stores the data blocks as it would objects. The master controller then maps the blocks and physical address sent with the block to the actual locations determined by the master controller . The mapping is stored in the object index. Typically for block emulation a block device application program interface API is provided in a driver in a computer such as the computing system or other device wishing to use the storage device solid state storage device as a block storage device.

In another embodiment the master controller coordinates with NIC controllers and embedded RDMA controllers to deliver just in time RDMA transfers of data and command sets. NIC controller may be hidden behind a non transparent port to enable the use of custom drivers. Also a driver on a computing system may have access to the computer network through an I O memory driver using a standard stack API and operating in conjunction with NICs .

In one embodiment the master controller is also a redundant array of independent drive RAID controller. Where the data storage device solid state storage device is networked with one or more other data storage devices solid state storage devices the master controller may be a RAID controller for single tier RAID multi tier RAID progressive RAID etc. The master controller also allows some objects to be stored in a RAID array and other objects to be stored without RAID. In another embodiment the master controller may be a distributed RAID controller element. In another embodiment the master controller may comprise many RAID distributed RAID and other functions as described elsewhere. In one embodiment the master controller controls storage of data in a RAID like structure where parity information is stored in one or more storage elements of a logical page where the parity information protects data stored in the other storage elements of the same logical page.

In one embodiment the master controller coordinates with single or redundant network managers e.g. switches to establish routing to balance bandwidth utilization failover etc. In another embodiment the master controller coordinates with integrated application specific logic via local bus and associated driver software. In another embodiment the master controller coordinates with attached application specific processors or logic via the external system bus and associated driver software. In another embodiment the master controller coordinates with remote application specific logic via the computer network and associated driver software. In another embodiment the master controller coordinates with the local bus or external bus attached hard disk drive HDD storage controller.

In one embodiment the master controller communicates with one or more storage controllers where the storage device solid state storage device may appear as a storage device connected through a SCSI bus Internet SCSI iSCSI fiber channel etc. Meanwhile the storage device solid state storage device may autonomously manage objects and may appear as an object file system or distributed object file system. The master controller may also be accessed by peer controllers and or application specific processors .

In another embodiment the master controller coordinates with an autonomous integrated management controller to periodically validate FPGA code and or controller software validate FPGA code while running reset and or validate controller software during power on reset support external reset requests support reset requests due to watchdog timeouts and support voltage current power temperature and other environmental measurements and setting of threshold interrupts. In another embodiment the master controller manages garbage collection to free erase blocks for reuse. In another embodiment the master controller manages wear leveling. In another embodiment the master controller allows the data storage device solid state storage device to be partitioned into multiple logical devices and allows partition based media encryption. In yet another embodiment the master controller supports a solid state storage controller with advanced multi bit ECC correction. One of skill in the art will recognize other features and functions of a master controller in a storage controller or more specifically in a solid state storage device .

In one embodiment the solid state storage device controller includes a memory controller which controls a dynamic random memory array and or a static random memory array . As stated above the memory controller may be independent or integrated with the master controller . The memory controller typically controls volatile memory of some type such as DRAM dynamic random memory array and SRAM static random memory array . In other examples the memory controller also controls other memory types such as electrically erasable programmable read only memory EEPROM etc. In other embodiments the memory controller controls two or more memory types and the memory controller may include more than one controller. Typically the memory controller controls as much SRAM as is feasible and by DRAM to supplement the SRAM .

In one embodiment the object index is stored in memory and then periodically off loaded to a channel of the solid state storage media or other non volatile memory. One of skill in the art will recognize other uses and configurations of the memory controller dynamic memory array and static memory array .

In one embodiment the solid state storage device controller includes a DMA controller that controls DMA operations between the storage device solid state storage device and one or more external memory controllers and associated external memory arrays and CPUs . Note that the external memory controllers and external memory arrays are called external because they are external to the storage device solid state storage device . In addition the DMA controller may also control RDMA operations with requesting devices through a NIC and associated RDMA controller .

In one embodiment the solid state storage device controller includes a management controller connected to a management bus . Typically the management controller manages environmental metrics and status of the storage device solid state storage device . The management controller may monitor device temperature fan speed power supply settings etc. over the management bus . The management controller may support the reading and programming of erasable programmable read only memory EEPROM for storage of FPGA code and controller software. Typically the management bus is connected to the various components within the storage device solid state storage device . The management controller may communicate alerts interrupts etc. over the local bus or may include a separate connection to a system bus or other bus. In one embodiment the management bus is an Inter Integrated Circuit I2C bus. One of skill in the art will recognize other related functions and uses of a management controller connected to components of the storage device solid state storage device by a management bus .

In one embodiment the solid state storage device controller includes miscellaneous logic that may be customized for a specific application. Typically where the solid state device controller or master controller is are configured using a FPGA or other configurable controller custom logic may be included based on a particular application customer requirement storage requirement etc.

The write data pipeline includes a packetizer that receives a data or metadata segment to be written to the solid state storage either directly or indirectly through another write data pipeline stage and creates one or more packets sized for the solid state storage media . The data or metadata segment is typically part of a data structure such as an object but may also include an entire data structure. In another embodiment the data segment is part of a block of data but may also include an entire block of data. Typically a set of data such as a data structure is received from a computer such as the computing system or other computer or device and is transmitted to the solid state storage device in data segments streamed to the solid state storage device . A data segment may also be known by another name such as data parcel but as referenced herein includes all or a portion of a data structure or data block.

Each data structure is stored as one or more packets. Each data structure may have one or more container packets. Each packet contains a header. The header may include a header type field. Type fields may include data attribute metadata data segment delimiters multi packet data structures data linkages and the like. The header may also include information regarding the size of the packet such as the number of bytes of data included in the packet. The length of the packet may be established by the packet type. The header may include information that establishes the relationship of the packet to a data structure. An example might be the use of an offset in a data packet header to identify the location of the data segment within the data structure. One of skill in the art will recognize other information that may be included in a header added to data by a packetizer and other information that may be added to a data packet.

Each packet includes a header and possibly data from the data or metadata segment. The header of each packet includes pertinent information to relate the packet to the data structure to which the packet belongs. For example the header may include an object identifier or other data structure identifier and offset that indicates the data segment object data structure or data block from which the data packet was formed. The header may also include a logical address used by the storage bus controller to store the packet. The header may also include information regarding the size of the packet such as the number of bytes included in the packet. The header may also include a sequence number that identifies where the data segment belongs with respect to other packets within the data structure when reconstructing the data segment or data structure. The header may include a header type field. Type fields may include data data structure attributes metadata data segment delimiters multi packet data structure types data structure linkages and the like. One of skill in the art will recognize other information that may be included in a header added to data or metadata by a packetizer and other information that may be added to a packet.

The write data pipeline includes an ECC generator that that generates one or more error correcting codes ECC for the one or more packets received from the packetizer . The ECC generator typically uses an error correcting algorithm to generate ECC check bits which are stored with the one or more data packets. The ECC codes generated by the ECC generator together with the one or more data packets associated with the ECC codes comprise an ECC chunk. The ECC data stored with the one or more data packets is used to detect and to correct errors introduced into the data through transmission and storage. In one embodiment packets are streamed into the ECC generator as un encoded blocks of length N. A syndrome of length S is calculated appended and output as an encoded block of length N S. The value of N and S are dependent upon the characteristics of the ECC algorithm which is selected to achieve specific performance efficiency and robustness metrics. In one embodiment there is no fixed relationship between the ECC blocks and the packets the packet may comprise more than one ECC block the ECC block may comprise more than one packet and a first packet may end anywhere within the ECC block and a second packet may begin after the end of the first packet within the same ECC block. In one embodiment ECC algorithms are not dynamically modified. In one embodiment the ECC data stored with the data packets is robust enough to correct errors in more than two bits.

Beneficially using a robust ECC algorithm allowing more than single bit correction or even double bit correction allows the life of the solid state storage media to be extended. For example if flash memory is used as the storage medium in the solid state storage media the flash memory may be written approximately 100 000 times without error per erase cycle. This usage limit may be extended using a robust ECC algorithm. Having the ECC generator and corresponding ECC correction module onboard the solid state storage device the solid state storage device can internally correct errors and has a longer useful life than if a less robust ECC algorithm is used such as single bit correction. However in other embodiments the ECC generator may use a less robust algorithm and may correct single bit or double bit errors. In another embodiment the solid state storage device may comprise less reliable storage such as multi level cell MLC flash in order to increase capacity which storage may not be sufficiently reliable without more robust ECC algorithms.

In one embodiment the write pipeline includes an input buffer that receives a data segment to be written to the solid state storage media and stores the incoming data segments until the next stage of the write data pipeline such as the packetizer or other stage for a more complex write data pipeline is ready to process the next data segment. The input buffer typically allows for discrepancies between the rate data segments are received and processed by the write data pipeline using an appropriately sized data buffer. The input buffer also allows the data bus to transfer data to the write data pipeline at rates greater than can be sustained by the write data pipeline in order to improve efficiency of operation of the data bus . Typically when the write data pipeline does not include an input buffer a buffering function is performed elsewhere such as in the solid state storage device but outside the write data pipeline in the computing system such as within a network interface card NIC or at another device for example when using remote direct memory access RDMA .

In another embodiment the write data pipeline also includes a write synchronization buffer that buffers packets received from the ECC generator prior to writing the packets to the solid state storage media . The write synchronization buffer is located at a boundary between a local clock domain and a solid state storage clock domain and provides buffering to account for the clock domain differences. In other embodiments synchronous solid state storage media may be used and synchronization buffers may be eliminated.

In one embodiment the write data pipeline also includes a media encryption module that receives the one or more packets from the packetizer either directly or indirectly and encrypts the one or more packets using an encryption key unique to the solid state storage device prior to sending the packets to the ECC generator . Typically the entire packet is encrypted including the headers. In another embodiment headers are not encrypted. In this document encryption key is understood to mean a secret encryption key that is managed externally from a solid state storage controller .

The media encryption module and corresponding media decryption module provide a level of security for data stored in the solid state storage media . For example where data is encrypted with the media encryption module if the solid state storage media is connected to a different solid state storage controller solid state storage device or server the contents of the solid state storage media typically could not be read without use of the same encryption key used during the write of the data to the solid state storage media without significant effort.

In a typical embodiment the solid state storage device does not store the encryption key in non volatile storage and allows no external access to the encryption key. The encryption key is provided to the solid state storage controller during initialization. The solid state storage device may use and store a non secret cryptographic nonce that is used in conjunction with an encryption key. A different nonce may be stored with every packet. Data segments may be split between multiple packets with unique nonces for the purpose of improving protection by the encryption algorithm.

The encryption key may be received from a computing system a server key manager or other device that manages the encryption key to be used by the solid state storage controller . In another embodiment the solid state storage media may have two or more partitions and the solid state storage controller behaves as though it was two or more solid state storage controllers each operating on a single partition within the solid state storage media . In this embodiment a unique media encryption key may be used with each partition.

In another embodiment the write data pipeline also includes an encryption module that encrypts a data or metadata segment received from the input buffer either directly or indirectly prior sending the data segment to the packetizer the data segment encrypted using an encryption key received in conjunction with the data segment. The encryption keys used by the encryption module to encrypt data may not be common to all data stored within the solid state storage device but may vary on an per data structure basis and received in conjunction with receiving data segments as described below. For example an encryption key for a data segment to be encrypted by the encryption module may be received with the data segment or may be received as part of a command to write a data structure to which the data segment belongs. The solid sate storage device may use and store a non secret cryptographic nonce in each data structure packet that is used in conjunction with the encryption key. A different nonce may be stored with every packet. Data segments may be split between multiple packets with unique nonces for the purpose of improving protection by the encryption algorithm.

The encryption key may be received from a computing system another computer key manager or other device that holds the encryption key to be used to encrypt the data segment. In one embodiment encryption keys are transferred to the solid state storage controller from one of a solid state storage device computing system computer or other external agent which has the ability to execute industry standard methods to securely transfer and protect private and public keys.

In one embodiment the encryption module encrypts a first packet with a first encryption key received in conjunction with the packet and encrypts a second packet with a second encryption key received in conjunction with the second packet. In another embodiment the encryption module encrypts a first packet with a first encryption key received in conjunction with the packet and passes a second data packet on to the next stage without encryption. Beneficially the encryption module included in the write data pipeline of the solid state storage device allows data structure by data structure or segment by segment data encryption without a single file system or other external system to keep track of the different encryption keys used to store corresponding data structures or data segments. Each requesting device or related key manager independently manages encryption keys used to encrypt only the data structures or data segments sent by the requesting device .

In one embodiment the encryption module may encrypt the one or more packets using an encryption key unique to the solid state storage device . The encryption module may perform this media encryption independently or in addition to the encryption described above. Typically the entire packet is encrypted including the headers. In another embodiment headers are not encrypted. The media encryption by the encryption module provides a level of security for data stored in the solid state storage media . For example where data is encrypted with media encryption unique to the specific solid state storage device if the solid state storage media is connected to a different solid state storage controller solid state storage device or computing system the contents of the solid state storage media typically could not be read without use of the same encryption key used during the write of the data to the solid state storage media without significant effort.

In another embodiment the write data pipeline includes a compression module that compresses the data for metadata segment prior to sending the data segment to the packetizer . The compression module typically compresses a data or metadata segment using a compression routine known to those of skill in the art to reduce the storage size of the segment. For example if a data segment includes a string of 512 zeros the compression module may replace the 512 zeros with code or token indicating the 512 zeros where the code is much more compact than the space taken by the 512 zeros.

In one embodiment the compression module compresses a first segment with a first compression routine and passes along a second segment without compression. In another embodiment the compression module compresses a first segment with a first compression routine and compresses the second segment with a second compression routine. Having this flexibility within the solid state storage device is beneficial so that computing systems or other devices writing data to the solid state storage device may each specify a compression routine or so that one can specify a compression routine while another specifies no compression. Selection of compression routines may also be selected according to default settings on a per data structure type or data structure class basis. For example a first data structure of a specific data structure may be able to override default compression routine settings and a second data structure of the same data structure class and data structure type may use the default compression routine and a third data structure of the same data structure class and data structure type may use no compression.

In one embodiment the write data pipeline includes a garbage collector bypass that receives data segments from the read data pipeline as part of a data bypass in a garbage collection system. A garbage collection system typically marks packets that are no longer valid typically because the packet is marked for deletion or has been modified and the modified data is stored in a different location. At some point the garbage collection system determines that a particular section of storage may be recovered. This determination may be due to a lack of available storage capacity the percentage of data marked as invalid reaching a threshold a consolidation of valid data an error detection rate for that section of storage reaching a threshold or improving performance based on data distribution etc. Numerous factors may be considered by a garbage collection algorithm to determine when a section of storage is to be recovered.

Once a section of storage has been marked for recovery valid packets in the section typically must be relocated. The garbage collector bypass allows packets to be read into the read data pipeline and then transferred directly to the write data pipeline without being routed out of the solid state storage controller . In one embodiment the garbage collector bypass is part of an autonomous garbage collector system that operates within the solid state storage device . This allows the solid state storage device to manage data so that data is systematically spread throughout the solid state storage media to improve performance data reliability and to avoid overuse and underuse of any one location or area of the solid state storage media and to lengthen the useful life of the solid state storage media .

The garbage collector bypass coordinates insertion of segments into the write data pipeline with other segments being written by computing systems or other devices. In the depicted embodiment the garbage collector bypass is before the packetizer in the write data pipeline and after the depacketizer in the read data pipeline but may also be located elsewhere in the read and write data pipelines . The garbage collector bypass may be used during a flush of the write pipeline to fill the remainder of the virtual page in order to improve the efficiency of storage within the solid state storage media and thereby reduce the frequency of garbage collection.

In one embodiment the write data pipeline includes a write buffer that buffers data for efficient write operations. Typically the write buffer includes enough capacity for packets to fill at least one virtual page in the solid state storage media . This allows a write operation to send an entire page of data to the solid state storage media without interruption. By sizing the write buffer of the write data pipeline and buffers within the read data pipeline to be the same capacity or larger than a storage write buffer within the solid state storage media writing and reading data is more efficient since a single write command may be crafted to send a full virtual page of data to the solid state storage media instead of multiple commands.

While the write buffer is being filled the solid state storage media may be used for other read operations. This is advantageous because other solid state devices with a smaller write buffer or no write buffer may tie up the solid state storage when data is written to a storage write buffer and data flowing into the storage write buffer stalls. Read operations will be blocked until the entire storage write buffer is filled and programmed. Another approach for systems without a write buffer or a small write buffer is to flush the storage write buffer that is not full in order to enable reads. Again this is inefficient because multiple write program cycles are required to fill a page.

For depicted embodiment with a write buffer sized larger than a virtual page a single write command which includes numerous subcommands can then be followed by a single program command to transfer the page of data from the storage write buffer in each solid state storage element to the designated page within each solid state storage element . This technique has the benefits of eliminating partial page programming which is known to reduce data reliability and durability and freeing up the destination bank for reads and other commands while the buffer fills.

In one embodiment the write buffer is a ping pong buffer where one side of the buffer is filled and then designated for transfer at an appropriate time while the other side of the ping pong buffer is being filled. In another embodiment the write buffer includes a first in first out FIFO register with a capacity of more than a virtual page of data segments. One of skill in the art will recognize other write buffer configurations that allow a virtual page of data to be stored prior to writing the data to the solid state storage media .

In another embodiment the write buffer is sized smaller than a virtual page so that less than a page of information could be written to a storage write buffer in the solid state storage media . In the embodiment to prevent a stall in the write data pipeline from holding up read operations data is queued using the garbage collection system that needs to be moved from one location to another as part of the garbage collection process. In case of a data stall in the write data pipeline the data can be fed through the garbage collector bypass to the write buffer and then on to the storage write buffer in the solid state storage media to fill the pages of a virtual page prior to programming the data. In this way a data stall in the write data pipeline would not stall reading from the solid state storage device .

In another embodiment the write data pipeline includes a write program module with one or more user definable functions within the write data pipeline . The write program module allows a user to customize the write data pipeline . A user may customize the write data pipeline based on a particular data requirement or application. Where the solid state storage controller is an FPGA the user may program the write data pipeline with custom commands and functions relatively easily. A user may also use the write program module to include custom functions with an ASIC however customizing an ASIC may be more difficult than with an FPGA. The write program module may include buffers and bypass mechanisms to allow a first data segment to execute in the write program module while a second data segment may continue through the write data pipeline . In another embodiment the write program module may include a processor core that can be programmed through software.

Note that the write program module is shown between the input buffer and the compression module however the write program module could be anywhere in the write data pipeline and may be distributed among the various stages . In addition there may be multiple write program modules distributed among the various states that are programmed and operate independently. In addition the order of the stages may be altered. One of skill in the art will recognize workable alterations to the order of the stages based on particular user requirements.

The read data pipeline includes an ECC correction module that determines if a data error exists in ECC blocks a requested packet received from the solid state storage media by using ECC stored with each ECC block of the requested packet. The ECC correction module then corrects any errors in the requested packet if any error exists and the errors are correctable using the ECC. For example if the ECC can detect an error in six bits but can only correct three bit errors the ECC correction module corrects ECC blocks of the requested packet with up to three bits in error. The ECC correction module corrects the bits in error by changing the bits in error to the correct one or zero state so that the requested data packet is identical to when it was written to the solid state storage media and the ECC was generated for the packet.

If the ECC correction module determines that the requested packets contains more bits in error than the ECC can correct the ECC correction module cannot correct the errors in the corrupted ECC blocks of the requested packet and sends an interrupt. In one embodiment the ECC correction module sends an interrupt with a message indicating that the requested packet is in error. The message may include information that the ECC correction module cannot correct the errors or the inability of the ECC correction module to correct the errors may be implied. In another embodiment the ECC correction module sends the corrupted ECC blocks of the requested packet with the interrupt and or the message.

In one embodiment a corrupted ECC block or portion of a corrupted ECC block of the requested packet that cannot be corrected by the ECC correction module is read by the master controller corrected and returned to the ECC correction module for further processing by the read data pipeline . In one embodiment a corrupted ECC block or portion of a corrupted ECC block of the requested packet is sent to the device requesting the data. The requesting device may correct the ECC block or replace the data using another copy such as a backup or mirror copy and then may use the replacement data of the requested data packet or return it to the read data pipeline . The requesting device may use header information in the requested packet in error to identify data required to replace the corrupted requested packet or to replace the data structure to which the packet belongs. In another embodiment the solid state storage controller stores data using some type of RAID and is able to recover the corrupted data. In another embodiment the ECC correction module sends an interrupt and or message and the receiving device fails the read operation associated with the requested data packet. One of skill in the art will recognize other options and actions to be taken as a result of the ECC correction module determining that one or more ECC blocks of the requested packet are corrupted and that the ECC correction module cannot correct the errors.

The read data pipeline includes a depacketizer that receives ECC blocks of the requested packet from the ECC correction module directly or indirectly and checks and removes one or more packet headers. The depacketizer may validate the packet headers by checking packet identifiers data length data location etc. within the headers. In one embodiment the header includes a hash code that can be used to validate that the packet delivered to the read data pipeline is the requested packet. The depacketizer also removes the headers from the requested packet added by the packetizer . The depacketizer may directed to not operate on certain packets but pass these forward without modification. An example might be a container label that is requested during the course of a rebuild process where the header information is required for index reconstruction. Further examples include the transfer of packets of various types destined for use within the solid state storage device . In another embodiment the depacketizer operation may be packet type dependent.

The read data pipeline includes an alignment module that receives data from the depacketizer and removes unwanted data. In one embodiment a read command sent to the solid state storage media retrieves a packet of data. A device requesting the data may not require all data within the retrieved packet and the alignment module removes the unwanted data. If all data within a retrieved page is requested data the alignment module does not remove any data.

The alignment module re formats the data as data segments of a data structure in a form compatible with a device requesting the data segment prior to forwarding the data segment to the next stage. Typically as data is processed by the read data pipeline the size of data segments or packets changes at various stages. The alignment module uses received data to format the data into data segments suitable to be sent to the requesting device and joined to form a response. For example data from a portion of a first data packet may be combined with data from a portion of a second data packet. If a data segment is larger than a data requested by the requesting device the alignment module may discard the unwanted data.

In one embodiment the read data pipeline includes a read synchronization buffer that buffers one or more requested packets read from the solid state storage media prior to processing by the read data pipeline . The read synchronization buffer is at the boundary between the solid state storage clock domain and the local bus clock domain and provides buffering to account for the clock domain differences.

In another embodiment the read data pipeline includes an output buffer that receives requested packets from the alignment module and stores the packets prior to transmission to the requesting device . The output buffer accounts for differences between when data segments are received from stages of the read data pipeline and when the data segments are transmitted to other parts of the solid state storage controller or to the requesting device . The output buffer also allows the data bus to receive data from the read data pipeline at rates greater than can be sustained by the read data pipeline in order to improve efficiency of operation of the data bus .

In one embodiment the read data pipeline includes a media decryption module that receives one or more encrypted requested packets from the ECC correction module and decrypts the one or more requested packets using the encryption key unique to the solid state storage device prior to sending the one or more requested packets to the depacketizer . Typically the encryption key used to decrypt data by the media decryption module is identical to the encryption key used by the media encryption module . In another embodiment the solid state storage media may have two or more partitions and the solid state storage controller behaves as though it was two or more solid state storage controllers each operating on a single partition within the solid state storage media . In this embodiment a unique media encryption key may be used with each partition.

In another embodiment the read data pipeline includes a decryption module that decrypts a data segment formatted by the depacketizer prior to sending the data segment to the output buffer . The data segment may be decrypted using an encryption key received in conjunction with the read request that initiates retrieval of the requested packet received by the read synchronization buffer . The decryption module may decrypt a first packet with an encryption key received in conjunction with the read request for the first packet and then may decrypt a second packet with a different encryption key or may pass the second packet on to the next stage of the read data pipeline without decryption. When the packet was stored with a non secret cryptographic nonce the nonce is used in conjunction with an encryption key to decrypt the data packet. The encryption key may be received from a computing system a client key manager or other device that manages the encryption key to be used by the solid state storage controller .

In another embodiment the read data pipeline includes a decompression module that decompresses a data segment formatted by the depacketizer . In one embodiment the decompression module uses compression information stored in one or both of the packet header and the container label to select a complementary routine to that used to compress the data by the compression module . In another embodiment the decompression routine used by the decompression module is dictated by the device requesting the data segment being decompressed. In another embodiment the decompression module selects a decompression routine according to default settings on a per data structure type or data structure class basis. A first packet of a first object may be able to override a default decompression routine and a second packet of a second data structure of the same data structure class and data structure type may use the default decompression routine and a third packet of a third data structure of the same data structure class and data structure type may use no decompression.

In another embodiment the read data pipeline includes a read program module that includes one or more user definable functions within the read data pipeline . The read program module has similar characteristics to the write program module and allows a user to provide custom functions to the read data pipeline . The read program module may be located as shown in may be located in another position within the read data pipeline or may include multiple parts in multiple locations within the read data pipeline . Additionally there may be multiple read program modules within multiple locations within the read data pipeline that operate independently. One of skill in the art will recognize other forms of a read program module within a read data pipeline . As with the write data pipeline the stages of the read data pipeline may be rearranged and one of skill in the art will recognize other orders of stages within the read data pipeline .

The solid state storage controller includes control and status registers and corresponding control queues . The control and status registers and control queues facilitate control and sequencing commands and subcommands associated with data processed in the write and read data pipelines . For example a data segment in the packetizer may have one or more corresponding control commands or instructions in a control queue associated with the ECC generator . As the data segment is packetized some of the instructions or commands may be executed within the packetizer . Other commands or instructions may be passed to the next control queue through the control and status registers as the newly formed data packet created from the data segment is passed to the next stage.

Commands or instructions may be simultaneously loaded into the control queues for a packet being forwarded to the write data pipeline with each pipeline stage pulling the appropriate command or instruction as the respective packet is executed by that stage. Similarly commands or instructions may be simultaneously loaded into the control queues for a packet being requested from the read data pipeline with each pipeline stage pulling the appropriate command or instruction as the respective packet is executed by that stage. One of skill in the art will recognize other features and functions of control and status registers and control queues .

The solid state storage controller and or solid state storage device may also include a bank interleave controller a synchronization buffer a storage bus controller and a multiplexer MUX which are described in relation to .

The bank interleave controller directs one or more commands to two or more queues in the bank interleave controller and coordinates among the banks of the solid state storage media execution of the commands stored in the queues such that a command of a first type executes on one bank while a command of a second type executes on a second bank . The one or more commands are separated by command type into the queues. Each bank of the solid state storage media has a corresponding set of queues within the bank interleave controller and each set of queues includes a queue for each command type.

The bank interleave controller coordinates among the banks of the solid state storage media execution of the commands stored in the queues. For example a command of a first type executes on one bank while a command of a second type executes on a second bank . Typically the command types and queue types include read and write commands and queues but may also include other commands and queues that are storage media specific. For example in the embodiment depicted in erase and management queues are included and would be appropriate for flash memory NRAM MRAM DRAM PRAM etc.

For other types of solid state storage media other types of commands and corresponding queues may be included without straying from the scope of the invention. The flexible nature of an FPGA solid state storage controller allows flexibility in storage media. If flash memory were changed to another solid state storage type the bank interleave controller storage bus controller and MUX could be altered to accommodate the media type without significantly affecting the data pipelines and other solid state storage controller functions.

In the embodiment depicted in the bank interleave controller includes for each bank a read queue for reading data from the solid state storage media a write queue for write commands to the solid state storage media an erase queue for erasing an erase block in the solid state storage an a management queue for management commands. The bank interleave controller also includes corresponding read write erase and management agents . In another embodiment the control and status registers and control queues or similar components queue commands for data sent to the banks of the solid state storage media without a bank interleave controller .

The agents in one embodiment direct commands of the appropriate type destined for a particular bank to the correct queue for the bank . For example the read agent may receive a read command for bank and directs the read command to the bank read queue . The write agent may receive a write command to write data to a location in bank of the solid state storage media and will then send the write command to the bank write queue . Similarly the erase agent may receive an erase command to erase an erase block in bank and will then pass the erase command to the bank erase queue . The management agent typically receives management commands status requests and the like such as a reset command or a request to read a configuration register of a bank such as bank . The management agent sends the management command to the bank management queue

The agents typically also monitor status of the queues and send status interrupt or other messages when the queues are full nearly full non functional etc. In one embodiment the agents receive commands and generate corresponding sub commands. In one embodiment the agents receive commands through the control status registers and generate corresponding sub commands which are forwarded to the queues . One of skill in the art will recognize other functions of the agents .

The queues typically receive commands and store the commands until required to be sent to the solid state storage banks . In a typical embodiment the queues are first in first out FIFO registers or a similar component that operates as a FIFO. In another embodiment the queues store commands in an order that matches data order of importance or other criteria.

The bank controllers typically receive commands from the queues and generate appropriate subcommands. For example the bank write queue may receive a command to write a page of data packets to bank . The bank controller may receive the write command at an appropriate time and may generate one or more write subcommands for each data packet stored in the write buffer to be written to the page in bank . For example bank controller may generate commands to validate the status of bank and the solid state storage array select the appropriate location for writing one or more data packets clear the input buffers within the solid state storage memory array transfer the one or more data packets to the input buffers program the input buffers into the selected location verify that the data was correctly programmed and if program failures occur do one or more of interrupting the master controller retrying the write to the same physical location and retrying the write to a different physical location. Additionally in conjunction with example write command the storage bus controller will cause the one or more commands to multiplied to each of the each of the storage I O buses with the logical address of the command mapped to a first physical addresses for storage I O bus and mapped to a second physical address for storage I O bus and so forth as further described below.

Typically bus arbiter selects from among the bank controllers and pulls subcommands from output queues within the bank controllers and forwards these to the Storage Bus Controller in a sequence that optimizes the performance of the banks . In another embodiment the bus arbiter may respond to a high level interrupt and modify the normal selection criteria. In another embodiment the master controller can control the bus arbiter through the control and status registers . One of skill in the art will recognize other means by which the bus arbiter may control and interleave the sequence of commands from the bank controllers to the solid state storage media .

The bus arbiter typically coordinates selection of appropriate commands and corresponding data when required for the command type from the bank controllers and sends the commands and data to the storage bus controller . The bus arbiter typically also sends commands to the storage control bus to select the appropriate bank . For the case of flash memory or other solid state storage media with an asynchronous bi directional serial storage I O bus only one command control information or set of data can be transmitted at a time. For example when write commands or data are being transmitted to the solid state storage media on the storage I O bus read commands data being read erase commands management commands or other status commands cannot be transmitted on the storage I O bus . For example when data is being read from the storage I O bus data cannot be written to the solid state storage media .

For example during a write operation on bank the bus arbiter selects the bank controller which may have a write command or a series of write sub commands on the top of its queue which cause the storage bus controller to execute the following sequence. The bus arbiter forwards the write command to the storage bus controller which sets up a write command by selecting bank through the storage control bus sending a command to clear the input buffers of the solid state storage elements associated with the bank and sending a command to validate the status of the solid state storage elements associated with the bank . The storage bus controller then transmits a write subcommand on the storage I O bus which contains the physical addresses including the address of the logical erase block for each individual physical erase solid stage storage element as mapped from the logical erase block address. The storage bus controller then muxes the write buffer through the write synchronization buffer to the storage I O bus through the MUX and streams write data to the appropriate page. When the page is full then storage bus controller causes the solid state storage elements associated with the bank to program the input buffer to the memory cells within the solid state storage elements . Finally the storage bus controller validates the status to ensure that page was correctly programmed.

A read operation is similar to the write example above. During a read operation typically the bus arbiter or other component of the bank interleave controller receives data and corresponding status information and sends the data to the read data pipeline while sending the status information on to the control and status registers . Typically a read data command forwarded from bus arbiter to the storage bus controller will cause the MUX to gate the read data on storage I O bus to the read data pipeline and send status information to the appropriate control and status registers through the status MUX .

The bus arbiter coordinates the various command types and data access modes so that only an appropriate command type or corresponding data is on the bus at any given time. If the bus arbiter has selected a write command and write subcommands and corresponding data are being written to the solid state storage media the bus arbiter will not allow other command types on the storage I O bus . Beneficially the bus arbiter uses timing information such as predicted command execution times along with status information received concerning bank status to coordinate execution of the various commands on the bus with the goal of minimizing or eliminating idle time of the busses.

The master controller through the bus arbiter typically uses expected completion times of the commands stored in the queues along with status information so that when the subcommands associated with a command are executing on one bank other subcommands of other commands are executing on other banks . When one command is fully executed on a bank the bus arbiter directs another command to the bank . The bus arbiter may also coordinate commands stored in the queues with other commands that are not stored in the queues .

For example an erase command may be sent out to erase a group of erase blocks within the solid state storage media . An erase command may take 10 to 1000 times more time to execute than a write or a read command or 10 to 100 times more time to execute than a program command. For N banks the bank interleave controller may split the erase command into N commands each to erase a virtual erase block of a bank . While Bank is executing an erase command the bus arbiter may select other commands for execution on the other banks . The bus arbiter may also work with other components such as the storage bus controller the master controller etc. to coordinate command execution among the buses. Coordinating execution of commands using the bus arbiter bank controllers queues and agents of the bank interleave controller can dramatically increase performance over other solid state storage systems without a bank interleave function.

In one embodiment the solid state controller includes one bank interleave controller that serves all of the storage elements of the solid state storage media . In another embodiment the solid state controller includes a bank interleave controller for each column of storage elements . For example one bank interleave controller serves one column of storage elements SSS . SSS N. . . . a second bank interleave controller serves a second column of storage elements SSS . SSS N. . . . etc.

The solid state storage controller includes a synchronization buffer that buffers commands and status messages sent and received from the solid state storage media . The synchronization buffer is located at the boundary between the solid state storage clock domain and the local bus clock domain and provides buffering to account for the clock domain differences. The synchronization buffer write synchronization buffer and read synchronization buffer may be independent or may act together to buffer data commands status messages etc. In one embodiment the synchronization buffer is located where there are the fewest number of signals crossing the clock domains. One skilled in the art will recognize that synchronization between clock domains may be arbitrarily moved to other locations within the solid state storage device in order to optimize some aspect of design implementation.

The solid state storage controller includes a storage bus controller that interprets and translates commands for data sent to and read from the solid state storage media and status messages received from the solid state storage media based on the type of solid state storage media . For example the storage bus controller may have different timing requirements for different types of storage storage with different performance characteristics storage from different manufacturers etc. The storage bus controller also sends control commands to the storage control bus .

In one embodiment the solid state storage controller includes a MUX that comprises an array of multiplexers where each multiplexer is dedicated to a row in the solid state storage array . For example multiplexer is associated with solid state storage elements . MUX routes the data from the write data pipeline and commands from the storage bus controller to the solid state storage media via the storage I O bus and routes data and status messages from the solid state storage media via the storage I O bus to the read data pipeline and the control and status registers through the storage bus controller synchronization buffer and bank interleave controller .

In one embodiment the solid state storage controller includes a MUX for each column of solid state storage elements e.g. SSS . SSS . SSS N. . A MUX combines data from the write data pipeline and commands sent to the solid state storage media via the storage I O bus and separates data to be processed by the read data pipeline from commands. Packets stored in the write buffer are directed on busses out of the write buffer through a write synchronization buffer for each column of solid state storage elements SSS .to SSS N.x to the MUX for each column of solid state storage elements SSS .to SSS N.x . The commands and read data are received by the MUXes from the storage I O bus . The MUXes also direct status messages to the storage bus controller .

The storage bus controller includes a mapping module . The mapping module maps a logical address of an erase block to one or more physical addresses of an erase block. For example a solid state storage media with an array of twenty storage elements e.g. SSS . to SSS .M per bank may have a logical address for a particular erase block mapped to twenty physical addresses of the erase block one physical address per storage element. Because the storage elements are accessed in parallel erase blocks at the same position in each storage element in a column of storage elements will share a physical address. To select one erase block e.g. in storage element SSS . instead of all erase blocks in the row e.g. in storage elements SSS . . . . . N. one bank in this case Bank is selected.

This logical to physical mapping for erase blocks is beneficial because if one erase block becomes damaged or inaccessible the mapping can be changed to map to another erase block. This mitigates the loss of losing an entire virtual erase block when one element s erase block is faulty. The remapping module changes a mapping of a logical address of an erase block to one or more physical addresses of a virtual erase block spread over the array of storage elements . For example virtual erase block may be mapped to erase block of storage element SSS . to erase block of storage element SSS . . . . and to storage element .M virtual erase block may be mapped to erase block of storage element SSS . to erase block of storage element SSS . . . . and to storage element .M etc. Alternatively virtual erase block may be mapped to one erase block from each storage element in an array such that virtual erase block includes erase block of storage element SSS . to erase block of storage element SSS . to storage element .M and erase block of storage element SSS . to erase block of storage element SSS . . . . and to storage element .M for each storage element in the array up to erase block of storage element N.M

If erase block of a storage element SSS . is damaged experiencing errors due to wear etc. or cannot be used for some reason the remapping module could change the logical to physical mapping for the logical address that pointed to erase block of virtual erase block . If a spare erase block call it erase block of storage element SSS . is available and currently not mapped the remapping module could change the mapping of virtual erase block to point to erase block of storage element SSS . while continuing to point to erase block of storage element SSS . erase block of storage element SSS . not shown . . . and to storage element .M . The mapping module or remapping module could map erase blocks in a prescribed order virtual erase block to erase block of the storage elements virtual erase block to erase block of the storage elements etc. or may map erase blocks of the storage elements in another order based on some other criteria.

In one embodiment the erase blocks could be grouped by access time. Grouping by access time meaning time to execute a command such as programming writing data into pages of specific erase blocks can level command completion so that a command executed across the erase blocks of a virtual erase block is not limited by the slowest erase block. In other embodiments the erase blocks may be grouped by wear level health etc. One of skill in the art will recognize other factors to consider when mapping or remapping erase blocks.

In one embodiment the storage bus controller includes a status capture module that receives status messages from the solid state storage media and sends the status messages to the status MUX . In another embodiment when the solid state storage media is flash memory the storage bus controller includes a NAND bus controller . The NAND bus controller directs commands from the read and write data pipelines to the correct location in the solid state storage media coordinates timing of command execution based on characteristics of the flash memory etc. If the solid state storage media is another solid state storage type the NAND bus controller would be replaced by a bus controller specific to the storage type. One of skill in the art will recognize other functions of a NAND bus controller .

While the power management apparatuses in the depicted embodiment are illustrated as part of the storage device and the components in other embodiments as described above with regard to the power management apparatuses may be implemented as software drivers of the computing system as a combination of software drivers and firmware hardware controllers of the storage device and the components or the like. In one embodiment the computing system loads separate software drivers for each power management apparatus 

The power supply in the depicted embodiment provides electric power to the computing system and through it the attached storage device and components . In certain embodiments the computing system distributes power to various components such as the storage device and the components shown. The power may be distributed via a number of different approaches in one embodiment the storage device and the components connect to a motherboard which provides the necessary power for the computing system . In other embodiments the storage device and or one or more of the components are remote from the computing system and network attached.

The components provide computing capabilities for the computing system . The components may be additional storage devices such as Flash memory hard disk drives optical drives or the like graphics processors network cards or other computer components. Those of skill in the art will appreciate the different types of components that may be in a computing system . The storage device and the components may be internal or external to the computing system . In one embodiment the components are PCI or PCI e cards that connect to the computing system . The components may receive power through the computing system .

In certain embodiments the power management apparatuses may dynamically adjust power allocations for the storage device and the components . The computing system and or the power management apparatuses may dedicate more or less power to a particular component or the storage device at a particular time based on power consumption rates of other components and of the storage device . The computing system and or the power management apparatuses may thus dynamically change the allocation of power and send appropriate parameters to the storage device and the components 

In certain embodiments the power management apparatuses monitor the activity of the storage device and the components to determine whether or not the storage device and the components are active. If a particular component or the storage device is inactive the power management apparatus for that component or storage device may inform the other power management apparatuses that the component or storage device is inactive. For example if a component that is a wireless card is not currently connected to a wireless network or if the wireless connection and or card is powered off the power management apparatus on that component may inform the other power management apparatuses and correspondingly increase the power consumption target for any of the other active components and or the storage device .

In the depicted embodiment the power management apparatuses are arranged in a master slave relationship with the power management apparatus of the storage device as the master. In other embodiments the system may include a single system power management apparatus that manages power consumption rates for the storage device and the components the power management apparatuses may operate independently one of the component power management apparatuses may serve as a master or the like.

In one embodiment the slave power management apparatuses follow adjustments that the master power management apparatus makes. For example in one embodiment instead of monitoring power consumption rates for the storage device and each of the components the master power management apparatus monitors the power consumption rate of a single component the storage device in the depicted embodiment and the power management apparatuses each adjust operations on the storage device and the components based on the power consumption rate of the single component.

In certain embodiments adjusting operations of the storage device and the components similarly in parallel may be more efficient than managing them separately. For example in an embodiment where the storage device and the components comprise a storage array that is accessed in parallel it may be inefficient to throttle or slow operations on a single storage device without throttling or slowing operations on the other components . In the example embodiment power consumption rates may also be similar in the storage device and the components due to parallel accesses which also makes parallel adjustments efficient by allowing the power management apparatus to monitor a single power consumption rate.

The master power management apparatus in various embodiments may communicate an adjustment to be made a power consumption rate a power consumption target and or other information to the slave power management apparatuses . In another embodiment instead of basing adjustments on a static master component or master power management apparatus the power management apparatuses dynamically adjust operations based on any one of the power consumption rates failing to satisfy a power consumption target.

In one embodiment either a master power management apparatus or a single system power management apparatus not shown dynamically adjusts operations on the storage device and the components based on a system power consumption target for the storage device and the components so that a total power consumption rate satisfies the system power consumption target. In one embodiment using a system power consumption target allows the power management apparatus to dynamically allocate and balance power from the power supply between the storage device and the components allowing greater power use for certain components executing power intensive operations by lowering a power allocation of other components or the like.

In another embodiment instead of having a master slave arrangement the power management apparatuses are arranged as peers. The power management apparatuses in one embodiment communicate information such as power consumption rates power consumption targets operation adjustments and or other information to other power management apparatuses . In a further embodiment the power management apparatuses adjust operations adjust a power consumption target or the like based on power consumption rates reported by peer power management apparatuses . In a further embodiment a power management apparatus may request an additional allocation of power from a master power management apparatus a peer power management apparatus or the like.

In one embodiment the power management apparatuses manage thermal states of the storage device and the components . The power management apparatuses in various embodiments may manage the thermal states substantially as described above with regard to power consumption as master slave as peers or the like. For example in various embodiments a single system power management apparatus may manage the thermal states for the storage device and the components one power management apparatus may be selected as a master while other power management apparatus may be selected as slaves for thermal management the power management apparatuses may be arranged as peers for thermal management purposes or the like.

In one embodiment the power management apparatuses or a single system power management apparatus may monitor a single temperature for the storage device and the components instead of monitoring separate individual temperatures. The single temperature or system temperature in one embodiment may be a temperature of an enclosure that houses the storage device and the components . In another embodiment the single temperature may be the temperature of a single one of the storage device and the components . In a further embodiment the power management apparatuses or a single system power management apparatus monitor individual temperatures for each of the storage device and the components . The power management apparatuses or a single system power management apparatus in various embodiments may manage the thermal states adjusting execution of operations etc. of each of the storage device and the components individually in parallel or some combination of the two.

In a further embodiment the power management apparatuses manage both power consumption and thermal states of the storage device and the components . In one embodiment the power management apparatuses combine adjustments made for power consumption with adjustments made for thermal management into a single adjustment to operations for each of the storage device and the components . For example in one embodiment adjusting execution of operations for the storage device and or for the components may serve a dual purpose of adjusting power consumption which in certain embodiments may also have a direct correlation to reducing thermal heating.

In one embodiment a master power management apparatus determines a system adjustment to make for power consumption and each power management apparatus determines an individual thermal adjustment and separately combines the individual thermal adjustment with the system adjustment into a single independent adjustment for each respective storage device and component . In such an embodiment the power management apparatuses make adjustments for power consumption substantially identically while making adjustments for thermal management independently.

The power management apparatuses in one embodiment combine power consumption adjustments and thermal adjustments cumulatively summing the adjustments. In other embodiments the power management apparatuses may combine power consumption adjustments and thermal adjustments by selecting the largest adjustment for use factoring in previous adjustments and or otherwise combining the adjustments.

In general the audit module monitors a power consumption rate of a component such as the storage device relative to a power consumption target the throttle module adjusts operations on the component in response to the power consumption rate failing to satisfy the power consumption target and the verification module verifies whether the power consumption rate of the component satisfies the power consumption target in response to adjusting the execution of operations on the component . In a further embodiment the audit module monitors a temperature for a component such as the storage device relative to a thermal ceiling or other thermal target the throttle module adjusts execution of operations on the component in response to the temperature approaching the thermal ceiling and the verification module verifies whether the temperature is moving away from the thermal ceiling in response to adjusting the execution of the operations on the component .

The audit module in one embodiment monitors a power consumption rate of the associated component relative to a power consumption target. In a further embodiment the audit module monitors a temperature for the associated component relative to a thermal target such as a thermal ceiling or the like. Determining a power consumption target and or a thermal target is described in greater detail below with regard to the target module of . In one embodiment the audit module determines whether or not the power consumption rate satisfies the power consumption threshold. In a further embodiment the audit module determines whether the temperature satisfies the thermal target.

The audit module in one embodiment receives power consumption and or temperature information for the component from one or more sensors such as voltage sensors current sensors temperature sensors and the like. The audit module in one embodiment determines a power consumption rate using a known or estimated voltage and a reading from a current sensor. For example protocols such as PCI e Mini PCI e and the like often provide relatively stable voltage levels at 12V 3.3V 1.5V and the like. The audit module in one embodiment uses provided voltage levels and measured current levels to determine a power consumption rate for the component .

In one embodiment the audit module monitors a power consumption rate that is an instantaneous power consumption rate based on current power usage of the corresponding component . In a further embodiment the audit module monitors a power consumption rate that is an average of power consumption over time such as a decaying average or the like. In one embodiment a period of the average power consumption for the power consumption rate is selected to smooth spikes in power usage.

The audit module in one embodiment monitors a temperature for the corresponding component based on a reading from a temperature sensor such as a thermometer. In one embodiment a controller processor FPGA or other hardware of the component includes a temperature sensor and the audit module accesses temperature readings from the hardware of the component . In a further embodiment the component includes a separate temperature sensor that provides the audit module with a temperature for the component . Other embodiments of the audit module are described below with regard to .

The throttle module in one embodiment adjusts execution of operations on the corresponding component in response to the power consumption rate for the component failing to satisfy the power consumption target. As described above in various embodiments a power consumption rate fails to satisfy a power consumption target by approaching a power consumption ceiling defined by a power consumption target exceeding a power consumption target or threshold set below a power consumption ceiling being outside a range of allowable power consumption defined by a power consumption target or the like.

In one embodiment the throttle module adjusts operations by throttling or slowing down operations on the component to lower the power consumption rate of the component . In a further embodiment the throttle module adjusts operations by scheduling the operations so that power consumed by concurrently executed operations satisfies the power consumption target. Other ways that the throttle module may adjust operations are described below with regard to .

The throttle module in one embodiment adjusts operations on the corresponding component by reducing a frequency with which the operations are executed on the component . In certain embodiments the throttle module reduces the frequency of operations by reducing a clock rate clock speed or clock frequency of a synchronous circuit of the component . In one embodiment the throttle module reduces the frequency of operations by setting a timer value specifying a frequency with which operations are submitted to the component for execution. In another embodiment the throttle module reduces the frequency of operations by setting a timer value specifying a frequency with which hardware of the component polls to verify that an operation has completed.

For example in one embodiment where the corresponding component includes a nonvolatile solid state storage device such as the storage device the throttle module may adjust a tPROG time for the component and increasing the tPROG time reduces the frequency of program operations for the component . For nonvolatile solid state storage such as Flash memory the tPROG time is the period of time between submitting a program command to a memory area such as a die a bank a physical or logical page or the like and submitting a subsequent command to the same memory area. In certain embodiments the subsequent command may be a status command such as a READ STATUS command or the like to verify that the program command has completed was successful or the like.

In a further embodiment the power consumption target specifies a number of quanta or units that define an allowable amount of energy or power that the component may use during a period of time. Operations on the component in one embodiment may also be assigned various amounts of quanta or units based on the amount of energy or power used to perform each operation. The throttle module in one embodiment adjusts execution of operations on the component by scheduling the operations such that the quanta or units associated with concurrently executing operations satisfies the number of quanta or units permitted to satisfy the power consumption target. Assigning quanta to operations and scheduling the operations based on the quanta to satisfy a power consumption target is discussed in greater detail below with regard to .

In another embodiment as described in greater detail below with regard to the throttle module adjusts execution of the operations on the component by selecting operations from one or more queues of pending operations so that the power or energy used by concurrently executed operations satisfies the power consumption target. The throttle module in certain embodiments may adjust certain types of operations without adjusting other types of operations may adjust different types of operations in different manners or may otherwise dynamically or selectively adjust operations by type.

For example in one embodiment the throttle module may adjust write or program operations but not erase operations or vice versa. In another example embodiment the throttle module may adjust write or program operations differently than erase operations. In one embodiment the throttle module may ignore certain operations such as read operations or the like and may not adjust them at all. In a further example embodiment the throttle module may reduce a frequency of write or program operations by setting a timer adjusting a tPROG time or the like and may schedule erase operations to satisfy a power consumption target. Further embodiments of the throttle module are described below with regard to .

The verification module in one embodiment verifies whether the power consumption rate of the corresponding component satisfies the power consumption target in response to the throttle module adjusting execution of operations on the component . In another embodiment the verification module verifies whether the temperature of the component satisfies the thermal target is moving away from the thermal ceiling or the like in response to the throttle module adjusting execution of operations on the component .

The verification module in one embodiment is substantially similar to is integrated with is in communication with cooperates with and or performs similar functions to the audit module . For example in one embodiment the verification module may be part of or integrated with the audit module and may compare a power consumption rate from prior to the adjustment of operations to a power consumption rate from after the adjustment of operations. In a further embodiment the verification module may compare a power consumption rate directly to the power consumption target to determine whether the power consumption rate satisfies the power consumption target. In one embodiment the verification module may verify that a temperature for the component satisfies a thermal target in a similar manner.

In one embodiment the verification module waits a predefined period of time after the throttle module adjusts the operations before verifying that the power consumption rate and or the temperature of the component satisfies the corresponding target. For example in various embodiments the predefined period of time may be selected to account for thermal inertia throughput of the adjustments or the like. In one embodiment if the verification module determines that the power consumption rate fails to satisfy the power consumption target the throttle module readjusts execution of subsequent operations on the component . Similarly in a further embodiment the throttle module may readjust execution of subsequent operations on the component in response to the verification module determining that the temperature fails to satisfy the thermal target.

The power management apparatus in one embodiment is arranged in a system of a plurality of power management apparatuses as described above with regard to . Depending on whether the system is arranged with a single system power management apparatus with master slave power management apparatuses with peer power management apparatuses or in another arrangement such a system may include one or more audit modules one or more throttle modules and or one or more verification module which may function as described above with regard to .

For example in one embodiment each power management apparatus may include an audit module a throttle module and a verification module . In a further embodiment just a master or system power management apparatus may include an audit module a throttle module and a verification module . In another embodiment a master power management apparatus may include an audit module a throttle module and a verification module and one or more slave power management apparatuses may include just a throttle module or the like. In view of this disclosure one of skill in the art will recognize other arrangements and combinations of audit modules throttle modules and verification modules suitable for use in a system with a plurality of components as described above with regard to .

In one embodiment the target module determines an appropriate power consumption target for the component that the power management apparatus is responsible for. As used herein references to a component include the storage device which is a specific example of a component . The power consumption target sets the power limits for the component . In one embodiment the power consumption target may be expressed in watts. The power consumption target may be a power consumption ceiling defining a maximum amount of power for the component a power consumption threshold set below a power consumption ceiling an average amount of power for the component with allowances for deviation during burst operations a range of allowable power consumptions levels for the component or another manner of expressing the power limits for the component . Those of skill in the art will appreciate the other ways in which the power consumption target may be expressed in view of this disclosure.

In another embodiment the target module determines an appropriate thermal target for the corresponding component . The target module in one embodiment may determine the thermal target in substantially the same manner as the power consumption target. The thermal target in one embodiment sets temperature limits for the component . The thermal target in various embodiments may be expressed as degrees Celsius degrees Fahrenheit Kelvin units or the like. As described above with regard to the power consumption target the thermal target in various embodiments may be a thermal ceiling defining a maximum allowable temperature or thermal rating for the component a thermal threshold set below a thermal ceiling an average temperature for the component with allowances for deviation a range of allowable temperatures for the component or another manner of expressing temperature limits for the component .

In one embodiment the thermal target includes a thermal ceiling and a thermal threshold below the thermal ceiling to account for thermal inertia. In a further embodiment if the temperature for the component approaches the thermal ceiling by exceeding or passing the thermal threshold the temperature fails to satisfy the thermal target and the throttle module adjusts operations on the component . In one embodiment setting a thermal threshold below a thermal ceiling provides the power management apparatus time to account for thermal inertia so that a rate of change of the temperature slows and reverses before the temperature for the component hits and or exceeds the thermal ceiling.

For example in one embodiment a thermal rating for a consumer grade component may be around about 85 degrees Celsius or for an industrial grade component around about 100 degrees Celsius. In one embodiment the thermal ceiling for the component is set at the thermal rating for the component . A thermal threshold in various embodiments may be set at a number of degrees below the thermal ceiling such as about 1 to 15 degrees below the thermal ceiling. In one embodiment a thermal threshold is set at about 7 degrees below the thermal ceiling to account for thermal inertia. In the example embodiments the thermal threshold for the consumer grade component may be set at around 78 degrees Celsius and the thermal threshold for the industrial grade component may be set at around 93 degrees Celsius or the like.

In one embodiment the power consumption target and or the thermal target is communicated to a software driver by a user another module another power management apparatus or the like and the software driver sends the power consumption target and or the thermal target to a physical card or device on which the component is realized. The target module may be part of the software driver part of the physical card or device or both. While the power consumption target is one parameter that the target module may determine the target module may also determine other parameters for example the target module may determine parameters specifying allowable burst limits and times power ramping limits the thermal target and others.

In one embodiment the target module retrieves or otherwise determines the power consumption target and or other parameters. These parameters may be stored in nonvolatile storage within the power management apparatus in RAM for a computing system or on a persistent storage device such as a parameter file in a file system. For example in certain embodiments the power consumption target is coded into the power management apparatus as a hardware defined value software defined value firmware defined value and or some combination thereof. The power consumption target and other parameters may be retrieved by the target module or retrieved by the target module during start up. In certain embodiments updates may be made to the firmware or software on the power management apparatus to change the power consumption target and other parameters directly. In such embodiments the computing system may not be aware or have a need to be aware of the power management activities on the components 

In certain embodiments the target module may receive initial parameters from the BIOS of the computing system when the computing system is starting up. In one embodiment the BIOS may provide a set of parameters that puts the component into a mode that provides lower performance but is a safe setting as that term is commonly used in the computing industry. The computing system may update revise the set of parameters at a later point in the initialization process.

In embodiments where the computing system is aware of the power management apparatuses on the components the computing system may communicate with the power management apparatuses . The target module may receive the power consumption target and other parameters from the system itself or it may receive the power consumption target and other parameters from other components that are aware of the power management apparatus .

In certain embodiments the power management apparatuses may share and communicate information with one another. The information may be shared in accordance with a protocol as described below. The power management apparatus may have a master module described below to facilitate sharing this information. The power management apparatus in the system may also be configured to act as a single logical power management apparatus . Such embodiments may allow certain components to be given more power at a particular time by lowering the power allotment for other components in the system. As noted above this power allotment may be dynamic.

The power management apparatus may respond to direction from a user indicating that the functions of one particular component are more important than those of another. For example a user may indicate that establishing a strong wireless connection is more important than storage performance at a particular time. Thus a component that is a wireless card may be allowed to operate at full power while the power allotted to the component that is a storage device such as the storage device is decreased. In other embodiments the dynamic reallocation of power may be initiated by other components or processes transparently to the user. For example a particular solid state storage device may be falling behind in reclaiming storage space as part of garbage collection due to the restrictions on power. The power management apparatus may allocate more power to that solid state storage device for a period of time to allow it to catch up while decreasing the power allocation to other components during that period.

In one embodiment the throttle module uses the operations module to adjust operations for the component . The operations module in one embodiment associates the power consumption target and other parameters received by the target module with performance limits on the component . Performance limits in one embodiment include the adjustments of operations described above with regard to the throttle module of . For example a power consumption target of 25 watts may be associated with one set of performance limits i.e. operation adjustments on the component while a power consumption target of 20 watts may be associated with another set of performance limits i.e. operation adjustments on the component .

The parameters may include any data that specifies limitations on or information on how the component can use power. The operations module selects performance limits for the component that correspond to the power consumption target provided to the target module . Performance limits as used herein are values operation modes adjustments and or rules that manage limit control or otherwise adjust the performance of the component to a level that corresponds to the parameters received or otherwise determined by the target module . In certain embodiments the operations module uses tables indexes and other structures to determine the appropriate performance limits for the given parameter inputs to the target module . By determining appropriate performance limits for the given parameter inputs such as the power consumption target the thermal target or the like the operations module identifies a level of performance for the component that the operations module determines will provide the desired power consumption and or thermal state of the component .

In certain embodiments the operations module associates the power consumption target and or the thermal target with timer values or counter values used in controlling the component thus limiting the performance power consumption and temperature of the component . Timers in certain embodiments may be used to control when after an operation is started on a storage device the controller polls to determine if the prior operation completed and before sending a new operation. Timers in other embodiments may be used to control how frequently operations are submitted to a storage device for execution.

For example a solid state storage device may use timers such as the tPROG time discussed above to control when the software polls the hardware of the solid state storage device such as the banks discussed above to verify that an operation has completed before initiating a new operation. In one embodiment the operations module provides an increased time between the polling such that the controller polls the hardware of the solid state storage device less frequently. This causes a reduction in the number of operations per unit time and a corresponding decrease in power consumption per unit time. In one embodiment reducing a frequency with which operations are executed introduces downtime on a communications bus of the component reducing bandwidth throughput and or other performance of the component as a tradeoff for reducing power consumption and or temperature. This is discussed in more detail in connection with .

The schedule module executes operations on the component according to the performance limits selected by the schedule module . As a result the component may not operate as effectively as it would if full power were available for example the component may operate at a lower bandwidth. However in one embodiment the component may also consume less power during operation than it would without the performance limits. In one embodiment the performance limits are set such that the actual power consumed by the component under the performance limits is within a tolerable range either above or below the power consumption target satisfying the power consumption target.

Thus if the power consumption target is 50 mW the actual power consumed by the component when operating under the performance limits associated with a power consumption target of 50 mW is within a tolerable range of 50 mW. For example the timers controlling the polling of the banks may be slowed to a point where average power usage is approximately 50 mW. Those of skill in the art will appreciate how to specify a tolerable range in one embodiment the tolerable range allows for a difference of 4 for example above or below the power consumption target.

While this application discusses specific approaches to reducing bandwidth on a storage device those of skill in the art in view of this disclosure will appreciate that there are other ways to reduce bandwidth of a storage device in a predictable fashion such that the power consumption of the storage device may be reduced. Slowing timers and clocks scheduling operations and other approaches discussed herein are examples of approaches to achieve power management.

In certain embodiments the power management apparatus may also include the audit module . The audit module in one embodiment measures and or monitors the actual power consumed by the component and compares the actual power with a projected or expected power consumption of the component that is a predicted amount of power that the component uses when executing operations in accordance with the performance limits or other adjustments. The projected power consumption may be the power consumption target. In other embodiments the projected power consumption is based on the power consumption target and the other parameters or adjustments governing the component . For example the operations module may determine that a first set of performance limits are appropriate for the parameters provided to the target module and that the component operating under the first set of performance limits will have a projected power consumption of 50 mW.

In one embodiment the audit module measures the actual power and compares it with the projected power consumption to determine whether the actual power consumption of the component is within an allowable tolerance of the projected power consumption of the component as specified by the power management apparatus i.e. whether the actual power consumption satisfies the power consumption target. The actual power may be a measured power value in other embodiments the actual power is an approximation based on at least one measurement. For example assuming that voltage provided to the component is constant current measurements alone may be sufficient to determine an actual power value.

In one embodiment the audit module communicates with the throttle module and makes appropriate adjustments and optimizations to bring the actual power usage of the component closer to the projected power usage of the component and or closer to satisfying the power consumption target. For example if the audit module determines that the component is consistently using more power than the power allocated to it the audit module may provide this information to the throttle module . The throttle module may then adjust the performance limits and or the operations of the component to account for the discrepancy between the actual power and the projected power. For example the throttle module may use the operations module to decrease the frequency of polling to reduce the actual power consumed by the device. The throttle module in one embodiment may store the adjustments such that in the future the performance limits selected for the particular set of parameters are the adjusted performance limits.

Thus the audit module may be used to ensure that the actual power usage of the component is sufficiently close to the power usage that the power management apparatus estimates to match the selected performance limits. The audit module may also be used to allow the performance limits and or operations of the component to be adjusted and or corrected during the life of the component as the power consumption characteristics of components often change over the life of the component . In the depicted embodiment the audit module includes the verification module . As described above with regard to in certain embodiments the verification module works with the audit module to verify that specific adjustments to operations of the component bring the power consumption rate within the power consumption target and or bring the temperature of the component away from the thermal ceiling.

In one embodiment the power management apparatus may also include a master module . The master module allows the power management apparatus to share and receive information from other power management apparatuses in the system. In certain embodiments the master module may assign operations to other power management apparatuses and receive operations for execution from other power management apparatuses in the system. The master module may also receive operations for execution from the computing system . The master module may also manage priorities for operations in the system and dynamically reconfigure priorities. The master module may dynamically adjust the power consumption target for the component if there are changes in the power consumption or the power consumption needs of other components in the system.

The master module may also maintain a global or system power consumption target for the set of components not just the component on which the master module is operating. The master module may apportion the global or system power consumption target among the components and change the local power consumption targets dynamically while ensuring that the components do not cumulatively exceed the global power consumption target. As described herein in certain embodiments the components may include the storage device and or each component may comprise a storage device .

In certain embodiments there is only one master module that manages all of the power management apparatus in the system. In other embodiments the master module is distributed across the power management apparatuses which master modules share information. In certain embodiments the power management apparatuses use a default set of settings if the master module goes offline or is otherwise unavailable for cooperative power management.

The master module may also be used to coordinate power allocation within a single component . For example as noted above a solid state storage device may include multiple DIMMs and the master module may coordinate power allocation between the multiple DIMMs. The master module may dynamically reallocate the power made available to each DIMM while ensuring that the component itself does not exceed the power consumption target.

In one embodiment the power management apparatus includes a state module that specifies various possible modes of operation for the component for which the power management apparatus is responsible. For example one state may be a power disruption state which causes the power management apparatus to manage operations on the component to manage the power disruption. Another state may be a power reduction state in which the power management apparatus throttles performance to achieve power savings i.e. when a computing system hosting the power management apparatus moves to operate on battery power or other secondary power sources . Another state may be a thermal reduction state in which the power management apparatus throttles performance to reduce the temperature of the component the temperature output of the component or the temperature of the enclosure housing the component .

While the above discussion has been directed primarily to power given the relationship between power and thermal energy the modules discussed above may be tailored to allow for thermal monitoring and to allow performance throttling as a thermal management tool for managing thermal temperature levels affected by the component . For example the audit module may also include the necessary hardware i.e. sensors temperature sub systems and the like to monitor temperature on the component as described above. In other embodiments the audit module monitors the temperature of an enclosure that contains the components . The audit module in certain embodiments may use thermistors or other known hardware to monitor the temperature.

If temperature detected by the audit module is greater than a temperature threshold value or otherwise does not satisfy the thermal target the state module may put the component in a temperature reduction state. In certain embodiments the throttle module may also store data that describes the thermal characteristics of operations for example the operations module may define the average amount of heat given off during an erase operation a read operation or a program operation. The throttle module may use the thermal characteristics to adjust or schedule execution of the operations on the component as described above.

Those of skill in the art will appreciate that given the close relationship between power consumption and heat the techniques described in this application in connection to power management based on reducing power consumption may be effectively applied to reduce the thermal energy radiated by the component . In some cases the application of these techniques may require an inverse adjustment such as activating a cooling system instead of or in addition to terminating certain operations and or adjusting the operating speed in completing queued operations. Similarly the audit module may monitor the actual temperature and adjust the thermal characteristics as necessary to ensure the component is providing the expected level of thermal energy for the given thermal characteristics.

The schedule module of the throttle module may manage the queue and select operations from the queue for execution. The schedule module may use the information about the energy and time requirements for particular operations which information may be provided by the operations module in selecting and scheduling operations for execution. In typical embodiments the schedule module selects operations such that the power consumed by the component in performing the operations is at or below the power consumption target or otherwise satisfies the power consumption target.

In certain embodiments the schedule module uses a credit or token system to manage the queue . In one embodiment the schedule module may receive a credit when an operation is placed in the queue and delete a credit when an operation is removed from the queue for execution or the like. The schedule module may stop accepting operations when the number of credits reaches or exceeds a predetermined amount. Other approaches to managing the size of the queue may be used. Other approaches to queue management may also be used. The credit approach may be used for scheduling the start of operations.

In certain embodiments the schedule module may communicate the credit status of the queue with the computing system or with other components . In such embodiments the entity receiving the credit status for example the number of credits held by the schedule module the number of credits in a pool or the like may dynamically assign operations based on the credit status. Such an embodiment may be particular useful where more than one component can execute the particular operation . In certain embodiments each component has its own queue . In other embodiments where the power management apparatus may communicate with one another the queue may be shared across components 

For example there may be multiple components that are storage devices configured as a RAID mirror. In such an embodiment a read request can be handled by either storage device. The power management apparatus operating on the storage device components may communicate their credit status. The credit status may be the number of credits held by the respective schedule modules a message stating whether the queues are full or not full or other approach to indicating the status of the queue . The computing system may use this information to balance the workload and send requests to the component with the most available processing capability. Balancing may occur across multiple dual in line memory modules DIMMs multiple components or in certain embodiments across multiple systems. In other embodiments the workload balancing is managed by a master module described below.

In a further embodiment the schedule module may provide a provide a pool of a predetermined number of credits or tokens to one or more queues or other sources of operations such as bank controllers bank read queues bank write queues bank erase queues bank management queues control queues or the like . Each queue or other source of operations in one embodiment may access a shared pool of credits or tokens. In a further embodiment the schedule module grants each queue or other source of operations credits or tokens individually. Credits or tokens in one embodiment are a logical construct that the schedule module uses to track schedule and or monitor concurrently executing operations on the component .

The schedule module in one embodiment receives a token for each concurrently executing operation and prevents other pending operations from executing once the predetermined number of credits or tokens have been used. In one embodiment a credit or token has an associated duration after which the schedule module returns the credit or token to the pool. In one embodiment the associated duration is about equal to an amount of time that it takes for an operation of a predefined type to execute. In a further embodiment the associated duration is shorter than an amount of time that it takes for an operation of the predefined type to execute such that execution of sets of operations of the predefined type overlap or are staggered. Certain types of operations such as the erase operation described below with regard to may require a greater amount of power toward a beginning of an operation than toward an end of the operation or may otherwise have nonlinear power consumption over time during execution. Staggering and or overlapping such operations may decrease or prevent a spike in power consumption that would otherwise occur if all of the operations were started concurrently.

In a further embodiment the schedule module does not receive or distribute credits or tokens but uses a counter or the like to schedule operations in a similar manner. For example in one embodiment the schedule module receives a set of concurrent operations for execution on a component such as the storage device . The schedule module in various embodiments may receive the set of concurrent operations from the depicted queue and or from one or more other sources of operations such as the bank controllers the bank read queues the bank write queues the bank erase queues the bank management queues the control queues or the like.

Each operation in the set in one embodiment has an associated operation type such as erase write program read or the like. The operations in one embodiment have a plurality of different operation types. In another embodiment the operations have a single operation type. For example in one embodiment the set of concurrent operations come from a queue of a single type of operations such as erase operations or the like.

The schedule module in one embodiment determines a type for at least two operations in the set of concurrent operations. The schedule module in a further embodiment schedules two or more concurrent operations from the set of concurrent operations for execution based on the type of the operations so that the power consumption rate of the component does not exceed a power consumption ceiling while the component is executing the concurrent operations.

The schedule module in one embodiment maintains a count of what types of operations are currently executing on a component . The schedule module in a further embodiment may access stored definitions of allowable sets of concurrently executable combinations of operation types to determine which operations from the set of concurrent operations to schedule for execution. As depicted in and described below different types of operations may use different amounts of power or energy during execution and may have different durations. The schedule module in one embodiment dynamically schedules concurrently executing operations to satisfy the power consumption target.

In a further embodiment the schedule module schedules certain types of operations separately or independently or may ignore certain types of operations. In one embodiment the schedule module schedules no more than a predefined number of erase operations to start concurrent execution on a component . Once the predefined number of erase operations have started in one embodiment the schedule module delays execution of other erase operations for at least a predefined duration before allowing the other erase operations to execute. As described above with regard to the tokens or credits the predefined duration in one embodiment is less than an execution time for the operation causing sequential sets of concurrently started operations to stagger or overlap. For an erase operation in certain embodiments the predefined duration is sized such that execution of an erase operation consumes more power during the predefined duration than after the predefined duration. Such sizing of the predefined duration in certain embodiments prevents the power consumption rate from spiking above the power consumption target while still allowing for concurrently executing operations.

In one embodiment the schedule module may include or be in communication with a bus arbiter such as the bus arbiter and may schedule operations for execution on the component by allowing the operations to be performed on a bus as described above with regard to . The bus and or the bus arbiter may control access to two or more banks or other components of a storage device . For example in one embodiment the bus may be a storage control bus and individual storage components such as banks dies or other storage elements may receive commands for operations over the storage control bus. One example of a storage control bus is the storage control bus of and . In other embodiments the schedule module may schedule operations for execution by placing them in a queue or buffer sending them to the component or the like.

As noted above the operations module may store information concerning the time and power necessary for particular operations for the component . The power and time characteristics of the particular operations may be part of the performance limits or other adjustments by the throttle module . The schedule module may use this information in controlling the flow of operations on the component .

As shown in the schedule module may schedule an erase operation at time unit along with a write operation and four read operations. Even if the component has the ability to perform more read write or erase operations the operations cannot be performed without exceeding the power consumption target. As a result the component may use less than the unregulated power limit for the component and experience a corresponding decrease in performance.

In certain embodiments the schedule module does not verify that the operations have completed before removing additional operations from the queue . The schedule module may assume that the operations will complete in the time specified by the operations module . As noted above the audit module may conduct audits to verify the accuracy of the timing operations and the energy consumption of particular operations or may monitor the power consumption rate of the component . In addition the operations module may adjust the energy and timing characteristics of operations as the component ages based on information about how the age of the component affects its performance.

As a result the schedule module may ensure that the component does not exceed the power consumption target based on the information provided by the operations module . In addition the schedule module may select operations from the queue according to priorities. For example in certain embodiments read operations may be given the highest priority followed by writes and finally by erase operations. The schedule module may maintain these priority values.

In addition priorities may change dynamically based on the nature of the queue . For example the schedule module may give write operations the highest priority if the component is running out of volatile storage capacity to hold the data before it is written the schedule module may determine that executing the write operations will free up necessary volatile storage. The schedule module may also determine that the component is approaching a situation where a secondary power supply for the component will have insufficient energy to move the data into solid state storage media in the event of a power failure.

The schedule module may increase the priority of write operations and keep the priority at the higher level until the amount of data is reduced to an acceptable level that does not put data at risk. If the component is running low on available space in the solid state storage media the schedule module may increase the priority of erase operations that allow the component to recover storage. Those of skill in the art will appreciate other situations in which it may be advantageous to dynamically alter a priority scheme for operations on a component in response to conditions on the component .

In certain embodiments when certain operations are continuously delayed or aborted due to lack of priority the schedule module may report to a user or to another module such as a master module that additional resources are required if the continuously delayed operations are to execute. This may be used to prevent what is commonly referred to as starvation. For example a groomer on a storage device may not be able to recover storage area in the storage device because erase operations are continuously being delayed or aborted. The schedule module may alert the user the master module the computing system or all of them of the consequences if additional resources are not allocated to the storage device.

In certain embodiments the schedule module may establish priorities for operations based on parameters provided to the target module . For example the schedule module may establish different priorities for operations based on the computing system originating the operation . The schedule module may establish priorities based on the traffic type of the operation for example different priorities may apply to system read and write operations in comparison to application read and write operations. Priority may also be based on the nature of the storage such as partition or volume and or function for example areas acting as a cache may have a different priority scheme than those acting as static storage. In addition different priorities may be established for background processes such as data grooming deduplication progressive RAIDing or others. Those of skill in the art will recognize other advantageous basis for establishing priorities for operations .

In other embodiments the schedule module may provide information concerning the queue to the relevant computing system or to other components . As noted above multiple power management apparatus operating on multiple components may act as a logical power management apparatus and communicate information and dynamically adjust loads on components . One or more of the power management apparatus may include a master module that can receive information from the other power management apparatus in the computing system . The master module may adjust the power consumption targets of the various components based on the information provided by the schedule modules .

For example the schedule module may determine that the queue is filling rapidly and request a higher allocation of power. The master module may increase the power consumption target for one component and reduce the power consumption target for another. If the queue is far below an acceptable level the schedule module may inform the master module . The master module may determine that the associated component does not need the entire power consumption target allocated and reduce it in response to a request from another component requesting a larger power allotment.

The schedule module may provide other information to the master module for example the schedule module may inform the master module that the component is running out of storage because erase operations cannot be executed. The schedule module may inform the master module that write operations cannot be executed quickly enough and that the queue is filling and approaching overflow. The schedule module may inform the master module that it is approaching a point where there is more data in the volatile memory on the component than a secondary power supply can move to solid state storage media in the event of a power failure. The master module may dynamically adjust power allocations among the components in the system based on the information.

In certain embodiments the schedule module can interrupt operations that are executing to allow higher priority operations to execute. The schedule module may interrupt operations by canceling them and then moving them back into the queue or pausing them and resuming them once the higher priority operation is complete. For example at time unit in the component may receive an operation such as a critical write with a priority level that requires immediate execution. The schedule module may choose to cancel one of the two erase operations or both erase operations to create availability for the critical write operation.

In one embodiment when the schedule module must interrupt an existing operation the schedule module chooses the currently executing operation with the lowest priority. In the event that two currently executing operations have the same priority the schedule module may choose the operation that is farthest from completion to cancel.

In such embodiments the schedule module may stagger the execution of the operations during the time period t such that less than the maximum amount of operations occur but such that the component does not exceed the power consumption target t for that time period. This approach may also be used in conjunction with the scheduling of operations in parallel as shown in which combination of scheduling operations is shown in .

The staggered execution may be implemented by increasing the period of time that the controller waits before verifying that a bank such as bank has completed an operation. For example at full speed a bank may be given a read operation and after a wait period of 3 microseconds poll to ensure that the read operation completed and send another read operation. In one embodiment the schedule module increases the wait period from 3 microseconds to 4 microseconds. As a result fewer reads occur and less power is used. Similar delays may be introduced to erase and write operations to obtain similar power savings. As described above one embodiment of a wait period for write program operations is the tPROG value.

In certain embodiments and as shown in the component may be a solid state storage device with banks and . In a storage device with interleaved controllers for the banks and as described in the Bank Interleave Application referenced above the storage device can process multiple read write and erase operations simultaneously as the banks can act in parallel. In one embodiment the banks and perform read operations while the bank performs write operations and the bank performs an erase operation. In the depicted embodiment the banks and may perform less than the maximum number of operations possible during the time period t if there were no constraint on power usage for the storage device. For example bank may perform only two writes as shown instead of the three possible writes during the time period t.

In certain embodiments the schedule module coordinates the execution of operations on the banks and . For example the schedule module may assign the bank to perform one or more erase operations in the queue and have bank clear out as many read operations as possible while awaiting the bank to complete the erase operation. The schedule module may coordinate the activities of the banks and to ensure that the total operations for the component do not exceed the power consumption target.

The performance of the component may also vary depending on the operations of the schedule module and the nature of the workload. For example if the schedule module is managing only read operations from the queue in a component that is a solid state storage device the component may be able to operate at full bandwidth that is the component may be able to perform reads as quickly as possible because read operations require less energy. The same component may operate at less than full bandwidth when performing write operations and may operate at even less than full bandwidth if the component has numerous erase operations to the extent that write and erase operations need to be scheduled as described above in order to ensure that the component does not exceed the power consumption target.

In a further embodiment operations associated with certain types of workloads may be given priority such as reading or writing a video file or other real time workloads of the component . To satisfy the power consumption target in one embodiment operations not associated with a priority workload may be adjusted or throttled to provide power to process the operations associated with the priority workload at full speed or the like.

In certain embodiments the operations executing on one bank are relevant to what operations may execute on another bank in the component . The schedule module may schedule operations using an algorithm based on allowable combinations of operations for a particular power consumption target. For example all banks and may be able to execute read operations simultaneously. As noted above this situation may not require any limiting on the number of read operations if the component can execute read operations without exceeding the power consumption target. In one embodiment the banks and may all be able to execute write operations while the bank performs read operations. Two banks such as and may be able to read while two banks and erase. Those of skill in the art in view of this disclosure will appreciate how operations can be organized on a per bank basis to keep the component below a power consumption target.

In the power consumption target may be set to the value 10. At time 0 the bank begins an erase operation bank begins a program and banks and begin read operations. In one time unit the read operations on banks and complete. In certain embodiments as discussed above the timers are configured such that the banks are not polled to determine whether or not the read operations completed until time 2. As a result the banks and may not begin new read operations until time 2. Similarly at time 3 the completion of the program operation on bank and the read operations on banks and are detected. Bank may initiate a new program operation bank a new read operation and bank a new program operation.

During the time period from 4 to 5 and from 6 to 7 the cumulative power used by the storage device is more than the power consumption target of 10. However in certain embodiments the solid state storage device is allowed to operate above the power consumption target for brief bursts so long as the total power used by the storage device over a set amount of time is kept below the power consumption target on average. Thus for example the power consumption target is exceeded during the time periods from 4 to 5 6 to 7 and 14 to 21. However the total area of the power used by the solid state storage device during time 0 to 22 is still less than the area below the power consumption target line thus the momentary bursts above the power consumption target in certain embodiments are permissible since on average the storage device is using less than the power consumption target. In a further embodiment the audit module determines that the power consumption between time 4 and time 7 exceeds the power consumption target and the throttle module adjusts execution of operations as described above.

In certain embodiments one or more banks sit idle in order to ensure that the power consumption target is not exceeded for example at time 14 banks and begin erase operations. Due to the large power requirements of the erase operations the banks and are kept idle until the erase operations on the other banks complete so that the power consumption target is not exceeded.

While shows a period of inactivity after each operation by a bank in certain embodiments there is no period of inactivity. In certain embodiments the schedule module may actively manage the sequential execution of the operations as well as contrasted with systems that use a parallel or simultaneous execution of operations and provide pauses when necessary. For example the schedule module may arrange the operations such that there are only sufficient pauses in sequential execution to ensure that the area of the operations in is equal to the area under the power consumption target. Thus fewer pauses may be introduced and the schedule module may dynamically manage when pauses are introduced and for what period of time.

Each quantum shown in represents a given amount of power used over an increment of time in connection with operations on the component . In one embodiment the quantum is a lowest common denominator in terms of energy for all of the operations that are represented in terms of quanta. The quantum may also be a lowest common denominator in terms of time for all operations . The operations may be divided into time periods and each time period may be assigned one or more quanta to represent the amount of power required by the operation during that particular time period.

For example a read operation may use the least amount of energy of all operations and may be the fastest operation. One half of a read operation may be the lowest common denominator in terms of power for the operations . The read operation may take the least amount of time and the period of time for the read operation may represent the least common denominator for the operations in terms of time. Thus the read operation may be represented as two quanta during a time period 1 as shown in .

In contrast more complicated operations may be expressed using different number of quanta over a number of time periods. Thus the erase operation shown in may last a total of 8 time periods. It should be noted that the number of quanta illustrated and the number of quanta per time period are used only for illustration. Certain embodiments of the invention may require a different number of quanta for each operation and the number of quanta for each unit of time may vary depending on a number of factors not included in this example for the sake of clarity. During the time period 1 the erase operation may use 12 quanta of energy. During the second and third time period of the example erase operation shown the erase operation requires 9 quanta. Five quanta are required for each of the fourth and fifth time units 2 quanta for the sixth and seventh and 9 for the eighth time unit.

Such an approach may allow for a more granular analysis and treatment of various operations on the component than is obtained by treating each operation as a single unit as shown in connection with . As a result the schedule module may adjust the performance of the component more effectively using the more detailed breakdown of the energy requirements of particular operations and how they change over time. One of skill in the art will appreciate how for example could be modified to show the more granular picture of power usage and how this might affect operations scheduling and the sharing distribution of power among multiple components .

The quanta characteristics of operations may be hard coded into the component or stored in software or firmware. In certain embodiments the quanta characteristics such as the power requirements the time periods and the make up of operations in terms of quanta may be changed over time. For example the quanta characteristics may be updateable through firmware updates downloads or other techniques. In certain embodiments the audit module may adjust the quanta values such as the time and energy values of the quanta and the quanta characteristics of operations such as the number of quanta per time period number of time periods per operation based on the results of audits.

In certain embodiments the operations may be managed through use of a credit or token system as described above with regard to . Banks such as banks in a storage device may be allocated a certain amount of credits for use during a predetermined period of time. Operations may require a certain number of credits to execute for example each quantum in an operation as illustrated in may be one credit. Once a bank has used the credit allocated for the time period the bank waits for a new time period to begin with a corresponding new allocation of credits. In such an embodiment the total power consumption of the banks may periodically exceed the power consumption target but the average power consumption for the banks during the time period will be equal to or less than the power consumption target.

A credit system may be used to effectively reduce or stop power consumption by removing or reallocating credits to another component . For example certain banks may be given more credits than others and certain components may be given more credits than others. In one embodiment the power disruption procedure discussed above is initiated by removing all credits for a particular component . In certain embodiments the allocation of credits is tracked to help prevent starvation.

The power management apparatus may communicate with its peers with the computing system and with others using a power protocol that enables communications between relevant components that need to send and receive information concerning power with participants in the power management system. The power protocol may specify the rules by which information is communicated between participants in the system. For example the computing system may pack the power consumption target and other parameters according to the power protocol and send the package to the target module . The target module may receive the packet and unpack the power consumption target and other parameters.

In certain embodiments the power protocol provides a discovery routine which allows the power management apparatus to discover its peers and to enable the computing system to communicate with the power management apparatus . In other embodiments this information is provided by a user. Various approaches may be taken to establish communications between the components of the power management system.

The power protocol may provide an Application Programming Interface API that dictates the manner in which information is exchanged. The API may provide method and routines which may be invoked to facilitate power management. The power protocol may allow components to read base requirements and understand configuration options allow adjustments to be made to parameters and operational information such as for example the energy and time requirements of particular operations allow components to read and report status and to provide status to interested parties such as a user.

In the depicted embodiment if the audit module determines that the power consumption rate fails to satisfy the power consumption target the throttle module adjusts execution of one or more operations on the storage device . The verification module in the depicted embodiment verifies whether the power consumption rate of the storage device satisfies the power consumption target in response to the throttle module adjusting execution of operations.

In the depicted embodiment if the verification module determines that the power consumption rate fails to satisfy the power consumption target the throttle module readjusts execution of operations on the storage device and the method continues. If the verification module in the depicted embodiment verifies that the power consumption rate satisfies the power consumption target the method returns to the monitoring step and the audit module continues to monitor the power consumption rate of the data storage device relative to the power consumption target.

In the depicted embodiment if the audit module determines that the temperature is approaching the thermal ceiling the throttle module adjusts execution of one or more operations on the storage device . The verification module in the depicted embodiment verifies whether the temperature for the storage device is moving away from the thermal ceiling in response to the throttle module adjusting execution of operations.

In the depicted embodiment if the verification module determines that the temperature for the storage device is not moving away from the thermal ceiling the throttle module readjusts execution of operations on the storage device and the method continues. If the verification module in the depicted embodiment verifies that the temperature is moving away from the thermal ceiling the method returns to the monitoring step and the audit module continues to monitor the temperature for the data storage device relative to the thermal ceiling.

The schedule module in the depicted embodiment schedules two or more concurrent operations from the set of concurrent operations based on the type of the operations and the method ends. The two or more scheduled concurrent operations in one embodiment have the same type. In a further embodiment the two or more scheduled concurrent operations have a plurality of types.

In various embodiments the method of the method of and or the method of may be combined into a single system method or apparatus. For example in one embodiment the method for managing a thermal state of a storage device may be used in conjunction with the method and or the method for managing power consumption in the same storage device . In another embodiment the method may be used to manage power consumption of one or more types of operations on a storage device while the method may be used to manage power consumption for one or more other types of operations on the same storage device . For example the method may manage power consumption for write or program operations and the method may be used to manage power consumption for erase operations or the like. In view of this disclosure one of skill in the art will recognize other combinations of the methods for managing power consumption and or a thermal state of a storage device .

The method also includes determining the performance limits that correspond to the power consumption target and the other parameters. In certain embodiments the operations module determines which performance limits correspond to the parameters. In certain embodiments the performance limits are specified in terms of timing and or polling values that slow the execution of operations on the component . The performance limits may be specified in terms of the energy requirements and time requirements for particular operations . As discussed above each operation may be defined as taking a certain amount of time and using a certain amount of energy during that time. Operations may also be defined in terms of quanta. Those in the art will appreciate other approaches to setting performance limits for a component .

The method may also include executing the operations on the component in accordance with the performance limits. In one embodiment the schedule module schedules the operations such that the operations are executed in accordance with the performance limits as described above. In certain embodiments the performance limits decrease the performance and bandwidth of the component in other embodiments the performance limits may be sufficiently high that the performance of the component is not affected.

As a result the component executes operations in such a way that the power used by the component is sufficiently close to the power consumption target. In certain embodiments the method also includes auditing the power performance of the component . The audit module may audit the device and compare the actual power consumption of the component with the projected power consumption at the particular performance limits being used. If the audit module determines that the actual power consumption of the component is sufficiently close to the projected power consumption the power performance is deemed acceptable. The audit module may allow some tolerance of variations above the power consumption target. If the power performance is unacceptable for example the actual power consumption is too far above or below the projected power consumption the audit module may adjust the performance limits for the component to more accurately reflect the power performance of the component .

The method may also include determining whether or not there are any changes in the parameters. As noted above the parameters such as the power consumption target may be altered dynamically by the computing system the power management apparatus in the system or others. If the parameters are altered the method begins again with determining the performance limits that correspond to the changed parameters. The performance of the component may thus be changed dynamically to account for changes in the demands on the component user preference and other changes.

The present invention may be embodied in other specific forms without departing from its spirit or essential characteristics. The described embodiments are to be considered in all respects only as illustrative and not restrictive. The scope of the invention is therefore indicated by the appended claims rather than by the foregoing description. All changes which come within the meaning and range of equivalency of the claims are to be embraced within their scope.

