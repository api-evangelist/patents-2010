---

title: System, apparatus and methods to implement high-speed network analyzers
abstract: Systems, apparatus and methods for the implementation of high-speed network analyzers are provided. A set of high-level specifications is used to define the behavior of the network analyzer emitted by a compiler. An optimized inline workflow to process regular expressions is presented without sacrificing the semantic capabilities of the processing engine. An optimized packet dispatcher implements a subset of the functions implemented by the network analyzer, providing a fast and slow path workflow used to accelerate specific processing units. Such dispatcher facility can also be used as a cache of policies, wherein if a policy is found, then packet manipulations associated with the policy can be quickly performed. An optimized method of generating DFA specifications for network signatures is also presented. The method accepts several optimization criteria, such as min-max allocations or optimal allocations based on the probability of occurrence of each signature input bit.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09185020&OS=09185020&RS=09185020
owner: Reservoir Labs, Inc.
number: 09185020
owner_city: New York
owner_country: US
publication_date: 20100429
---
This application is related to and claims the benefit of priority to U.S. Provisional Application Ser. No. 61 174 325 entitled COMPILATION AND OPTIMIZATION OF PROTOCOL ANALYZERS filed Apr. 30 2009 the entirety of which is hereby incorporated by reference.

Portions of this invention were made with U.S. Government support under contract instrument Department of Energy SBIR DE FG02 08ER85046 The U.S. Government has certain rights.

The present invention generally concerns high speed network analyzers. More particularly the invention concerns a system and its methods for identifying intelligent patterns within streams of data observed at high speed rates.

The process of developing technology often involves two parallel efforts 1 the development of a new technology and 2 the development of peripheral tools for the performance analysis and behavior evaluation of the technology itself. While the field of computer science provides uncountable examples of technologies that have been developed hand in hand with such peripheral tools two of these technologies have taken a pivotal place during the past fifty years the computer and the Internet.

There exist at least two broad reasons for the need of analytical tools to measure the performance and behavior of Internet computing systems. First at the early stages of development systems are simple and their performance can usually be characterized through direct naked eye observation. As the technology matures its complexity increases often to a point where its behavior is no longer easy to predict. The Internet provides a good example of such evolutionary transition. Its current behavior is both a function of arguably predictable computer behavior and unpredictable psychology based human behavior. Such is the case that computer scientists have long tried to model its behavior borrowing tools from branches of math such as stochastic processes game theory or even fractal and chaos theory. A. B. Mackenzie S. B. Wicker Game theory in communications motivation explanation and application to power control IEEE Global Telecommunications Conference 2001 M. E. Crovella A. Bestavros Self similarity in World Wide Web Traffic Evidence and Possible Causes IEEE ACM Transactions on Networking 1996. This evolutionary need for performance analysis tools arises in most of the technologies that achieve certain complexity.

Second for the past fifty years our economic and social superstructures have evolved to a point where almost any transaction both economic and social requires some form of involvement of the Internet and our computer systems. Key resources such as energy water communication or the stock market to name a few depend on the proper functioning of these two technologies. Such is the case that they are recognized as national security infrastructures subject to possible cyber attacks. V. Paxson Bro A System for Detecting Network Intruders in Real Time Proceedings of the 7th USENIX Security Symposium 1998. To protect the well being of our society it is therefore crucial to dispose high performance peripheral tools capable of analyzing the behavior of the networks to detect malicious usages.

Current architectures of these analysis tools are being driven to a breaking point by two independent challenges first as network data rates increase these tools are being overwhelmed by the quantity of computation they must perform to continuously analyze the network second as computer network systems become more sophisticated the parsing of the network flows requires ever more complex traffic analysis heuristics that further stress the system s processing capacity.

Therefore there exists a need for a set of systems and methods that focus on the high performance implementation of peripheral tools.

Various provided embodiments include a system apparatus and methods for addressing many of the two challenges introduced above. In an exemplary embodiment a method to generate data plane specifications of a network analyzer capable of running in a variety of hardware platforms is provided. The method is based on two core facilities a high level protocol specification language HLPSL used to write input protocols and events specifications and a compiler capable of translating such specifications into actual native code the data plane specification executable on the targeted hardware platform.

In another embodiment a fast and slow path implementation of a component within the network analysis is provided. The separation of the implementation between fast and slow paths allows for the decoupling of those logical elements that are slow to execute but rarely used from those that are faster and commonly invoked. It is argued that this approach differs from previous work by providing scalability to large number of signatures and hence that one exemplary application of the present invention is that of large systems supporting a large variety of analysis events.

In yet another embodiment a packet dispatcher workflow is provided. The dispatcher provides a high performance optimization framework in which a subset of the network analyzer functions can be offloaded onto a more specialized hardware and software facility. For instance in an exemplary embodiment the dispatcher is used to leverage dedicated DFA engines to search for regular expressions of interest in the flow of packets offloading the network analyzer from such task.

In a further embodiment a level 1 L1 policy caching flowchart is provided that enables the following basic dispatcher behavior upon arrival of a packet P if a policy associated with P is found in the L1 cache then execute it otherwise forward the packet to the network analyzer. This L1 cache provides yet another framework upon which high performance optimizations can be implemented. For instance the cache provides a facility to make early decisions on whether a specific flow requires any further processing or can be bypassed yielding net savings of processing resources from the network analyzer.

In yet another embodiment a workflow is presented which illustrates how a single protocol analyzer engine can be used to resolve a large number of signatures in parallel. Given a fixed number of supported protocols this approach is shown to scale up with the number of signatures up to a certain saturation point.

In yet another embodiment a method to translate and optimize signatures into binary decision diagrams BDD is provided. The method includes an optimization phase which accepts several optimization criteria. In one specific embodiment a method is provided yielding min max BDD cuts that can be run in parallel across multiple DFA engines. In another embodiment a probabilistic method is provided that shows how knowledge of the probabilities of occurrence of the protocol header fields can be used to minimize the average computational cost of resolving a large set of signatures.

It will be recognized that some or all of the Figures are schematic representations for purposes of illustration and do not necessarily depict the actual relative sizes or locations of the elements shown. The Figures are provided for the purpose of illustrating one or more embodiments with the explicit understanding that they will not be used to limit the scope or the meaning of the claims.

In the following paragraphs the present invention will be described in detail by way of example with reference to the attached drawings. While this invention is capable of embodiment in many different forms there is shown in the drawings and will herein be described in detail specific embodiments with the understanding that the present disclosure is to be considered as an example of the principles of the invention and not intended to limit the invention to the specific embodiments shown and described. That is throughout this description the embodiments and examples shown should be considered as exemplars rather than as limitations on the present invention. Descriptions of well known components methods and or processing techniques are omitted so as to not unnecessarily obscure the invention. As used herein the present invention refers to any one of the embodiments of the invention described herein and any equivalents. Furthermore reference to various feature s of the present invention throughout this document does not mean that all claimed embodiments or methods must include the referenced feature s .

As is known in the art a network may employ wireless wired and optical media as the media for communication. Further in some embodiments portions of network may comprise the Public Switched Telephone Network PSTN . Networks as used herein may be classified by range. For example local area networks wide area networks metropolitan area networks and personal area networks. Additionally networks may be classified by communications media such as wireless networks and optical networks. Further some networks may contain portions in which multiple media are employed. For example in modern television distribution networks Hybrid Fiber Coax networks are typically employed. In these networks optical fiber is used from the head end out to distribution nodes in the field. At a distribution node communications content is mapped onto a coaxial media for distribution to a customer s premises. In many environments the Internet is mapped into these Hybrid Fiber Coax networks providing high speed Internet access to customer premises through a cable modem. In these types of networks electronic devices may comprise computers laptop computers and servers to name a few. Some portions of these networks may be wireless through the use of wireless technologies such as a technology commonly known as WiFi which is currently specified by the IEEE as 802.11 and its variants which are typically alphabetically designated as 802.11a 802.11b 802.11g and 802.11n to name a few.

Portions of a network may additionally include wireless networks that are typically designated as cellular networks . In many of these networks Internet traffic is routed through high speed packet switched or circuit switched data channels that may be associated to traditional voice channels. In these networks electronic devices may include cell phones PDAs laptop computers or other types of portable electronic devices. Additionally metropolitan area networks may include 3and 4generation wireless networks employing an alternate wide area or metropolitan area wireless technology. 3G and 4G wireless networks are currently specified by both of the 3Generation Partnership Projects 3GPP and 3GPP2 Further personal area networks are known in the art. Many of these personal area networks employ a frequency hopping wireless technology. Other personal area networks may employ a technology known as Ultra Wideband UWB . The hallmark of personal area networks is their limited range and in some instances very high data rates. Since many types of networks and underlying communication technologies are known in the art various embodiments of the present invention will not therefore be limited with respect to the type of network or the underlying communication technology.

For purposes of clarity the term network as used herein specifically includes but is not limited to the following networks a wireless communication network a local area network a wide area network a client server network a peer to peer network a wireless local area network a wireless wide area network a cellular network a public switched telephone network and the Internet.

As used in this application the terms component module system and the like are intended to refer to a computer related entity either hardware firmware a combination of hardware and software software or software in execution. For example a component can be but is not limited to being a process running on a processor an integrated circuit an object an executable a thread of execution a program and or a computer. By way of illustration both an application running on a computing device and the computing device can be a component. One or more components can reside within a process and or thread of execution and a component can be localized on one computer and or distributed between two or more computers. In addition these components can execute from various non transitory computer readable media having various data structures stored thereon. The components can communicate by way of local and or remote processes such as in accordance with a signal having one or more data packets e.g. data from one component interacting with another component in a local system distributed system and or across a network such as the Internet with other systems by way of the signal .

Referring to which illustrates an exemplary embodiment of a network analyzer for data plane specifications generation. As illustrated the network analyzer contains interfaces a management plane and a data plane. In this embodiment network analyzer contains a network analyzer compiler with an event interface and a protocol interface . In some embodiments event interface is configured to receive an event specification . Additionally protocol interface is configured to receive a protocol specification .

In some embodiments the inputs to the network analyzer compiler are expressed in a high level protocol specification language HLPSL . The input to the protocol interface originates from a protocol specification source . This specification can be expressed in a form different than the HLPSL. Examples of protocol source specifications are Internet Engineering Task Force IETF Request For Comments RFCs such as HTTP specification R. Fielding Hypertext Transfer Protocol HTTP 1.1 Request for Comments RFC2616 1999 or protocols specified in Backus Naur Form BNF . The protocol specification source is translated into an equivalent protocol specification written in the HLPSL . The HLPSL defines a human readable grammar that is close to the language used to represent the protocol specification source so that the process of translating one to another is generally straightforward. This translation process can be made manually or automatically via a simple domain specific compiler.

Two examples of HLPSL that can be used within the scope of various embodiments are GAPAL N. Borisov D. J. Brumley H. J. Wang J. Dunagan P. Joshi and C. Guo A Generic Application Level Protocol Analyzer and its Language Proceedings of the 14th Annual Network Distributed System Security Symposium March 2007 and BinPAC R. Pang V. Paxson R. Sommer L. Peterson binpac A yacc for Writing Application Protocol Parsers Proceedings of ACM Internet Measurement Conference October 2006 . The protocol specification expressed in the HLPSL is used as an input to the protocol interface in the network analyzer compiler module . Protocol specifications written in HLPSL can be kept in storage and be reused across different applications.

In some embodiments the same HLPSL or in some embodiments an extension of the same HLPSL is used to describe the events that are relevant to the analysis. This event specification is used as another input to the network analyzer compiler module through event interface .

These two sets of specifications form the inputs to the network analyzer compiler also referred as the management plane which emits a set of data plane specifications for a particular data plane module implementing the actual network analyzer. In an exemplary embodiment the data plane specifications correspond to native binary code that runs on various hardware engines within the data plane module such as processors FPGAs GPUs ASICs DFA engines or embedded network processors and hardware acceleration engines among others.

In various provided embodiments the network analyzer compiler module derives a data plane specification from a protocol specification and an event specification . In some instances the data plane specification contains a set of target events that are grouped or segmented into a fast path and a slow path. In these embodiments the fast path is typically implemented by a single logical OR of at least two of the target events while the slow path implements each of the target events individually.

In an exemplary description of the blocks presented in is presented. In one embodiment the process of generating an event specification in the HLPSL is obtained from the analysis of the set of actions triggered by a network message. This process starts with receiving a network message or packet in which is used to stress a signature generation module . In one embodiment the signature generation module can be implemented via a computer system which reports traces of the actions triggered by the input message. Such traces are then analyzed by the same signature generation module to emit the actual signature . A signature can be understood as a Boolean function that takes as input a network message and returns as output a TRUE value if some relevant action is triggered by the input message.

In one embodiment the signature generation module emits signatures that return a TRUE statement whenever the input network message attacks a vulnerability of a specific network system. Such signature can then be used by an intrusion detection system IDS to protect a network facility from a cyber attack. An example of this type of signature generation module is presented in J. Newsome D. Song Dynamic Taint Analysis for Automatic Detection Analysis and Signature Generation of Exploits on Commodity Software Proceedings of the Network and Distributed System Security Symposium NDSS 2005.

In the signature is used as input to a HLPSL signature compiler which is responsible for converting the signature into a specification that conforms to the HLPSL grammar. An example of HLPSL signature compiler is BSC J. Ros Giralt P. Szilagyi J. Ezick D. Wohlford R. Lethin Generation of High Performance Protocol Aware Analyzers with Applications in Intrusion Detection Systems in Proceedings of SPIE April 2010 which translates signatures expressed in VinE into BinPAC language. In general the process will involve an additional stage an event packing module where the signature is packaged into an event specification in module . In this stage the set of actions that need to be triggered if the signature returns a TRUE statement are added to the HLPSL specification to form the event specification . For instance when used in the context of an IDS the event can decide to drop divert or normalize a flow among other actions upon detecting that such flow is malicious i.e. when the signature returns a TRUE value . The end result is a complete event specification of an event using the HLPSL. Logically we have that EVENT 126 SIGNATURE 124 ACTIONS 125 where ACTIONS is the set of actions that must be performed when the signature returns TRUE.

Notice that if the first condition does not hold then for most of the cases the system will need to invoke both the fast path and the slow path rendering the fast path redundant. The second condition is also required to be consistent with the definition of fast path and slow path . It is observed that in the particular embodiment of an IDS the above conditions are generally satisfied. In an IDS in average only a small percentage of the traffic corresponds to an attack. Even though there might exist certain windows of time for which malicious traffic might come in bursts . In addition notice also that in general the OR signature can be resolved faster than each signature separately because typically signatures will expose redundancies that the OR operator can simplify away.

In previous work approach 1 relevant results were presented on the acceleration of signatures based on the independent optimization of each signature s protocol parser. This approach focuses on the optimization of the protocol parser of each signature by removing from the protocol definition those elements that are not relevant to the processing of the signature. In embodiments of the present invention approach 2 in contrast as the number of events and therefore signatures that need to be handled increases the parts of the protocol parser that can be omitted become fewer since a larger variety of signatures will require in general a larger variety of parsed information. In the limit when the number of signatures is very large one should expect the need to process most of the protocol elements carried by a flow in order to resolve all the signatures. Therefore the savings introduced by approach 1 will tend to decrease as the number of signatures increase. In various embodiments of the present invention instead of optimizing each signature separately with its own protocol parser the union of all of the signatures is optimized this allows both the simplification of redundancies across signatures and the execution of all the signatures in parallel leveraging one single complete protocol parser. Further as it will be described later in a separate embodiment this approach allows for the parallel execution of the signatures using DFA deterministic finite automata hardware engines. In summary both solutions approach 1 and approach 2 have their practical applications and market niches approach 1 is likely to operate well in the case of low number of signatures whereas approach 2 will tend to do better as the number of signatures increases.

In another exemplary embodiment of the present invention presents an implementation of the data plane module introduced in . The figure describes the architecture and processing of an incoming packet from the time it is received at the physical layer to the time it is delivered to the network analyzer at the application layer. In some embodiments the workflow distinguishes two contexts the bottom half context 1 reference and the upper half context 2 reference . This is done to optimize the matching of tasks and resources. For instance in one embodiment the bottom half offers a more limited set of resources and functionalities while delivers its services at high performance whereas the upper half provides a richer set of functionalities at a lower performance. While not specific to in an exemplary embodiment this infrastructure is mapped onto the operating system of a computer where the bottom half corresponds to the kernel OS and the upper half corresponds to user space.

In what follows and throughout the description of the various provided embodiments the terms connection and flow are used interchangeably. Upon arrival a packet is intercepted from the main flow by a dispatcher . In one embodiment the dispatcher can be implemented as a driver module that is dynamically inserted to the system. The packet is first handed to a first worker worker . Using the packet s IP connection tuple defined by the IP source and destination address transport layer source and destination port and protocol number the first worker performs a look up operation onto a table of flows in memory block to obtain the connection state flow associated with the packet . Next the first worker creates a working item made of the packet buffer and flow i and inserts it to a working queue . In general the first worker can decide to coalesce multiple packets that belong to the same connection into a scatter gather list of packets packets and queue it together with the flow state flow i packets as indicated in . To ensure a correct processing if a flow requires in order packet delivery the first worker will also perform the task of packet re ordering for instance in the case of TCP flows .

A deterministic finite automata DFA engine is used to process each working item flow i packets to extract the relevant regular expressions found within the packet . In some embodiments the DFA engines are loaded at booting time with a set of specifications the DFA graphs that define which regular expressions are of interest to the analysis. In one embodiment this loading operation is triggered by the network analyzer using an out of band control channel and such facility can also be used to dynamically modify the set of DFA specifications. In yet another provided implementation the DFA engines is implemented in hardware providing a facility to process regular expressions in parallel. Depending on the type of application such dedicated facility can save substantial amount of computational work from the system s core processors. For instance previous work by the authors of the present invention J. Ros Giralt P. Szilagyi J. Ezick D. Wohlford R. Lethin Generation of High Performance Protocol Aware Analyzers with Applications in Intrusion Detection Systems SPIE Cyber Security Situation Management and Impact Assessment Conference April 2010 which is incorporated by reference in its entirety shows that for a typical HTTP request message offloading of regular expressions onto hardware DFA engines can save about half the total amount of cycles spent by the CPU in processing the complete message potentially doubling the throughput of the system. In another embodiment the DFA engines are implemented in software. While such approach does not benefit from the parallel and optimized nature of the hardware DFA engines the system still benefits from doing this processing up front for instance by enabling early filtering policies that can save future cycles. This will be described in more detail in a separate embodiment.

The DFA engines return the offsets of the regular expressions found within the scatter gather list of packets putting a working item of the form flow i packets regex offsets back to a queue of results. A second worker Worker pulls elements from this queue and delivers them back to the system s IP stack . The tuple flow i packets regex offsets is ultimately delivered to the network analyzer . In an exemplary embodiment the control data flow i regex offsets is delivered using a separate control channel . The network analyzer can then process the packets using the control information provided by the tuple flow i regex offsets.

In one embodiment DFA specifications are automatically generated at compile time by the network analyzer compiler . The network analyzer compiler upon reading the input HLPSL specifications extracts the relevant regular expressions and generates their corresponding DFA specification. Such regular expressions can come from multiple sources presents examples of regular expressions that can be offloaded using this method for the case in which the HLPSL is Bro BinPAC R. Pang V. Paxson R. Sommer and L. Peterson binpac A yacc for Writing Application Protocol Parsers Proc. ACM IMC October 2006 . The figure provides four examples 

1. Regular expressions obtained from the HLPSL reference these type of regular expressions are typically explicitly defined following the HLPSL grammar for instance in they are defined via the expression RE regular expression .

2. Regular expressions obtained from the code emitted by the compiler these regular expressions can be implicitly identified from the code emitted by the compiler the bold line defines the search for a regular expression equal to carrier return CR and line feed LF .

3. Regular expressions obtained from protocol agnostic filters these regular expression can typically be identified from filters that are based on plain pattern matching or minimal protocol interpretation for instance in these regular expressions are defined by the expressions payload regular expression .

4. Regular expressions obtained from protocol aware filters these regular expressions can be found in filters that are protocol intelligent for instance these regular expressions can be defined with expressions such as if regular expression in buffer 

In one embodiment the network analyzer compiler is capable of identifying a complete taxonomy of regular expressions such as the one presented above and generates at compile time the DFA specifications.

Another provided embodiment illustrated in is a method that can be implemented within the scope of the dispatcher module presented in . These embodiments include apparatus and methods to implement the passing of control data from the dispatcher to the network analyzer. The data flow for this method starts in block the remaining data flow previously described with respect to . The DFA engines process the incoming data to obtain the set of offsets where the regular expressions of interest are found. One objective is to convey these offsets to the network analyzer in an efficient way. To that end a cache table is used to store the control information. In some embodiments this cache is indexed with the memory pointers of the packets scanned by the DFA engines. For each scanned packet the DFA engines store in the cache a record containing the offsets of the regular expressions. Following this procedure a STORE operation takes as input parameters the pointer to the packet and the results of the DFA engines the offsets . A record containing these offsets is created and put in the cache using the pointer as the index to store and later retrieve the record.

In a one embodiment the cache can have limited storage capacity. If such limit is reached the STORE operation is allowed to fail or alternatively a STORE operation can cause an existing record to be dropped from the cache. In these situations certain packets may not have a corresponding record in the cache. Hence certain look up operations may also fail what is known as a cache miss . In a particular configuration this storage capacity can be set to infinite or more practically to the maximum storage capacity given by the system. The cache can also be maintained by a variety of garbage collection or record replacement heuristics as is done in traditional caching methods. In yet another embodiment the cache can be implemented using high performance storage access algorithms including but not limited to hash tables or bloom filters.

Building on the same data path previously described above packets arrive at the network analyzer where without loss of generality the work is partitioned into three stages a preliminary work stage a regular expression processing stage and a third stage where the remaining tasks take. Upon arriving at the regular expression processing stage a packet is processed by a worker which performs a LOOKUP operation onto the cache . The LOOKUP operation passes as argument the pointer to the packet which is then used as the index to retrieve the record associated with such packet. If the LOOKUP operation returns a record cache hit then the worker uses the offsets found in it if any to identify the location of the regular expressions of interest in the packet. Otherwise if no record is found cache miss the worker falls back to the normal path and invokes the software regular expression module .

The usage of a caching system to convey control information from the dispatcher to the network analyzer provides two key advantages on one hand portability and interoperability are greatly enhanced since all the control logic is kept separate from the rest of the system blocks and with only a few simple hooks the control path can be easily attached to the dispatcher and the network analyzer on the other hand this comes at no or negligible performance cost since the cache can be implemented using traditional high performance storage access algorithms such as hash tables or bloom filters.

The flowchart in assumes the existence of two caches 1 a protocol based policy cache PBPC 181 which given a protocol type P returns a policy applicable to any flow conforming to P and 2 a tuple based policy cache TBPC 182 which given an IP tuple T of the form IP source and destination addresses transport source and destination ports and protocol number returns a policy applicable to the flow associated with such tuple T.

The method runs as follows. At upon receiving a packet it checks if the packet can be parsed. If the packet cannot be parsed then it is forwarded directly to the network analyzer in . Otherwise at it extracts from the packet its protocol type and looks up the PBPC. If a policy entry was not found in the table at it goes to . Otherwise it executes the policy at . If the policy requires it to drop the packet at then it is dropped at . Otherwise it goes to . At the method extracts the tuple from the packet and looks up the TBPC in the tuple based policy cache . If a policy is not found at it forwards the packet to the network analyzer at . Otherwise it executes the policy at . In some embodiments on a final check if the policy requires it to drop the packet at then it is dropped at . Otherwise it is forwarded to the network analyzer at .

One advantage of the presented methods and apparatus is that of providing a logical level 1 L1 caching facility to implement packet policies before the packets reach the network analyzer. If the caches return a policy cache hit then such policy can be executed immediately by the dispatcher at . Otherwise cache miss the packet is handed to the network analyzer which in all cases has the required information to process the packet. From a performance perspective this caching facility allows for the offloading of processing cycles from the network analyzer onto the dispatcher. For instance if the policy states that the packet must be dropped then such decision can be made up front without the need to involve the network analyzer.

In an exemplary embodiment a cache miss could occur when the incoming packet is IP fragmented. If the dispatcher does not support IP defragmentation then it will not be able to compute the tuple of an IP fragmented packet hence it will not be able to look up the cache table and retrieve a policy yielding a cache miss. Another example of cache miss could occur when packets of the same connection arrive out of order. If a policy requires in order delivery and the dispatcher does not support packet reordering then such scenario should be treated as a cache miss too. In general any cache miss can be avoided by adding more functionality to the dispatcher. For instance these two examples of cache misses could be avoided by implementing the functions of IP defragmentation and packet reordering in the dispatcher. Thus there exists a natural trade off between number of cache misses and logical complexity of the dispatcher. In some provided embodiments IP defragmentation and packet reordering are provided in others they are not.

In one embodiment policies are loaded from the network analyzer to the dispatcher as shown in . In this embodiment there can be two types of policies static policies which do not change and do not depend on the input traffic and dynamic policies which can change with time and are a function of the input traffic.

This could happen for multiple reasons for instance in some scenarios the network analyzer compiler in could decide at compile time that certain regular expressions cannot be offloaded because they would consume too many resources from the DFA engines . To those skilled in the art this condition is known as DFA state explosion. If the entry is found then the protocol parser uses the offset in the entry to identify the position of the regular expression within the packet. In some embodiments a negative offset can be used to indicate that the regular expression was not found. The protocol parser gathers information for each flow and as packets are processed it stores the results of this analysis in a per flow status table referred as the status control block or SCB . In the notation SCB Ti refers to the status control block of the connection with IP tuple Ti.

Once a status control block SCB Tj is fully completed or the analysis for a connection with tuple Tj is finalized SCB Tj is passed to a module that can resolve all the events signatures in one single pass. This strategy differs from previous work N. Schear D. R. Albrecht N. Borisov High speed Matching of Vulnerability Signatures Symposium on Recent Advances in Intrusion Detection September 2008 in that instead of implementing a protocol parser for each even signature embodiments of the present invention use a single protocol parser to first extract all the required information and then compute all the signatures at once. This strategy allows for the implementation of the fast and slow path as previously introduced in . In union implements the union of all the signatures which can now be resolved with the information provided in SCB Tj . If the union returns a FALSE statement then no further analysis is required for this connection otherwise SCB Tj is passed to an event classifier the slow path which resolves each signature separately to determine which of them is triggered leading to the set of specific events that must be invoked.

One advantage of the above method resides in its scalability. shows how the performance of the system scales up with the number of signatures and protocols supported. The illustration makes an important assumption that the execution of events is negligible with respect to the end to end cost of executing the complete workflow in . This assumption is more likely to be true in systems that focus on the passive analysis of the network traffic for instance in intrusion detection systems and less likely to hold when the system performs heavy duty tasks for each event for instance in intrusion detection and prevention systems . It is also most likely to hold for network traffic patterns where flows that generate events occupy a small percentage of the total traffic. Because in some embodiments each connection is only parsed once regardless of the number of signatures the system yields a flat region of operation. Within this region the performance is invariant with respect to the number of signatures. This flat region has a saturation point which defines the maximum number of signatures beyond which the performance will start to degrade. As long as the system operates within the flat region the performance is invariant with respect the number of signatures. The performance also depends on the number of protocols supported because for each protocol the system will replicate the infrastructure in . As shown the performance will degrade as the number of protocols increases.

In another embodiment of the present invention a method to generate DFA specifications capable of resolving large number of signatures in parallel is illustrated in . J. Ros Giralt J. Ezick P. Szilagyi R. Lethin High Speed Parallel Processing of Protocol Aware Signatures Thirteenth Annual Workshop of High Performance Embedded Computing 2009 which is incorporated herein in its entirety. In this method a signature is modeled as a Boolean function and the objective is to find a DFA specification than can map such input into N DFA engines with N larger or equal to 1. A signature or the union of more than one signature is first treated by CNF generator which interprets its input as a Boolean function and converts it into conjunctive normal form CNF . This CNF expression is then treated by CNF partitioner which partitions it into N smaller CNF expressions following a predefined optimization criteria. The output of this module is then processed by BDD converter which converts each CNF expression into a binary decision diagram BDD yielding N BDDs. Since BDDs are a form of DFA specifications this outcome can be directly loaded onto a set of N DFA engines. In some embodiments the method includes a feedback loop between BDD converter and CNF partitioner to allow for an iterative optimization process in which if the output is not good enough it can be fed back to the CNF partitioner which can then adjust the partition in search for an optimal outcome. As it will be made evident in the following embodiments a variety of metrics can be implemented including but not limited to the maximum number of signatures that can be mapped onto a fixed size DFA engine or the resulting average throughput of the system.

Turning to and which describe a reduction to practice of an embodiment of the method presented in . Block in shows an example of a signature expressed in its Boolean form. The signature is expressed as a function of certain HTTP protocol header fields. Block shows a pseudo code implementation of the signature using a language called SALT which is described in J. Ezick Salt 1.5 Closing the Programming Gap for Boolean Satisfiability Solvers Reservoir White Paper August 2007 which is incorporated by reference herein in its entirety. In block shows the CNF representation this would correspond to an instance of an output of block in . In this block lines that start with the keyword c are comments and the rest of the lines correspond to the clauses of the CNF. Each integer identifies an input binary variable of the original signature except for the last integer which is always 0 and defines the end of a clause and negative signs represent the negation of a variable. Hence the CNF expression in block is as follows bit1 bit2 bit3 bit4 0 and bit6 bit7 bit9 bit6 bit7 bit8 bit5 bit6 bit9 bit5 bit6 bit8 bit5 bit7 bit9 bit5 bit7 bit8 where and represent logical operators AND OR and NEGATION respectively.

Block in shows a BDD representation of this CNF expression written in the BuDDy language which is described in J. Lind Nielsel BuDDy Binary Decision Diagram Package manual white paper November 2002. In a graphical representation of this BDD is presented. Next suppose that this BDD is to be mapped onto three DFA engines. In this instance some embodiments follow different optimizations criteria. For instance presents a partition of the initial BDD into three smaller BDDs that prove to be min max i.e. that the maximum number of nodes of any of the BDDs is minimal. The final DFA specification can be obtained by OR ing the output of these three BDDs.

In another exemplary embodiment provides an optimization method for the partitioning of the BDDs based on the following constraints 1 there exist two DFA engines represented by E and E 2 DFA engine E has unlimited amount of memory whereas DFA engine E has bounded amount of memory and 3 the usage of DFA engine E should be maximal. One embodiment of this configuration corresponds to a host computer configured with a DFA hardware acceleration engine. In this case E could be implemented as a regular expression software library running on the host which would effectively enjoy unlimited amounts of memory and E would correspond to the hardware DFA engine with limited amounts of embedded memory.

In S and S correspond to the set of clauses of the BDDs that are to be mapped on E and E respectively computeBDD S is a function that emits a BDD given a CNF expression S shiftClauses S S is a function that moves a subset of clauses from S to S following a certain optimization criterion and the target size is defined as the maximum BDD size allowed by E. The method starts at with S being initialized with the complete CNF expression and S being empty. First computeBDD module calculates the BDD for S. In If its size is equal to the target size or within a tolerance then the method terminates at else if it is determined in that the size is larger than the target size at clauses are shifted from S to S and the method returns to otherwise the size is smaller and at it is determined if S is empty. If it is empty the method terminates at otherwise clauses are shifted from S to S at and the method returns to .

In yet another provided embodiment the logic order of the systems presented in and are reversed producing different sets of DFA specifications that are logically equivalent.

Illustrated in are computing apparatus and computer software products consistent with provided embodiments. Computing apparatus includes processor memory storage medium and in some embodiments input port and network interface . In many provided embodiments non transitory storage medium contains a set of processor executable instructions that when executed by processor configure computing apparatus to implement the modules and methods described herein. In one embodiment storage medium containing the set of processor executable instructions resides in another computing apparatus across network . In an embodiment of a computer software product computer software product is a computer readable storage medium containing processor executable instructions sufficient that when executed by processor configure computing apparatus to implement the above described modules and methods. Further computer software product in some embodiments consists of a physical medium configured to interface with input port to allow its contents to be copied to storage medium . In other embodiments computer software product is an internal storage medium such as . An additional embodiment of computing apparatus includes a plurality of processors a plurality of memories a storage medium and in some embodiments input port and network connection . In some embodiments one or more processors is a host while others are modeled in the form of a grid.

Thus it is seen that methods apparatus and computer software products for implementation of high speed network analyzers is provided. One skilled in the art will appreciate that embodiments of the present invention can be practiced by other than the above described embodiments which are presented in this description for purposes of illustration and not of limitation. The specification and drawings are not intended to limit the exclusionary scope of this patent document. It is noted that various equivalents for the particular embodiments discussed in this description may practice the invention as well. That is while the present invention has been described in conjunction with specific embodiments it is evident that many alternatives modifications permutations and variations will become apparent to those of ordinary skill in the art in light of the foregoing description. Accordingly it is intended that the present invention embrace all such alternatives modifications and variations as fall within the scope of the appended claims. The fact that a product process or method exhibits differences from one or more of the above described exemplary embodiments does not mean that the product or process is outside the scope literal scope and or other legally recognized scope of the following claims.

