---

title: Operating system image management
abstract: In a data processing system including multiple logical partitions (LPARs), an application executes on a first logical partition (LPAR) of the multiple LPARs, where the application uses a first operation system stored in a first memory partition of a shared pool memory of the data processing system. A virtualization management component (a) initiates an update process that quiesces operations of the first LPAR, (b) pages in, via a virtual input/output server coupled to a first paging device, a first image of a second operating system from the first paging device to the shared pool memory; (c) changes one or more pointers associated with the application to point to one or more portions of the second operating system, such that the application uses the second operating system, when resumed; and (b) resumes execution the application.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08473692&OS=08473692&RS=08473692
owner: International Business Machines Corporation
number: 08473692
owner_city: Armonk
owner_country: US
publication_date: 20101027
---
The present invention relates in general to distributed data processing systems and in particular to operating systems within distributed data processing systems associated with virtual input output servers and client logical partitions. Still more particularly the present invention relates to a method data processing system and computer program product that creates and or replaces operating systems within one or more client logical partitions of distributed data processing systems.

Virtualized data processing system configuration which provides the virtualization of processor memory and operating system OS resources are becoming more and more common in the computer and particularly the computer server industry. To a lesser extent storage virtualization is also known and provided in various environments. However within the virtualization computing environment storage virtualization and management is implemented as a separate virtualization model from server virtualization and management. As such different client logical partitions LPARs associated with different virtualized server systems may execute one or more operating systems that need to be upgraded and or replaced. In the past however each client LPARs are shutdown the OS of the client logical partition LPAR is upgraded or replaced and the client LPAR is booted or started after the OS of the client LPAR is upgraded or replaced.

Disclosed are a method data processing system and a computer program product that creates an operating system OS image and replaces an existing OS on one or more logical partitions LPARs according to one or more embodiments. For example in a data processing system including multiple LPARs an application executes on a first logical partition LPAR of the multiple LPARs where the application uses a first operation system stored in a first memory partition of a shared pool memory of the data processing system. In one embodiment a virtualization management component or hypervisor initiates an update process that quiesces operations of the first LPAR pages in via a virtual input output server VIOS coupled to a first paging device a first image of a second operating system from the first paging device to the shared pool memory changes one or more pointers associated with the application to point to one or more portions of the second operating system such that the application uses the second operating system when resumed and resumes execution the application.

In another embodiment the virtualization management component or hypervisor creates a second logical partition of the multiple LPARs loads the second operating system into a second memory partition of the shared pool memory such that the second logical partition uses the second memory partition as a second memory of the second logical partition pages out via the VIOS coupled to a second paging device of the plurality of paging devices a second image of the second operating system and produces the first image of the second operating system by copying one or more of metadata and data associated with the second operating system to the first image. In one embodiment the first paging device and the second paging device are associated with respective files in a file system and the VIOS provides the file via a virtual SCSI device to the virtualization management component or hypervisor as the first paging device and the second paging device. According to another embodiment a third OS can be produced such that the third OS that includes the first image of the second operating system first metadata changes and first data changes differing from the first image of the second operating system that are specific to a second logical partition.

In one or more embodiments a method data processing system and a computer program product establishes cluster awareness for a VIOS created within an OS partition of a computing electronic complex within the data processing system. One embodiment provides a computing electronic complex CEC comprising a processor one or more physical input output I O adapters that support I O communication with an external network and a memory coupled to the one or more processors and having a virtualization management component that supports creation of and communication between one or more virtualized operating system OS partitions. The memory also includes at least one operating system OS partition including a first virtual input output I O server VIOS partition having a cluster aware CA OS that executes on a virtual processor resource of the VIOS. When executed the CA OS performs the function of registering the first VIOS with a VIOS cluster comprising at least a second VIOS. The registering of the first VIOS to the VIOS cluster enables the first VIOS to receive cluster specific data to make the first VIOS aware of the VIOS cluster and the first VIOS is thus able to communicate information with other VIOSes within the VIOS cluster.

According to one embodiment registering of the first VIOS with the VIOS cluster comprises initializing a cluster registration module that assigns the VIOS with a unique ID and forwards VIOS registration data to a shared VIOS database DB receiving from the shared VIOS DB cluster configuration data and status data of the VIOS cluster for local storage at the first VIOS and updating a local storage of the first VIOS with the received cluster configuration data and status data.

In one embodiment the CA OS further performs the functions of initializing an I O emulation module of the CA OS to enable the VIOS to provide virtual I O VIO services to one or more client logical partitions LPARs existing within one of the OS partitions registering a first client LPAR with the VIOS assigning a unique VIO adapter to the client LPAR and enabling I O operations for the client LPAR by linking the unique VIO adapter to one of the one or more physical I O adapters.

The above summary contains simplifications generalizations and omissions of detail and is not intended as a comprehensive description of the claimed subject matter but rather is intended to provide a brief overview of some of the functionality associated therewith. Other systems methods functionality features and advantages of the claimed subject matter will be or will become apparent to one with skill in the art upon examination of the following figures and detailed written description.

The above as well as additional objectives features and advantages of the present invention will become apparent in the following detailed written description.

The illustrative embodiments provide a method data processing system and computer program product that efficiently creates an operating system OS image and replaces an existing OS on one or more logical partitions LPARs According to the method which is executed within a data processing system that includes multiple logical partitions LPARs with at least one cluster aware Virtual Input Output I O Server VIOS an application executes on a first logical partition LPAR of the multiple LPARs where the application uses a first operation system stored in a first memory partition of a shared pool memory of the data processing system. A virtualization management component a initiates an update process that quiesces operations of the first LPAR b pages in via a virtual input output server coupled to a first paging device a first image of a second operating system from the first paging device to the shared pool memory c changes one or more pointers associated with the application to point to one or more portions of the second operating system such that the application uses the second operating system when resumed and b resumes execution the application.

In the following detailed description of exemplary embodiments of the invention specific exemplary embodiments in which the invention may be practiced are described in sufficient detail to enable those skilled in the art to practice the invention and it is to be understood that other embodiments may be utilized and that logical architectural programmatic mechanical electrical and other changes may be made without departing from the spirit or scope of the present invention. The following detailed description is therefore not to be taken in a limiting sense and the scope of the present invention is defined by the appended claims and equivalents thereof.

Within the descriptions of the different views of the figures similar elements are provided similar names and reference numerals as those of the previous figure s . The specific numerals assigned to the elements are provided solely to aid in the description and are not meant to imply any limitations structural or functional or otherwise on the described embodiment.

It is understood that the use of specific component device and or parameter names such as those of the executing utility logic firmware described herein are for example only and not meant to imply any limitations on the invention. The invention may thus be implemented with different nomenclature terminology utilized to describe the components devices parameters herein without limitation. References to any specific protocol or proprietary name in describing one or more elements features or concepts of the embodiments are provided solely as examples of one implementation and such references do not limit the extension of the invention to embodiments in which different element feature or concept names are utilized. Thus each term utilized herein is to be given its broadest interpretation given the context in which that terms is utilized. For example as utilized herein the term cluster aware refers to the operational state of each VIOS within the cluster where the VIOSes contain information about which other VIOSes are connected within the cluster the configuration of the different CECs within the DPS supported by the cluster information about which client LPARs are supported by each VIOS and other state and operating information and data related to performing VIO operations using the physical I O devices of the DPS and those of the distributed storage repository storage repository . Cluster awareness is supported by both a shared networked VIOS database and locally maintained copies of VIOS cluster data within each VIOS.

As further described below implementation of the functional features of the invention is provided within processing devices structures and involves use of a combination of hardware firmware as well as several software level constructs e.g. program code . The presented figures illustrate both hardware components and software components within example data processing architecture having a specific number of processing nodes e.g. computing electronic complexes . The illustrative and described embodiments assume that the system architecture may be scaled to a much larger number of processing nodes.

In the following descriptions headings or section labels are provided to separate functional descriptions of portions of the invention provided in specific sections. These headings are provided to enable better flow in the presentation of the illustrative embodiments and are not meant to imply any limitation on the invention or with respect to any of the general functions described within a particular section. Material presented in any one section may be applicable to a next section and vice versa. The following sequence of headings and subheadings are presented within the specification 

With specific reference now to there is depicted a block diagram of an example cluster aware CA distributed data processing system DPS architecture within which the functional aspects of the described embodiments may advantageously be implemented. For simplicity cluster aware distributed DPS architecture shall be referred to herein simply as DPS . DPS comprises a plurality of computing nodes each referred to herein as a computing electronic complex CEC of which CECs A and B are illustrated. The number of CECs within DPS may vary ranging from a single CEC in a smaller system extending up to hundreds or thousands of CECs in larger scaled systems. For simplicity the embodiments shall be described from the perspective of a single CEC CEC A or two CECs CECs A B . Each CEC A B comprises at least one and in most instances a plurality of Virtual Input Output Server also referred to herein as a VIO Server or VIOS with functionality as described below. The actual number of VIOSes within each CEC of DPS is a design feature and may vary. Also supported within each CEC A B are client logical partitions interchangeably referred to as client LPARs or clients of which a first two clients clientA and clientB are illustrated. As described below with reference to client LPARs are logical partitions of a virtualized or operating system partitioned computing system. The actual number of clients within each CEC may vary and could range from a single client to hundreds or thousands of clients without limitation. For efficiency in presenting the inventive concepts herein only two clients are presented within each CEC of the various illustrative and described embodiments.

DPS also comprises a distributed storage facility accessible to each of the CECs and the components within the CECs . Within the described embodiments the distributed storage facility will be referred to as distributed storage repository and the distributed storage repository enables several of the client level functional features provided by the embodiments described herein. Distributed storage repository provides a single view of storage that is utilized by each CEC and for each client of each CEC within a cluster aware distributed system. Distributed storage repository comprises local physical storage and network storage both of which comprise multiple physical storage units e.g. disks. solid state drives etc. . The physical disks making up distributed storage repository may be distributed across a storage network e.g. a SAN . Additionally distributed storage repository provides a depository within which is stored and maintained the software utility instruction code OS images client images data system node and client level and or other functional information utilized in maintaining the client level system management and storage level operations features of DPS . In addition to distributed storage repository DPS also comprises a VIOS database DB which may also be a distributed storage facility comprising physical disks across a storage network. VIOS DB or DB is a repository that stores and provides access to various cluster configuration data and other functional components modules and data structures that enable the various cluster aware functionality described herein. In one embodiment portions of distributed storage repository may be allocated to provide storage pools for a cluster. Each VIOS of the cluster maintains a local view of the DB and updates the cluster level information data data structures within DB as such information data is created or updated.

Communication between each VIOS of each CEC as well as with the VIOSes of at least one other CEC is generally supported via a plurality of inter CEC interconnects illustrated as bi directional dashed lines connecting pairs of VIOSes . The arrows indicated two way data exchange or communication between components. In addition to the inter CEC interconnects each VIOS is also connected to distributed storage repository via VIOS to Store or CEC to Store interconnects which are also illustrated as full lined bi directional arrows. Also each VIOS is connected to DB via VIOS to DB interconnects presented as dashed and dotted lines. With the exception of the inter CEC connectors running from a first VIOS e.g. VIOS of a first CEC to a second VIOS e.g. VIOS on the same CEC the various interconnects represent a network level connectivity between the VIOS nodes of the cluster and the DB and the distributed storage repository . As utilized herein references to one or more nodes are assumed to refer specifically to a VIOS within the cluster. DPS also comprises a management console on which a management tool not shown executes.

Turning now to there is illustrated another view of DPS illustrating the network based connection of the CECs to the distributed storage repository and DB . illustrates in greater detail the network connectivity of VIOSes and CECs to each other and to Distributed storage repository . With this view CEC A Node A A and CEC B Node B B comprise similar constructs as presented in . Each CEC within DPS connects to distributed storage repository via one or more networks and or I O interconnect switch fabric generally illustrated as interconnect network fabric . The descriptions and illustrations assume that at least some of the CECs of DPS and distributed storage repository are located remotely from each other including being located in different countries for example such that no direct physical connectivity exists between the respective devices. For simplicity the embodiments are described as having primary interconnect network comprising a private wide area network WAN or a public WAN such as the Internet although other network types e.g. a local area network are possible and supported.

As depicted in one or more embodiments each CEC is also connected to one or more neighbor CECs in order to provide efficient fail over and or mobility support and other functions as described hereinafter. As utilized herein the term neighbor refers to a connected second CEC with which a first CEC is able to communicate and references to a neighbor CEC is not limited to a second CEC in geographic proximity to the first CEC. CEC A A and CEC B B are illustrated connected to each other via some connecting medium which may include a different network such as a local area network or some type of direct interconnect e.g. a fiber channel connection when physically close to each other. The connection between neighbor CECs A and B is illustrated as a direct line connection or a secondary network connection between CECs A and B. However it is appreciated that the connections are not necessarily direct and may actually be routed through the same general interconnect network as with the other CEC connections to distributed storage repository . In one or more alternate embodiments the connections between CECs may be via a different network e.g. network such as a local area network LAN .

Also illustrated by is an initial view of the component make up of an example distributed storage repository and an initial listing of some components of DB . As depicted each CEC comprises one or more network interfaces and one or more I O adapters to enable the CEC and thus the other components i.e. client partitions of the CEC to engage in network level communication as described below. Specifically each VIOS emulates virtual client I O adapters to enable communication by the client LPARs with distributed storage repository and or other clients within the same CEC or on a different CEC. The VIOSes emulate virtual I O adapters and communicates with distributed storage repository by connecting with corresponding virtual sever I O adapters at distributed storage repository . The VIOSes within each CEC are thus able to support client level access to distributed storage and enable the exchange of system level and client level information with distributed storage repository .

In addition each VIOS also comprises the functional components modules and data to enable the VIOSes within DPS to be aware of the other VIOSes anywhere within the cluster DPS . From this perspective the VIOSes are referred to herein as cluster aware and their interconnected structure within DPS thus enables DPS to also be interchangeably referred to as cluster aware DPS . As a part of being cluster aware each VIOS also connects to DB via network and communicates cluster level data with DB to support the cluster management functions described herein.

To support the virtual I O operations with the VIOSes and the associated virtual client I O adapters distributed storage repository comprises communication infrastructure . Communication infrastructure comprises network interface s and a plurality of server I O adapters utilized for cluster level communication and enabling access to data code software utility stored on distributed storage repository to complete I O operations thereto. Specifically these server I O adapters are also presented as virtual sever I O adapters which are paired with virtual I O adapters that are assigned to clients of CECs .

As shown with distributed storage repository DSR also comprises a plurality of software firmware and or software utility components including DSR configuration utility DSR configuration data e.g. inodes for basic file system access metadata authentication and other processes and DSR management utility .

To support the cluster awareness features of the DPS and in accordance with the illustrative embodiment distributed storage repository also comprises VIOS database DB in which is stored various data structures generated during set up and or subsequent processing of the VIOS cluster connected processing components e.g. VIOSes and management tool . DB comprises a plurality of software or firmware components and or and data data modules or data structures several of which are presented in for illustration. Among these components are cluster management CM utility VIO AdapterID data structure cluster configuration data Client identifying ID data active nodes list and I O redundancy data among others. These various components support the various clustering functionality and cluster aware I O operations of the one or more VIOSes as described herein. Additional features of DB and distributed storage repository as well as the specific components or sub components that enable the various clustering functionality are presented within the description of the remaining figures and throughout the description of the various embodiments.

These various data structures are created maintained and or updated and or deleted by the various operations of one or more of the processing components. In one embodiment the initial set up of the storage pools VIOS DB and corresponding data structures is activated by execution of a cluster aware operating system by management tool . Once the infrastructure has been established however maintenance of the infrastructure including expanding the number of nodes where required is performed by the VIOSes in communication with DB and the management tool .

Also associated with DPS and communicatively coupled to distributed storage repository and DB and VIOSes is management console which may be utilized by an administrator of DPS or of distributed storage repository or DB to access DB or distributed storage repository and configure resources and functionality of DB and of distributed storage repository for access usage by the VIOSes and clients of the connected CECs within the cluster. As shown in and described throughout the specification management tool is implemented within management console . However it is appreciated that resources of any node within DPS may be selected elected to perform the functions of management tool and the selected node would then perform one or more of the below described cluster creation and the other cluster monitoring and management functions utilizing the availability of the resources provided by DB and distributed storage repository .

In an alternate embodiment management tool is an executable module that is executed within a client partition at one of the CECs within DPS . In one embodiment the management tool controls the operations of the cluster and enables each node within the cluster to maintain current updated information regarding the cluster including providing notification of any changes made to one or more of the nodes within the cluster.

With reference now to there is presented a third view of an example DPS emphasizing a processing system architecture i.e. architecture of the individual CECs and specifically CEC A A . CEC A A CEC A serves as the example CEC that is described in greater detail in and throughout the specification. CEC A is presented as a server that comprises hardware components and software firmware OS components that are logically partition to create a plurality of virtualized machine partitions which are assigned as client logical partitions LPARs and virtual I O servers VIOSes . Hardware components of example CEC A comprises one or more processors A P one or more memories A M and local storage . The processors A P are interconnected with one or a plurality of memories A M and with local storage via a bus interconnect switch or an interconnect fabric not specifically shown . The specific internal connectivity of components which may be distributed across a large scale interconnect fabric is not germane to the described embodiments and no further detail is presented regarding the particular type of interconnectivity between the system hardware components.

Also included within hardware components are one or more physical network interfaces by which CEC A A connects to an external network such as network among others. Additionally hardware components comprise a plurality of I O adapters A E which provides the I O interface for CEC A A. I O adapters A E are physical adapters that enable CEC A to support I O operations via an I O interface with both locally connected and remotely networked connected I O devices including SF storage . Examples of I O adapters include Peripheral Component Interface PCI PCI X or PCI Express Adapter and Small Computer System Interconnect SCSI adapters among others. CEC is logically partitioned such that different I O adapters are virtualized and the virtual I O adapters may then be uniquely assigned to different logical partitions.

Logically located above the hardware level is a virtualization management component provided as a Power Hypervisor PHYP trademark of IBM Corporation as one embodiment. While illustrated and described throughout the various embodiments as PHYP it is fully appreciated that other types of virtualization management components may be utilized and are equally applicable to the implementation of the various embodiments. PHYP has an associated service processor coupled thereto within CEC . Service processor may be used to provide various services for one or more logical partitions. PHYP is also coupled to hardware management controller HMC which exists outside of the physical CEC . Operations of the different logical partitions may be controlled through HMC which is a separate data processing system from which a system administrator may perform various functions such as reallocation of resources to different logical partitions.

CEC A A further comprises a plurality of user level logical partitions LPARs of which a first two are shown represented as individual client LPARs A B within CEC A. According to the various illustrative embodiments CEC A supports multiple clients and other functional operating OS partitions that are created within a virtualized environment. Each LPAR e.g. client LPAR A receives an allocation of specific virtualized hardware and OS resources including virtualized CPU A Memory A OS A local firmware and local storage LStore . Each client LPAR includes a respective host operating system that controls low level access to hardware layer of CEC A and or to virtualized I O functions and or services provided through VIOSes . In one embodiment the operating system s may be implemented using OS 400 which is designed to interface with a partition management firmware such as PHYP and is available from International Business Machines Corporation. It is appreciated that other types of operating systems such as Advanced Interactive Executive AIX operating system a trademark of IBM Corporation Microsoft Windows a trademark of Microsoft Corp or GNU Linux registered trademarks of the Free Software Foundation and The Linux Mark Institute for example may be utilized depending on a particular implementation and OS 400 is used only as an example.

Additionally according to the illustrative embodiment CEC A also comprises one or more VIOSes of which two VIOS A and B are illustrated. In one embodiment each VIOS is configured within one of the memories A M and comprises virtualized versions of hardware components including CPU memory local storage and I O adapters among others. According to one embodiment each VIOS is implemented as a logical partition LPAR that owns specific network and disk I O adapters. Each VIOS also represents a single purpose dedicated LPAR. The VIOS facilitates the sharing of physical I O resources between client logical partitions. Each VIOS allows other OS LPARs which may be referred to as VIO Clients or as Clients to utilize the physical resources of the VIOS via a pair of virtual adapters. Thus VIOS provides virtual small computer system interface SCSI target and shared network adapter capability to client LPARs within CEC . As provided herein VIOS supports Virtual real memory and Virtual shared storage functionality with access to Distributed storage repository as well as clustering functionality.

Within CEC A VIOSes and client LPARs utilize an internal virtual network to communicate. This communication is implemented by API calls to the memory of the PHYP . The VIOS then bridges the virtual network to the physical I O adapter to allow the client LPARs to communicate externally. The client LPARs are thus able to be connected and inter operate fully in a VLAN environment.

Those of ordinary skill in the art will appreciate that the hardware firmware software utility and software components and basic configuration thereof depicted in and may vary. The illustrative components of DPS and specifically those within CEC A are not intended to be exhaustive but rather are representative to highlight some of the components that are utilized to implement certain of the described embodiments. For example different configurations of data processing systems CECs devices may be provided containing other devices components which may be used in addition to or in place of the hardware depicted and may be differently configured. The depicted example is not meant to imply architectural or other limitations with respect to the presently described embodiments and or the general invention. The CEC depicted in the various figures may be for example an IBM eServer pSeries system a product of International Business Machines Corporation in Armonk N.Y. running the Advanced Interactive Executive AIX operating system or LINUX operating system.

Certain of the features associated with the implementation of a cluster aware VIOS e.g. VIOS of and are introduced above with reference to the description of the previous figures and particularly . Descriptions of the specific functionality of the VIOS will continue to be provided with reference to the illustrations of and . As presented by each VIOS is a virtual machine instance that emulates hardware in a virtualized environment. The VIOS is tasked with emulating SCSI storage devices and the VIOS provides client LPARs with access to distributed storage repository in cooperation with the PHYP . Configuration of the VIOS is performed through the hardware management tools of HMC . SCSI storage devices support a set of commands that allow SCSI initiators the ability to control access to storage . Database programs for example may manage access to distributed storage repository through a set of SCSI commands commonly referred to as persistent reserve. Other types of reserves are also supported by VIOS and the collective group of such commands is referred to herein as reserve commands.

As provided herein each VIOS allows sharing of physical I O resources between client LPARs including sharing of virtual Small Computer Systems Interface SCSI and virtual networking These I O resources may be presented as internal or external SCSI or SCSI with RAID adapters or via Fibre Channel adapters to distributed storage repository . The client LPAR however uses the virtual SCSI device drivers. In one embodiment the VIOS also provides disk virtualization for the client LPAR by creating a corresponding file on distributed storage repository for each virtual disk. The VIOS allows more efficient utilization of physical resources through sharing between client LPARs and supports a single machine e.g. CEC to run multiple operating system OS images concurrently and isolated from each other.

In one or more embodiments the VIOS operating system s is an enhanced OS that includes cluster aware functionality and is thus referred to as a cluster aware OS CA OS . One embodiment for example utilizes cluster aware AIX CAA as the operating system. According to one embodiment cluster awareness enables multiple independent physical systems to be operated and managed as a single system. As provided within VIOS of CEC A VIOS comprises cluster aware CA OS kernel or simply CA OS as well as LPAR function code for performing OS kernel related functions for the VIOS LPARs . When executed within two or more nodes of DPS CA OS enables various clustering functions such as forming a cluster adding members to a cluster and removing members from a cluster as described in greater detail below. CA OS manages the VIOS LPARs and enables the VIOSes within a cluster to be cluster aware. CA OS comprises several functional modules. In the described embodiments CA OS comprises cluster management CM utility which supports the configuration of the VIOS to enable cluster awareness and cluster level functionality such as redundant virtual I O. Each of these additional software components of CA OS may be a functional module within CM utility in one embodiment and each module is thus described as such throughout the remainder of this specification. In one embodiment CM utility may be a separate utility that is locally installed or downloaded from DB for example as an enhancement to an existing OS within a CEC or VIOS when initially configured for operation within the VIOS cluster. CM utility is then executed when configuring the individual VIOS to create or join a cluster and or become a cluster aware node within the VIOS cluster. With this implementation structure CM utility enables the OS to support the various cluster awareness and other cluster level features and functionality. In an alternate embodiment CA OS includes all the clustering features and functionality and established the various features when the CEC VIOS joins the cluster and or during configuration of VIOS to become cluster aware.

In one implementation functional components of CM utility are encoded on local device storage of a corresponding VIOS such that the VIOS becomes automatically configured as a part of the VIOS cluster when the VIOS is initially activated. On initial set up of the VIOS VIOS API kernel extensions and virtual adapters are configured within VIOS to enable communication with the other VIOSes the VIOS DB and with the distributed storage repository . During this initial setup of the VIOS the VIOS executes a registration module of CM utility to register VIOS with the cluster. The registration module enables VIOS to retrieve download or have forwarded from DB on successful registration with the cluster any additional CM software components and or cluster level information and or data required to establish full cluster awareness when the VIOS has completed installation and is activated within the CEC . Thus in one embodiment in addition to the locally stored CA OS components and software modules of CM utility other functional components of CM utility may be downloaded from DB when CEC is powered on or when one or more VIOSes are enabled on CEC . Once the VIOS has completed its setup one or more client LPARs that are activated within CEC may be assigned to VIOS and VIOS subsequently performs the various I O operations initiated by the client as initiator or directed to the client as target . Updates to the local VIOS data may periodically be made as changes are made within the VIOS cluster and or as one or more new client LPARs are added to the CEC requiring VIOS support. In one embodiment CM utility may also enable retrieval and presentation of a comprehensive view of the resources of the entire cluster.

It is appreciated that while various functional aspects of the clustering operations are described as separate components modules and or utility and associated data constructs the entire grouping of different components utility data may be provided by a single executable utility application such as CA OS or CM utility . Thus in one embodiment CA OS executes within VIOS and generates a plurality of functional components within VIOS and within DB . Several of these functional components are introduced within and and others are described throughout the various embodiments provided herein. For simplicity in the descriptions which follow references to CM utility and CA OS will be assumed to be referring to the same general component i.e. CM utility being a subcomponent of CA OS and the terms may be utilized interchangeably throughout the specification.

As further presented by the illustrative embodiments e.g. VIOS includes one or more additional functional modules components such as VIO adapter s interface and virtual I O drivers utility which provides I O functionality to VIOS and enables VIOS to route data traffic to and from data structures and storage within distributed storage repository and or DB . Virtual I O adapter s and CM utility also enable the VIOS to provide each client LPAR with access to the full range of storage accessible within distributed storage repository and other cluster supported functionalities as described herein.

In the illustrative embodiment each client LPAR communicates with VIOS via PHYP . VIOS and client LPAR A B are logically coupled to PHYP which enables supports communication between both virtualized structures. Each component forwards information to PHYP and PHYP then routes data between the different components in physical memory A M . In one embodiment a virtualized interface of I O adapters is also linked to PHYP such that I O operations can be communicated between the different logical partitions and one or more local and or remote I O devices. As with local I O routing data traffic coming in and or out of I O adapter interface or network interface from a remote I O device is passed to the specific VIOS via PHYP .

With the above introduced system configuration of and a first VIOS through a communication channel established via PHYP grants access to another VIOS through one or more virtual adapters. VIOS includes the functionality to query PHYP for the identity of the Client LPAR on the CEC where the VIOS is currently running.

With the cluster aware VIOS infrastructure different VIOSes associated with different CECs access the distributed storage repository and cluster level information is shared communicated across the VIOS cluster via VIOS DB while each client I O process is being performed. In this manner the VIOS associated with a first client on a first CEC is aware of which SAN disk resources are being accessed by a second client on a second CEC or on the same CEC . With this awareness factored into the I O exchange with the distributed storage repository the VIOS associated with the first client can avoid accessing the same storage resource that is concurrently being utilized by the second client thus preventing data integrity issues which could potentially cause data corruption and client partition crashes.

In one embodiment VIOS functionality is enhanced to enable assigning of client identifiers ID and unique virtual I O adapter IDs in a secure manner while enabling storage pooling within virtual storage within distributed storage repository . According to the described implementation the different clientID vioAdapterID pairings are unique throughout the cluster so that no two clients throughout the entire cluster can share a same virtual adapter and no two vioAdapterIDs are the same within a single client. is a flow chart illustrating the method by which a VIOS on a CEC with DPS enables cluster level communication between a client LPAR and distributed storage repository according to one embodiment. The process begins at block and proceeds to block at which the VIOS queries PHYP for the identity of the client LPAR . At block the VIOS creates a unique identifier ID for the client i.e. a ClientID . The VIOS then stores the unique ClientID in ClientID data structure within DB block . The DB and by extension the ClientID data structure are accessible to each VIOS partition in the cooperating cluster DPS . At block the VIOS also generates an identifier for each virtual IT nexus virtual I O AdapterID that is utilized for each virtual adapter assigned to the client LPAR . In one embodiment a client LPAR can have multiple virtual adapters assigned thereto. These vio AdapterIDs are stored in the AdapaterID data structure block and are associated with their corresponding clientIDs block . The method illustrated by ends at termination block with each clientID having been associated with the corresponding one or more vio AdapterIDs with DB .

As described herein a cluster is a set of one or more networked VIOS partitions where each VIOS within the cluster has access to a common set of physical volumes. The physical volume resides within the VIOS cluster and is utilized to provide block storage. Implementation of the cluster awareness with the VIOSes of the cluster enables the VIOSes to provide cluster storage services to virtual clients client LPARs . The VIOS software stack provides the following advanced capabilities among others Storage Aggregation and Provisioning Thin Provisioning Virtual Client Cloning Virtual Client Snapshot Virtual Client Migration Distributed Storage Repository Virtual Client Mirroring and Server Management Infrastructure integration. More generally the VIOS protocol allows distributed storage to be viewed as centralized structured storage with a namespace location transparency serialization and fine grain security. The VIOS protocol provides storage pooling distributed storage and consistent storage virtualization interfaces and capabilities across heterogeneous SAN and network accessible storage NAS . In order to provide block storage services utilizing the distributed repository each VIOS configures virtual devices to be exported to virtual clients. Once each virtual device is successfully configured and mapped to a virtual host VHOST adapter the clients may begin utilizing the devices as needed. In one embodiment the virtualization is performed utilizing POWER virtual machine VM virtualization technology which allows the device configuration process to occur seamlessly because the physical block storage is always accessible from the OS partition.

One embodiment provides a communication protocol that enables efficient communication between the Clients and distributed storage repository via the respective VIOS and virtual I O adapters assigned within the VIOSes to the specific client . The embodiment further provides storage virtualization and management via the specific communication mechanisms protocols implemented with respect to the use of cluster awareness and the Distributed storage repository such that the virtualization is presented within the context of the server CEC virtualization and management. With the presented protocol different VIOSes associated with different CECs access the same single distributed DB and cluster level information is shared communicated with each Client I O process such that a first client on a first CEC is aware of which SAN disk resources are being accessed by a second client on a second CEC or on the same CEC . With this awareness factored into the I O exchange with the distributed storage repository the first client can avoid accessing the same storage resource that is concurrently being utilized by the second client thus preventing data integrity issues which would potentially cause data corruption and client partition crashes.

The communication protocol provides a highly integrated server based storage virtualization as well as distributed storage across clustered VIOS partitions. This protocol comprises one or more query features which enables dynamic tracking of storage resource usage across the entire cluster. Throughout the following description the communication and management protocol shall be described as a VIOS protocol. VIOS protocol provides distributed storage across clustered VIOS partitions. With the VIOS protocol the storage is considered as a one large storage pool which chunks of storage i.e. logical units or LUs allocated to each client . The VIOSes within the overall system DPS are now structured as part of the cluster with each VIOS being a node in the cluster. Each VIOS node communicates with other VIOS nodes utilizing the VIOS protocol. With this configuration of VIOSes when two or more client LPARs belonging to different CECs share storage on the SAN e.g. two clients assigned overlapping LUs the VIOS protocol enables each node to query each client within the cluster to determine the current usage of the storage device. When this information is received the VIOS may then disseminate this information to other VIOSes. Each client is thus made aware of whether the SAN storage device that the client is trying to access is currently being used by some other client.

Referring now to there is illustrated an example VIOS communication infrastructure having an application programming interface API controlling the various exchanges between XML components over a virtual Small Computing Systems Interface vSCSI topology. Central to the Cluster VIOS communication paradigm is a plurality of APIs of which API is provided in the illustrative embodiment. The VIOS API is utilized to manage objects within a VIOS cluster. The API includes the necessary information about how to connect to and or exchange information with internal VIOS functional modules as well as with DB DDS and management tool . In one embodiment management tool is implemented within a cluster aware server module and includes server management sub agents which represents the structures utilized by the managing tool to communicate with the operating system. The internal functional modules within VIOS comprises command line interface CLI Daemon socket kernel extension vKE and vSCSI host . The vSCSCI host includes the enhancements to VIOS that enable the cluster aware functionality. These enhancements are illustrated as a connected block structure by which advanced VIOS operations and emulation are provided as described in greater detail below. VIOS with its various internal components is connected within CEC via PHYP as previously illustrated by described above.

Each component that connects with API and makes one or more requests through API is generally referred to as a caller throughout this specification. As presented by the figure any one or management tool via management agent CLI Daemon and vSCSI host may be a caller requesting specific types of information exchange via API . In one embodiment the API comprises an XML interface as well as a C programming language interface. The various callers use the VIOS API to initiate actions on these objects. Some actions may change the state of one or more objects in the VIOS cluster. The VIOS API may be used by multiple callers at any given time. While callers are not aware of other callers using the VIOS API and do not have the ability to notify all callers of actions that they initiate the VIOS API event notification protocol provides cluster level awareness of caller modifications to prevent data contamination during processing of multiple caller requests. Callers that need awareness of actions taken on VIO objects are able to register for event notification and receive notification about changes to VIO objects that occur within the cluster. The callers then utilize the notifications as a trigger to go to the shared storage DB and retrieve the necessary information from the shared VIOS cluster DB to keep the caller s locally stored VIO object data current. Additionally in one embodiment VIOS API event notification provides participating callers with results to actions that have occurred on one or more VIO objects. As described herein these VIO object events are categorized as Lifecycle events or Alert events.

In one embodiment to decrease the amount of APIs required be each consumer only a few high level APIs are exposed. Each API provides various actions on an object by object basis. Interaction between the API and a consumer a caller receiving data in response to a requestor a caller registered to receive notification of an event is performed by the consumer providing a VIO request extensible markup language XML buffer with sufficient amount of data provided in order for the request to be processed. Once the request has been processed a VIO response XML steam is written back to the caller for response processing. When the response indicates a successful processing of the request the XML steam contains the status and the requested object information that is needed. However if the request fails the response XML stream contains VIO exception information. The common format of each object API is to provide a vioRequest structure that contains the required information needed for request processing.

Returning to in the illustrative embodiments a VIOS emulates SCSI devices using a kernel extension vscsi host kernel extension kernel extension in the VIOS partition which also includes the code modules for providing VCSI host and Daemon . VSCSI host includes one or more driver s and sub driver s which provide separate functions. A first set of drivers provides emulation functionality while other drivers provide transport and messaging functionality. VSCSI host includes VIOS enhanced operational functionality illustrated via additional structure coupled to VSCSI host . Structure includes software modules that enable the various messaging structures used for implementing VIOS cluster awareness functionality and VIOS Client emulation. Client logs into the VIOS as part of the transport layer protocol. At the time the client logs into the VIOS the PHYP provides information to the VIOS regarding the identity ID of the client relative to the CEC . The VKE services SCSI requests sent by the VIOS through a transport layer supported by PHYP . The kernel code does not complete the login request until the VKE sends a message with the CEC relative client ID using a socket to Daemon which is also running on the VIOS . VKE also transmits other messages within the cluster environment. The user daemon has access through API to Database DB which is maintained by all VIOS partitions servicing the client s within the cluster.

As described herein implementation of the cluster awareness with the VIOSes of the cluster enables the VIOSes to provide cluster storage services to virtual clients . The VIOS software stack provides the following advanced capabilities among others Storage Aggregation and Provisioning Thin Provisioning Virtual Client Cloning Virtual Client Snapshot Virtual Client Migration Distributed Storage Repository Virtual Client Mirroring and Server Management Infrastructure integration. More generally the VIOS protocol allows distributed storage to be viewed as centralized structured storage with a namespace location transparency serialization and fine grain security. The VIOS protocol provides storage pooling distributed storage and consistent storage virtualization interfaces and capabilities across heterogeneous SAN and network accessible storage NAS . In order to provide block storage services utilizing the distributed repository each VIOS configures virtual devices to be exported to virtual clients. Once each virtual device is successfully configured and mapped to a virtual host VHOST adapter the clients may begin utilizing the devices as needed. In one embodiment the virtualization is performed utilizing POWER virtual machine VM virtualization technology which allows the device configuration process to occur seamlessly because the physical block storage is always accessible from the OS partition. When a virtual target device is removed the local OS cache local storage data entries are deleted. Within the clustered environment removal of any of the LUs is noticed to the other VIOSes. According to the described method a distributed device repository and local repository cache are utilized to ensure the nodes within the cluster become device level synchronized from each node VIOS in the cluster.

According to one embodiment information needed to configure a virtual target device VTD is stored in DB . This database DB can be accessed by all the nodes in the VIOS cluster utilizing services provided by Cluster Aware OS such as but not limited to Cluster Aware AIX CAA . Additionally certain small levels of cluster data are stored in a local database ODM e.g. virtualized portions of storage on each node for the devices which exist on that node. This local storage is necessary in order for the processes running on the local node to be able to match the VIOS device with the correct information in the distributed database.

With information about each device being stored in the DB operations on those devices can be performed from any VIOS node in the cluster not just the node on which the device resides. When an operation on a device is performed on a remote non local node i.e. one other than the node where the device physically resides the operation is able to make any changes to the device s information in the DB as necessary. When corresponding changes are needed in the device s local database the corresponding CM utility enables the remote node to send a message using cluster services to the local node to notify the local node to make the required changes. Additionally when a node in the cluster is booted up or when the node rejoins the cluster after having been lost for any period of time the node will autonomously reference the DB in order to synchronize the data there with the local data of the node.

As an example if an operation to delete a VIOS device from the local node is executed on a remote node the operation will remove the information associated with that device from the DB and send a message to the local node to tell the local node to remove the device from the local database. If the local node is down or not currently a part of the cluster when the local node first boots up or rejoins the cluster the local node will automatically access the DB retrieve current data information that indicates that the information for one of the local devices has been removed and delete that device from the local database records.

Referring now to there is illustrated a high level logical flowchart of the method by which a fabric loss condition for a VIOS I O operation is handled within the cluster aware VIOS infrastructure. The method begins at block and proceeds to block at which a cluster aware CA operating system OS executing on a processor resource within the first VIOS partition initializes the first VIOS for VIO functionality. At block the CA OS initializes the cluster management utility and the cluster registration utility. CA OS generates a software stack and assigns a unique ID to the VIOS block . CA OS or CM utility initiates registration of the VIOS with the VIOS forwarding VIOS registration data to the shared VIOS DB block . Registration with the VIOS DB triggers the VIOS DB to return cluster configuration information and enables the first VIOS to receive cluster specific data to make the first VIOS aware of the VIOS cluster. Once the first VIOS receives the information the first VIOS is made cluster aware and is able to communicate information with other VIOSes within the VIOS cluster. At decision block CA OS determines whether the cluster configuration data and cluster state status data have been received from the VIOS DB . When the cluster data is not received within a pre established timeout period as determined at decision block the registration information is resent block and the initial registration process ends with an initial failure block . The CA OS waits until the timeout period expires before resending the registration request and the number of registration retries is a design parameter that is variable.

Returning to decision block in response to receiving the configuration data from the VIOS DB the CA OS stores the cluster configuration and cluster state status data within a local storage of the first VIOS block . CM utility then registers VIOS with the VIOS DB for one or more event notifications of cluster events and or other node events block . When cluster or other node event notifications are received by VIOS as determined at block the CM utility updates the local cluster information within the local storage of the VIOS with relevant changes to the cluster information block . The CM utility then initiates performance of any responsive cluster level functions based on the received event notifications block . As an example in one embodiment when the notification indicates that the primary node of the cluster has relegated from its primary node position the CM utility may initiate a primary node election module which performs the primary node election process with the other nodes in the VIOS cluster. When a local event occurs at the first node that requires cluster notification as determined at decision block CM utility sends a notification of the local event to the VIOS cluster i.e. to the VIOS DB and or to the other nodes within the VIOS cluster block .

Turning now to there is illustrated the method by which various ones of the I O functions of the CA VIOS are implemented. The method begins at block and proceeds to block at which CA OS initializes the I O emulation module of the first VIOS. The I O emulation module enables the VIOS to provide virtual I O VIO services to one or more client logical partitions LPARs existing within one or more of the OS partitions within the CEC. The client LPAR is registered with the VIOS block and is assigned a unique VIO adapter for I O processing block . The CA OS then links the unique VIO adapter to a physical adapter block . In one or more embodiments this linking actually only occurs at the time the VOS processes an I O request from the client LPAR. When an I O request is received from the client LPAR as determined at decision block the I O request is processed by the VIOS utilizing the VIO adapter and connectivity via the physical I O adapters to the connecting fabric block . As provided at decision block if a new client LPAR is initiated on the CEC and requires VIO services from the VIOS the process of registering a client to the VIOS is repeated for the new client LPAR. As also indicated by the dashed optional block the CA OS also enables the VIOS to propagate I O requests to a second VIOS providing redundant I O coverage to the first VIOS when the first VIOS cannot complete the I O request for the registered client LPAR. In one embodiment this scenario may occur when the VIOS detects that a problem exists with a fabric connection to the block storage.

One or more of the above processes and or method are implemented within a computer program product comprising a computer readable storage medium and program code for completing these functions stored on said computer readable storage medium.

Turning now to block diagrams of logical partitions and multiple operating systems are illustrated according to one or more embodiments. As illustrated VIOS A LPAR A and LPAR B are coupled to PHYP . In one embodiment a CEC e.g. CEC A includes PHYP VIOS A LPAR A and LPAR B. PHYP can access all system memory of the CEC which allows PHYP to manage and or to control one or more images included in a memory e.g. CEC memory A of the CEC and or included in paging devices A and B coupled to the CEC via VIOS A. In one example paging devices A and B are included in distributed storage repository DSR . For example paging devices A and B can be included in one or more of network storage and physical storage . In one embodiment LPAR B loads and or boots OS B from distributed storage repository into memory partition MP B that is provided by PHYP . For example PHYP can provide a shared pool memory of CEC memory A to two or more LPARs. For instance PHYP provides CEC memory A as a shared resource to LPARs A and B.

In one or more embodiments resource sharing allows multiple logical partitions to access a same resource under the control of PHYP . For example PHYP monitors load applies allocation rules and then time shares access to CEC memory A. For instance LPAR A treats the memory partition A as though it had complete access to shared pool memory while PHYP manages the real access avoiding conflicts or interferences and allowing access to LPARs e.g. LPARs A and B etc. that have resource requirements. Shared pool memory uses paging devices A and B to store excess memory pages on storage devices. Access to paging devices A and B associated with a shared pool memory is provided by a paging VIOS e.g. VIOS A . As shown shared pool memory is partitioned into memory partitions A and B that correspond to LPARs A and B respectively. In one embodiment each shared memory partition uses a dedicated paging device. For example a paging device is implemented as a file in distributed storage repository that is provided to PHYP as a vSCSI device by VIOS A.

When PHYP needs to free memory pages in shared pool memory contents of a memory partition are stored on a paging device and is restored when the contents are accessed again. For example PHYP stores via VIOS A contents of memory partition B on paging device B that includes memory pages B. In one embodiment a virtual asynchronous service interface VASI is a virtual device conducts communications between PHYP and VIOS A. For example one or more VASI devices of VIOS A enable VIOS A to be a paging VIOS. Each VASI device or adapter can support multiple shared pool memories. In one embodiment PHYP includes and or implements one or more of a VASI a pager and a virtual block storage device driver to provide read and or write services between VIOS A and PHYP .

In one embodiment an OS e.g. OS B treats the provided virtual memory e.g. memory partition B as memory B of the LPAR. For example PHYP provides memory partition B to LPAR B and PHYP pages out via VIOS A one or more portions of memory partition B to memory pages B of paging devices B. For instance OS B is stored in memory partition B and PHYP pages out one or more portions e.g. memory pages of memory partition B of shared pool memory to memory pages B of paging devices B such that one or more memory pages B stores OS B as OS image Img B.

In one or more embodiments PHYP provides and or shares via a paging VIOS first memory pages of a first paging device that is associated with a first memory partition and a first LPAR with a second memory partition and a second LPAR. For example PHYP provides and or shares via VIOS A memory pages B with memory partition A and LPAR A. For instance PHYP via VIOS A provides one or more memory images e.g. OS Img Copy B OS Img B etc. via memory pages A and or B to one or more of LPARs A and B and or VIOS A. In one or more embodiments providing access to one or more memory pages B includes providing reading and or writing services between paging devices A and B and a LPAR or a VIOS.

As illustrated VIOS A includes clone snapshot service . In one or more embodiments clone snapshot service of VIOS A clones OS Img B and or takes a snapshot of OS Img B and clone snapshot service stores a clone or a snapshot of OS Img B as OS Img copy B in memory pages A of paging device A. In one example clone snapshot service copies both metadata and data associated with OS Img B and stores the copy of both metadata and data associated with OS Img B as OS Img copy B. In another example clone snapshot service copies metadata associated with OS Img B and stores the copy of metadata associated with OS Img B as OS Img copy B. For instance the copy of the metadata associated with OS Img B can be considered a snapshot of OS Img B.

In one or more embodiments OS Img copy B is used by one or more LPARs. For example PHYP substitutes or redirects a portion of a memory partition or supplements the memory partition with memory pages A which includes OS Img copy B. For instance PHYP suspends operations of LPAR A pages out OS A and pages in OS Img copy B into memory partition A as OS B. After OS B is paged into memory partition A PHYP resumes operations of LPAR A and LPAR A uses OS B.

As shown memory partition A includes OS A and applications APPs A C that are coupled to and or use OS A. In one embodiment one or more of APPs A C are coupled to OS A by dynamically linking to one or more libraries e.g. shared libraries included in and or provided by OS A. In another embodiment one or more of APPs A C are coupled to OS A by communicating with one or more of a kernel a device driver and a logical device among others included in and or provided by OS A.

As illustrated in OS A has been paged out of memory partition A and OS B has been paged into memory partition A. In one embodiment PHYP replaces OS A with OS B and couples APPs A C to OS B while LPAR A is suspended. For example application APP A can be associated with one or more pointers e.g. memory pointers file pointers etc. and PHYP changes the one or more pointers to point to appropriate one or more portions of OS B such that APP A uses OS B. For instance PHYP replacing OS A with OS B can eliminate a restart or reboot of LPAR A.

In one embodiment PHYP suspends one or more of OS A OS B and APPs A C before replacing OS A with OS B and or while changing pointers of an application to point to appropriate one or more portions of OS B. In one embodiment data associated with OS B can be paged into memory partition A or can be made available to LPAR A as needed. For example one or more portions of OS B can be read only and can be shared between or among two or more LPARs. For instance one or more of shared libraries a kernel and programs e.g. init cat ls ln ksh bash chmod kill perl python cu tar touch true grep nroff rogue vi etc. associated with OS B can be read only and can be shared between or among two or more LPARs.

Turning now to block diagrams of logical partitions and multiple operating systems are illustrated according to one or more embodiments. As illustrated VIOS A LPAR A and LPARs C N are coupled to PHYP . In one embodiment a CEC e.g. CEC A includes VIOS A LPAR A LPARs C N and PHYP . As is apparent in LPAR B is not present. For example LPAR B may be utilized to create OS Img B and LPAR B can be removed and or stored by PHYP after OS Img B is created.

As shown LPARs C N can include respective memory partitions C N which can store respective operating systems OSes C N. In one example memory partition C can include APPs C and C which are coupled to and or use OS C. In another example memory partition N can include APPs N and N which are coupled to and or use OS N. In one embodiment PHYP pages memory pages A into one or more of memories C N. In one example OS Img copy B is paged into memory partition C as OS B. In another example OS Img copy B is paged into memory partition N as OS B. In one embodiment one or more of OSes C N are replaced after OS Img copy B has been paged into respective one or more of memory partitions C N as OS B.

As illustrated in OSes A and C N have been paged out of respective memory partitions A and C N and memory pages A including OS Img copy B have been paged into memories C N as OS B. In one embodiment PHYP replaces OSes C N with OS B couples one or more of APPs C and C to OS B included in memory partition C and couples one or more of APPs N and N to OS B included in memory partition N. For example APP N is associated with one or more pointers and PHYP changes the one or more pointers to point to appropriate one or more portions of OS B of memory partition N such that APP N uses OS B. In one embodiment PHYP suspends one or more of OS N OS B and APPs N and N before replacing OS N with OS B and or while changing pointers of an application to point to appropriate one or more portions of OS B.

Turning now to a block diagram of logical partitions and multiple operating systems are illustrated according to one or more embodiments. As illustrated memory pages A including OS Img copy B have been paged into shared pool memory as OS B. In one embodiment PHYP provides read only access of OS B to memory partitions C N that can be used within with respective OSes C N. As shown PHYP replaces OSes C N with respective OSes C N couples one or more of APPs C and C to OS C included in memory partition C and couples one or more of APPs N and N to OS N included in memory partition N. As illustrated OS C includes changes CH C and a read only accessible link to OS B and OS N includes CH C and a read only accessible link to OS B.

In one or more embodiments an OS e.g. OS C accesses a shared copy of an OS image e.g. OS Img Copy B and includes metadata changes and data changes e.g. CH C differing from the OS image that are local or specific to a LPAR e.g. LPAR C . In one example OS C includes OS B that is shared between or among two or more LPARs and can include CH C which includes metadata changes and data changes differing from OS B that are local or specific to LPAR C. In another example OS N includes OS B that is shared between or among two or more LPARs and includes CH N which includes metadata changes and data changes differing from OS B that are local or specific to LPAR N. In one embodiment OSes C and N can be respectively produced from a combination of a shared read only link to OS B and CH C and from a combination the shared read only link to OS B and CH N.

The one or more changes in metadata and or data differing from a shared OS image can include one or more of a network address a network mask a default gateway a mount point an initialization script or utility a users file e.g. etc passwd a group file e.g. etc group a device driver a loadable kernel module a kernel parameter and a library among others. For example one or more of LPAR C VOIS A and PHYP modifies OS C with the one or more changes in metadata and or data differing from OS B. For instance VIOS A receives a write request to modify a portion of OS C with a modification and or change and performs a redirection on write action method or process to store the modification and or change to OS C in CH C.

As illustrated memory pages C of paging device C include OS Img copy C. For example each of OS Img copies B and C can be different from another. In one or more embodiments OS Img copy C is produced and or used in a same or similar fashion as OS Img copy B.

Turning now to a method of producing an operating system image is illustrated according to one or more embodiments. The method begins at block and proceeds to block where PHYP creates LPAR B. In one embodiment LPAR B is a hidden client LPAR. For example other client LPARs are not able to communicate with and or control LPAR B.

At block LPAR B loads and or boots OS B from distributed storage repository . At block PHYP pages out OS B from memory partition B to memory pages B of paging device B as OS Img B. In one embodiment storing OS B via memory pages B creates and or produces an image of OS B that can be paged into another memory partition.

At block clone snapshot service of VIOS A produces OS Img copy B. In one embodiment producing OS Img copy B clone snapshot service includes cloning OS Img B and or taking a snapshot of OS Img B. In one example clone snapshot service copies both metadata and data associated with OS Img B and stores the copy of both metadata and data associated with OS Img B as OS Img copy B in memory pages A of paging device A. In another example clone snapshot service copies metadata associated with OS Img B and stores the copy of metadata associated with OS Img B as OS Img copy B in memory pages A of paging device A. For instance the copy of the metadata associated with OS Img B is considered a snapshot of OS Img B. At block clone snapshot service stores a clone e.g. duplication or a snapshot of OS Img B as OS Img copy B in memory pages A of paging device A. The method then ends at block .

Turning now to a method of replacing an operating system is illustrated according to one or more embodiments. The method begins at block and proceeds to block where PHYP suspends operations of LPAR A. In one embodiment suspending one or more operations of LPAR A includes suspending one or more of OS A and APPs A C. At block PHYP pages out OS A. At block PHYP pages in OS Img copy B into memory partition A associated with LPAR A. In one embodiment PHYP pages in OS Img copy B from pages A of paging device A. In one embodiment paging in OS Img copy B includes paging in metadata associated with OS B. For example PHYP pages in metadata associated with OS B and data associated with OS B is paged in at a later point in time or as the data is needed or required.

At block PHYP changes one or more pointers of each of APPs A C to point to appropriate one or more portions of OS B such that APPs use A C OS B. At block PHYP resumes one or more suspended operations of LPAR A. For example one or more of APPs A C resume operations and or execution using OS B. The method then ends at block . In one or more embodiments the method illustrated in can be repeated for one or more other LPARs and or other OS images. In one example the method illustrated in can be repeated for one or more of LPARs C N. In another example the method illustrated in can be repeated for OS Img copy C.

Turning now to a method of replacing an operating system is illustrated according to one or more embodiments. The method begins at block and proceeds to block where PHYP pages in OS Img copy B into shared pool memory . In one embodiment PHYP pages in OS Img copy B from memory pages A of paging device A. In another embodiment paging in OS Img copy B includes paging in metadata associated with OS B. For example PHYP pages in metadata associated with OS B and data associated with OS B is paged in at a later point in time or as the data is needed or required. In one embodiment the data associated with OS B is paged in as read only data. For example OS Img copy B is shared between or among two or more LPARs. For instance OS Img copy B is shared between LPARs C and N.

At block PHYP suspends operations of LPAR C. At block PHYP pages out OS C of LPAR C. In one embodiment suspending one or more operations of LPAR C can include suspending one or more of OS C and APPs C and C. At block PHYP produces OS C from OS B and CH C where CH C can include metadata changes and data changes differing from OS B that are local or specific to LPAR C. In one embodiment CH C does not include any changes until a redirect on write operation occurs where CH C stores metadata changes and data changes differing from OS B when a write operation to modify OS C is redirected.

At block PHYP replaces OS C with OS C. In one embodiment replacing OS C with OS C includes coupling APPs C and C to OS C. For example APPs C C are coupled to OS C such that APPs C and C uses OS C. In one embodiment an APP e.g. APP C is associated with one or more pointers and PHYP changes the one or more pointers to point to appropriate one or more portions of OS C such that the APP uses OS C. At block PHYP resumes one or more suspended operations of LPAR C. For example one or more of APPs C and C resume operations and or execution using OS C. The method then ends at block . In one or more embodiments the method illustrated in can be repeated for one or more other LPARs. For example the method illustrated in can be repeated for one or more of LPARs A N.

The flowcharts and block diagrams in the various figures presented and described herein illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present invention. In this regard each block in the flowcharts or block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It should also be noted that in some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

In the flow charts above one or more of the methods are embodied in a computer readable medium containing computer readable code such that a series of steps are performed when the computer readable code is executed by a processing unit on a computing device. In some implementations certain processes of the methods are combined performed simultaneously or in a different order or perhaps omitted without deviating from the spirit and scope of the invention. Thus while the method processes are described and illustrated in a particular sequence use of a specific sequence of processes is not meant to imply any limitations on the invention. Changes may be made with regards to the sequence of processes without departing from the spirit or scope of the present invention. Use of a particular sequence is therefore not to be taken in a limiting sense and the scope of the present invention extends to the appended claims and equivalents thereof.

As will be appreciated by one skilled in the art aspects of the present invention may be embodied as a system method or computer program product. Accordingly aspects of the present invention may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. Furthermore aspects of the present invention may take the form of a computer program product embodied in one or more computer readable medium s having computer readable program code embodied thereon.

Any combination of one or more computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this document a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable R.F etc. or any suitable combination of the foregoing. Computer program code for carrying out operations for aspects of the present invention may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Aspects of the present invention are described below with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

These computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks. The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

As will be further appreciated the processes in embodiments of the present invention may be implemented using any combination of software firmware or hardware. As a preparatory step to practicing the invention in software the programming code whether software or firmware will typically be stored in one or more machine readable storage mediums such as fixed hard drives diskettes optical disks magnetic tape semiconductor memories such as ROMs PROMs etc. thereby making an article of manufacture in accordance with the invention. The article of manufacture containing the programming code is used by either executing the code directly from the storage device by copying the code from the storage device into another storage device such as a hard disk RAM etc. or by transmitting the code for remote execution using transmission type media such as digital and analog communication links The methods of the invention may be practiced by combining one or more machine readable storage devices containing the code according to the present invention with appropriate processing hardware to execute the code contained therein. An apparatus for practicing the invention could be one or more processing devices and storage systems containing or having network access to program s coded in accordance with the invention.

Thus it is important that while an illustrative embodiment of the present invention is described in the context of a fully functional computer server system with installed or executed software those skilled in the art will appreciate that the software aspects of an illustrative embodiment of the present invention are capable of being distributed as a program product in a variety of forms and that an illustrative embodiment of the present invention applies equally regardless of the particular type of media used to actually carry out the distribution.

While the invention has been described with reference to exemplary embodiments it will be understood by those skilled in the art that various changes may be made and equivalents may be substituted for elements thereof without departing from the scope of the invention. In addition many modifications may be made to adapt a particular system device or component thereof to the teachings of the invention without departing from the essential scope thereof. Therefore it is intended that the invention not be limited to the particular embodiments disclosed for carrying out this invention but that the invention will include all embodiments falling within the scope of the appended claims. Moreover the use of the terms first second etc. do not denote any order or importance but rather the terms first second etc. are used to distinguish one element from another.

The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

The corresponding structures materials acts and equivalents of all means or step plus function elements in the claims below are intended to include any structure material or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present invention has been presented for purposes of illustration and description but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention. The embodiment was chosen and described in order to best explain the principles of the invention and the practical application and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.

