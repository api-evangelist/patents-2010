---

title: Storage server with embedded communication agent
abstract: A storage server receives a data access request in a standard communication format, such as the Storage Management Initiative-Specification (SMI-S). A single mode request is received at a disk module of the storage server and a cluster mode request is received at a management host of the storage server. The request is forwarded to a communication agent in a management module of the storage server. The communication agent translates the request from the standard communication format to a proprietary format used by the storage server to communicate with an attached storage subsystem. The storage server services the request from the attached storage subsystem through a disk module. In cluster mode, the request is forwarded to corresponding disk modules in each storage server in the cluster.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08756338&OS=08756338&RS=08756338
owner: NetApp, Inc.
number: 08756338
owner_city: Sunnyvale
owner_country: US
publication_date: 20100429
---
This invention relates to the field of data storage systems and in particular to a storage server having an embedded communication agent.

Various forms of network storage systems are known today. These forms include network attached storage NAS storage area networks SANs and others. Network storage systems are commonly used for a variety of purposes such as providing multiple users with access to shared data backing up critical data e.g. by data mirroring etc.

A network storage system can include at least one storage system which is a processing system configured to store and retrieve data on behalf of one or more storage client processing systems clients . In the context of NAS a storage system may be a file server which is sometimes called a filer. A filer operates on behalf of one or more clients to store and manage shared files in a set of mass storage devices such as magnetic or optical disks or tapes or flash drives. The mass storage devices may be organized into one or more volumes of a Redundant Array of Inexpensive Disks RAID . In a SAN context the storage server provides clients with block level access to stored data rather than file level access. Some storage servers are capable of providing clients with both file level access and block level access.

In a typical storage system client devices and storage servers from different manufacturers may be used together. To enable effective communication between the different devices a storage communication standard may be used. One example is the Storage Management Initiative Specification SMI S maintained by the Storage Networking Industry Association SNIA . A communication agent i.e. SMI S agent is used to translate standard SMI S commands into a proprietary format used by the storage server. In conventional systems the communication agent runs outside the storage server on a host computing device running an operating system such as Windows or Linux. This host machine requires additional hardware increasing the cost of the system. In addition being outside the storage server limits the scalability of the agent and limits the number of nodes in the storage system that it can support. As the communication agent on the host machine communicates with one or more nodes in the storage system a latency may develop in communications between the different devices. The latency may be increased especially if the host machine and storage servers are in different subnets. In addition if the storage system includes storage devices from a number of different vendors there may need to be a different communication agent for each storage vendor. Running multiple communication agents on a single host machine may cause conflicts and result in decreased performance. Having a separate host machine for each communication agent can lead to increased costs.

A storage server receives a data access request in a standard communication format such as the Storage Management Initiative Specification SMI S . A single mode request is received at a disk module of the storage server and a cluster mode request is received at a management host of the storage server. The request is forwarded to a communication agent in a management module of the storage server. The communication agent translates the request from the standard communication format to a proprietary format used by the storage server to communicate with an attached storage subsystem. The storage server services the request from the attached storage subsystem through a disk module. In cluster mode the request is forwarded to corresponding disk modules in each storage server in the cluster.

In the following detailed description of embodiments of the invention reference is made to the accompanying drawings in which like references indicate similar elements and in which is shown by way of illustration specific embodiments in which the invention may be practiced. These embodiments are described in sufficient detail to enable those skilled in the art to practice the invention and it is to be understood that other embodiments may be utilized and that logical mechanical electrical functional and other changes may be made without departing from the scope of the present invention. The following detailed description is therefore not to be taken in a limiting sense and the scope of the present invention is defined only by the appended claims.

Embodiments are described for a communication agent embedded in a storage server. In one embodiment the communication agent may be a Storage Management Initiative Specification SMI S agent configured to use data in the SMI S format. The SMI S agent translates a received data access request from the SMI S format to a proprietary format used by the storage server to communicate with an attached storage subsystem. An internal architecture of the storage server enables a single SMI S agent to handle data access requests in both a single mode and a cluster mode.

For a single mode data access request the request is received at a logical interface in a disk module D module of the storage server. The disk module forwards the single mode request to the SMI S agent which resides in a management module M host of the storage server. The SMI S agent translates the request and selects and calls the corresponding application programming interfaces APIs so that the request may be serviced from the attached storage subsystem through the D module.

For a cluster mode request the request from client is received directly at a logical interface of a management module such as M host of storage server as shown in . The request is processed by M host as discussed below with respect to . A communication agent such as an SMI S agent not shown in M host performs the translation and calls a different set of APIs. In the cluster mode the storage server is arranged in a clustered configuration with one or more other storage servers and which together make up a storage cluster. The cluster mode request is forwarded by the M host of storage server to a disk module such as D modules and in each storage server and so that the request may be executed on a storage subsystem attached to each storage server in the cluster. Each of storage servers and may have attached mass storage devices which the D module in each storage server may access using for example the commands and protocols discussed below with respect to .

The logical interfaces at each of D module and M host have a separate Internet Protocol IP address which may be known to the clients in the storage system. The clients may be configured e.g. through user configurable or default settings to send data access requests to one of the IP addresses depending on whether the request is a single mode request or a cluster mode request.

A single mode request received at D module is routed for example by data access routing module as shown in to communication agent running in M host . In one embodiment communication agent is an SMI S agent run by a processor such as processor as shown in which translates standard SMI S commands into a proprietary format used by the storage server to access data in mass storage device . Additional details of communication agent will be described below with respect to . A cluster mode request received at M host is passed directly from the logical interface to communication agent .

Communication agent makes an application programming interface API call through API server . API server may include a known set of APIs corresponding to the SMI S standard. For a single mode request communication agent calls an interface module . In one embodiment interface module is a single mode API tunnel module configured to form an API tunnel between M Host and D Module . Interface module in turn makes an API call back to D module . Using the modules described below with respect to such as for example RAID system and disk driver system D module may service the request e.g. read data or write data to mass storage device . For a cluster mode request communication agent calls simple management framework SMF module . SMF module may include tables and schema to store information pertaining to the APIs including API names API parameters and API values. SMF module makes an API call to D module of storage server as well as the corresponding D modules in each of the other storage servers in the cluster as shown in .

In one embodiment SMI S agent includes Common Information Model Object Manager CIMOM . CIMOM receives data access requests in single mode routed through the D module and directly from the logical interface of the M host in cluster mode. The Common Information Model CIM is a standard that defines how elements in a storage system are represented as a common set of objects and the relationships between them. CIMOM accesses schema to define the specific set of objects e.g. disks clients networks etc. managed by the storage server as well as the relationships between them. Repository is a compilation of the data in schema . CIMOM determines the information in the request based on the contents of schema and repository . For example CIMOM may determine what client the data access request was received from what the type of the data access request is e.g. read or write and what data in the storage system the data access request pertains to. This information is passed to the provider dynamic link library DLL which uses application specific APIs from the API server to complete the data access request.

In one embodiment SMI S agent identifies whether a received data request is a single mode request or a cluster mode request. The type of data access request can be determined based on the interface at which it is received. Single mode data access requests are received at a logic interface in D module and forwarded to the agent in M Host . Cluster mode data access requests are received at a logical interface directly in M Host . Each of the logical interfaces may have a unique IP address so that they can be distinguished by storage clients such as client . As discussed above CIMOM may parse the received message to determine which logical interface it was received at and thus whether the request is single mode or cluster mode.

Once the type of data access request is determined provider DLLs selects the appropriate API from API server to complete the data access request. The request may include an identifier specifying a known action e.g. read or write as well as one or more input parameters specifying a particular piece of data in mass storage . Provider DLLs may include a list of APIs in API server and may select one or more of the APIs based on information in the data access request. API server may include separate APIs for single mode request and for cluster mode requests. As discussed above the APIs for single mode requests may be executed through API tunnel while APIs for cluster mode requests may be executed through SMF .

At block method receives a single mode data access request at the logical interface of D module . The request may be in the SMI S format and may include for example a request to read from or write to a data block in a storage device managed by the storage server. At block method forwards the data access request to an SMI S agent in M host . In one embodiment a data access routing module in D module routes the request to the SMI S agent such as SMI S agent . At block SMI S agent translates the request from the SMI S standard into the proprietary format used by the storage server . The translation is described above with respect to and may include identifying data in the request such as an action to be performed and data on which the action may be performed.

At block method selects the appropriate APIs for servicing the single mode data access request from API server . The appropriate APIs may be selected by determining whether the request is a single mode access request or a cluster mode access request. Provider DLLs may select APIs from API server based on the type of request and the data identified in the translation at block . At block method makes the API call through the single mode API tunnel. In one embodiment API tunnel encodes the APIs from API server in an extensible markup language XML format for communication between M Host and D module . At block through the API call method forwards the request to D module . Using the modules described below such as for example RAID system and disk driver system at block method services the request e.g. read data or write data to the attached mass storage device. In the case of a read request data is returned from the storage device to the D module forwarded to the M host where it is translated to the SMI S format by SMI S agent and routed back to the requesting client through D module .

At block method receives a cluster mode data access request at the logical interface of M host . The request may be in the SMI S format and may include for example a request to read from or write to a data block in a storage device managed by the storage server. At block method forwards the data access request to an SMI S agent such as SMI S agent in M host . At block SMI S agent translates the request from the SMI S standard into the proprietary format used by the storage server . The translation is described above with respect to .

At block method selects the appropriate APIs for servicing the cluster mode data access request from API server . The appropriate APIs may be selected by determining whether the request is a single mode access request or a cluster mode access request. Provider DLLs may select APIs from API server based on the type of request and the data identified in the translation at block . At block method makes the API call through the Simple Management Framework . In one embodiment SMF encodes the APIs from API server in an extensible markup language XML format for communication between M Host and D module . At block through the API call method forwards the request to D module as well as the D module of each storage server in the cluster. Using the modules described below such as for example RAID system and disk driver system at block method services the request e.g. read data or write data to the attached mass storage device. In the case of a read request data is returned from the storage device to the D module forwarded to the M host where it is translated to the SMI S format by SMI S agent and routed back to the requesting client through D module .

Storage of data in storage units A B is managed by storage servers A B which receive and respond to various read and write requests from clients directed to data stored in or to be stored in storage units A B. Storage units A B may constitute mass storage devices which can include for example flash memory magnetic or optical disks or tape drives. The storage devices in storage units A B can further be organized into arrays not shown in this figure implementing a Redundant Array of Inexpensive Disks Devices RAID scheme whereby storage servers A B access storage units A B using one or more RAID protocols known in the art.

Storage servers A B can provide file level service such as used in a network attached storage NAS environment block level service such as used in a storage area network SAN environment a service which is capable of providing both file level and block level service or any other service capable of providing other data access services. Although storage servers A B are each illustrated as single units in a storage server can in other embodiments constitute a separate network element or module an N module and disk element or module a D module . In one embodiment the D module includes storage access components for servicing client requests. In contrast the N module includes functionality that enables client access to storage access components e.g. the D module and may include protocol components such as Common Internet File System CIFS Network File System NFS or an Internet Protocol IP module for facilitating such connectivity. Details of a distributed architecture environment involving D modules and N modules are described further below with respect to .

In yet other embodiments storage servers A B are referred to as network storage subsystems. A network storage subsystem provides networked storage services for a specific application or purpose. Examples of such applications include database applications web applications Enterprise Resource Planning ERP applications etc. e.g. implemented in a client. Examples of such purposes include file archiving backup mirroring etc. provided for example on archive backup or secondary storage server connected to a primary storage server. A network storage subsystem can also be implemented with a collection of networked resources provided across multiple storage servers and or storage units

Illustratively one of the storage servers e.g. storage server A functions as a primary provider of data storage services to client . Data storage requests from client are serviced using disks A organized as one or more storage objects. A secondary storage server e.g. storage server B takes a standby role in a protection relationship with the primary storage server replicating storage objects from the primary storage server to storage objects organized on disks of the secondary storage server e.g. disks B . In operation the secondary storage server does not service requests from client until data in the primary storage object becomes inaccessible such as in a disaster with the primary storage server such event considered a failure at the primary storage server. Upon a failure at the primary storage server requests from client intended for the primary storage object are serviced using replicated data i.e. the secondary storage object at the secondary storage server.

It will be appreciated that in other embodiments network storage system may include more than two storage servers. In these cases protection relationships may be operative between various storage servers in system such that one or more primary storage objects from storage server A may be replicated to a storage server other than storage server B not shown in this figure . Secondary storage objects may further implement protection relationships with other storage objects such that the secondary storage objects are replicated e.g. to tertiary storage objects to protect against failures with secondary storage objects. Accordingly the description of a single tier protection relationship between primary and secondary storage objects of storage servers should be taken as illustrative only.

Nodes A B may be operative as multiple functional components that cooperate to provide a distributed architecture of system . To that end each node A B may be organized as a network element or module N module A B a disk element or module D module A B and a management element or module M host A B . In one embodiment each module includes a processor and memory for carrying out respective module operations. For example N module A B may include functionality that enables node A B to connect to client via network and may include protocol components such as a media access layer Internet Protocol IP layer Transport Control Protocol TCP layer User Datagram Protocol UDP layer and other protocols known in the art.

In contrast D module A B may connect to one or more storage devices A B via cluster switching fabric and may be operative to service access requests on devices A B. In one embodiment the D module A B includes storage access components such as a storage abstraction layer supporting multi protocol data access e.g. Common Internet File System protocol the Network File System protocol and the Hypertext Transfer Protocol a storage layer implementing storage protocols e.g. RAID protocol and a driver layer implementing storage device protocols e.g. Small Computer Systems Interface protocol for carrying out operations in support of storage access operations. Illustratively a storage abstraction layer e.g. file system of the D module divides the physical storage of devices A B into storage objects. Requests received by node A B e.g. via N module A B may thus include storage object identifiers to indicate a storage object on which to carry out the request.

Also operative in node A B is M host A B which provides cluster services for node A B by performing operations in support of a distributed storage system image for instance across system . Illustratively M host A B provides cluster services by managing a data structure such as a RDB A B which contains information used by N module A B to determine which D module A B owns services each storage object. The various instances of RDB A B across respective nodes A B may be updated regularly by M host A B using conventional protocols operative between each of the M hosts e.g. across network to bring them into synchronization with each other. A client request received by N module A B may then be routed to the appropriate D module A B for servicing to provide a distributed storage system image.

It should be noted that while shows an equal number of N modules and D modules constituting a node in the illustrative system there may be different number of N modules and D modules constituting a node in accordance with various embodiments of the present invention. For example there may be a number of N modules and D modules of node A that does not reflect a one to one correspondence between the N modules and D modules of node B. As such the description of a node comprising one N module and one D module for each node should be taken as illustrative only.

In one embodiment the processor reads instructions from the memory and executes the instructions. The memory may include any of various types of memory devices such as for example random access memory RAM read only memory ROM flash memory one or more mass storage devices e.g. disks etc. The memory stores instructions of an operating system . The processor retrieves the instructions from the memory to run the operating system . In one embodiment operating system includes SMI S agent which will be described further below. The storage system may interface with one or more storage systems via the storage adaptor which may include a small computer system interface SCSI adaptor fiber channel adaptor etc.

Multi protocol engine includes a media access layer of network drivers e.g. gigabit Ethernet drivers that interface with network protocol layers such as the IP layer and its supporting transport mechanisms the TCP layer and the User Datagram Protocol UDP layer . A file system protocol layer provides multi protocol file access and to that end includes support for the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the Hypertext Transfer Protocol HTTP protocol . A VI layer implements the VI architecture to provide direct access transport DAT capabilities such as RDMA as required by the DAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the storage server. In certain cases a Fibre Channel over Ethernet FCoE layer not shown may also be operative in multi protocol engine to receive and transmit requests and responses to and from the storage server. The FC and iSCSI drivers provide respective FC and iSCSI specific access control to the blocks and thus manage exports of luns to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing blocks on the storage server.

The storage operating system also includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on storage devices implementing secure storage e.g. storage devices A B . Information may include data received from a client e.g. client in addition to data accessed by the storage operating system in support of storage server operations such as program application data or other system data. Preferably client data may be organized as one or more logical storage objects e.g. volumes that comprise a collection of storage devices cooperating to define an overall logical arrangement. In one embodiment the logical arrangement may involve logical volume block number vbn spaces wherein each volume is associated with a unique vbn.

File system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustratively embodied as e.g. a SCSI target module . SCSI target module is generally disposed between drivers and file system to provide a translation layer between the block lun space and the file system space where luns are represented as blocks. File system illustratively implements the WAFL file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using a data structure such as index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . File system uses files to store metadata describing the layout of its file system including an inode file which directly or indirectly references points to the underlying data blocks of a file.

Operationally a request from a client is forwarded as a packet over the network and onto the storage server where it is received at a network adapter e.g. adapter . A network driver such as layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to file system . There file system generates operations to load or retrieve the requested data from the disks if it is not resident in core i.e. in memory . If the information is not in memory file system accesses the inode file to retrieve a logical vbn and passes a message structure including the logical vbn to the RAID system . There the logical vbn is mapped to a disk identifier and device block number e.g. disk dbn and sent to an appropriate driver e.g. SCSI of disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the storage server. Upon completion of the request the node and operating system returns a reply to the client over the network. In one embodiment where the request is a single mode SMI S request the request is received directly at D module . Data access routing module detects the request and routes the request to an SMI S agent in M host for processing as described with respect to .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the storage server adaptable to the teachings of the invention may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by the storage server in response to a request issued by a client. Moreover in another alternate embodiment of the invention the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the storage server. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

When implemented in a cluster data access components of the storage operating system may be embodied as D module for accessing data stored on disk. In contrast multi protocol engine may be embodied as N module to perform protocol termination with respect to a client issuing incoming access over the network as well as to redirect the access requests to any other N module in the cluster. A cluster services system may further implement an M host e.g. M host to provide cluster services for generating information sharing operations to present a distributed file system image for the cluster. For instance media access layer may send and receive information packets between the various cluster services systems of the nodes to synchronize the replicated databases in each of the nodes.

In addition a cluster fabric CF interface module may facilitate intra cluster communication between N module and D module . For instance D module may expose a CF application programming interface API to which N module or another D module not shown issues calls. To that end CF interface module can be organized as a CF encoder decoder using local procedure calls LPCs and remote procedure calls RPCs to communicate a file system command to between D modules residing on the same node and remote nodes respectively.

Illustratively the storage operating system issues a read or write command to a storage device controller e.g. device controller through disk driver system for accessing a physical storage object e.g. disk using the disk identifier mapped from the logical vbn by RAID system .

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write anywhere file system the teachings of the present invention may be utilized with any suitable file system including conventional write in place file systems.

The above description sets forth numerous specific details such as examples of specific systems components methods and so forth in order to provide a good understanding of several embodiments of the present invention. It will be apparent to one skilled in the art however that at least some embodiments of the present invention may be practiced without these specific details. In other instances well known components or methods are not described in detail or are presented in simple block diagram format in order to avoid unnecessarily obscuring the present invention. Thus the specific details set forth are merely exemplary. Particular implementations may vary from these exemplary details and still be contemplated to be within the scope of the present invention.

Embodiments of the present invention include various operations which are described above. These operations may be performed by hardware components software firmware or a combination thereof. As used herein the term coupled to may mean coupled directly or indirectly through one or more intervening components. Any of the signals provided over various buses described herein may be time multiplexed with other signals and provided over one or more common buses. Additionally the interconnection between circuit components or blocks may be shown as buses or as single signal lines. Each of the buses may alternatively be one or more single signal lines and each of the single signal lines may alternatively be buses.

Certain embodiments may be implemented as a computer program product that may include instructions stored on a machine readable medium. These instructions may be used to program a general purpose or special purpose processor to perform the described operations. A machine readable medium includes any mechanism for storing or transmitting information in a form e.g. software processing application readable by a machine e.g. a computer . The machine readable medium may include but is not limited to magnetic storage medium e.g. floppy diskette optical storage medium e.g. CD ROM magneto optical storage medium read only memory ROM random access memory RAM erasable programmable memory e.g. EPROM and EEPROM flash memory or another type of medium suitable for storing electronic instructions.

Additionally some embodiments may be practiced in distributed computing environments where the machine readable medium is stored on and or executed by more than one computer system. In addition the information transferred between computer systems may either be pulled or pushed across the communication medium connecting the computer systems.

The digital processing devices described herein may include one or more general purpose processing devices such as a microprocessor or central processing unit a controller or the like. Alternatively the digital processing device may include one or more special purpose processing devices such as a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or the like. In an alternative embodiment for example the digital processing device may be a network processor having multiple processors including a core unit and multiple microengines. Additionally the digital processing device may include any combination of general purpose processing devices and special purpose processing device s .

Although the operations of the methods herein are shown and described in a particular order the order of the operations of each method may be altered so that certain operations may be performed in an inverse order or so that certain operation may be performed at least in part concurrently with other operations. In another embodiment instructions or sub operations of distinct operations may be in an intermittent and or alternating manner.

In the above descriptions embodiments have been described in terms of objects in an object oriented environment. It should be understood that the invention is not limited to embodiments in object oriented environments and that alternative embodiments may be implemented in other programming environments having characteristics similar to object oriented concepts.

In the foregoing specification the invention has been described with reference to specific exemplary embodiments thereof. It will however be evident that various modifications and changes may be made thereto without departing from the broader scope of the invention as set forth in the appended claims. The specification and drawings are accordingly to be regarded in an illustrative sense rather than a restrictive sense.

