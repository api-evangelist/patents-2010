---

title: Reactive monitoring of guests in a hypervisor environment
abstract: A system, method and computer program product for detecting and reacting to a network discrepancy within a data center network environment. The method comprises monitoring network traffic between a first computer configured as a first hypervisor inter-networked within the data center network environment and a second computer inter-networked within the data center network environment in order to detect a presence of a predetermined condition representing a presence of the network discrepancy in the data center network environment between a recorded state of the data center network environment and an observed state of the data center network environment. The monitoring includes determining the observed state of the data center network environment using one of: polling, event-based or a combination thereof. The method also comprises rectifying, upon the detection of the presence of the predetermined condition, an effect associated with the presence of the predetermined condition in the data center network environment.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08775590&OS=08775590&RS=08775590
owner: International Business Machines Corporation
number: 08775590
owner_city: Armonk
owner_country: US
publication_date: 20100902
---
The disclosed technology is generally related to computing infrastructure services and in particular to operation of virtualized computing platforms.

Hypervisor technology efficiently facilitates the concurrent virtual operation of multiple guest operating systems including multiple instances of the same operating system on a non virtual host computer. The hypervisor presents each guest operating system with a virtual operating platform and monitors the execution of each guest operating system in a way that all guest operating systems can properly and efficiently share the hardware resources available to the host computer. Through the use of a hypervisor each guest operating systems appears identical to the host computer. Users of each guest operating system may be able to install applications on each guest operating system without impacting users of other guest operating systems operating on the host computer. When one guest operating system experiences a malfunction or a failure the hypervisor will efficiently isolate that malfunction or failure from impacting the operation of other concurrently running guest operating systems. A proper distribution of guest operating systems over hypervisors can allow hypervisors to efficiently share their compute resources and avoid resource contention bottlenecks. Hypervisor vendors and other software providers have tools for managing guest operating systems in cloud computing environments and determining the proper guest operating systems placement.

Hypervisors are an important tool often used in consolidation of computers in order to reduce electricity cooling facility and other costs involved in running data centers. Hypervisors can also be used to migrate entire data centers from one geographic location to another and to construct various cloud computing environments. These environments are composed of hypervisors which together may host a multitude of guest operating systems. These environments are notable for their scalability i.e. if more compute power is needed to handle an increased workload the hypervisors may provide additional resources to guest operating systems they host. The hypervisors may also increase the number of hosted virtual guest operating systems and if necessary additional hypervisors can be added to the compute cloud. As workload eases the hypervisors may be powered off or be put into a lower power state in order to conserve electricity and other operating costs or removed from the compute cloud entirely.

There are many types of hypervisors in the marketplace and a platform virtualization infrastructure may be composed of a combination of hypervisors. This is called a hybrid infrastructure. A hybrid compute cloud is a composite cloud which includes virtual resources from at least two independent compute clouds. Often a company may operate a private compute cloud within its own data center. If the company needs additional compute resources to handle increased demand in a busy time of the year for example the company may purchase virtual servers from a vendor compute cloud such as AMAZON WEB SERVICES .

The hypervisors are typically connected to each other through a computer network. Computer networks are well known in the art and Ethernet based computer network based on an IEEE 802 standard is predominant. Currently Ethernet is standardized as IEEE 802.3 which is incorporated by reference as if fully set forth herein. An Ethernet computer network typically connects computers via Network Interface Cards NIC and one or more network routers and network switches. Network firewalls are used to limit information flow between computer networks and provide data and other resource protection.

Cloud computing networks provide a management interface such as a web page or a communication protocol by which guest operating systems can be controlled. New guest operating systems can be provisioned for started stopped paused or deleted via the management interface. Guest operating systems can also be moved from one hypervisor to another. Because the hypervisors provide a common level of virtualization guest operating systems may be moved from being hosted on one hypervisor to another without the knowledge or intervention of the users of the cloud. Other interactions with the guest operating systems via the management interface involve controlling hibernation which is an operation where the compute state of the virtual guest operating system is recorded and the guest operating system is powered down. The compute state of the guest operating system can then be restored possibly on a different hypervisor or even in a different compute cloud at a later time. Further by initiation via the management interface one or more copies of a guest operating system s compute image may also be cloned and started as additional guest operating systems. Hence the compute cloud offers clients a great deal of agility in the creation and management of guest operating systems since guest operating systems can be created destroyed hibernated cloned and moved.

The high agility of guest operating systems virtualized by hypervisors introduces many problems. Data centers are typically designed for a static environment where computers are physical entities which often do not move. Computers are connected to network switches and other devices in a planned fashion and they are not disconnected except during maintenance and decommissioning. The tools and information repositories used to maintain data centers reflect these older best practices.

The increased frequency of guest operating systems entering and leaving a compute cloud or physical computers off loading computing needs to guest operating systems in a hybrid cloud manner introduces many complications. It is important but difficult to maintain and track an accurate inventory of guest operating systems and know which cloud or hypervisor is hosting which guest operating systems at any given time. Auditing techniques are desired to ensure the integrity of the extended data center. An accurate inventory of guest operating systems is also important for purposes of accounting billing customer support security and right sizing of compute resources.

Furthermore errors may occur during the creation deletion movement cloning and or hibernation of guest operating systems. For example an error in the hypervisor may leave a guest operating system which is supposed to be powered off in a powered on state. This error in combination with guest operating system movement may cause an unwanted copy of the guest operating system to be running. If an error occurs during guest operating system movement then it may be difficult to determine where the guest operating system is.

Also tools for guest operating system placement and redistribution may introduce errors in the hypervisors and inventory records particularly if two or more guest placement tools or engines are used. There is no established standard for orchestrating guest operating system placement engines and they may inadvertently compete against each other. Placement engines typically operate directly in conjunction with the hypervisors and do not directly notify other inventory systems.

Often polling and an adherence to data center best practices and management mandates are used to alleviate these problems. That is hypervisors are periodically polled to determine which guest operating systems they are hosting and the managers of data centers typically allow only a single guest operating system placement engine to be used in a cloud. Also strict adherence to documentation and process are mandated in order to maintain cloud synchronization and reduce the creation of rogue guest operating systems. However even these practices may have limitations because the polling is typically slow and cannot keep pace with the agility of the cloud. Users also may desire features offered by their preferred placement engines. Finally best practices of documentation and process are also subject to human error.

In accordance with one embodiment there is provided a computer implemented method for detecting and reacting to a network discrepancy within a data center network environment. The method comprises monitoring network traffic between a first computer configured as a first hypervisor inter networked within the data center network environment and a second computer inter networked within the data center network environment in order to detect a presence of a predetermined condition representing a presence of the network discrepancy in the data center network environment between a recorded state of the data center network environment and an observed state of the data center network environment. The monitoring includes determining the observed state of the data center network environment using one of polling event based or a combination thereof. The method also comprises rectifying upon the detection of the presence of the predetermined condition an effect associated with the presence of the predetermined condition in the data center network environment.

In accordance with yet another embodiment there is provided a computer system for detecting and reacting to a network discrepancy within a data center network environment. The system comprises a memory and a processor in communications with the computer memory. The computer system is capable of performing a method comprising monitoring network traffic between a first computer configured as a first hypervisor inter networked within the data center network environment and a second computer inter networked within the data center network environment in order to detect a presence of a predetermined condition representing a presence of the network discrepancy in the data center network environment between a recorded state of the data center network environment and an observed state of the data center network environment. The monitoring includes determining the observed state of the data center network environment using one of polling event based or a combination thereof. The method also comprises rectifying upon the detection of the presence of the predetermined condition an effect associated with the presence of the predetermined condition in the data center network environment.

In accordance with yet another embodiment there is provided a computer program product for detecting and reacting to a network discrepancy within a data center network environment. The computer program product comprising a storage medium readable by a processing circuit and storing instructions for execution by the processing circuit for performing a method. The method comprising monitoring network traffic between a first computer configured as a first hypervisor inter networked within the data center network environment and a second computer inter networked within the data center network environment in order to detect a presence of a predetermined condition representing a presence of the network discrepancy in the data center network environment between a recorded state of the data center network environment and an observed state of the data center network environment. The monitoring includes determining the observed state of the data center network environment using one of polling event based or a combination thereof. The method also comprises rectifying upon the detection of the presence of the predetermined condition an effect associated with the presence of the predetermined condition in the data center network environment.

The following are definitions of terms used in the following description of the embodiments of the disclosed technology.

A computer network often simply referred to as a network is a collection of computers and devices connected by communications channels that facilitates communications among computers and allows computers to share resources with other computers. Often the computers and devices are connected using twisted pair wires and other Ethernet technology.

A data center is a facility used to house computer systems and associated components such as telecommunications and storage systems. Often the data center includes redundant or backup power supplies redundant data communications connections environmental controls and security devices.

A data center network environment is the computer network or networks used for communication within the data center. The data center network environment also includes gateway devices which may interconnect one data center network environment to another.

Cloud computing is network based computing whereby shared resources software and information are dynamically provided to computers and other devices on demand like the electricity grid. Typical cloud computing providers deliver common business applications online that are accessed from another Web service or software like a Web browser while the software and data are stored on servers.

Network traffic are digital communications between a computer and a device within the data center network environment. Often the network traffic follows Ethernet protocols where computers and devices exchange packets of data. Each packet may contain a control header containing addressing information including source and destination Media Access Control MAC addresses and a payload.

A recorded state is information stored in various inventory and topology databases and records. Generally this information is intended in part to model the data center network environment.

An observed state is information obtained through communication with a network device such as a network switch.

A network discrepancy is a logical difference found between the recorded state of the network environment and the observed state. For example some network discrepancies include a MAC address observed in network packets that is not found in the recorded state a MAC address observed as being associated with a first computer or device but being associated with a second computer or device in the recorded state and a computer or device listed in the recorded state but not being observed.

An event based technique is a computer programming paradigm where sensors or computer programs communicate events to listening computer programs. A sensor such as a network device may send a Link Up event using a Simple Network Management Protocol SNMP communication protocol when a network port is initialized. Event based computer programs can listen for these SNMP events and take responsive action. Similarly a RADIUS server may send an Accounting Start event to a listening computer program each time network access is granted to a computer or a network device and an Accounting Stop event when network access is closed.

A predetermined condition is a condition which may be indicative of a network discrepancy. For example the condition of a MAC address appearing in a network switch forward table may indicate a network discrepancy and the condition of a MAC address expiring from a network switch forward table may also indicate a network discrepancy.

A zombie guest operating system is a guest operating system which is still at least partially running despite a hypervisor attempt to shut it down.

Non virtual computers A B function as hypervisors and thus host zero or more virtual computers also known as guests. Each guest is running an operating system such as MICROSOFT WINDOWS or LINUX . As shown in in an example implementation five guests are hosted on non virtual computers A B . Boxes A B indicate provision of any logical components in non virtual computers A B respectively. For example guests A B C are hosted on non virtual computer A . Guests D E are hosted non virtual computer B .

Host operating systems A B which run on respective non virtual computers A B are run as hypervisors. Some examples of such operating systems include LINUX distributions such as REDHAT ENTERPRISE LINUX with kernels supporting XEN or KVM hypervisors MICROSOFT WINDOWS and VMWare ESX . The hypervisors each have various management interfaces A B which can be queried to obtain guests information. Some examples of these queries include requests to list all guests that the hypervisor is hosting to determine each guest s resource capabilities e.g. how many virtual network cards are defined and storage memory and CPU constraints to determine each guest s resource consumption and to determine each guest s current run state. In one embodiment the run state is typically a keyword such as powered on powered off suspended or hibernated . Management interfaces A B represent an interface into a generic machine emulator and virtualizer such as Quick Emulator QEMU . Further provided at boxes A and B are respective management interfaces A B that each represent an interface into a respective network management daemon such as Simple Network Management Protocol Daemon SNMPd . However the disclosed technology should not be construed to be limited to the above management interfaces. Any other management interface that performs a similar function as known in the art is contemplated to be within the scope of the disclosed technology.

In order to enable communication between guests A B C D E and other computing machines non virtual computers A B emulate networking hardware. Virtual network bridges and switches A B C D are connected through software to non virtual Ethernet NICs A B . Virtual NICs A B C D E are assigned to each guest A B C D E respectively. Some examples of implementations of virtual network bridges and switches include a virtual switch vSwitch on VMWARE ESX Server and brctl and Open vSwitch on LINUX .

In one embodiment non virtual computers A B are connected to network switch via dedicated ports A B of network switch . For simplicity a single network switch is shown. However more than one network switch may be used in a data center. In one embodiment port traffic from port C is mirrored onto port D .

Each NIC both non virtual and virtual has a unique MAC address. Non virtual computers A B include the identifier of the transmitting NIC within each data packet sent over the network. For example if guest A transmits an Ethernet data packet destined for guest D then that data packet will originate at virtual NIC A and contain the MAC address of virtual NIC A . The data packet will then flow over virtual network bridge A and be transmitted by non virtual Ethernet NIC A to network switch through switch port A . The source MAC address within the data packet will contain the MAC address of virtual NIC A . If guest A and guest D are peers on the same network subnet then to transmit the data packet to guest D guest A sets the target MAC address to that of virtual NIC D . The data packet would flow from network switch through switch port B into non virtual Ethernet NIC B . Virtual network bridge D which in one embodiment is also used for management of data traffic would deliver the data packet to guest D through virtual NIC D .

As further explained in reference to each network bridge both virtual and non virtual and network switch both virtual and non virtual include a data structure such as a forward table which is a lookup table that maps a MAC address to one of the ports A B C D of network switch . In one embodiment the contents of forward table enable network switch to retransmit the packet from guest A onto switch port B . In one embodiment forward table enables network switch to conserve network bandwidth by only forwarding the packet to port B and not broadcasting the packet to all ports.

Managed network switches such as network switch which are switches which can be queried and or configured through software have a management network switch interface . By using a protocol such as SNMP management interface allows computers and administrators to determine the contents of a network switch forward table . On non virtual switches such as network switch network forward table is typically populated by packet inspection. Upon packet entry network switch examines packet s source MAC address and entries are made into network switch forward table . Many implementations of network switch forward tables such as network switch forward table include a timeout value that indicates when an entry is considered stale and removed from the table. Network switch forward table of virtual bridges such as brctl may also be populated by running commands in Linux.

Network router is connected to port C and firewall . Network router and firewall are used to connect non virtual computers A B and other computing resources with other networks. One example of such network is network which may include the public interne and or other private intranets. Computers A B and C on network may be non virtual computers including those functioning as hypervisors or virtual guests served by other cloud environments. Hybrid cloud environment could be part of a broader hybrid or non hybrid cloud infrastructure if for instance one or more guests A B and C were served by vendor clouds. Other types of hybrid clouds are ones where a mix of hypervisor technology is used such as non virtual computer A being VMWARE ESX and non virtual computer B being LINUX KVM .

As further shown in hybrid cloud environment may include a first server and a second server which are both used for monitoring and management infrastructure of hybrid cloud environment . First server and second server may be guests on a non virtual computer functioning as a hypervisor such as non virtual computers A B . Alternatively first server and second server may be non virtual servers. First server and second server are operably interconnected within hybrid cloud environment and can communicate with network switch via ports F E and non virtual computers A B .

As further shown in box indicates provision of logical components in first server which may include for example a Configuration Management Database CMDB a Data Center Model DCM and a topology database . Optionally first server may include a RADIUS Remote Authentication Dial In User Service server .

More particularly CMDB is a database related to all logical components of an information system and it provides some high level details about the configuration of non virtual computers A B and their respective guests.

DCM is a database which provides a lower level of information about non virtual computers A B and their guests. Generally in contrast to CMDB which provides high level business oriented details DCM has lower level more technical details about the configuration of non virtual computers A B and their respective guests. DCM is typically used in allocating and provisioning guests onto non virtual computers A B by maintaining and updating an inventory of which guests are provisioned to non virtual computers A B the amount of computing resources such as CPU storage memory network bandwidth that each guest has been assigned and an estimate of the computing resources remaining on non virtual computers A B for future guests.

Topology database models the network topology of hybrid cloud environment . Topology database contains topological information such as which if any ports A B C D of network switch are connected to other switches how router and firewall are connected within hybrid cloud environment and a mapping which shows how non virtual computers A B are connected to ports A B C D . In one embodiment topology database may only be populated with information about the non virtual infrastructure and does not concern itself with the virtual guest network connections.

In one embodiment network switch may communicate with RADIUS server in order to authenticate devices authorize the devices for network communication and provide accounting metrics. In one embodiment network switch configured to communicate with RADIUS server does so via the 802.1x protocol.

Box associated with second server indicates provision of some logical components of second server which includes one or more guest placement engines a network discovery engine and an auditing engine .

Guest placement engines determine an optimal placement for new guests. An example of guest placement engines is the TIVOLI PROVISIONING MANAGER TPM available from International Business Machines Inc. IBM used in conjunction with an IBM TIVOLI SERVICE AUTOMATION MANAGER TSAM which includes a placement engine which consults DCM to select which non virtual computer A B should host new guests. In one embodiment this consultation occurs through a database connectivity API such as Open Database Connectivity ODBC or Java Database Connectivity JDBC over TCP IP. Another example of guest placement engine is a VMWARE CONTROL CENTER which uses non virtual computer A B management interfaces such as QEMUs A B to determine the current load of non virtual computer A B and recommend guest placement. Guest placement engines may also perform guest migrations where a guest is moved from being hosted on one non virtual computer A B to another. An example migration may include moving guest A away from non virtual computer A and onto non virtual computer B . Often particularly when more than one placement engine is in use data contained in DCM and CMDB can become out of date.

In one embodiment network discovery engine populates topology database with the topological information. This population is typically conducted by running ping scans over the network and through SNMP queries to some network devices such as switches routers firewalls. An example of network discovery engine is included in IBM s NETCOOL PRECISION IP EDITION product.

Auditing engine executes a process to determine whether a network discrepancy exists in hybrid cloud environment . In one embodiment upon receipt auditing engine invokes one or more workflows see in order to reduce at least one negative effect associated with network discrepancy.

The placement of the components and shown in two servers is for illustrative purposes only. Those skilled in the art of data center design will be familiar with other layouts. Further CMDB and DCM need not be formal databases. In an exemplary cloud environment having twenty hypervisors and fifty guest operating systems a spreadsheet implementation may suffice.

Managed network switches e.g. switches which can be queried and or configured through software have a management interface. This management interface typically through the SNMP protocol allows computers and administrators to determine the contents of the network switch forward table. On non virtual switches the network switch forward table is typically populated by packet inspection. As packets enter the network switch the network switch examines their source MAC address and entries are made into the network switch forward table. Many implementations of network switch forward tables include a timeout value that indicates when an entry is considered stale and removed from the table. The network switch forward table of virtual bridges such as brctl may also be populated by running commands in Linux.

In one embodiment network switch forward table includes two columns a first column and a second column . The content of cells in first column includes network switch port identifiers. The content of cells in second column includes MAC addresses of Ethernet NICs which are connected directly or indirectly to the corresponding port of the network switch referenced in column .

The content of first column and second column indicates that as shown in network switch is connected to non virtual computer A through network switch port A and NIC A . Guests A B are also connected to switch port A through their virtual NICs A and B respectively. Similarly non virtual computer B is connected to network switch port B. The content of first column and second column also indicates that guest D is connected to network switch port A. This configuration differs from what is shown in and indicates a discrepancy between a recorded state of hybrid cloud environment as recorded in a published inventory table see table in and an observed state of hybrid cloud environment as recorded in network switch forward table .

In one embodiment network switch forward table may be retrieved from a managed network switch using a communication protocol such as SNMP.

In one embodiment hypervisor forward table has four columns a first column a second column a third column and a fourth column . The content of cells in first column includes hypervisor identifiers which identify operating hypervisors. The content of cells in second column includes MAC addresses which references MAC addresses associated with hypervisors identifiers referenced in first column . The content of cells in third column includes network switch identifiers which identify network switches associated with MAC addresses in second column . The content of cells in fourth column includes port identifiers which identify ports on corresponding network switches listed in third column . Therefore hypervisor forward table associates one or more MAC addresses of each hypervisor with a port on a switch. In some computer systems with a hierarchy of network switches or other network topologies such as redundant network links there may be multiple entries for each MAC and each port.

The content of first column second column third column and fourth column indicates that as shown in network switch is connected to non virtual computer A through Ethernet NIC A and switch port A. Non virtual computer B is connected to switch through Ethernet NIC B and switch port B.

Hypervisor forward table provides information similar to that of network switch forward table as discussed in reference to . However these tables differ in how they are populated. The content for network table is retrieved from managed network devices such as network switch . The content for hypervisor forward table is retrieved from a network topology database such as a topology database as previously discussed in reference to . In a stable networking environment both tables would typically show the same hypervisor network connectivity. In reference to discrepancies between the two tables indicate a difference between the observed state of hybrid cloud environment as recorded in network switch forward table and the recorded state of hybrid cloud environment as recorded in network topology database .

In one embodiment published inventory table includes four columns a first column a second column a third column and a fourth column . The content of entries in first column includes hypervisor identifiers which identify operating hypervisors. The content of cells in second column includes guest operating system identifiers which identify guest operating systems running on corresponding hypervisors referenced in first column . The content of cells in third column includes MAC addresses associated with guest operating systems references in second column . The content of cells in fourth column includes a run state indicator e.g. a keyword indicating the operating state of guest operating systems referenced in second column . In one embodiment some typical keywords include powered on powered off suspended and crashed . Other keywords may be used as well.

The content of first column second column third column and fourth column indicates that as shown in non virtual computer A hosts three guest operating systems A B and C . Guest operating systems A and C are powered on and guest operating system B is powered off. Non virtual computer B hosts two guest operating systems D and E . Guest operating system D is powered on and guest operating system E is powered off.

In reference to some of the contents to be included within published inventory table can be retrieved by communicating with individual non virtual computers A B through management interfaces such as QEMU A SNMPD A QEMU B and SNMPD B to obtain data corresponding to their respective table rows. Some hypervisor products such as VMWARE ESX and KVM may provide centralized views over a collection of hypervisors and data for published inventory table can be retrieved by consulting a centralized virtualization controller.

Process begins at step . Process is performed by auditing engine such as auditing engine . In one implementation all network traffic is performed using at least one of the TCP IP and UDP network protocols.

In step a list is initialized. List is a list of guest operating system identifiers as would be seen in one embodiment in second column of published inventory table . Initially list is empty.

In step process begins a loop over each managed network switch. In one implementation the managed network switch is network switch .

In step a network switch forward table is retrieved. In one implementation network switch forward table is network switch over table .

In step another inner loop is begun. This loop iterates over each entry in the network switch forward table that contains a guest operating system MAC address such as MAC addresses disclosed in third column of published inventory table . As discussed above in one implementation network switch forward table can be retrieved by communicating to the managed network switch through a network protocol such as SNMP. Network switch forward table will typically contain many MAC addresses. This process only concerns itself with the MAC addresses which are associated with guest operating systems. These MAC addresses can be found by consulting second column of published inventory table .

In one implementation the MAC addresses of guest operating systems are constructed with a known prefix. For example each MAC address begins with 54 52 00 and is followed by three pairs of random hexadecimal digits. This identification system allows for a rapid identification of guest operating systems MAC addresses disclosed within switch forward table . This identification system also allows process to subsequently determine the presence of guest operating systems MAC addresses not disclosed within published inventory table . For example the ESX and KVM hypervisors have a reserved range of MAC addresses to provide to guest operating systems.

In step a check is made to see if a guest operating system MAC address as disclosed by second column in network switch forward table is listed within column of published inventory table . If the MAC address is not found then execution of process branches to step where a report or a message of a discrepancy is generated. In one implementation the report or the message could be an e mail a message on a paging device an entry in a database a message sent to a message queue or a message sent over a REST Representational State Transfer API. In a preferred embodiment the reports are messages which are sent to a workflow manager See for example . This branching typically indicates that a new guest has been powered on and is being hosted by a hypervisor. The report includes an identifier associated with managed network switch as disclosed in step MAC address as disclosed in second column and network switch port as disclosed in first column of network switch forward table . Such reports are sent either individually or collectively to reaction workflows such as reaction workflows as described in reference to . Consequently method proceeds to step .

However if in step guest operating system is found within published inventory table having MAC address then the execution of process continues to step .

In at least one row is selected from a hypervisor forward table. Specifically in one implementation all rows of hypervisor forward table which correspond to the hypervisor hosting the found guest operating system the current network switch and the current network switch port are selected. For example the current selection is made from hypervisor forward table using the following criteria 

In step if no rows were found a report of a discrepancy is generated. This report is similar to the report generated in step . This report indicates that a guest operating system either moved was cloned or that two hypervisors are concurrently hosting the same guest operating system. Here the symptom of network discrepancy is that the network switch has detected network activity of a guest operating system on an unexpected network switch port. Consequently the process proceeds to step .

If any rows were found in the hypervisor forward table by the check performed in step then execution of process continues to step . Those familiar with database queries will note that since process does not reference the contents of any rows selected in step alternatively steps and may be written as existence tests.

In step published inventory table is consulted in order to determine the known run state of guest operating system . If guest operating system disclosed in second column was known to be in state where it should not be communicating e.g. if it is powered off and the network switch is reporting that guest operating system is communicating then a discrepancy has been detected and process execution branches to step where a report is generated. This report is similar to the report generated in step . In one implementation this type of discrepancy may indicate a hypervisor error where the hypervisor continues to run the guest operating system although the guest operating system is listed as powered off in its inventory table. In one implementation the discrepancy may be due to a lag between the network switch not expiring the network switch forward table entry if the guest operating system was recently powered off. Regardless of the branch taken in execution of process continues to step .

In step the execution of process iterates to the next guest MAC in the network switch forward table and loops back to step . If there are no guest operating systems left in the network switch forward table for network switch then the execution of process continues to step .

In step process iterates via the outer loop to step for the next network switch. If there are no further network switches left then the execution of process continues to step .

In step process begins another loop iterating over any guests in the published inventory table that are not disclosed in list which has been populated with the identifiers of guest operating systems who are communicating over the network via network switch . Hence those guest operating systems in the published inventory table not in list are those who are not communicating with the network.

In step a check is made in order to determine if guest operating system is in a state where it should be communicating. If guest operating system is in a state such as powered on but was not found to be communicating step is executed i.e. a discrepancy report is generated. This report is similar to the report generated in step . Some network devices such as printers will have long periods of inactivity and generate network traffic. However typically guest operating system such as guest operating system visiting a cloud maintains a steady flow of network traffic. Accordingly an absence of network traffic could indicate a problem. In one implementation this problem is indicative of guest operating system crashing or being frozen and this state e.g. crash or freeze may not be known to the hypervisor. In one implementation the guest operating system could have both been moved to another hypervisor and remain in the original hypervisors inventory table.

In step a discrepancy report is generated. This report is similar to the report generated in step . This branching typically indicates that a new guest has been powered on and is being hosted by a hypervisor. In one implementation the report includes guest identifier and optionally other information in published inventory table hypervisor MAC and run state .

In step process loops back to step in order to determine if there are more guest operating systems which are not communicating. When all the guest operating systems have been exhausted process ends at step .

In one implementation the selection of guest operating systems performed in step could also include the criteria of step i.e. the condition of step could be incorporated into step .

Workflow manager may be a software program or a module or a hardware device capable of reacting to the receipt and interpretation of the discrepancy reports A B by invoking one or more workflows. In one embodiment the invocation may be an automated process performed by a workflow engine or through an e mail system. The workflow engine or the email system can be configured to launch scripts upon receipt of e mail from certain addresses or with certain subject lines. Workflow manager may execute at least one of the following workflows.

Workflow cross references the network discrepancy against work scheduled to be performed within the CMDB . Discrepancies could be a normal artifact of work performed during for example a scheduled outage.

Workflow delays or ignores reaction for a period of time. In one embodiment as described in in reference to step first zombie guest operating system discrepancy might be discarded because network switch forward table entries may not be up to date. However generation of subsequent zombie guest operating system reports could cause some action to be taken.

Workflow opens an incident ticket to a ticketing system and or sends an email to a system operator notifying the operator of the observed network discrepancy.

Workflow forces the guest operating system into a powered off state. In one embodiment this may occur when there appears to be a duplication of guest operating systems running in the system.

Mobile IP is an Internet Engineering Task Force IETF standard communications protocol that is designed to allow mobile device users to move from one network to another while maintaining a permanent IP address. Mobile IPv4 is described in IETF RFC 3344 Obsoleting both RFC 3220 and RFC 2002 and updates are added in IETF RFC 4721. Mobile IPv6 is described in IETF RFC 3775. RFCs 3344 3220 2002 4721 and 3775 are all incorporated by reference as if fully set forth herein.

The start event contains a network identifier A and a location B . Stop event contains a network identifier A and expiration event also contains a network identifier A .

In one implementation a network monitor process A listens for accounting events from RADIUS server see . RADIUS server receives access to accounting messages from network devices such as managed network switch during the 802.1x network handshaking protocol when a non virtual or virtual NICs A A connect or disconnect to a network switch port such as network switch port A of a network switch see .

A RADIUS Accounting Request packet includes an Acct Status Type attribute with the value start is sent by network switch to the RADIUS server to signal the start of the NIC s network access. The start records typically include both the MAC address of the NIC and the identifier of the network switch port such as A . A RADIUS Accounting Request packet containing an Acct Status Type attribute with the value stop is sent to the RADIUS server when the NIC disconnects from the managed switch. Consequently the start record would be the start event and the stop record would be the stop event . In one implementation network identifiers A A are MAC addresses and location B is a switch and port identifier.

In one implementation a network monitor process B is a process which listens to network traffic through a mirrored port such as D of a network switch or otherwise sniffs the traffic on the network. Specifically network monitor process B listens for IPv6 packets which contain a Binding Update Mobile IPv6 Destination option.

In implementing Mobile IPv6 computers are enabled to move throughout a data center or other interconnected network while retaining network connectivity and established network communication sessions. A Home Agent HA coordinates communication between the mobile node MN and correspondent nodes CN . Each time the mobile node moves it obtains a local network address called a care of address and this address is made known to the HA through in part the Mobile IPv6 Destination options. When a correspondent node wishes to communicate to the mobile node it acquires the care of address through in part Mobile IPv6 Destination options. Mobile IPv6 and its Destination options are well known in the art. See for example http www.slideshare.net guestbeb9a5 introduction to mobile ipv6 for more description of the Mobile IPv6 and its Destination options.

Accordingly by listening to network traffic network monitor process B may determine if a guest virtual computer such as guest A is operating as a home agent a correspondent node and or a mobile node. Start events and stop events are generated when the Mobile IPv6 Destination options indicate that a node has become a mobile node ceased being a mobile node or that it has changed its care of address. In one implementation network identifiers A A are IPv6 addresses. Location B is the subnet of the mobile node s care of address.

Process maintains a list of network identifiers and expiration dates . Network identifiers are MAC addresses where the network monitor process is RADIUS server based A . The network identifiers are IPv6 addresses in the case of network monitor process is Mobile IPv6 traffic based B .

In one implementation the execution of the event based process begins at step upon receipt of a start event . Each start event contains a network identifier and this network identifier is added to list . Expiration date for the new list entry is set to a time value in the future. In one implementation expiration date is the current time plus a fixed number of seconds. If list already contains an entry with the same network identifier that entry is removed from list .

Process then determines if a guest operating system is associated with network identifier A . Where network identifier A is a MAC address this determination is done by examining published inventory table see . Where network identifier A is an IPv6 address the determination is done by examining one or more of CMDB DCM and topology databases see . If identifier A is not associated with a known guest operating system a discrepancy report is generated similar to step of process in . Consequently the execution of process then ends at step .

In step if a guest operating system is found then a test is performed to determine if the guest operating system is operating from the proper location. Where location B is a switch and port identifier topology database is consulted to determine if the switch and port values match what is contained in topology database . Where location B is a subnet CMDB and or DCM are consulted to determine if the subnet value matches what is contained in the database s . If the test fails to match a discrepancy report is generated similar to step of process in . Consequently the execution of process then ends at step .

In step if the guest is determined to be at the proper location then a test is performed similar to step of process in in order to determine if the guest is known to be in a powered on state. Where A is an IPv6 address the known state may be determined by examination of the CMDB and or DCM databases. If CMDB and or DCM databases do not include such state information then the performance of this step is omitted and the test is considered to have passed.

If the test in step failed a discrepancy report is generated similar to step of process in . Consequently the execution of process then ends at step .

In step a test as described in reference to step is performed in order to determine if a guest associated with network identifier A is known to be in a powered on state. If the test of step fails then a discrepancy report is generated similar to step of process in . Regardless of the result of the test the execution of process continues with step when the entry if present of list that contains network identifier A is deleted. The execution of process then ends at step .

An expiration event is generated when expiration date of an entry in list has already expired. Expiration event includes network identifier A field which is equal to network identifier of the expired list entry.

In one implementation the execution of the event based process begins at step upon receipt of expiration event . As in steps and a test is performed to determine if a guest associated with network identifier A is known to be in a powered on state. If the test of step is passed then a discrepancy report is generated similar to step of process in . Regardless of the result of the test the execution of process continues with the performance of step when the entry if present of list that includes the network identifier A is deleted. The execution of process then ends at step .

While the foregoing is directed to embodiments of the presently disclosed technology other and further embodiments of the disclosed technology may be devised without departing from the basic scope thereof and the scope thereof is determined by the claims that follow.

As will be appreciated by one skilled in the art aspects of the present disclosure may be embodied as a system method or computer program product. Accordingly aspects of the present disclosure may take the form of an entirely hardware embodiment an entirely software embodiment including firmware resident software micro code etc. or an embodiment combining software and hardware aspects that may all generally be referred to herein as a circuit module or system. 

Furthermore aspects of the present disclosure may take the foam of a computer program product embodied in one or more computer readable medium s having computer readable program code embodied thereon. Any combination of one or more computer readable medium s may be utilized. The computer readable medium may be a computer readable signal medium or a computer readable storage medium. A computer readable storage medium may be for example but not limited to an electronic magnetic optical electromagnetic infrared or semiconductor system apparatus or device or any suitable combination of the foregoing. More specific examples a non exhaustive list of the computer readable storage medium would include the following an electrical connection having one or more wires a portable computer diskette a hard disk a random access memory RAM a read only memory ROM an erasable programmable read only memory EPROM or Flash memory an optical fiber a portable compact disc read only memory CD ROM an optical storage device a magnetic storage device or any suitable combination of the foregoing. In the context of this document a computer readable storage medium may be any tangible medium that can contain or store a program for use by or in connection with an instruction execution system apparatus or device.

A computer readable signal medium may include a propagated data signal with computer readable program code embodied therein for example in baseband or as part of a carrier wave. Such a propagated signal may take any of a variety of forms including but not limited to electro magnetic optical or any suitable combination thereof. A computer readable signal medium may be any computer readable medium that is not a computer readable storage medium and that can communicate propagate or transport a program for use by or in connection with an instruction execution system apparatus or device.

Program code embodied on a computer readable medium may be transmitted using any appropriate medium including but not limited to wireless wireline optical fiber cable RF etc. or any suitable combination of the foregoing. Computer program code for carrying out operations for aspects of the present disclosure may be written in any combination of one or more programming languages including an object oriented programming language such as Java Smalltalk C or the like and conventional procedural programming languages such as the C programming language or similar programming languages. The program code may execute entirely on the user s computer partly on the user s computer as a stand alone software package partly on the user s computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario the remote computer may be connected to the user s computer through any type of network including a local area network LAN or a wide area network WAN or the connection may be made to an external computer for example through the Internet using an Internet Service Provider .

Aspects of the present disclosure are described below with reference to flowchart illustrations and or block diagrams of methods apparatus systems and computer program products according to embodiments of disclosed herein. It will be understood that each block of the flowchart illustrations and or block diagrams and combinations of blocks in the flowchart illustrations and or block diagrams can be implemented by computer program instructions. These computer program instructions may be provided to a processor of a general purpose computer special purpose computer or other programmable data processing apparatus to produce a machine such that the instructions which execute via the processor of the computer or other programmable data processing apparatus create means for implementing the functions acts specified in the flowchart and or block diagram block or blocks. These computer program instructions may also be stored in a computer readable medium that can direct a computer other programmable data processing apparatus or other devices to function in a particular manner such that the instructions stored in the computer readable medium produce an article of manufacture including instructions which implement the function act specified in the flowchart and or block diagram block or blocks. The computer program instructions may also be loaded onto a computer other programmable data processing apparatus or other devices to cause a series of operational steps to be performed on the computer other programmable apparatus or other devices to produce a computer implemented process such that the instructions which execute on the computer or other programmable apparatus provide processes for implementing the functions acts specified in the flowchart and or block diagram block or blocks.

The flowchart and block diagrams in illustrate the architecture functionality and operation of possible implementations of systems methods and computer program products according to various embodiments of the present disclosure. In this regard each block in the flowchart or block diagrams may represent a module segment or portion of code which comprises one or more executable instructions for implementing the specified logical function s . It should also be noted that in some alternative implementations the functions noted in the block may occur out of the order noted in the figures. For example two blocks shown in succession may in fact be executed substantially concurrently or the blocks may sometimes be executed in the reverse order depending upon the functionality involved. It will also be noted that each block of the block diagrams and or flowchart illustration and combinations of blocks in the block diagrams and or flowchart illustration can be implemented by special purpose hardware based systems that perform the specified functions or acts or combinations of special purpose hardware and computer instructions.

Although the embodiments of the present disclosure have been described in detail it should be understood that various changes and substitutions can be made therein without departing from spirit and scope of the disclosure as defined by the appended claims. Variations described for the present disclosure can be realized in any combination desirable for each particular application. Thus particular limitations and or embodiment enhancements described herein which may have particular advantages to a particular application need not be used for all applications. Also not all limitations need be implemented in methods systems and or apparatus including one or more concepts of the present disclosure.

Reference in the specification to one embodiment or to an embodiment means that a particular feature structure or characteristic described in connection with the embodiments is included in at least one embodiment. The appearances of the phrase one embodiment in various places in the specification are not necessarily all referring to the same embodiment.

