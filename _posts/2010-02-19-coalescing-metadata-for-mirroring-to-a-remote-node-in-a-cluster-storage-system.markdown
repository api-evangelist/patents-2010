---

title: Coalescing metadata for mirroring to a remote node in a cluster storage system
abstract: Described herein are a system and method for remote mirroring/copying data and metadata sets from a local node to a remote node that reduces the number of metadata sets that are mirrored. In some embodiments, the local node may coalesce metadata sets into metadata chains, each metadata chain comprising a grouping of two or more metadata sets. In some instances, a “representative” metadata set of a metadata chain may be selected for sending to the remote node for storing, wherein the other metadata sets of the metadata chain are not sent to the remote node. In these embodiments, the selected metadata set may represent all the metadata sets in the chain and be the only metadata set in the chain that is transmitted and stored to the remote node. As such, the network congestion between the local and remote nodes may be reduced.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08386433&OS=08386433&RS=08386433
owner: NetApp, Inc.
number: 08386433
owner_city: Sunnyvale
owner_country: US
publication_date: 20100219
---
The present invention relates to storage systems and particularly to coalescing metadata for mirroring to a remote node in a cluster storage system.

A storage system typically comprises one or more storage devices into which information may be entered and from which information may be obtained as desired. The storage system includes a storage operating system that functionally organizes the system by inter alia invoking storage operations in support of a storage service implemented by the system. The storage system may be implemented in accordance with a variety of storage architectures including but not limited to a network attached storage environment a storage area network and a disk assembly directly attached to a client or host computer. The storage devices are typically disk drives organized as a disk array wherein the term disk commonly describes a self contained rotating magnetic media storage device. The term disk in this context is synonymous with hard disk drive HDD or direct access storage device DASD . The storage operating system of the storage system may implement a high level module such as a file system to logically organize the information stored on volumes as a hierarchical structure of data containers such as files and logical units LUs . For example each on disk file may be implemented as set of data structures i.e. disk blocks configured to store information such as the actual data for the file. These data blocks are organized within a volume block number vbn space that is maintained by the file system. The file system may also assign each data block in the file a corresponding file offset or file block number fbn . The file system typically assigns sequences of fbns on a per file basis whereas vbns are assigned over a larger volume address space. The file system organizes the data blocks within the vbn space as a logical volume each logical volume may be although is not necessarily associated with its own file system.

A known type of file system is a write anywhere file system that does not overwrite data on disks. If a data block is retrieved read from disk into a memory of the storage system and dirtied i.e. updated or modified with new data the data block is thereafter stored written to a new location on disk to optimize write performance. A write anywhere file system may initially assume an optimal layout such that the data is substantially contiguously arranged on disks. The optimal disk layout results in efficient access operations particularly for sequential read operations directed to the disks. An example of a write anywhere file system that is configured to operate on a storage system is the Write Anywhere File Layout WAFL file system available from NetApp Inc. Sunnyvale Calif.

The storage system may be further configured to operate according to a client server model of information delivery to thereby allow many clients to access data containers stored on the system. In this model the client may comprise an application such as a database application executing on a computer that connects to the storage system over a computer network such as a point to point link shared local area network LAN wide area network WAN or virtual private network VPN implemented over a public network such as the Internet. Each client may request the services of the storage system by issuing access requests read write requests as file based and block based protocol messages in the form of packets to the system over the network.

A plurality of storage systems may be interconnected to provide a storage system architecture configured to service many clients. In some embodiments the storage system architecture provides one or more aggregates and one or more volumes distributed across a plurality of nodes interconnected as a cluster. The aggregates may be configured to contain one or more volumes. The volumes may be configured to store content of data containers such as files and logical units served by the cluster in response to multi protocol data access requests issued by clients.

Each node of the cluster may include i a storage server referred to as a D blade adapted to service a particular aggregate or volume and ii a multi protocol engine referred to as an N blade adapted to redirect the data access requests to any storage server of the cluster. In the illustrative embodiment the storage server of each node is embodied as a disk element D blade and the multi protocol engine is embodied as a network element N blade . The N blade receives a multi protocol data access request from a client converts that access request into a cluster fabric CF message and redirects the message to an appropriate D blade of the cluster.

The nodes of the cluster may be configured to communicate with one another to act collectively to increase performance or to offset any single node failure within the cluster. Each node in the cluster may have a predetermined failover partner node that may take over resume storage functions of the node upon failure of the node. When a node failure occurs where the failed node is no longer capable of processing access requests for clients the access requests sent to the failed node may be re directed to the partner node for processing. As such the cluster may be configured such that a partner node may take over the work load of a failed node. As used herein a local source node may have data and metadata that is mirrored copied to a remote destination node in the cluster storage system as discussed below . The remote node may comprise a predetermined failover partner node of the local node. As used herein various components residing on the local node may likewise be referred to as a local component e.g. local memory local de staging layer etc. and various components residing on a remote node may likewise be referred to as a remote component e.g. remote memory remote de staging layer etc. .

A cluster provides data access service to clients by providing access to shared storage comprising a set of storage devices . Typically clients will connect with a node of the cluster for data access sessions with the node. During a data access session with a node a client may submit access requests read write requests that are received and performed by the node. For the received write requests the node may produce write logs that represent the write requests and locally store the write logs to a volatile storage device from which the node may at a later time perform the write logs on the storage devices .

To ensure data consistency and provide high data availability the write logs may also be stored to two non volatile storage devices. Typically the write logs of the node may be locally stored to a non volatile storage device and also be stored remotely to a non volatile storage device at a partner node sometimes referred to herein as mirroring data to a remote node . As such if the local node fails the remote partner node will have a copy of the write logs and will still be able to perform the write logs on the storage devices. Also if the write logs stored at the partner node is corrupted or lost the write logs stored locally in the non volatile storage device at the local node can be extracted retrieved and used to perform the write logs on the storage devices.

As such data in a local non volatile storage device at a local node may be mirrored to a remote non volatile storage device of a remote node to provide failover protection e.g. in case the local node crashes and high availability of data in the cluster storage system. The mirrored data may comprise write logs or any other data that is to be stored to the non volatile storage devices.

Typically for a group of related data sets e.g. data sets X Y there may also be a metadata set e.g. metadata set Z that describes each of the related data sets e.g. metadata set Z describes data sets X Y the metadata set to also be stored to the local and remote non volatile storage devices. As used herein a related group of data and metadata sets may comprise one or more data sets and one metadata set that describes and is associated with each of the one or more data sets. For example the data sets of a related group may comprise data sets X Y and metadata set Z where metadata set Z specifies that there are 2 valid data sets. A valid data set may comprise user client data that is pending to be stored to the local and remote non volatile storage devices. The ratio of data sets to a metadata set produced may vary depending on the mirroring client. For example a particular mirroring client may continually produce and send one metadata set for every two data sets whereby each related group comprises two data sets and one metadata set.

Metadata sets may be produced at the local node to assist in the processing of the data sets the metadata sets also being mirrored to the remote node. However additional software and hardware resources are expended to process the metadata sets and the metadata sets cause additional network congestion between the local and remote nodes consuming the network connection s valuable data bandwidth. Since metadata sets do not comprise user client data the metadata sets may be considered overhead that should be limited as much as possible.

Described herein are a system and method for remote mirroring copying data and metadata sets from a local node to a remote node that reduces the number of metadata sets that are mirrored. In some embodiments the local node may coalesce metadata sets into metadata chains each metadata chain comprising a grouping of two or more metadata sets. In some instances a representative metadata set of a metadata chain may be selected for sending to the remote node for storing wherein the other metadata sets of the metadata chain are not sent to the remote node. In these embodiments the selected metadata set may represent all the metadata sets in the chain and be the only metadata set in the chain that is transmitted and stored to the remote node. As such the network congestion between the local and remote nodes may be reduced.

In some embodiments the local source node executes software layers or applications referred to as mirroring clients that may require data and metadata to be stored to a local non volatile storage device and mirrored stored to a remote non volatile storage device on the remote destination node. In some embodiments a mirroring client comprises a software layer e.g. file system layer of a storage operating system executing on the local node. For storing data and metadata to the local non volatile storage device a mirroring client may send the data and metadata to software layers of the storage operating system that store the data and metadata using methods known in the art. For storing data and metadata to the remote non volatile storage device each mirroring client may also send a stream of data and metadata to a mirroring layer engine sometimes referred to as an interconnect IC layer engine of the storage operating system that stores the data and metadata using methods described herein.

In some embodiments the mirroring layer engine may perform embodiments described herein. The mirroring layer may receive the stream of data and metadata from each mirroring client and store the received data and metadata to a remote node while reducing the number of metadata sets transmitted and stored to the remote node. In some embodiments the mirroring layer produces a data and metadata request DMR data structure and a metadata request MR data structure for each mirroring client. The mirroring layer may treat each received data and metadata set as a request having a unique request identifier XID from the mirroring client to mirror store the data or metadata set to the remote node. In some embodiments herein the terms data or metadata set may be used interchangeably with the terms data or metadata request. The mirroring layer may queue store each received data and metadata set request to the DMR data structure for the mirroring client. The mirroring layer may also queue store each received metadata set request to the MR data structure for the mirroring client.

In some embodiments the metadata sets produced by a mirroring client may specify the total accumulated number of data sets currently produced by the mirroring client and sent to the mirroring layer. Because of the accumulative nature of the metadata sets the data contained in a new metadata set may encompass or encapsulate data contained in a previous metadata set. In certain circumstances only sending a new metadata set to the remote node may produce the same end result as sending the previous metadata set and then the new metadata set.

The mirroring layer may queue store received requests to the DMR and MR data structures based on the time order the requests are received. For example earlier received requests may be stored towards the top of the DMR and MR data structures and later received requests may be stored towards the bottom of the DMR and MR data structures. As such a new metadata request stored below a previous metadata request may encompass or encapsulate the data contained in the previous metadata set. As data and metadata requests are completed i.e. successfully stored to the remote node the mirroring layer may remove completed requests from the DMR and MR data structures.

In some embodiments the mirroring layer may use the MR data structure to manage metadata chains e.g. produce break and or delete metadata chains . As used herein each metadata chain may comprise a head metadata request a tail metadata request and zero or more middle metadata requests. The head metadata request may comprise a metadata request that is received earlier than the tail metadata request and is stored above the tail metadata request in the DMR and MR data structures.

In some embodiments the mirroring layer may coalesce a newly received metadata request with a previously received metadata request in the MR data structure to produce a chain if certain conditions exist. For example the metadata requests may be coalesced if both metadata requests are to be stored to the same storage address location at the remote non volatile storage device at the remote node both metadata requests have the same data size or the previous metadata request has not yet been transmitted to the remote node. In other embodiments the mirroring layer may require all three conditions or any combination of the three conditions for coalescing the two metadata requests to produce a chain.

As used herein a representative metadata request comprises a metadata request in a metadata chain that is the only metadata request in the chain that is sent to the remote node. A represented metadata request may comprise the other metadata requests in the metadata chain that are not sent to the remote node. In some embodiments the mirroring layer may send to the remote node only the representative metadata request in a metadata chain and not send represented metadata requests to the remote node only in certain circumstances.

In some embodiments only the representative metadata request in a chain is sent when the metadata requests in the chain comprise the top most requests currently stored in the DMR data structure i.e. the metadata requests in the chain comprise the most earliest received requests currently remaining stored in the DMR data structure and the representative metadata request comprises the tail metadata request in the chain. For example if the chain comprises two metadata requests only the representative metadata request in the chain is sent if the two metadata requests in the chain comprise the top two requests in the DMR data structure and the representative metadata request comprises the tail metadata request that is received after the represented metadata request that comprises the head metadata request in the chain. As such the representative metadata request is the last received metadata request in the chain and may encompass or encapsulate data contained in the represented metadata request.

In the following description numerous details are set forth for purpose of explanation. However one of ordinary skill in the art will realize that the embodiments described herein may be practiced without the use of these specific details. In other instances well known structures and devices are shown in block diagram form in order not to obscure the description with unnecessary detail.

The description that follows is divided into three sections. Section I describes a cluster environment in which some embodiments operate. Section II describes a storage operating system having a mirroring layer for remote mirroring of data and metadata to a remote node. Section III describes a method and apparatus for reducing metadata in remote mirroring of data and metadata to a remote node.

As shown in each node may be organized as a network element N blade and a disk element D blade . The N blade includes functionality that enables the node to connect to clients over a computer network while each D blade connects to one or more storage devices such as disks of a disk array . The nodes are interconnected by a connection system such as a cluster switching fabric discussed below .

It should be noted that although disks are used in some embodiments described below any other type of storage device may be used as well. For example a solid state storage device may be used instead the solid state device having no mechanical moving parts for reading and writing data. Some examples of solid state devices include flash memory non volatile storage device NVRAM Magnetic Random Access Memory MRAM Phase Change RAM PRAM etc. In other embodiments other storage devices other than those mentioned here may also be used.

Also it should be noted that while there is shown an equal number of N and D blades in the illustrative cluster there may be differing numbers of N and or D blades and or different types of blades implemented in the cluster in accordance with various embodiments. For example there may be a plurality of N blades and or D blades interconnected in a cluster configuration that does not reflect a one to one correspondence between the N and D blades. As such the description of a node comprising one N blade and one D blade should be taken as illustrative only. For example a node may also have one N blade and a plurality of D blades a plurality of N blades and one D blade or a plurality of N blades and a plurality of D blades.

The clients may be general purpose computers configured to interact with the node in accordance with a client server model of information delivery. That is each client may request the services of the node e.g. by submitting read write requests and the node may return the results of the services requested by the client by exchanging packets over the network . The client may submit access requests by issuing packets using file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of files and directories. Alternatively the client may submit access requests by issuing packets using block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of blocks.

In some embodiments a client connects to a node for a data access session with the node . During a data access session the client may submit access requests that are received and performed by the node . Such access requests may include storage state requests a storage state request comprising a request that alters the data state of a storage device . Examples of storage state requests include requests for storing new data to a file deleting a file changing attributes of a file etc. For illustrative purposes storage state requests may be generically referred to herein as write requests.

In some embodiments the totality of storage space provided by the disks and disk arrays of the cluster comprise a total shared storage space referred to as shared storage of the cluster . In other embodiments the shared storage comprises the totality of storage space provided by other types of storage devices such as solid state storage devices . The shared storage is accessible by each D blade of each node in the cluster . In some embodiments the cluster may provide high availability of service to clients in accessing the shared storage . For example the nodes may be configured to communicate with one another e.g. via cluster switching fabric to act collectively to offset any single node failure within the cluster .

To ensure data consistency and provide high data availability a local source node e.g. local node A may have data and metadata stored to a local non volatile storage device that is mirrored copied to a remote non volatile storage device at a remote destination node e.g. remote node B in the cluster . Likewise remote node B may have data and metadata stored to the remote non volatile storage device that is mirrored copied to a local non volatile storage device at the local node A. The remote node B may comprise a predetermined failover partner node of the local node A. Likewise the local node A may comprise a predetermined failover partner node of the remote node B. As used herein various software and hardware components residing on the local node may be referred to as a local component e.g. local non volatile storage device local de staging layer etc. and various components residing on a remote node may be referred to as a remote component e.g. remote non volatile storage device remote de staging layer etc. .

The data and metadata mirrored from the local node A to remote node B may comprise for example write logs. As such if the local node A fails the remote partner node B will have a copy of the write logs and will still be able to perform the write logs on the storage devices. In other embodiments the data and metadata mirrored from the local node A to remote node B may comprise any other type of data and metadata. As such data in a local non volatile storage device at a local node may be mirrored to a remote non volatile storage device of a remote node to provide failover protection e.g. in case the local node crashes and high availability of data in the cluster storage system.

The cluster access adapter comprises a plurality of ports adapted to couple the node to other nodes of the cluster through the cluster switching fabric . In the illustrative embodiment Ethernet is used as the clustering protocol and interconnect media although it will be apparent to those skilled in the art that other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternate embodiments where the N blades and D blades are implemented on separate storage systems or computers the cluster access adapter is utilized by the N D blade for communicating with other N D blades in the cluster .

Each node is illustratively embodied as a dual processor storage system executing a storage operating system that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of named data containers such as directories files and special types of files called virtual disks hereinafter generally blocks on the disks. However it will be apparent to those of ordinary skill in the art that the node may alternatively comprise a single or more than two processor system. Illustratively one processor executes the functions of the N blade on the node while the other processor executes the functions of the D blade .

The network adapter comprises a plurality of ports adapted to couple the node to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Illustratively the computer network may be embodied as an Ethernet network or a Fibre Channel FC network. Each client may communicate with the node over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system executing on the node to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on the disks of array . The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC link topology.

Storage of information on each array is preferably implemented as one or more storage volumes that comprise a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number vbn space on the volume s . Each logical volume is generally although not necessarily associated with its own file system. The disks within a logical volume file system are typically organized as one or more groups wherein each group may be operated as a Redundant Array of Independent or Inexpensive Disks RAID . Most RAID implementations such as a RAID 4 level implementation enhance the reliability integrity of data storage through the redundant writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of parity information with respect to the striped data. An illustrative example of a RAID implementation is a RAID 4 level implementation although it should be understood that other types and levels of RAID implementations may be used in accordance with the inventive principles described herein.

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data used in some embodiments. The processors and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data stored in the memory . In some embodiments the memory may comprise a form of random access memory RAM comprising volatile memory that is generally cleared by a power cycle or other reboot operation.

The storage operating system portions of which is typically resident in memory and executed by the processing elements functionally organizes the node by inter alia invoking storage operations in support of the storage services implemented by the node. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the invention described herein. In some embodiments the storage operating system comprises a plurality of software layers including a mirroring layer engine that are executed by the processors.

The local non volatile storage device may comprise one or more storage devices utilized by the node to locally store data. The local non volatile storage device may be employed as a backup memory that ensures that the storage system does not lose received information e.g. CIFS and NFS requests in the event of a system shutdown or other unforeseen problem. In some embodiments the non volatile storage device may comprise a rewritable computer memory for storing data that does not require power to maintain data information stored in the computer memory and may be electrically erased and reprogrammed. Some examples of non volatile storage devices include disks flash memory non volatile storage device NVRAM Magnetic Random Access Memory MRAM Phase Change RAM PRAM etc. In other embodiments other non volatile storage devices are used other than those listed here.

In some embodiments the local non volatile storage device may locally store various data and metadata from software layers or applications referred to as mirroring clients executing on the node. For example a mirroring client may comprise a software layer e.g. file system layer or RAID layer of a storage operating system executing on the node. In other embodiments the mirroring client may comprise any other software layer or application that requests data and metadata to be stored to the local non volatile storage device and mirrored stored to a remote non volatile storage device on a remote node. For storing data and metadata to the local non volatile storage device a mirroring client may send the data and metadata to software layers of the storage operating system that store the data and metadata using methods known in the art. For storing data and metadata to the remote non volatile storage device each mirroring client may also send a stream of data and metadata to the mirroring layer engine sometimes referred to as an interconnect IC layer engine that mirrors stores the data and metadata to the remote node using methods described herein.

To facilitate access to the disks the storage operating system implements a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by disks . The file system logically organizes the information as a hierarchical structure of named directories and files on the disks. Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the file system to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers luns .

In the illustrative embodiment the storage operating system is preferably the Data ONTAP software operating system available from NetApp Inc. Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such where the term WAFL is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.

A file system protocol layer provides multi protocol file access and to that end includes support for the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the Hypertext Transfer Protocol HTTP protocol . A VI layer implements the VI architecture to provide direct access transport DAT capabilities such as RDMA as required by the DAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the node. The FC and iSCSI drivers provide FC specific and iSCSI specific access control to the blocks and thus manage exports of luns to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing the blocks on the node .

In addition the storage operating system includes a series of software layers organized to form a storage server D blade that provides data paths for accessing information stored on the disks of the node . To that end the storage server includes a file system module a de staging layer a storage RAID system layer and a disk driver system module . The RAID system layer manages the storage and retrieval of information to and from the volumes disks in accordance with I O operations while the disk driver system implements a disk access protocol such as e.g. the SCSI protocol.

The file system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustratively embodied as e.g. a virtual disk vdisk module not shown and a SCSI target module . The SCSI target module is generally disposed between the FC and iSCSI drivers and the file system to provide a translation layer of the virtualization system between the block lun space and the file system space where luns are represented as blocks.

The file system is illustratively a message based system that allocates storage space for itself in the disk array and controls the layout of information on the array. The file system further provides logical volume management capabilities for use in access to the information stored on the storage devices such as disks. That is in addition to providing file system semantics the file system provides functions normally associated with a volume manager. These functions include i aggregation of the disks ii aggregation of storage bandwidth of the disks and iii reliability guarantees such as mirroring and or parity RAID . The file system illustratively implements the WAFL file system hereinafter generally the write anywhere file system having an on disk format representation that is block based using e.g. 4 kilobyte kB blocks and using index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . The file system uses files to store metadata describing the layout of its file system these metadata files include among others an inode file. A file data container handle i.e. an identifier that includes an inode number is used to retrieve an inode from disk.

All inodes of the write anywhere file system may be organized into the inode file. A file system fs info block specifies the layout of information in the file system and includes an inode of a data container e.g. file that includes all other inodes of the file system. Each logical volume file system has an fsinfo block that may be stored at a fixed or variable location within e.g. a RAID group. The inode of the inode file may directly reference point to data blocks of the inode file or may reference indirect blocks of the inode file that in turn reference data blocks of the inode file. Within each data block of the inode file are embedded inodes each of which may reference indirect blocks that in turn reference data blocks of a file.

Operationally an access request read write request from the client is forwarded as a packet over the computer network and onto the node where it is received at the network adapter . A network driver of layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the write anywhere file system . Here the file system produces operations to load retrieve the requested data from disk if it is not resident in core i.e. in memory . If the information is not in memory the file system indexes into the inode file using the inode number to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system the logical vbn is mapped to a disk identifier and disk block number disk dbn and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the access request the node and storage operating system returns a reply to the client over the network .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the node may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by node in response to a request issued by client . Moreover in another alternate embodiment of the invention the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the node. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows NT or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present invention may be utilized with any suitable file system including a write in place file system.

In some embodiments the storage server is embodied as D blade of the storage operating system to service one or more volumes of array . In addition the multi protocol engine is embodied as N blade to i perform protocol termination with respect to a client issuing incoming data access request packets over the network as well as ii redirect those data access requests to any storage server of the cluster . Moreover the N blade and D blade cooperate to provide a highly scalable distributed storage system architecture of the cluster . To that end each blade includes a cluster fabric CF interface module adapted to implement intra cluster communication among the blades e.g. communication between blades of the same node or communication between blades of different nodes using CF protocol messages.

For example the protocol layers e.g. the NFS CIFS layers and the iSCSI FC layers of the N blade may function as protocol servers that translate file based and block based access requests from clients into CF protocol messages used for communication with the D blade . In some embodiments the N blade servers convert the incoming client access requests into file system primitive operations commands that are embedded within CF protocol messages by the CF interface module for transmission to the D blades of the cluster . Notably the CF interface modules cooperate to provide a single file system image across all D blades in the cluster . Thus any network port of an N blade that receives a client request can access any data container within the single file system image located on any D blade of the cluster.

In some embodiments the N blade and D blade are implemented as separately scheduled processes of storage operating system . In other embodiments the N blade and D blade may be implemented as separate software components code within a single operating system process. Communication between an N blade and D blade in the same node is thus illustratively effected through the use of CF messages passing between the blades. In the case of remote communication between an N blade and D blade of different nodes such CF message passing occurs over the cluster switching fabric .

A known message passing mechanism provided by the storage operating system to transfer information between blades processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF application programming interface API . Examples of such an agnostic protocol are the SpinFS and SpinNP protocols available from NetApp Inc.

The CF interface module implements the CF protocol for communicating file system commands messages among the blades of cluster . Communication is illustratively effected by the D blade exposing the CF API to which an N blade or another D blade issues calls. To that end the CF interface module is organized as a CF encoder and CF decoder. The CF encoder of e.g. CF interface on N blade encapsulates a CF message as i a local procedure call LPC when communicating a file system command to a D blade residing on the same node or ii a remote procedure call RPC when communicating the command to a D blade residing on a remote node of the cluster . In either case the CF decoder of CF interface on D blade de encapsulates the CF message and processes the file system command. As used herein the term CF message may be used generally to refer to LPC and RPC communication between blades of the cluster.

In some embodiments the storage operating system also comprises a de staging layer that operates in conjunction with the file system and other software layers of the storage operating system to produce and store write logs to the local non volatile storage device . In general the de staging layer may receive write requests for files and perform the received write requests in two stages. In a first stage write requests received by the file system layer are sent to the de staging layer . The de staging layer produces a write log for each received write request a write log representing the write request. The write logs may be stored to the local non volatile storage device . In a second stage upon occurrence of a predetermined initiating event referred to as a consistency point accumulated local write logs stored in the local volatile storage device may be performed on the storage devices. To do so the accumulated local write logs may be sent to the RAID system layer that then performs the write logs. The consistency point may be initiated by various predetermined initiating events such as the occurrence of a predetermined time interval etc.

In some embodiments the storage operating system also comprises a mirroring layer that may reside underneath the storage RAID system layer and be a peer layer of the disk driver system layer as shown in . In other embodiments the mirroring layer may reside near other layers of the storage operating system . In some embodiments the mirroring layer may be pre included in storage operating system software. In other embodiments the mirroring layer may comprise an external auxiliary plug in type software module that works with the storage operating system to enhance its functions.

To ensure data consistency and provide failover protection the write logs may be stored to the local non volatile storage device as described above and also be mirrored stored to a remote non volatile storage device at a remote partner node. The file system and or the de staging layer may comprise mirroring clients that utilize and request data mirroring services of the mirroring layer . In some embodiments described below the file system may comprise a mirroring client that produces the write logs the write logs comprising the data and metadata that are to be mirrored stored to the remote non volatile storage device for illustrative purposes only. In other embodiments other mirroring clients may use the mirroring layer and or produce other types of data and metadata other than write logs that are to be mirrored stored to the remote non volatile storage device by the mirroring layer .

As used herein a mirroring client executing on a local node e.g. local node A may produce local data and metadata stored to the local non volatile storage device . The mirroring client may also send its local data and metadata to the mirroring layer engine for storing to a remote node e.g. remote node B using methods described herein. Likewise a mirroring client on the remote node may send its data and metadata shown as remote data and metadata to its mirroring layer engine for storing to the local non volatile storage device of the local node using methods described herein.

Currently remote mirroring of data and metadata may produce a significant amount of metadata overheard for each mirroring client whereby each metadata set produced by the mirroring client is sent from the local node to the remote node for storage. The metadata overhead caused by the mirroring clients contributes to network congestion between the nodes and consume data bandwidth of the network connection.

Note that in both the local and remote non volatile storage devices for each mirroring client there is typically assigned a predetermined storage address location for storing metadata sets of the mirroring client. The metadata sets of a mirroring client are typically stored to the same predetermined storage address location rather than to new locations in the local and remote non volatile storage devices . As such any new metadata set received from a particular mirroring client typically overwrites updates the prior received metadata set at the same predetermined storage address location for the particular mirroring client. This is shown in where the new metadata set Count 5 has overwritten the previous metadata set Count 3 in the same storage location in the local and remote non volatile storage devices .

The mirroring client may send the data sets and metadata sets to a conventional mirroring layer executing on the local node A for mirroring storing to the remote non volatile storage device on the remote node B. The data and metadata sets may be transmitted from the local node A to the remote node B through a connection system . The mirroring layer may typically transmit all of the received data sets and metadata sets to the remote node B which may cause connection congestion between the nodes.

In some embodiments the connection system may provide a single connection path between the local and remote nodes which results in in order delivery IOD of data and metadata between the local and remote nodes. For IOD the data and metadata is expected to be received at the remote node in the same time order as it was sent at the local node. For example if data sets are sent at the local node in a time order comprising data sets W X and then Y the remote node receives the data sets in the same time order i.e. receive in order W X and then Y .

In other embodiments the connection system may comprise the cluster switching fabric that provides multiple connection paths between the local and remote nodes in the cluster . The cluster switching fabric may utilize any type of network connection switches and or protocols known in the art. For example the cluster switching fabric may comprise a Fibre Channel interconnect using Fibre Channel fabric switches an Ethernet interconnect using Gigabit Ethernet switches and Ethernet clustering protocol etc. The multiple connection paths may be provided through multiple hardware connections between the nodes. Multiple connection paths may be implemented to increase data throughput and bandwidth between the nodes. Use of multiple connection paths between the nodes may result in out of order delivery OOD of data and metadata. For OOD the data and metadata is not expected to be received at the remote node in the same time order as it was sent at the local node and may arrive in any order. As such in the above example data set Y may arrive at the remote node prior to data sets W and X in OOD.

Some embodiments below may describe a single or multiple connection path and or IOD or OOD of data and metadata between the nodes for illustrative purposes. However methods described herein may be used to reduce the number of metadata sets transmitted between the nodes regardless of the type of connection system or the delivery type IOD or OOD used between the nodes.

In the example of for illustrative purposes the connection system may provide a single connection path between the nodes resulting in IOD of data and metadata between the local and remote nodes. In the example of the data and metadata sets are transmitted from the local node A in the following time order Data is sent first Data is sent second Data is sent third and Count 3 is sent fourth. As such the data and metadata sets are received and stored to the remote storage at the remote node B in the same following time order Data is received and stored first Data is received and stored second Data is received and stored third and Count 3 is received and stored fourth.

Similarly in the example of the data and metadata sets are transmitted from the local node A in the following time order Data Data and then Count 5 then the data and metadata sets are received and stored at the remote node B in the same time order Data Data and then Count 5. Note that the new metadata set Count 5 overwrites updates the prior received metadata set Count 3 at the same predetermined storage address location for the particular mirroring client in the local and remote non volatile storage devices .

For example the mirroring client may comprise the file system that writes data and metadata to the local non volatile storage device and also sends the data and metadata to the mirroring layer for mirroring storing to the remote non volatile storage device . In this example file system may periodically produce metadata sets to assist in tracking the data sets it produces. For example the file system may produce metadata Count that may specify the number of valid client data sets that have been currently produced so far by the file system for storage to the local and remote non volatile storage devices . As such in the above example the metadata set Count 3 specifies that 3 valid client data sets Data Data and Data have been produced so far for storage to the local and remote non volatile storage devices . When 2 more valid client data sets Data and Data are later produced by the file system the file system may then produce an updated metadata set Count 5 that specifies that a total of 5 valid client data sets Data Data have been produced so far.

As such the metadata sets produced by a mirroring client e.g. file system may specify the total accumulated number of data sets currently produced by the mirroring client and sent to the mirroring layer for storing to the remote node. Because of the accumulative nature of the metadata sets the data contained in a new metadata set may encompass or encapsulate data contained in a previous metadata set. For example the new metadata set Count 5 may be viewed as encompassing or encapsulating data contained in the previous metadata set Count 3 . In certain circumstances only sending and storing the new metadata set Count 5 to the remote node may produce the same end result as sending the previous metadata set Count 3 and then the new metadata set Count 5 . This is because both the metadata sets are overwritten to the same predetermined storage location in the remote non volatile storage device and the new metadata set encompasses the previous metadata set.

In some embodiments the mirroring layer coalesces two or more metadata sets requests to produce a metadata chain comprising a grouping two or more metadata sets requests. In some instances a metadata set of a metadata chain may be selected for sending to the remote node for storing wherein the other metadata sets of the metadata chain are not sent to the remote node. In these embodiments the selected metadata set may represent all the metadata sets in the chain and be the only metadata set in the chain that is transmitted and stored to the remote node. For example in some circumstances the metadata set Count 3 and the metadata set Count 5 may be coalesced to produce a metadata chain and only the metadata set Count 5 may be transmitted to the remote node for storage wherein the metadata set Count 3 is not transmitted to the remote node. Typically however each metadata set received by the mirroring layer is sent to the remote node as shown in the examples of .

In some examples described herein the mirroring client comprises the file system and the data and metadata pertains to write logs. In other embodiments the mirroring client may comprise another software layer or application and the data and metadata may not pertain to write logs.

Each mirroring client may produce and send its own separate stream of data and metadata to the mirroring layer for processing the stream comprising related groups of data and metadata. As such related groups of data and metadata will be produced and received from the same mirroring client. The storage size for the data and metadata sets typically vary depending on how the mirroring client produces the data and metadata sets although there is typically a maximum storage size to a single data or metadata set e.g. 64 KB . The ratio of data sets to a metadata set produced may vary depending on the mirroring client. For illustrative purposes some embodiments described below describe a mirroring client that produces and sends one metadata set for every two data sets whereby each related group comprises two data sets and one metadata set. In other embodiments however other ratios of data sets to a metadata set may be used.

Described herein are a system and method for remote mirroring copying data and metadata sets from a local node to a remote node that reduces the number of metadata sets that are mirrored. In some embodiments the local node may coalesce metadata sets into metadata chains each metadata chain comprising two or more metadata sets. In some instances a metadata set of a metadata chain may be selected for sending to the remote node for storing wherein the other metadata sets of the metadata chain are not sent to the remote node. In these embodiments the selected metadata set may represent all the metadata sets in the chain and be the only metadata set in the chain that is transmitted and stored to the remote node. As such the network congestion between the local and remote nodes may be reduced.

In some embodiments a mirroring layer of a storage operating system executing on the local node may be configured to perform embodiments described herein. For each mirroring client sending data and metadata sets to the mirroring layer the mirroring layer may produce a data and metadata request DMR data structure and a metadata request MR data structure. The mirroring layer may treat each received data and metadata set as a request having an assigned unique request identifier XID from the mirroring client to mirror store the data or metadata set to the remote node. The mirroring layer may queue store each received data and metadata set request to the DMR data structure for the mirroring client. The mirroring layer may also queue store each received metadata set request to the MR data structure for the mirroring client.

The mirroring layer may queue store received requests to the DMR and MR data structures based on the time order the requests are received. For example earlier received requests may be stored towards the top of the DMR and MR data structures and later received requests may be stored towards the bottom of the DMR and MR data structures. As such the request ordering from the top to the bottom of the DMR and MR data structures comprises the earliest received request to the latest received request. As data and metadata requests are completed i.e. successfully stored to the remote node the mirroring layer may remove completed requests from the DMR and MR data structures.

In some embodiments the mirroring layer may use the MR data structure to manage metadata chains e.g. produce break and or delete metadata chains . As used herein each metadata chain may comprise a head metadata request a tail metadata request and zero or more middle metadata requests. The head metadata request may comprise a metadata request that is received earlier than the tail metadata request and is stored above the tail metadata request in the DMR and MR data structures. In some embodiments a metadata chain comprises two or more adjacent consecutive metadata requests stored in the MR data structure.

In some embodiments the mirroring layer may coalesce a newly received metadata request with a previously received metadata request in the MR data structure to produce a chain if certain conditions exist. For example the metadata requests may be coalesced if both metadata requests are to be stored to the same storage address location at the remote non volatile storage device at the remote node both metadata requests have the same data size or the previous metadata request has not yet been transmitted to the remote node. In other embodiments the mirroring layer may require all three conditions or any combination of the three conditions for coalescing the two metadata requests to produce a chain.

In some embodiments a metadata chain is specified by two pointers to two metadata requests stored in the MR data structure comprising the first top and last bottom metadata requests of the chain. In these embodiments two chain pointers may be used to specify each metadata chain a chain head pointer comprising an address pointer to the head metadata request in the MR data structure and a chain tail pointer comprising an address pointer to the tail metadata request in the MR data structure.

As used herein a representative metadata request comprises a metadata request in a metadata chain that is the only metadata request in the chain that is sent to the remote node. A represented metadata request may comprise the other metadata requests in the metadata chain that are not sent to the remote node. In some embodiments the mirroring layer may send to the remote node only the representative metadata request in a metadata chain and not send represented metadata requests to the remote node only in certain circumstances.

In some embodiments only the representative metadata request in a chain is sent when the metadata requests in the chain comprise the top most requests currently stored in the DMR data structure i.e. the metadata requests in the chain comprise the earliest received requests currently stored in the DMR data structure with no intervening requests between the metadata requests in the chain. For example if the chain comprises two metadata requests only the representative metadata request in the chain is sent if the two metadata requests in the chain comprise the top two requests in the DMR data structure. For a chain comprising three metadata requests only the representative metadata request in the chain is sent if the three metadata requests in the chain comprise the top three requests in the DMR data structure and so forth.

In further embodiments the representative metadata request comprises the tail metadata request in the chain. As such the representative metadata request is the last received metadata request in the chain. Thus the representative metadata request may encompass or encapsulate data contained in the previous received metadata requests in the chain. For example a chain may comprise metadata set request Count 3 and later received metadata set request Count 5 . The metadata request Count 5 may comprise the representative metadata request that encompasses the previous received metadata request Count 3 .

Typically each mirroring client will continually query the mirroring layer to determine if its data and metadata sets requests sent to the mirroring layer have been completed. The mirroring layer may also produce a request field last complt request for indicating the request identifiers XIDs of sets requests that have been currently completed thus far.

In some embodiments the mirroring layer may perform remote mirroring without use of a processor executing on the remote node. In these embodiments the mirroring layer may perform the remote mirroring using remote direct memory access RDMA methods without requiring use or involvement of a processor of the remote node. As known in the art RDMA comprises a communications protocol that provides transmission of data from the memory e.g. local non volatile storage device of one computer e.g. local node A to the memory e.g. remote non volatile storage device of another computer e.g. remote node B without involving the processor of the other computer.

Some steps of the methods and are described and conceptually illustrated in relation to FIGS. and A I. In some embodiments some of the steps of the methods and are implemented by software and or hardware. In some embodiments some of the steps of methods and are performed by the mirroring layer engine residing and executing on the operating system of a local node . The mirroring layer engine may perform the methods and in conjunction with other software layers of the operating system. In some embodiments the remote node comprises a predetermined failover partner node of the local node. The order and number of steps of the methods and are for illustrative purposes only and in other embodiments a different order and or number of steps are used.

The mirroring layer engine may concurrently perform the methods and in parallel for each mirroring client that sends data and metadata sets to the mirroring layer engine for mirroring storing the data and metadata sets to the remote node. The mirroring client may comprise for example a software layer of the storage operating system or any application executing on the local node.

The request field may be used to indicate currently completed requests of the mirroring client at the remote node and be used to respond to completion queries from the mirroring client. Typically each mirroring client will continually submit completion queries to the mirroring layer to determine if its data and metadata sets requests sent to the mirroring layer have been completed i.e. stored to the remote node . The mirroring layer may produce the request field for indicating the request identifiers of sets requests that have been currently completed thus far. Each completion query may contain a request identifier for a prior submitted set request and the mirroring layer may respond to the completion query based on the request identifier value stored in the request field.

The method receives at a current data or metadata set request from the mirroring client and generates and assigns a unique request identifier XID for the received set request. In some embodiments the request identifiers may comprise increasing sequential numbers e.g. 1 2 3 . . . that are assigned to the received data and metadata sets requests in the time order they are received. In some embodiments a request having a lower request identifier XID is received before another request having a higher request identifier XID . As such the request identifiers may indicate the time ordering of when requests were received by the method relative to each other. The method also stores at each received data or metadata set request along with the assigned request identifier XID to the DMR data structure . Each set request and assigned request identifier may comprise an entry in the DMR data structure .

The method then determines at whether the received set request is a metadata set request. If not the received set request is a data set request and the method then transmits at the data set request to the remote node B for storage to the remote non volatile storage device . Note that transmitting a data set request in the DMR data structure does not remove delete the data set request from the DMR data structure . The method then continues at step where a next set request is received.

If the current received set request is a metadata set request at Yes the method stores at the received metadata set request along with the assigned request identifier XID to the MR data structure . Each metadata set request and assigned request identifier may comprise an entry in the MR data structure .

In some embodiments the method may store the requests to the DMR and MR data structures and based on the time order they are received where an earlier received set request is stored to a higher entry in the data structure than a later received set request. For example the method may fill the DMR data structure beginning from a first top entry to a last bottom entry and likewise for the MR data structure . In some embodiments a higher entry in the DMR data structure may comprise a request received before another request in a lower entry in the DMR data structure and likewise for the MR data structure . As such the entry positions of the data and metadata requests in the DMR data structure may indicate the time ordering of when the requests were received by the method relative to each other and likewise for the MR data structure .

The method then determines at whether the currently received metadata set request is to be coalesced with a previous received metadata set request to produce a metadata chain comprising the two metadata sets requests. The method may do so by examining the last bottom metadata set request stored in the MR data structure that comprises the just previous received metadata request. In some embodiments the method determines at Yes that the currently received metadata set request is to be coalesced with the previous metadata set request if the following conditions exist. First both the current and previous metadata requests are to be stored to the same storage address location at the remote non volatile storage device at the remote node. Note that metadata sets requests for the same mirroring client may be stored to the same storage address location at the remote node. Second both the current and previous metadata requests have the same data size. Third the previous metadata request has not yet been transmitted to the remote node. In other embodiments the method may require all three conditions to exist or any combination of the three conditions to exist to determine that the current and previous metadata requests are to be coalesced.

If the method determines at No that the current and previous metadata requests are not to be coalesced the method returns to step . If the method determines at Yes that the metadata requests are to be coalesced the method determines at whether the previous metadata request is already part of a metadata chain. For example the method may do so by examining the chain pointer fields that indicate metadata requests in the MR data structure comprising a metadata chain. If so the method adds at the current metadata request to the metadata chain of the previous metadata request. The method may do so by updating the chain tail pointer in the pointer fields that stores the address location of the previous metadata request to now store the address location of the current metadata request in the MR data structure. The current metadata request may now comprise the new tail metadata request of the chain. The method then continues at step where a next set request is received.

If not the method produces at a new metadata chain comprising the current and previous metadata requests. The method may do so by producing new chain pointers in the pointer fields including a new chain head pointer that stores the address location of the previous metadata request and a new chain tail pointer that stores the address location of the current metadata request. The previous metadata request may now comprise the head metadata request and the current metadata request may now comprise the tail metadata request of the new chain. The method then continues at step where a next set request is received.

The method then removes deletes at the completed data or metadata set request from the DMR data structure e.g. by locating and deleting the entry having the same request identifier as the completed set request . The method then determines at whether the completed request comprises a metadata request. If not the method continues at step . If so the method removes deletes at the completed metadata set request from the MR data structure e.g. by locating and deleting the entry having the same request identifier as the completed set request .

The method then determines at whether the completed metadata request is part of any metadata chain. The method may do so by examining the chain pointer fields that indicate metadata requests in the MR data structure comprising a metadata chain. If not the method continues at step . If the method determines at Yes that the completed metadata request is part of a particular metadata chain the method removes deletes at the particular metadata chain from the DMR data structure and the MR data structure by removing all metadata requests of the particular metadata chain from the DMR and MR data structures . The method then continues at step .

At step the method then updates overwrites at the request field last cmplt request based on the request identifier XID of the first top data or metadata set request in the DMR data structure . In some embodiments the request field last cmplt request is updated using the following equation last cmplt request XID of first set request 1.

The method then selects at a best send metadata request comprising the best metadata request to send to the remote node at this current point in time. The method may do so by calling the method of that selects the best send metadata request discussed below . The method of may return a best send metadata request field comprising a request identifier XID value of the selected best send metadata request.

The method then determines at whether a best send metadata request exists i.e. the value of the best send metadata request field is not null . If a best send metadata request does not exist at No i.e. the value of the best send metadata request field equals null the method ends. If a best send metadata request does exist at Yes the method then determines at whether the best send metadata request comprises the tail last metadata request of a particular metadata chain. If so the method sends at the best send metadata request to the remote node for storage.

If not the method breaks at the particular metadata chain to produce two new metadata chains in a manner so that the best send metadata request comprises a tail last metadata request of one of the new metadata chains. The method may produce the two new metadata chains by producing for each new metadata chain a chain tail pointer that points to a tail metadata request and a chain head pointer that points to a head metadata request in the MR data structure . The method then sends at the best send metadata request to the remote node for storage. As such the best send metadata request is sent to the remote node only when it is the tail last metadata request of a metadata chain. The method then ends.

The method begins by setting at a best send metadata request field to equal null and a current request field to equal the request identifier XID of the first request currently stored in the DMR data structure . The method then determines at whether the current request comprises a metadata request. If not the method returns at the current value of the best send metadata request field to the calling method of .

If the current request comprises a metadata request the method then determines at whether the current request is part of a metadata chain. If not the method determines at whether the current request has already been transmitted to the remote node for storage. If so the method continues at step . If not the method sets at the best send metadata request field to equal the request identifier XID of the current request and continues at step .

If the method determines at Yes that the current request is part of a metadata chain the method sets at the best send metadata request field to equal the request identifier XID of the current request. The method then determines at whether the current request is the tail last metadata request of the metadata chain. If so the method continues at step . If not the method sets at the value of the current request field to equal the request identifier XID of the next request stored in the DMR data structure . The method then continues at step .

In the examples of the received data sets are represented as boxes with D D D etc. and received metadata sets are represented as boxes with M M M etc. . The un shaded boxes indicate sets requests that have been transmitted to the remote node for storage. The shaded boxes indicate sets requests that have not yet been transmitted to the remote node for storage. The received data and metadata sets requests are stored in entries of the DMR data structure indicated as DMR DS and the received metadata sets requests are stored in entries of the MR data structure indicated as MR DS . The request identifier XID of each received set request is shown as a number to the left of the set request stored in the DMR data structure. The value of the request field last complt request at different points in time is also shown. The request field may be used to indicate currently completed requests of the mirroring client at the remote node and be used to respond to completion queries from the mirroring client.

In the example of the mirroring layer receives data sets requests D and D and stores to the DMR data structure DMR DS and sends the data sets requests D and D to the remote node for storage as indicated by the un shaded boxes . The request field last complt request is updated to equal the request identifier XID of the first request in the DMR data structure 1. Thus last complt request 1 1 0.

In the example of the mirroring layer receives metadata set request M and stores to the DMR data structure and to the MR data structure MR DS and does not send the metadata set request M to the remote node yet as indicated by the shaded box . The request field last complt request is still equal to 0. The sets requests D D and M may comprise a related group.

In the example of another related group comprising sets requests D D and M are received. The data sets requests D and D are stored to the DMR data structure and are sent to the remote node for storage. The metadata set request M is stored to the MR data structure and not sent to the remote node yet. The request field last complt request is still equal to 0.

In the example of the three conditions for coalescing the M and M metadata requests are found to exist 1 both M and M are to be stored to the same storage address location at the remote non volatile storage device at the remote node 2 both M and M have the same data size 3 the previous metadata request M has not yet been transmitted to the remote node. Since M is not already a part of a metadata chain a new metadata chain is produced comprising M as the head metadata request and M as the tail metadata request. As shown in a new chain head pointer Chain head is produced that stores the address location of the M metadata request and a new chain tail pointer Chain tail is produced that stores the address location of the M metadata request the two new pointers specifying the new metadata chain metadata chain .

In the example of data request D has been completed whereby a request completion acknowledgement for data request D has been received from the remote node. The completed data request D is then removed from the DMR data structure . Since data request D has not been completed yet and is still the first request in the DMR data structure the request field last complt request is still equal to 0.

In the example of another related group comprising sets requests D D and M are received. The data sets requests D and D are stored to the DMR data structure and are sent to the remote node for storage. The metadata set request M is stored to the MR data structure and not sent to the remote node yet. The request field last complt request is still equal to 0.

In the example of the three conditions for coalescing the M and M metadata requests are found to exist 1 both M and M are to be stored to the same storage address location at the remote non volatile storage device at the remote node 2 both M and M have the same data size 3 the previous metadata request M has not yet been transmitted to the remote node. Since M is already a part of a metadata chain M is added to the metadata chain of M. The mirroring layer may do so by updating the chain tail pointer to now store the address location of metadata request M in the MR data structure whereby M now comprises the tail metadata request of the metadata chain .

In the example of another related group comprising sets requests D D and M are received and processed similar to the sets requests D D and M in the example of . As such M now comprises the tail metadata request of the metadata chain .

In the example of data requests D and D have been completed whereby request completion acknowledgements for data requests D and D have been received from the remote node. The completed data requests D and D are then removed from the DMR data structure . Since metadata request M having request identifier is now the first request in the DMR data structure the request field last complt request is set to equal 3 1 2.

In the example of the mirroring layer then determines the best send metadata request by first examining the first request in the DMR data structure which is metadata request M having request identifier . Since M is part of a metadata chain the best send metadata request is set to equal M. The next request in the DMR data structure is examined which is a data request D. As such M is selected as the best send metadata request comprising the best metadata request to currently send to the remote node. However since M is not the tail metadata request of the metadata chain the metadata chain is broken to produce metadata chain and metadata chain . Metadata chain comprises M as the head metadata request and M as the tail metadata request. Note that metadata chain now comprises M as the head and tail metadata request. As such M is now a tail metadata request of metadata chain and is sent to the remote node for storage.

In the example of data requests D D and D have been completed whereby request completion acknowledgements for data requests D D and D have been received from the remote node. The completed data requests D D and D are then removed from the DMR data structure . Since metadata request M having request identifier is now the first request in the DMR data structure the request field last complt request is set to equal 6 1 5.

Also in the example of metadata request M is then completed as well whereby a request completion acknowledgement for metadata request M has been received from the remote node. The completed metadata request M is then removed from the DMR data structure and the MR data structure . The mirroring layer then determines the best send metadata request by first examining the first request in the DMR data structure which is metadata request M having request identifier . Since M is part of a metadata chain the best send metadata request is set to equal M. The next request in the DMR data structure is examined which is a metadata request M. Since M is part of a metadata chain the best send metadata request is set to equal M. The next request in the DMR data structure is examined which is a data request D. As such M is selected and returned as the best send metadata request. However since M is not the tail metadata request of the metadata chain the metadata chain is broken to produce metadata chain and metadata chain . Metadata chain comprises M as the head metadata request and M as the tail metadata request. Metadata chain comprises M as the head and tail metadata request. As such M is now a tail metadata request of metadata chain and is sent to the remote node for storage.

In the example of requests D D and M have been completed whereby request completion acknowledgements for requests D D and M have been received from the remote node. The completed data requests D and D are then removed from the DMR data structure . The completed metadata request M is removed from the DMR data structure and the MR data structure . Since metadata request M having request identifier is now the first request in the DMR data structure the request field last complt request is set to equal 12 1 11. Since completed metadata request M is part of metadata chain all metadata requests of metadata chain are removed from the DMR data structure and the MR data structure . As such metadata request M is removed from the DMR and MR data structures without ever being transmitted to the remote node thus reducing network congestion.

As shown above in for metadata chain only the representative metadata request M in the chain is sent to the remote node and the represented metadata request M is not sent to the remote node. Note that this occurred when the metadata requests M and M in the chain comprised the top requests currently stored in the DMR data structure and the representative metadata request M comprises the tail metadata request in the chain. As such the representative metadata request M is the last received metadata request in the chain and may encompass or encapsulate data contained in the represented metadata request M.

Some embodiments may be conveniently implemented using a conventional general purpose or a specialized digital computer or microprocessor programmed according to the teachings herein as will be apparent to those skilled in the computer art. Some embodiments may be implemented by a general purpose computer programmed to perform method or process steps described herein. Such programming may produce a new machine or special purpose computer for performing particular method or process steps and functions described herein pursuant to instructions from program software. Appropriate software coding may be prepared by programmers based on the teachings herein as will be apparent to those skilled in the software art. Some embodiments may also be implemented by the preparation of application specific integrated circuits or by interconnecting an appropriate network of conventional component circuits as will be readily apparent to those skilled in the art. Those of skill in the art would understand that information may be represented using any of a variety of different technologies and techniques.

Some embodiments include a computer program product comprising a computer readable medium media having instructions stored thereon in and when executed e.g. by a processor perform methods techniques or embodiments described herein the computer readable medium comprising sets of instructions for performing various steps of the methods techniques or embodiments described herein. The computer readable medium may comprise a storage medium having instructions stored thereon in which may be used to control or cause a computer to perform any of the processes of an embodiment. The storage medium may include without limitation any type of disk including floppy disks mini disks MDs optical disks DVDs CD ROMs micro drives and magneto optical disks ROMs RAMs EPROMs EEPROMs DRAMs VRAMs flash memory devices including flash cards magnetic or optical cards nanosystems including molecular memory ICs RAID devices remote data storage archive warehousing or any other type of media or device suitable for storing instructions and or data thereon in.

Stored on any one of the computer readable medium media some embodiments include software instructions for controlling both the hardware of the general purpose or specialized computer or microprocessor and for enabling the computer or microprocessor to interact with a human user and or other mechanism using the results of an embodiment. Such software may include without limitation device drivers operating systems and user applications. Ultimately such computer readable media further includes software instructions for performing embodiments described herein. Included in the programming software of the general purpose specialized computer or microprocessor are software modules for implementing some embodiments.

Those of skill would further appreciate that the various illustrative logical blocks modules circuits techniques or method steps of embodiments described herein may be implemented as electronic hardware computer software or combinations of both. To illustrate this interchangeability of hardware and software various illustrative components blocks modules circuits and steps have been described herein generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application but such implementation decisions should not be interpreted as causing a departure from the embodiments described herein.

The various illustrative logical blocks modules and circuits described in connection with the embodiments disclosed herein may be implemented or performed with a general purpose processor a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or other programmable logic device discrete gate or transistor logic discrete hardware components or any combination thereof designed to perform the functions described herein. A general purpose processor may be a microprocessor but in the alternative the processor may be any conventional processor controller microcontroller or state machine. A processor may also be implemented as a combination of computing devices e.g. a combination of a DSP and a microprocessor a plurality of microprocessors one or more microprocessors in conjunction with a DSP core or any other such configuration.

The algorithm techniques processes or methods described in connection with embodiments disclosed herein may be embodied directly in hardware in software executed by a processor or in a combination of the two. In some embodiments any software application program tool module or layer described herein may comprise an engine comprising hardware and or software configured to perform embodiments described herein. In general functions of a software application program tool module or layer described herein may be embodied directly in hardware or embodied as software executed by a processor or embodied as a combination of the two. A software application layer or module may reside in RAM memory flash memory ROM memory EPROM memory EEPROM memory registers hard disk a removable disk a CD ROM or any other form of storage medium known in the art. An exemplary storage medium is coupled to the processor such that the processor can read data from and write data to the storage medium. In the alternative the storage medium may be integral to the processor. The processor and the storage medium may reside in an ASIC. The ASIC may reside in a user device. In the alternative the processor and the storage medium may reside as discrete components in a user device.

While the embodiments described herein have been described with reference to numerous specific details one of ordinary skill in the art will recognize that the embodiments can be embodied in other specific forms without departing from the spirit of the embodiments. Thus one of ordinary skill in the art would understand that the embodiments described herein are not to be limited by the foregoing illustrative details but rather are to be defined by the appended claims.

