---

title: Techniques and systems for enhancing touch screen device accessibility through virtual containers and virtually enlarged boundaries
abstract: Techniques for increasing accessibility of touch-screen devices are disclosed. In one aspect, container regions on a touch-sensitive user interface of a touch screen device are defined. A touch event corresponding to a location on the user interface is received, and it is determined that the location corresponds to a particular container region. When another touch event is received, content is determined according to a context of the particular container region. The content is then presented. In another aspect, data specifying locations of user interface items on a user interface is received. The data is modified to enlarge an area for a particular item. A touch input event corresponding to a particular location on the user interface is received. It is determined that the location is within the enlarged area for the item, and input is provided to an application indicating that the item was selected.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08386965&OS=08386965&RS=08386965
owner: Apple Inc.
number: 08386965
owner_city: Cupertino
owner_country: US
publication_date: 20100115
---
Touch screen devices allow users to provide touch inputs to select and modify user interface items displayed on the touch screen. For example a user can touch an icon to launch an application on a touch screen device or can touch user selectable controls displayed on a touch screen device to invoke certain device functionality.

However users especially visually impaired users may have difficulty locating items of interest that are displayed on a touch screen device. For example when the display itself is relatively large and the content displayed on the device is relatively small it may be difficult for visually impaired users to locate the area of the display that they are interested in. As another example when different information is clustered together on a screen such as when an e mail application is adjacent to a calendar application a user may have difficulty locating the needed information on the screen.

Techniques and systems are disclosed that allow users to navigate within and between groups of similar items displayed on a user interface of a touch screen device. These techniques can also allow users to more easily touch desired items displayed on a user interface of a touch screen device.

In one aspect container regions corresponding to physical regions on a touch sensitive user interface of a touch screen device are defined. A first touch input at a first location on the user interface is received. The device determines that the first location corresponds to a particular container region. When a second touch input is received the device determines appropriate content in response to the second touch input. The appropriate content is determined according to a context of the particular container region and independent of the location where the other touch event was received. The content can then be presented.

In another aspect data specifying the locations of user interface items on a touch sensitive user interface of a touch screen device is received. The data is modified to enlarge an area on the touch sensitive user interface associated with a particular item displayed on the touch screen interface. Touch input at a particular location on the user interface is received. It is determined that the particular location is within the enlarged area for the particular user interface item and input is provided to an application executing on the touch screen device indicating that the particular user interface item was selected.

Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. Users can more easily navigate the screen of a touch screen device without needing to see the screen. Users can easily move between areas of the screen of a touch screen device and can easily move between individual items displayed on the screen. Users can more easily navigate screen touch screen devices with large sections of empty space on the screen. Users can reduce the amount of time they spend exploring a large touch screen space in search of a particular item displayed on the screen. Users can quickly obtain information about the meta layout of items displayed on the screen by obtaining information about containers corresponding to the layout of the items.

In some implementations device includes touch sensitive display . Touch sensitive display can implement liquid crystal display LCD technology light emitting polymer display LPD technology or some other display technology. Touch sensitive display can be sensitive to haptic and or tactile contact with a user. In some implementations touch sensitive display is also sensitive to touch inputs received in proximity to but not actually touching display . In addition device can include a touch sensitive surface e.g. a trackpad or touchpad .

In some implementations touch sensitive display can include a multi touch sensitive display. A multi touch sensitive display can for example process multiple simultaneous points of input including processing data related to the pressure degree and or position of each point of input. Such processing facilitates gestures and interactions with multiple fingers chording and other interactions. Other touch sensitive display technologies can also be used e.g. a display in which contact is made using a stylus or other pointing device.

A user can interact with device using various touch inputs for example when a user touches touch sensitive display . Gesture inputs can also be derived from multiple touch inputs e.g. where a user moves his or her finger or other input tool across touch sensitive display . An example gesture input is a swipe input where a user swipes his or her finger or other input tool across the screen of touch sensitive display . In some implementations device can detect inputs that are received in direct contact with display or that are received within a particular vertical distance of display e.g. within one or two inches of display . Users can simultaneously provide input at multiple locations on display . For example inputs simultaneously touching at two or more locations can be received.

In some implementations device can display one or more graphical user interfaces on touch sensitive display for providing the user access to various system objects and for conveying information to the user. In some implementations the graphical user interface can include one or more user interface items e.g. user interface items and . In the example shown user interface items and are graphic representations of system objects. Some examples of system objects include device functions applications windows files alerts events or other identifiable system objects. In some implementations the user interface items can be configured by a user e.g. a user may specify which user interface items are displayed and or may download additional applications or other software that provides other functionalities and corresponding user interface items.

In some implementations device can implement various device functionalities. As part of one or more of these functionalities device presents graphical user interfaces on touch sensitive display of device and also responds to input received from a user for example through touch sensitive display . For example a user can invoke various functionality by launching one or more applications on the device. A user can invoke functionality for example by touching one of the user interface items in menu bar of the device. For example touching user interface item invokes an address book application on the device for accessing stored contact information. A user can alternatively invoke particular functionality in other ways including for example using one of user selectable menus included in the user interface.

Once an application has been selected one or more windows or pages corresponding to the application can be displayed on display of device . A user can navigate through the windows or pages by touching appropriate places on display . For example window corresponds to an e mail application. The user can interact with the window using touch input much as the user would interact with the window using mouse or keyboard input. For example the user can navigate through various folders in the e mail application by touching one of user selectable controls corresponding to the folders listed in window . As another example a user can specify that he or she wishes to reply forward or delete the current e mail by touching one of user selectable controls on the display.

Touch sensitive display of device can be difficult for a user with disabilities for example a user who is visually impaired to navigate. The display provides little useful feedback to help a user navigate the presented content or to know when input to the device is needed.

In some implementations device is configured to provide accessible information about the content on the display to a user. For example device can be configured to provide information on the display e.g. aural feedback including synthesized speech and sound effects haptic feedback including vibration and device motion Braille feedback or other feedback accessible to the user to a visually impaired user. When the user touches or otherwise selects a particular location on the display device can provide feedback describing the contents of the display at the particular location. For example if a user touches the display near area device could present content corresponding to the text To Me Me.com to the user. As another example if a user touches the display near user interface item the device could present content describing the user interface item or the fact that selecting user interface item will invoke an address book application. The user can then use a particular input to indicate that he or she wants to select user interface item rather than merely being provided with information about user interface item .

However merely providing accessible feedback to a user is not always sufficient. Visually impaired users may have trouble navigating the touch screen interface. For example visually impaired users may have problems navigating a large touch screen interface that are different from problems the users have navigating smaller touch screen interfaces. These problems can include difficulty navigating a user interface with large amounts of empty space and or large numbers of content items displayed simultaneously.

As will be described in more detail below providing users with virtual container based navigation and or virtually enlarging item boundaries can make navigating a touch screen device easier for a visually impaired user. These features can be enabled for example when a user provides input requesting device switch to an accessibility mode.

Each virtual container corresponds to a region on user interface . Each region includes one or more items. For example container corresponds to a listing of folders for an e mail application. Container contains four items e.g. items and . Each item is a user selectable control that when selected by a user causes a different folder to be displayed. Container contains four items and . Each item corresponds to specific information about the e mail displayed in the e mail application. For example the item corresponds to the to information for the e mail To Me Me.com . Container contains one item corresponding to the text of the e mail. Container contains three entries and each corresponding to a user selectable control that causes the device to perform a different action with respect to the displayed e mail. Container contains four items and . Each item is a user selectable icon that when selected by a user causes a particular application to execute on device .

The virtual containers can be defined according to different criteria. In some implementations the virtual containers are defined by one or more software developers that developed the applications generating content for display in the user interface. For example the developer of the e mail application shown in can specify one or more containers appropriate to the e mail application. Alternatively or additionally the containers can be defined according to user interface elements that control how the user interface is displayed. In general a container is defined where similar items are visually grouped together and occupy a distinct region of the space on the screen of the device. For example if an application includes a table user interface element a virtual container corresponding to the table user interface element can be defined. Other example user interface elements that can be used to define containers include toolbars navigation bars groups of playback controls e.g. play pause track position etc. and frames in a web page.

The items in each container can be logically ordered. Various ordering conventions can be used. For example the items can be ordered according to their physical location on the screen. For example a Cartesian ordering can be used where items that appear more at the top left of the screen can be ordered before items that appear more to the bottom right of the screen. As another example the items in the container can be ordered according to an order assigned by the source code used to generate the user interface. For example if the source code lists the items in a particular order that order can be preserved. As another example the items in the container can be ordered according to a document object model corresponding to the content being displayed. Other conventions for defining an order can also be used. The containers can also be ordered for example according to their physical location on the screen. For example a Cartesian ordering system can be used.

Device can use the virtual containers to aid visually disabled users in several ways. For example one problem that users often have is that when things are close together on the screen of the user interface it is easy to touch one item on the screen while actually intending to touch another item. For example a user could be reading the text of the e mail in container and then run his or her finger onto reply control in container . The device can address this problem by providing feedback e.g. aural or tactile feedback that can be perceived by a visually disabled user when the user moves between virtual containers.

Device can also use the virtual containers to help users move between items in containers. Often containers are fairly large and contain a lot of empty space in addition to the items in the containers. This space can make it difficult for a visually disabled user to find a particular item of interest. Device can help a user navigate between items in a container for example by automatically moving from one item in a container to another item in the container when a particular input is received from a user. When a user moves to a particular item in the container information about that item is presented to the user for example in a format accessible to the user.

For example a user may enter one input to select the container and additional inputs to move to the first item in the container e.g. a four finger flick left or the last item in the container e.g. a four finger flick right . If a user is already on the first item in a container when he or she provides the input to move to the first item in the container the user can be presented with information about the first item in the previous container. Similarly if a user is already on the last item in a container when he or she provides the input to move to the last item in the container the user can be presented with information about the first item in the next container.

A user can also use inputs to move to the next item e.g. a single finger flick right or the previous item e.g. a single finger flick left . If there is a next or previous item in the current container the user is presented with information about that next or previous item. Otherwise the user is presented with information about the first item in the next container or the last item in the previous container .

For example a user could touch inside container use a particular input e.g. a particular gesture input to navigate to the first item in the container e.g. item and then use a different input e.g. a different gesture input to move to the next item in the container e.g. . When a user touches inside container the device can provide feedback to the user identifying the container. If a user touches a particular item within the container the device can identify both the container and the item that was touched.

A user can also use specific gesture inputs to move to the first item in the first container e.g. a four finger flick up or the last item in the last container e.g. a four finger flick down .

Device can also move between containers in response to user input. For example a user can be reading text in container about an e mail and want to move to the text of the e mail in container . The user can use a particular input e.g. a particular gesture input to indicate that he or she wants to move to the next container in an order of the containers. Similarly the user can indicate that he or she wants to move to the first last or previous container.

In some implementations device can be in a particular mode for certain container functionality to be activated. For example in one mode device can support moving between containers and moving between items in containers. In another mode device can support alerting users when they have moved outside of a container. In yet another mode device can provide no container functionality. A user can indicate a desired mode for device for example through one or more inputs to the device.

Device can use the virtual containers to help a visually disabled user navigate the user interface as follows. First the user provides touch input within container . Then the user provides touch input indicating that the first item in the container should be presented. Device navigates to container which is empty. The device presents content corresponding to container for example by presenting content indicating that the container corresponds to the date September 1 and that no calendar items are present. The user then provides touch input indicating that the next item should be presented. The device then navigates to container and presents content corresponding to container . For example the device can present content indicating that the container corresponds to the date Sep. 2 2009 and that there are two items included in the container. The user then provides touch input indicating that the next item should be presented. The device navigates to the first item in container corresponding to Meeting A and presents information about the item. The user then provides touch input indicating that the next item should be presented. The device navigates to the second item in container corresponding to Meeting B and presents information about the second item.

The system can define container regions corresponding to physical regions on a user interface . Each container region can include one or more user interface items. The system can define container regions by storing data indicating current coordinates and or size of the container regions and or the contents of the container regions. The coordinates of the container regions correspond to the physical location of the regions on a user interface of a user device. For example for the user interface illustrated in the system can maintain data indicating that there are four containers and data indicating the physical coordinates of each container on the user interface and data indicating the contents of each container e.g. container contains four items in the following order items and .

The container definitions can also include additional data about the containers for example data defining an order for the containers or a brief description of each container for example a title of each container.

The system can define container regions so that similar user interface items are displayed in each physical region corresponding to a container region. Similar user interface items are user interface items that share common properties. For example the folders illustrated in container of are each user selectable user interface items that have the same basic functionality e.g. they cause data corresponding to their respective folders to be displayed . As another example the e mail header data illustrated in container of all corresponds to header information extracted from an e mail message. As yet another example the user selectable items illustrated in container of are each a user selectable item that cause a particular action to be performed on the displayed e mail message.

The system can use various heuristics to identify the physical regions for the containers. For example the system can apply criteria such as the ones described above with reference to to identify containers and determine the boundaries of the containers. Alternatively or additionally the system can receive data from third parties for example software developers defining where containers should be relative to a particular application. For example a software developer can use a command specified in an application programming interface to specify that a particular user interface component is a container. The developer can associate a particular attribute with the user interface component or can call a method provided by the application programming interface to toggle the state of the component between container and not container. In these implementations the system can map the received container definitions to appropriate coordinates on the user interface.

The system can also maintain data indicating the current container e.g. the last container selected by a user and the current item in the container e.g. the last item for which content was presented . This data can be used by the system for example as described below.

The system can receive a first touch event corresponding to a first touch input received at a first location on the user interface . The system can receive the first touch event for example from a touch service. An example touch service is described below with reference to .

The system can determine that the first location corresponds to the physical region of a particular container region . The system can determine that the input is within the physical region for example by comparing the coordinates of the first input e.g. a location on the user interface corresponding to the input to coordinates associated with container regions.

The system can receive a second touch event corresponding to a second touch input received at a second location on the user interface . The system can receive the second touch event for example from a touch service.

The system can determine content to present to a user according to a context of the first container region e.g. in response to the second touch event. The context can include one or more of the user interface items in the container region the last user interface item in the container region for which content was presented where the container region is in an order of the container regions and the physical boundaries of the container region. The system can determine the appropriate content to present for example as follows. The system can look the second input event up in a database mapping particular input events to particular actions take the action associated with the second input and then output content that is associated with the action.

The system can process the input independently of the location where the input was received. For example the input can request a particular item or container relative to an order maintained by the system and the system can identify the requested item according to the order. For example if the requested action is to move to the a particular item e.g. the next or previous item or first or last item in the container the system can do so for example as described below with reference to . As another example if the requested action is to update the current container the system can do so for example as described below with reference to . Once the system moves to the particular item or particular container the system can present content appropriate to the item or container.

Alternatively the system can process the input based on the location where the input was received. For example the system can alert a user when the second input is outside the first container region as described below with reference to .

The system can define container regions on a user interface for example as described above with reference to .

The system can identify a first container region from a first touch input event . The system can receive the first touch input event for example from a touch service.

The system can identify the first container region in various ways. For example the system can receive a first touch input event within a particular container region and identify that region as described above with reference to . As another example the system can receive a first touch input event corresponding to a gesture that indicates that a particular region should be the new current container region as described in more detail below with reference to .

The system can receive a second touch input event . The system can receive the second touch input event from a touch service. The second touch input can correspond to a gesture requesting that a particular item be presented. The particular item can be identified by a relative location in the order of the items in the container region. For example the second touch input can be a gesture corresponding to a request that the first item in the container region be presented or that the last item in the container region be presented. As another example the second touch input can be a gesture corresponding to a request that the next or previous item be presented. Example input gestures are described above with reference to .

The system can determine that content for an item in the first container region should be presented in response to the second touch input . The system can make this determination by identifying the item requested by the input and then retrieving content associated with the item.

The system can identify the appropriate item in the first container region according to the order of items in the container region the input from the user and optionally the last item that was presented for the container and the order of the containers. For example if the input requests the first or last item in the container the system can identify the first or last item according to the order of items in the container. If the input requests the first item in the container and the last item that was presented was the first item the system can identify the first item in the previous container. If the input requests the last item in the container and the last item that was presented was the last item the system can identify the first item in the next container.

As another example if the input requests the next or previous item in the container the system can identify where the last item that it presented content about is in the order and then identify the next or previous item as appropriate. If the last item that was presented was the first item in the container and the input requests the previous item the system can present the last item in the previous container. Alternatively the system can loop and present the first item in the current container. Similarly if the last item that was presented was the last item in the container and the input requests the next item the system can present the first item in the next container or can loop and present the first item in the current container. The decision of whether to loop or move to the next or previous container can be made according to user preference or can be predetermined.

The content associated with the item can be a description of the requested item. This description can include a title and or a brief description of the item.

The system can present the content . The system can present the content in a manner accessible to the user for example haptic feedback or aural output such as synthesized speech can be used. The content can also be presented in accordance with user preferences. Example preferences can include preferences on how content is presented e.g. one or more of volume language voice quality tone and speaking speed for aural content .

The system can define container regions on a user interface for example as described above with reference to .

The system can identify a first container region as a current region . The first container region can be identified according to user input for example as described above with reference to . Alternatively the system can use stored data to identify the current region for example as described above with reference to .

The system can update the current region to a different second container region in response to user input for example by modifying data indicating the current region. The system can receive the input for example as a touch input event from a touch service. The touch input event can identify that a particular gesture was detected. The system can map the gesture to a particular location in the order for the containers that is requested by the user for example by retrieving a location associated with the gesture in a database. The system can determine the appropriate updated region according to the user input and an order for the regions. For example if the user input requests that the system move to another container e.g. the first or last container the system can identify the requested container according to the order of the containers. As another example if the user input requests that the system move to the next or last item in a container but the previous item presented by the system was the last item in the container the system can move to the next container. Similarly if the user input requests that the system move to the previous or first item in a container but the previous item presented by the system was the first item in the container the system can move to the previous container.

The system can present content corresponding to the updated current region . For example the system can present a brief description of the updated current container region. This description can include a title and or a brief description or count of the items in the updated current container. The system can alternatively or additionally present content about one or more items in the container for example the first item in the container.

In some implementations when the system updates the current container region the system can also display a larger representation of the container region on the screen. For example the system can enlarge the updated container region so that it is the only thing displayed on the device.

The system can define container regions on a user interface for example as described above with reference to .

The system can receive a first touch input event corresponding to first input within a first container region for example as described above with reference to .

The system can receive a second touch input event corresponding to second input and can determine that the second touch input is outside the first container region . The system can make this determination by comparing the coordinates of the second input to the coordinates in the definition for the current container.

The system can announce to the user that the second touch input is outside the first container region . For example the system can provide distinctive aural feedback e.g. sound effects or haptic feedback to indicate that the second input is outside the first container region.

When a user provides touch input at a particular location on user interface the device can determine whether the input is within one of the virtual enlarged boundaries for an item presented in the user interface. If so an application executing on the device can perform as if the user selected the item. For example a user could touch at the upper left edge of the virtual enlarged boundary and e mail application would perform as if the user had actually selected the Inbox user selectable control .

The system receives data specifying a location of one or more user interface items displayed on a user interface of a device . The data can be generated for example using conventional techniques for tracking where items are displayed on a user interface.

The data can include data identifying the boundaries of each item displayed on the user interface. For example the data can specify the four corners of a rectangle that makes up the boundary of the item.

The system modifies the received data to enlarge an area associated with a first user interface item . This modification expands the boundaries associated with the first user interface item in effect creating a virtual boundary for the user interface item. The modified boundary can be selected so that it does not overlap with the boundaries for any of the other user interface items.

In some implementations the system can first determine whether the boundaries for the item should be enlarged. To do this the system can consider whether the amount of empty space e.g. free from other user interface items around the item in one or more directions satisfies a threshold. The threshold can be a predetermined threshold or can be dynamic. For example the threshold can be a percentage of the size of the largest smallest or average user interface window displayed on the device at a current time. Alternatively or additionally the system can determine that the boundaries for an item should be enlarged according to certain properties for the item. Some items can be identified as needing enlargement for example at the time they are created.

The system can use various heuristics to determine how to modify the boundaries associated with the user interface item.

In some implementations the system determines an amount of empty space around the user interface item in one or more directions on the user interface. The system can then enlarge the boundary in a given direction by an amount derived from the amount of empty space in the direction. For example the system can multiply the amount of empty space by a scaling factor e.g. or can subtract a constant value from the amount of empty space. Alternatively the system can enlarge the boundary by the same amount in each direction e.g. by determining a possible enlargement amount for each direction as described above and then enlarging the boundary in each direction by the smallest determined enlargement amount.

In some implementations the system uses a predefined enlargement amount. The predefined enlargement amount can be specific to the type of user interface item. For example icons in a dock can have one enlargement amount and hyperlinks in a webpage can have a different enlargement amount. In some implementations the system can modify the predefined enlargement amount for example to prevent overlap between the enlarged boundary and a boundary for another item.

In some implementations when the item is included within a particular user interface component e.g. text inside a cell of a table the system expands the item to the size of the user interface component. For example the system could expand text to have the same height as the cell of the table and or the same width as the cell of the table.

The system receives a touch input event corresponding to touch input at a location on the user interface and determines that the touch input is within the enlarged area for the first user interface item . The system can receive the touch input event as an input event from a touch service. An example touch service is described below with reference to .

The system can determine whether the user has selected an item on the user interface as follows. First the system determines a location on the screen indicated by the input. Then the system compares the location to the boundaries indicated by the modified data.

The system provides input to an application on the device indicating that the first user interface item was selected e.g. in response to the user input.

In some implementations the system modifies the boundaries for multiple items in response to a triggering event. For example if the system receives a touch event within a container region the system can modify the boundaries for all items in the container and optionally revert the boundaries for any items not in the container region to their original values.

Software architecture can include operating system touch services module input modification layer and application s . This architecture can conceptually operate on top of a hardware layer not shown .

Operating system provides an interface to the hardware layer e.g. a capacitive touch display or device . Operating system can include one or more software drivers that communicates with the hardware. For example the drivers can receive and process touch input signals generated by a touch sensitive display or device in the hardware layer. The operating system can process raw input data received from the driver s . This processed input data can then made available to touch services layer through one or more application programming interfaces APIs . These APIs can be a set of APIs that are usually included with operating systems such as for example Linux or UNIX APIs as well as APIs specific for sending and receiving data relevant to touch input.

Touch services module can receive touch inputs from operating system layer and convert one or more of these touch inputs into touch input events according to an internal touch event model.

The touch input events can be in a format e.g. attributes that are easier to use in an application than raw touch input signals generated by the touch sensitive device. For example a touch input event can include a set of coordinates for each location at which a touch is currently occurring on a user interface.

In some implementations gesture touch input events can also be detected by combining two or more touch input events. The gesture input events can contain scale and or rotation information. The rotation information can include a rotation value that is a relative delta in degrees. The scale information can also include a scaling value that is a relative delta in pixels on the display device. Other gesture events are possible.

All or some of these touch input events can be made available to developers through a touch input event API. The touch input API can be made available to developers as a Software Development Kit SDK or as part of an application e.g. as part of a browser tool kit .

The input modification layer receives input from the touch services layer and processes the input according to virtual containers and or virtual enlargement of user interface items for example as described above with reference to . The input modification layer then provides instruction events to one or more applications. The applications can access these instruction events through an API.

The application s receive instruction events from the input modification layer and take appropriate action based on the contents of the instruction events.

Sensors devices and subsystems can be coupled to the peripherals interface to facilitate multiple functionalities. For example motion sensor light sensor and proximity sensor can be coupled to peripherals interface to facilitate various orientation lighting and proximity functions. For example in some implementations light sensor can be utilized to facilitate adjusting the brightness of touch screen . In some implementations motion sensor e.g. an accelerometer velicometer or gyroscope can be utilized to detect movement of the device. Accordingly user interface items and or media can be presented according to a detected orientation e.g. portrait or landscape.

Other sensors can also be connected to peripherals interface such as a temperature sensor a biometric sensor or other sensing device to facilitate related functionalities.

Location determination functionality can be facilitated through positioning system . Positioning system in various implementations can be a component internal to device or can be an external component coupled to device e.g. using a wired connection or a wireless connection . In some implementations positioning system can include a GPS receiver and a positioning engine operable to derive positioning information from received GPS satellite signals. In other implementations positioning system can include a compass e.g. a magnetic compass and an accelerometer as well as a positioning engine operable to derive positioning information based on dead reckoning techniques. In still further implementations positioning system can use wireless signals e.g. cellular signals IEEE 802.11 signals to determine location information associated with the device. Other positioning systems are possible.

Broadcast reception functions can be facilitated through one or more radio frequency RF receiver s . An RF receiver can receive for example AM FM broadcasts or satellite broadcasts e.g. XM or Sirius radio broadcast . An RF receiver can also be a TV tuner. In some implementations RF receiver is built into wireless communication subsystems . In other implementations RF receiver is an independent subsystem coupled to device e.g. using a wired connection or a wireless connection . RF receiver can receive simulcasts. In some implementations RF receiver can include a Radio Data System RDS processor which can process broadcast content and simulcast data e.g. RDS data . In some implementations RF receiver can be digitally tuned to receive broadcasts at various frequencies. In addition RF receiver can include a scanning function which tunes up or down and pauses at a next frequency where broadcast content is available.

Camera subsystem and optical sensor e.g. a charged coupled device CCD or a complementary metal oxide semiconductor CMOS optical sensor can be utilized to facilitate camera functions such as recording photographs and video clips.

Communication functions can be facilitated through one or more communication subsystems . Communication subsystem s can include one or more wireless communication subsystems and one or more wired communication subsystems. Wireless communication subsystems can include radio frequency receivers and transmitters and or optical e.g. infrared receivers and transmitters. Wired communication system can include a port device e.g. a Universal Serial Bus USB port or some other wired port connection that can be used to establish a wired connection to other computing devices such as other communication devices network access devices a personal computer a printer a display screen or other processing devices capable of receiving and or transmitting data. The specific design and implementation of communication subsystem can depend on the communication network s or medium s over which device is intended to operate. For example device may include wireless communication subsystems designed to operate over a global system for mobile communications GSM network a GPRS network an enhanced data GSM environment EDGE network 802.x communication networks e.g. Wi Fi WiMax or 3G networks code division multiple access CDMA networks and a Bluetooth network. Communication subsystems may include hosting protocols such that device may be configured as a base station for other wireless devices. As another example the communication subsystems can allow the device to synchronize with a host device using one or more protocols such as for example the TCP IP protocol HTTP protocol UDP protocol and any other known protocol.

Audio subsystem can be coupled to speaker and one or more microphones . One or more microphones can be used for example to facilitate voice enabled functions such as voice recognition voice replication digital recording and telephony functions.

I O subsystem can include touch screen controller and or other input controller s . Touch screen controller can be coupled to touch screen . Touch screen and touch screen controller can for example detect contact and movement or break thereof using any of a number of touch sensitivity technologies including but not limited to capacitive resistive infrared and surface acoustic wave technologies as well as other proximity sensor arrays or other elements for determining one or more points of contact with the touch screen or proximity to touch screen .

Other input controller s can be coupled to other input control devices such as one or more buttons rocker switches thumb wheel infrared port USB port and or a pointer device such as a stylus. The one or more buttons not shown can include an up down button for volume control of speaker and or microphone .

In one implementation a pressing of the button for a first duration may disengage a lock of touch screen and a pressing of the button for a second duration that is longer than the first duration may turn power to device on or off. The user may be able to customize a functionality of one or more of the buttons. Touch screen can for example also be used to implement virtual or soft buttons and or a keyboard.

In some implementations device can present recorded audio and or video files such as MP3 AAC and MPEG files. In some implementations device can include the functionality of an MP3 player such as an iPhone .

Memory interface can be coupled to memory . Memory can include high speed random access memory and or non volatile memory such as one or more magnetic disk storage devices one or more optical storage devices and or flash memory e.g. NAND NOR . Memory can store an operating system such as Darwin RTXC LINUX UNIX OS X WINDOWS or an embedded operating system such as VxWorks. Operating system may include instructions for handling basic system services and for performing hardware dependent tasks. In some implementations the operating system can be a kernel e.g. UNIX kernel .

Memory may also store communication instructions to facilitate communicating with one or more additional devices one or more computers and or one or more servers. Communication instructions can also be used to select an operational mode or communication medium for use by the device based on a geographic location obtained by the GPS Navigation instructions of the device. Memory may include graphical user interface instructions to facilitate graphic user interface processing sensor processing instructions to facilitate sensor related processing and functions e.g. the touch services module described above with reference to phone instructions to facilitate phone related processes and functions electronic messaging instructions to facilitate electronic messaging related processes and functions web browsing instructions to facilitate web browsing related processes and functions media processing instructions to facilitate media processing related processes and functions GPS Navigation instructions to facilitate GPS and navigation related processes and instructions e.g. mapping a target location camera instructions to facilitate camera related processes and functions and or other software instructions to facilitate other processes and functions such as the accessibility features and processes described with reference to . Memory may also store other software instructions not shown such as web video instructions to facilitate web video related processes and functions and or web shopping instructions to facilitate web shopping related processes and functions. In some implementations media processing instructions are divided into audio processing instructions and video processing instructions to facilitate audio processing related processes and functions and video processing related processes and functions respectively.

Each of the above identified instructions and applications can correspond to a set of instructions for performing one or more functions described above. These instructions need not be implemented as separate software programs procedures or modules. Memory can include additional instructions or fewer instructions. Furthermore various functions of device may be implemented in hardware and or in software including in one or more signal processing and or application specific integrated circuits.

Devices and can also establish communications by other means. For example wireless device can communicate with other wireless devices e.g. other devices or cell phones etc. over the wireless network . Likewise devices and can establish peer to peer communications e.g. a personal area network by use of one or more communication subsystems such as a Bluetooth communication device. Other communication protocols and topologies can also be implemented.

Devices or can for example communicate with one or more services over the one or more wired and or wireless networks . These services can include for example container services and virtual item enlargement services . Container services can manage the definition and use of virtual containers for example as described above with reference to . Virtual item enlargement services can manage the virtual enlargement of user interface items for example as described above with reference to .

Device or can also access other data and content over the one or more wired and or wireless networks . For example content publishers such as news sites RSS feeds web sites blogs social networking sites developer networks etc. can be accessed by device or . Such access can be provided by invocation of a web browsing function or application e.g. a browser in response to a user touching for example a Web object.

The features described can be implemented in digital electronic circuitry or in computer hardware firmware software or in combinations of them. The features can be implemented in a computer program product tangibly embodied in an information carrier e.g. in a machine readable storage device for execution by a programmable processor and method steps can be performed by a programmable processor executing a program of instructions to perform functions of the described implementations by operating on input data and generating output. Alternatively or in addition the program instructions can be encoded on a propagated signal that is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to a suitable receiver apparatus for execution by a programmable processor.

The described features can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from and to transmit data and instructions to a data storage system at least one input device and at least one output device. A computer program is a set of instructions that can be used directly or indirectly in a computer to perform a certain activity or bring about a certain result. A computer program can be written in any form of programming language e.g. Objective C Java including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment.

Suitable processors for the execution of a program of instructions include by way of example both general and special purpose microprocessors and the sole processor or one of multiple processors or cores of any kind of computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Generally a computer will also include or be operatively coupled to communicate with one or more mass storage devices for storing data files such devices include magnetic disks such as internal hard disks and removable disks magneto optical disks and optical disks. Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non volatile memory including by way of example semiconductor memory devices such as EPROM EEPROM and flash memory devices magnetic disks such as internal hard disks and removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in ASICs application specific integrated circuits .

To provide for interaction with a user the features can be implemented on a computer having a display device such as a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer.

The features can be implemented in a computer system that includes a back end component such as a data server or that includes a middleware component such as an application server or an Internet server or that includes a front end component such as a client computer having a graphical user interface or an Internet browser or any combination of them. The components of the system can be connected by any form or medium of digital data communication such as a communication network. Examples of communication networks include e.g. a LAN a WAN and the computers and networks forming the Internet.

The computer system can include clients and servers. A client and server are generally remote from each other and typically interact through a network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

One or more features or steps of the disclosed embodiments can be implemented using an Application Programming Interface API . An API can define on or more parameters that are passed between a calling application and other software code e.g. an operating system library routine function that provides a service that provides data or that performs an operation or a computation.

The API can be implemented as one or more calls in program code that send or receive one or more parameters through a parameter list or other structure based on a call convention defined in an API specification document. A parameter can be a constant a key a data structure an object an object class a variable a data type a pointer an array a list or another call. API calls and parameters can be implemented in any programming language. The programming language can define the vocabulary and calling convention that a programmer will employ to access functions supporting the API.

In some implementations an API call can report to an application the capabilities of a device running the application such as input capability output capability processing capability power capability communications capability etc.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be made. For example elements of one or more implementations may be combined deleted modified or supplemented to form further implementations. As yet another example the logic flows depicted in the figures do not require the particular order shown or sequential order to achieve desirable results. In addition other steps may be provided or steps may be eliminated from the described flows and other components may be added to or removed from the described systems. Accordingly other implementations are within the scope of the following claims.

