---

title: Segmented fingerprint datastore and scaling a fingerprint datastore in de-duplication environments
abstract: A storage server is coupled to a storage device that stores data blocks, and generates a fingerprint for each data block stored on the storage device. The storage server creates a master datastore and a plurality of datastore segments. The master datastore comprises an entry for each data block that is written to the storage device and a datastore segment comprises an entry for a new data block or a modified data block that is subsequently written to the storage device. The storage server merges the entries in the datastore segments with the entries in the master datastore in memory to free duplicate data blocks in the storage device. The storage server overwrites the master datastore with the entries in the plurality of datastore segments and the entries in the master datastore to create an updated master datastore in response to detecting that the number of datastore segments meets a threshold.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08904120&OS=08904120&RS=08904120
owner: NetApp Inc.
number: 08904120
owner_city: Sunnyvale
owner_country: US
publication_date: 20101215
---
This application is related to co filed U.S. patent application Ser. No. 12 969 527 entitled FINGERPRINTS DATASTORE AND STALE FINGERPRINT REMOVAL IN DE DUPLICATION ENVIRONMENTS which is assigned to the same assignee as the present invention.

At least one embodiment of the present invention pertains to data storage systems and more particularly to a technique for identifying and eliminating duplicate data blocks for sharing data blocks managed by a file system and for identifying and removing stale entries.

In a data storage system it is desirable to use storage space as efficiently as possible to avoid wasting storage space. One type of system in which this concern can be particularly important is a storage server such as a file server. File servers and other types of storage servers often are used to maintain extremely large quantities of data. In such systems efficiency of storage space utilization is critical.

Data containers e.g. files maintained by a file system generally are made up of individual blocks of data. A common block size is four kilobytes. In a large file system it is common to find duplicate occurrences of individual blocks of data. Duplication of data blocks may occur when for example two or more files have some data in common or where a given set of data occurs at multiple places within a given file. Duplication of data blocks results in inefficient use of storage space.

A de duplication process eliminates redundant data within a file system. A de duplication process can occur in line and offline. When a de duplication process occurs while data is being written to a file system the process can be referred to as in line de duplication. When a de duplication process occurs after data is written to a storage device e.g. disk the process can be referred to as offline de duplication. A de duplication process can further be described for example to include two operations such as a de duplication operation identify and eliminating duplicate data blocks and a verify operation identify and removing stale entries from a fingerprints datastore . The de duplication process keeps a fingerprint value for every block within a file system in a fingerprints datastore FPDS . This fingerprints datastore is used to find redundant blocks of data within the file system during a de duplication operation. For example typically the fingerprint datastore is sorted on the basis of fingerprints to efficiently find potential duplicates. However maintaining one entry for each block in a file system increases the size of the fingerprints datastore drastically. An increased fingerprints datastore size consumes more time during a de duplication operation and verify operation.

De duplication involves the fingerprints datastore having some fingerprint entries that are stale. A stale fingerprint entry is an entry that has a fingerprint that corresponds to a data block that has been deleted freed or overwritten for example during a de duplication operation. The stale entries do not contribute to any space savings and add significant overhead in subsequent operations on the fingerprints datastore. These stale entries can be removed for example using a verify operation. Current implementations of a verify operation include two stages. In stage one the fingerprints datastore is first sorted in order by to check whether a fingerprint entry is stale or not for each entry. The fingerprints datastore is then overwritten with only the stale free entries. In stage two the output from stage one is sorted back to its original order e.g. fingerprint Mode file block number . Several problems with this conventional approach include sorting the fingerprints datastore twice with each verify operation and the second sort is unnecessary to remove the stale entries. Moreover the conventional approach overwrites the entire FPDS with stale free entries even if the number of stale entries is a small percentage of the FPDS. In addition a verify operation is typically a blocking operation and thus if a verify operation is executing on the FPDS then no other de duplication sharing operation can execute because de duplication operations and verify operations should work from a consistent copy of the FPDS.

De duplication includes logging fingerprints of any new data block that is written or updated in the file system into a changelog file. The changelog file is merged with fingerprints datastore to find duplicate blocks and to eliminate the duplicate data blocks. During this process the fingerprints datastore is overwritten with the merged data with every de duplication operation. Overwriting the entire fingerprints datastore with every de duplication operation however can involve a large amount of write cost.

In addition current de duplication operations use logical information to identify blocks in a volume and their associated fingerprints. De duplication maintains a fingerprint entry in the fingerprints datastore for each . That means if a block is shared n times the fingerprints datastore will have n entries for a single fingerprint value. In cases however where there is a significant amount of logical data a fingerprints datastore cannot scale proportionately.

One aspect of a de duplication operation generates a fingerprint for each data block that is stored on a storage device in storage. The de duplication operation creates a master datastore and a plurality of datastore segments. The master datastore comprises an entry for each data block that is written to the storage device and a datastore segment comprises an entry for a new data block or a modified data block that is subsequently written to the storage device. The storage server merges the entries in the datastore segments with the entries in the master datastore in memory to free duplicate data blocks in the storage device. The storage server overwrites the master datastore with the entries in the plurality of datastore segments and the entries in the master datastore to create an updated master datastore in response to detecting that the number of datastore segments meets a threshold. Another aspect of the de duplication operation addresses a block in a volume uniquely by maintaining a single entry for each fingerprint in the fingerprints datastore using a VVBN Virtual Volume Block Number thus allowing the fingerprints datastore to scale easily.

During a verify operation stale fingerprint entries are identified in the fingerprints datastore. A stale fingerprint entry is an entry that has a fingerprint that corresponds to a data block that has been deleted freed or overwritten for example during a de duplication operation. One aspect of the verify operation detects a request to execute a verify operation and overwrites the master datastore with the entries in the plurality of datastore segments and the entries in the master datastore to create an updated master datastore in response to detecting a request to execute a verify operation. The verify operation identifies and removes stale entries in the updated master datastore.

The present invention is described in conjunction with systems clients servers methods and computer readable media of varying scope. In addition to the aspects of the present invention described in this summary further aspects of the invention will become apparent by reference to the drawings and by reading the detailed description that follows.

In the following detailed description of embodiments of the invention reference is made to the accompanying drawings in which like references indicate similar elements and in which is shown by way of illustration specific embodiments in which the invention may be practiced. These embodiments are described in sufficient detail to enable those skilled in the art to practice the invention and it is to be understood that other embodiments may be utilized and that logical mechanical electrical functional and other changes may be made without departing from the scope of the present invention. The following detailed description is therefore not to be taken in a limiting sense and the scope of the present invention is defined only by the appended claims.

Data containers e.g. files maintained by a file system generally are made up of individual blocks of data stored on storage devices. Duplication of data blocks may occur when for example two or more files have some data in common or where a given set of data occurs at multiple places within a given file. Duplication of data blocks results in inefficient use of storage space. De duplication eliminates redundant data within a file system. As described herein de duplication includes two operations a de duplication operation to identify and eliminate duplicate data blocks and a verify operation to identify and remove stale entries e.g. stale records from a fingerprints datastore. Embodiments of a de duplication operation and a verify operation are described in greater detail below in conjunction with .

One aspect of the de duplication operation divides a fingerprints datastore into multiple parts such as a primary fingerprints datastore and a secondary fingerprints datastore. De duplication operations can use this primary fingerprints datastore which will be smaller in size compared to a secondary fingerprints datastore to identify duplicate blocks of data to reduce the overall time taken to find potential duplicate blocks. Another aspect of the de duplication operation organizes the fingerprints datastore as master datastore and stores the entries in the changelogs as datastore segments to avoid overwriting the master datastore during every de duplication operation. Another aspect of a de duplication operation references a block in a volume uniquely by maintaining a single entry for each fingerprint in the fingerprints datastore using a VVBN Virtual Volume Block Number thus allowing the fingerprints datastore to scale easily.

During a verify operation stale fingerprint entries are identified and removed from the fingerprints datastore. One aspect of the verify operation optimizes current stale entries removal by reducing the time to sort the fingerprints datastore by recording stale entry information to a separate datastore e.g. stale entries file which would be proportional to size of stale entries in the fingerprints datastore rather than rewriting the entire fingerprints datastore. In response to detecting a request for a subsequent de duplication operation the stale entry datastore is merged with the fingerprints datastore and the stale entries are removed prior to the execution of the de duplication operation when there is a full read write of the entire fingerprints datastore. Thus the conventional second sort of the fingerprints datastore is eliminated. Another aspect of a verify operation manages the verify operation as a background operation so that if any de duplication request is made while a verify operation is executing the de duplication request can be served which in turn helps decrease customer response time.

Storage of data in storage units is managed by storage servers which receive and respond to various read and write requests from clients directed to data stored in or to be stored in storage units . Storage units constitute mass storage devices which can include for example flash memory magnetic or optical disks or tape drives illustrated as storage devices A B . The storage devices can further be organized into arrays not illustrated implementing a Redundant Array of Inexpensive Disks Devices RAID scheme whereby storage servers access storage units using one or more RAID protocols known in the art.

Storage servers can provide file level service such as used in a network attached storage NAS environment block level service such as used in a storage area network SAN environment a service which is capable of providing both file level and block level service or any other service capable of providing other data access services. Although storage servers are each illustrated as single units in a storage server can in other embodiments constitute a separate network element or module an N module and disk element or module a D module . In one embodiment the D module includes storage access components for servicing client requests. In contrast the N module includes functionality that enables client access to storage access components e.g. the D module and may include protocol components such as Common Internet File System CIFS Network File System NFS or an Internet Protocol IP module for facilitating such connectivity. Details of a distributed architecture environment involving D modules and N modules are described further below with respect to and embodiments of an D module and an N module are described further below with respect to .

In yet other embodiments storage servers are referred to as network storage subsystems. A network storage subsystem provides networked storage services for a specific application or purpose. Examples of such applications include database applications web applications Enterprise Resource Planning ERP applications etc. e.g. implemented in a client. Examples of such purposes include file archiving backup mirroring etc. provided for example on archive backup or secondary storage server connected to a primary storage server. A network storage subsystem can also be implemented with a collection of networked resources provided across multiple storage servers and or storage units.

In the embodiment of one of the storage servers e.g. storage server A functions as a primary provider of data storage services to client . Data storage requests from client are serviced using storage devices A organized as one or more storage objects. A secondary storage server e.g. storage server B takes a standby role in a mirror relationship with the primary storage server replicating storage objects from the primary storage server to storage objects organized on storage devices of the secondary storage server e.g. storage devices B . In operation the secondary storage server does not service requests from client until data in the primary storage object becomes inaccessible such as in a disaster with the primary storage server such event considered a failure at the primary storage server. Upon a failure at the primary storage server requests from client intended for the primary storage object are serviced using replicated data i.e. the secondary storage object at the secondary storage server.

It will be appreciated that in other embodiments network storage system may include more than two storage servers. In these cases protection relationships may be operative between various storage servers in system such that one or more primary storage objects from storage server OA may be replicated to a storage server other than storage server OB not shown in this figure . Secondary storage objects may further implement protection relationships with other storage objects such that the secondary storage objects are replicated e.g. to tertiary storage objects to protect against failures of secondary storage objects. Accordingly the description of a single tier protection relationship between primary and secondary storage objects of storage servers should be taken as illustrative only.

Nodes may be operative as multiple functional components that cooperate to provide a distributed architecture of system . To that end each node may be organized as a network element or module N module A B a disk element or module D module A B and a management element or module M host A B . In one embodiment each module includes a processor and memory for carrying out respective module operations. For example N module may include functionality that enables node to connect to client via network and may include protocol components such as a media access layer Internet Protocol IP layer Transport Control Protocol TCP layer User Datagram Protocol UDP layer and other protocols known in the art.

In contrast D module may connect to one or more storage devices via cluster switching fabric and may be operative to service access requests on devices . In one embodiment the D module includes storage access components such as a storage abstraction layer supporting multi protocol data access e.g. Common Internet File System protocol the Network File System protocol and the Hypertext Transfer Protocol a storage layer implementing storage protocols e.g. RAID protocol and a driver layer implementing storage device protocols e.g. Small Computer Systems Interface protocol for carrying out operations in support of storage access operations. In the embodiment shown in a storage abstraction layer e.g. file system of the D module divides the physical storage of devices into storage objects. Requests received by node e.g. via N module may thus include storage object identifiers to indicate a storage object on which to carry out the request.

Also operative in node is M host which provides cluster services for node by performing operations in support of a distributed storage system image for instance across system . M host provides cluster services by managing a data structure such as a relational database RDB RDB A RDB B which contains information used by N module to determine which D module owns services each storage object. The various instances of RDB across respective nodes may be updated regularly by M host using conventional protocols operative between each of the M hosts e.g. across network to bring them into synchronization with each other. A client request received by N module may then be routed to the appropriate D module for servicing to provide a distributed storage system image.

It should be noted that while shows an equal number of N and D modules constituting a node in the illustrative system there may be different number of N and D modules constituting a node in accordance with various embodiments. For example there may be a number of N modules and D modules of node A that does not reflect a one to one correspondence between the N and D modules of node B. As such the description of a node comprising one N module and one D module for each node should be taken as illustrative only.

Memory includes storage locations addressable by processor network adapter and storage adapter for storing processor executable instructions and data structures associated with embodiments. A storage operating system portions of which are typically resident in memory and executed by processor functionally organizes the storage server by invoking operations in support of the storage services provided by the storage server. It will be apparent to those skilled in the art that other processing means may be used for executing instructions and other memory means including various computer readable media may be used for storing program instructions pertaining to the embodiments described herein. It will also be apparent that some or all of the functionality of the processor and executable software can be implemented by hardware such as integrated currents configured as programmable logic arrays ASICs and the like.

Network adapter comprises one or more ports to couple the storage server to one or more clients over point to point links or a network. Thus network adapter includes the mechanical electrical and signaling circuitry needed to couple the storage server to one or more client over a network. Each client may communicate with the storage server over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

Storage adapter includes a plurality of ports having input output I O interface circuitry to couple the storage devices e.g. disks to bus over an I O interconnect arrangement such as a conventional high performance FC or SAS link topology. Storage adapter typically includes a device controller not illustrated comprising a processor and a memory for controlling the overall operation of the storage units in accordance with read and write commands received from storage operating system . As used herein data written by a device controller in response to a write command is referred to as write data whereas data read by device controller responsive to a read command is referred to as read data. 

User console enables an administrator to interface with the storage server to invoke operations and provide inputs to the storage server using a command line interface CLI or a graphical user interface GUI . In one embodiment user console is implemented using a monitor and keyboard.

When implemented as a node of a cluster such as cluster of the storage server further includes a cluster access adapter shown in phantom having one or more ports to couple the node to other nodes in a cluster. In one embodiment Ethernet is used as the clustering protocol and interconnect media although it will apparent to one of skill in the art that other types of protocols and interconnects can by utilized within the cluster architecture.

Multi protocol engine includes a media access layer of network drivers e.g. gigabit Ethernet drivers that interface with network protocol layers such as the IP layer and its supporting transport mechanisms the TCP layer and the User Datagram Protocol UDP layer . A file system protocol layer provides multi protocol file access and to that end includes support for the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the Hypertext Transfer Protocol HTTP protocol . A VI layer implements the VI architecture to provide direct access transport DAT capabilities such as RDMA as required by the DAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the storage server. In certain cases a Fibre Channel over Ethernet FCoE layer not shown may also be operative in multi protocol engine to receive and transmit requests and responses to and from the storage server. The FC and iSCSI drivers provide respective FC and iSCSI specific access control to the blocks and thus manage exports of luns to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing blocks on the storage server.

The storage operating system also includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on storage devices. Information may include data received from a client in addition to data accessed by the storage operating system in support of storage server operations such as program application data or other system data. Preferably client data may be organized as one or more logical storage objects e.g. volumes that comprise a collection of storage devices cooperating to define an overall logical arrangement. In one embodiment the logical arrangement may involve logical volume block number vbn spaces wherein each volume is associated with a unique vbn.

File system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustrated as a SCSI target module . SCSI target module is generally disposed between drivers and file system to provide a translation layer between the block lun space and the file system space where luns are represented as blocks. In one embodiment file system implements a WAFL write anywhere file layout file system having an on disk format representation that is block based using e.g. 4 kilobyte KB blocks and using a data structure such as index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . File system uses files to store metadata describing the layout of its file system including an inode file which directly or indirectly references points to the underlying data blocks of a file.

Operationally a request from a client is forwarded as a packet over the network and onto the storage server where it is received at a network adapter. A network driver such as layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to file system . There file system generates operations to load retrieve the requested data from the storage devices if it is not resident in core i.e. in memory . If the information is not in memory file system accesses the inode file to retrieve a logical vbn and passes a message structure including the logical vbn to the RAID system . There the logical vbn is mapped to a disk identifier and device block number disk dbn and sent to an appropriate driver of disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the storage server. Upon completion of the request the node and operating system returns a reply to the client over the network.

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the storage server adaptable to the embodiments may alternatively be implemented in hardware. That is in an alternate embodiment a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by the storage server in response to a request issued by a client. Moreover in another alternate embodiment the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the storage server. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

When implemented in a cluster data access components of the storage operating system may be embodied as D module for accessing data stored on a storage device e.g. disk . In contrast multi protocol engine may be embodied as N module to perform protocol termination with respect to a client issuing incoming access over the network as well as to redirect the access requests to any other N module in the cluster. A cluster services system may further implement an M host e.g. M host to provide cluster services for generating information sharing operations to present a distributed file system image for the cluster. For instance media access layer may send and receive information packets between the various cluster services systems of the nodes to synchronize the replicated databases in each of the nodes.

In addition a cluster fabric CF interface module CF interface modules A B may facilitate intra cluster communication between N module and D module using a CF protocol . For instance D module may expose a CF application programming interface API to which N module or another D module not shown issues calls. To that end CF interface module can be organized as a CF encoder decoder using local procedure calls LPCs and remote procedure calls RPCs to communicate a file system command to between D modules residing on the same node and remote nodes respectively.

The operating system also includes a user interface module and a de duplication module logically on top of the file system . The user interface module may implement a command line interface and or a graphical user interface which may be accessed by a network administrator from an attached administrative console or through a network. The de duplication module is an application layer which identifies and eliminates duplicate data blocks de duplication and triggers data block sharing in accordance with the embodiments introduced herein.

The operating system also includes or has access to data repositories that are used to implement the data block sharing. The data repositories can include but are not limited to a fingerprints datastore a changelog file an active map and reference count file. Embodiments of the data repositories are described in greater detail below in conjunction with and . Although embodiments are shown within the storage operating system it will be appreciated that embodiments may be implemented in other modules or components of the storage server. In addition embodiments may be implemented as one or a combination of a software executing processor hardware or firmware within the storage server. As such embodiments may directly or indirectly interface with modules of the storage operating system.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the embodiments described herein may apply to any type of special purpose e.g. file server or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the embodiments can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write anywhere file system the embodiments may be utilized with any suitable file system including conventional write in place file systems.

When de duplication runs for the first time the de duplication module scans the blocks and creates a fingerprints datastore which contains fingerprints for used blocks in the storage device . The fingerprints datastore can store an entry e.g. a fingerprint record for each data block that is written to the storage device in the storage . An entry includes a fingerprint fingerprint value for the data block. A fingerprint or fingerprint value may be a checksum for example. The fingerprints are used in a de duplication operation for efficiently identifying duplicate data blocks i.e. to identify data blocks that can be shared. A de duplication operation is described below in detail according to embodiments.

When new data blocks are written or updated in the file system new fingerprint entries are created and logged into a changelog . During a de duplication operation the entries in the fingerprints datastore are compared to the entries in the changelog to identify and free duplicate data blocks so as to leave only one instance of each unique data block in the file system.

In one embodiment the fingerprints datastore is divided into a primary datastore and a secondary datastore . The primary datastore includes a single entry for each unique fingerprint and the secondary datastore includes an entry having an identical fingerprint as an entry in the primary datastore . The de duplication module merges entries in a changelog with the entries in the primary datastore to identify duplicate data blocks in the storage device and frees the identified duplicate data blocks in the storage. The de duplication module stores the entries that correspond to the freed data blocks to a third datastore and overwrites the primary datastore with the entries from the merged data that correspond to the unique fingerprints to create an updated primary datastore.

Typically during a de duplication operation the fingerprints datastore is overwritten each time with the entries from the current fingerprints datastore and the changelog . Overwriting the entire fingerprints datastore with every de duplication operation however can involve a large amount of write cost. shows another embodiment of a fingerprints datastore that is a master datastore and one or more datastore segments to avoid overwriting the master datastore during every de duplication operation. The master datastore includes an entry for each data block that is written to the storage device . Each data block has a fingerprint that is generated by the de duplication module . The one or more datastore segments include an entry for a new data block or modified data block that is subsequently written to the storage device . Each new and modified data block has a fingerprint that is generated by the de duplication module . The de duplication module delays overwriting the master datastore until a segment count threshold is reached or until a verify operation is triggered. The de duplication module can then overwrite the master datastore with the entries in all of the datastore segments and the entries in the master datastore to create an updated master datastore.

The first phase of a de duplication process includes identifying and eliminating duplicate data blocks. The identifying and eliminating of duplicate data blocks is hereinafter referred to as a de duplication operation and block freeing operation. At instruction block the method identifies duplicate data blocks. One embodiment of a method for identifying duplicate data blocks using a fingerprints datastore that is divided into multiple parts such as a primary datastore and a secondary datastore is described in greater detail in conjunction with . Another embodiment of a method for identifying duplicate data blocks using a fingerprints datastore that is organized into a master datastore and datastore segments is described in greater detail in conjunction with . Another embodiment of a method for identifying duplicate data blocks using VVBNs virtual volume block numbers is described in greater detail in conjunction with .

Once the duplicate data blocks are identified the method eliminates the identified duplicate blocks e.g. actual duplicate data blocks at instruction block so as to leave only one instance of each unique data block. Eliminating the duplicate data blocks includes sharing the remaining instance of each data block that was duplicated and freeing the no longer used duplicate data block s . One embodiment of a method for eliminating a data block such as a duplicate block is described in greater detail in conjunction with . At instruction block the method updates a reference count file and an active map. An embodiment of updating a reference count file and an active map are described in greater detail below in conjunction with .

The fingerprint entries that correspond to the eliminated duplicate data blocks and remain in a fingerprints datastore FPDS are referred to as stale fingerprint entries. A verify operation identifies stale fingerprint entries from the FPDS. The identifying and removing of stale fingerprint entries is hereinafter referred to as a verify operation stale record removal operation verify phase verify scan and checking phase. In one embodiment a verify operation identifies stale fingerprint entries and the stale entries are removed from a fingerprints datastore during a subsequent de duplication operation. A stale fingerprint entry is an entry that has a fingerprint that corresponds to a block that has been deleted or overwritten for example at instruction block .

At instruction block the method determines whether to perform a verify operation. A verify operation can be automatically triggered when the number of stale entries in a fingerprints datastore reaches or exceeds a stale entries threshold. In another example a verify operation can be triggered from CLI command line interface . A verify operation can also be user driven for example by the de duplication module receiving instructions entered by a user via a command line interface.

If there is a trigger for the verify operation the method identifies and removes the stale fingerprint entries from the fingerprints datastore at instruction block . One embodiment of a method for identifying and removing stale fingerprint entries when a next de duplication operation is invoked is described in greater detail in conjunction with . Another embodiment of a method for identifying and removing stale fingerprint entries as a background job is described in greater detail in conjunction with .

If there is not a trigger for the verify operation the method determines whether a de duplication operation start request e.g. sis start command or SIS request is received at instruction block . New data blocks may be written or updated in storage e.g. storage A B in and can trigger the storage server e.g. a de duplication module in a storage server to compute fingerprints of the new data blocks and update the fingerprints datastore to reflect the new data blocks. When new data blocks are written or updated in a file system the storage server creates and logs new fingerprint entries into a changelog file. The method may detect a SIS request to perform de duplication on the updated fingerprints datastore. If a SIS request is detected the method returns to instruction block to identify duplicate blocks in the fingerprints datastore.

The method can be triggered automatically at predetermined intervals or at predetermined times or it may be triggered manually or in response to pre specified events such as deletion of a file or in response to a pre specified policy such as a given number of new blocks having been collected .

For each VBN maintained by the file system a reference count file includes a value REFCOUNT indicating the number of references to that VBN. The reference count file contains an entry e.g. record for each data block maintained by the storage server wherein each entry includes a value REFCOUNT indicating the number of references to that data block. For example a data block which is shared by two files would have a REFCOUNT value of 2. A data block can be shared by more than two files or other entities in which case the REFCOUNT value would reflect this accordingly. A data block which is allocated but not shared would have a REFCOUNT value of 1. A data block which is not yet allocated would have a REFCOUNT value of 0. In certain embodiments the REFCOUNT value for each data block is a two byte binary value which allows each data block to be the target of up to 2 1 references. In the example of for VBNs 1 2 3 4 the REFCOUNT values are 1 1 1 1 respectively indicating that each VBN is the target of one reference.

Refer now to which is a variation of the example of in which VBNs 3 and 4 of file Bar have the same data A as VBN 1 of file Foo. That is VBNs 3 are 4 are duplicates of VBN 1 and of each other. Initially when a data block is allocated by the file system its REFCOUNT value in the reference count file is set equal to 1. Accordingly before duplicate data blocks are identified in the example of the REFCOUNT values for the example of are the same as in i.e. 1 1 1 1 as shown.

In contrast shows what the example of would look like after duplicate data blocks have been identified e.g. at instruction block in method of and sharing is implemented e.g. at instruction block in method of . Sharing involves giving to each entity which owns a shareable data block a pointer to that data block. Accordingly in the example of this involves giving file Bar two pointers to VBN 1 file Foo already had a pointer to VBN 1 . Data sharing also involves eliminating the duplicate data blocks VBNs 3 and 4 and freeing them for reuse e.g. at step of . Once the data sharing is completed the REFCOUNT values for VBNs 1 2 3 4 are adjusted to be 3 1 0 0 respectively to reflect the fact that VBNs 3 and 4 have been freed and VBN 1 now has three references to it i.e. VBN 1 is shared . One embodiment of a method for freeing duplicate data blocks for reuse is described in greater detail in conjunction with .

Returning to the data sharing continually updates the reference count file to reflect events that affect these blocks. For example if file Foo is now deleted the REFCOUNT values for VBNs 1 2 3 4 would be adjusted to be 2 0 0 0 respectively reflecting that VBN 2 has been freed in addition to VBNs 3 and 4. Note that VBN 1 has not been freed i.e. its REFCOUNT value is not zero since VBN 1 is still in use by file Bar instead the REFCOUNT value for VBN 1 has been decremented from 3 to 2. If file Bar is now deleted the REFCOUNT values for VBNs 1 2 3 4 would be adjusted to be 0 0 0 0 respectively.

In one embodiment the data sharing uses a file system that adheres to the copy on write principle that is anytime a data block is modified it is written to a different VBN rather than modifying the data in place. Referring back to the example of therefore assume that a write request from a client causes the data A in file Bar to be changed to F . In this case VBN 1 which contains the data A is not modified. However since the new data F is written to a new logical and physical block the REFCOUNT value for VBN 1 must still be updated. Hence the REFCOUNT value for VBN 1 in this case would be decremented by one. In addition the REFCOUNT value for whichever VBN is allocated to store the new data F would be incremented by one.

In another embodiment data sharing uses a file system which does not impose copy on write in all instances. For example the data sharing can be implemented by requiring copy on write only when the REFCOUNT value for given data block is greater than one.

To avoid data inconsistencies when a data container e.g. file which contains one or more shared blocks is modified its REFCOUNT values and block pointers are updated in a single atomic transaction. This updating may be done for example during a consistency point i.e. when a set of accumulated write transactions are committed from temporary storage to persistent storage.

The data in the reference count file may become corrupted for any of various reasons. The storage server scans the entire active file system for consistency with the reference count file before boot up of the file system to ensure the consistency between the reference count file and the actual state of the file system according to one embodiment. The scanning can include creating a separate temporary reference count file in main memory of the file server scanning all data blocks in the file system to identify shared data blocks and updating the temporary reference count file to reflect any shared data blocks. The temporary reference count file is then compared to the regular persistent on disk reference count file to determine whether they match. If they do not match an inconsistency is identified and appropriate corrective action is taken.

In another embodiment the consistency check is run while the file system is in operation. The storage server creates a temporary reference count file on a storage device e.g. disk so as not to consume main memory in the storage server. In that case however if the user modifies a particular block while the consistency check is running it is necessary to update both the temporary and the persistent reference count files.

Various other optimizations can be added to the above described data sharing. For example a SHARED flag can be provided for each data container e.g. file in the file system to indicate whether the file contains any shared blocks. The SHARED flag can be stored in a convenient location such as in the file s inode a container of metadata about the file used by the file system to allow fast determination of whether it is necessary to read the reference count file when modifying a block. This avoids unnecessarily having to read the large reference count file when the file includes no shared blocks. Similarly another flag can be implemented for each volume in the storage system to indicate whether the volume is allowed to implement block sharing. The benefit as in the previous example is avoiding the need to read the reference count file in all cases.

Further one or more counters can be implemented in the file system to track the total number of shared blocks. These counters can be used to provide an output to a user e.g. a storage network administrator indicating the amount of storage device space e.g. disk space being saved as a result of block sharing.

According to certain embodiments at any particular point in time a block will be in one of the following states free in use fingerprinted shared and zombie. A free block is a block that is not being used not allocated . An in use block is a block that is being used and has not yet been processed by the de duplication operation. A fingerprinted block is a block that has been processed by the de duplication operation and for which an entry e.g. record has been added into the fingerprints datastore to track the block. A shared block is a block that has become shared and for which one or more duplicates of this block have been identified and eliminated. A zombie is a block that was shared but now is no longer used by any files but the block has not yet been freed. illustrates how a block can transition through the various states in response to various events according to certain embodiments.

FPDSFPDSentriesFPDSFPDSentriesFPDSentriesentriesentriesFPDSentries entriesFPDS illustrates the elements of a de duplication module e.g. de duplication module in coupled to a primary FPDS and a secondary FPDS according to certain embodiments. The elements include a de duplication engine a gatherer module a fingerprint manager a fingerprint handler a block sharing engine and a stale fingerprint manager . The de duplication module can be coupled to a fingerprints datastore that stores fingerprints of data blocks that have been written to storage e.g. storage A B in .

Conventional de duplication solutions include a single significantly large FPDS. Therefore during a de duplication operation when the FPDS is sorted and merged with a sorted changelog to identify potential duplicate data blocks there is a significant time taken to sort the large FPDS. According to certain embodiments a FPDS is improved by dividing it into more than one datastore e.g. a primary datastore and a secondary datastore to reduce the overall time taken to identify potential duplicate blocks. Another embodiment of an improved FPDS includes a FPDS that is organized into segments as described in conjunction with .

When de duplication runs for the first time on a flexible volume with existing data the de duplication module scans the blocks in the flexible volume and creates a fingerprints datastore FPDS which contains a sorted list of all fingerprints for used blocks in the flexible volume according to some embodiments. The FPDS can store an entry e.g. fingerprint record for each data block that is written to the storage.

In one embodiment the fingerprint manager divides and manages the fingerprints datastore as multiple datastores such as a primary fingerprints datastore and a secondary fingerprints datastore . A primary FPDS contains an entry e.g. fingerprint record for each unique fingerprint value. A secondary FPDS contains fingerprints entries that have the same fingerprint value as an entry e.g. record in the primary FPDS . Unlike conventional de duplication solutions the sorted primary FPDS is significantly smaller by storing entries for only unique fingerprints and the entries in this smaller datastore is merged with the entries in a changelog to reduce the overall time taken to identify potential duplicate blocks.

The de duplication engine schedules and triggers operations of the other modules. In particular the de duplication engine triggers operation of the gatherer module which may be done according to a pre specified schedule timing algorithm or in response to a manual input. The de duplication engine can detect a request to start a de duplication operation e.g. sis start command and start a de duplication operation. For example the de duplication engine invokes the gatherer module . In one embodiment where a verify operation is currently in progress the de duplication engine detects a de duplication start request and notifies the stale fingerprint manager . One embodiment for invoking a de duplication operation and performing a verify operation in the background is described in detail in conjunction with .

When de duplication runs for the first time the gatherer module identifies each data block that has been written to storage e.g. storage A B in and triggers the fingerprint handler to compute fingerprints for the data blocks and return them to the gatherer module . In one embodiment the gatherer module operates as an initial scanner and also as a scanner for subsequent de duplication operations. In another embodiment the gatherer module operates as an initial scanner and a de duplication scanner not shown identifies new data blocks that are written to storage in subsequent de duplication operations. The fingerprint handler is responsible for computing the fingerprints of data blocks. In certain embodiments the fingerprint handler calculates a checksum such as an MD5 checksum to compute a fingerprint. One embodiment of a method for computing the fingerprints of data blocks is described in greater detail in conjunction with .

The fingerprint manager receives the fingerprints of the data blocks that have been written to storage e.g. storage A B in from the gatherer module and stores an entry e.g. a fingerprint record for each data block that is written to the storage in a FPDS. The fingerprint manager divides and manages the fingerprints datastore as multiple datastores such as a primary fingerprints datastore and a secondary fingerprints datastore . One embodiment for dividing a FPDS into a primary datastore and a secondary datastore described in greater detail in conjunction with . The fingerprint manager creates an entry for each data block that has been written to storage. In one embodiment an entry can include and is not limited to the fingerprint of the block the inode number of the file to which the block belongs and the FBN file block number of the block. In one embodiment the fingerprint manager sorts the entries in the FPDS e.g. primary FPDS secondary FPDS by fingerprint value. One embodiment of a sorting process described in greater detail in conjunction with .

The gatherer module also identifies new data blocks that are written or updated in storage and triggers the fingerprint handler to compute fingerprints of the new data blocks and return them to the gatherer module . In certain embodiments the fingerprint manager also maintains a changelog file e.g. changelog that is coupled to the de duplication module for identifying blocks that are new or modified since the last time the process of was executed.

When new data blocks are written or updated in a file system the fingerprint manager logs an entry e.g. fingerprint record into the changelog . In one embodiment the changelog contains information of the same type as the fingerprints datastore e.g. primary FPDS secondary FPDS i.e. fingerprint of the block inode number of the file to which the block belongs and the FBN of the block but only for new or modified blocks. In one embodiment the fingerprint manager sorts the entries the changelog for example by fingerprint value.

The fingerprint manager detects subsequent de duplication start requests e.g. sis start commands and re executes a sorting process on the entries in a FPDS e.g. primary FPDS and the changelog by fingerprint value. The fingerprint manager merges entries in a sorted FPDS e.g. primary FPDS with entries in a sorted changelog to identify potentially duplicate data blocks by finding entries with matching fingerprints.

The block sharing engine is responsible for comparing potentially duplicate data blocks identified by the fingerprint manager to each other to identify actual duplicate data blocks. The blocks represented by any entries which have identical fingerprints are considered to be potential duplicate blocks rather than actual duplicates since there is always a possibility that two non identical blocks could have the same fingerprint regardless of the fingerprint scheme being used. The block sharing engine also eliminates the actual duplicate data blocks and implements block sharing by calling functions of a file system e.g. file system in . Eliminating the duplicate data blocks can include sharing the remaining instance of each data block that was duplicated and freeing the no longer used duplicate data block s . For performance reasons multiple block share operations may be ongoing at any given time. One embodiment of a method for eliminating a data block such as a duplicate block is described in greater detail in conjunction with .

In some embodiments the fingerprint manager writes the fingerprint entries that correspond to the eliminated actual duplicate data blocks to a third datastore e.g. file and overwrites the primary FPDS with the fingerprint entries that correspond to the unique data blocks to create an updated primary FPDS. The updated primary FPDS can be used for a verify operation to identify and remove stale fingerprint entries that correspond to eliminated data blocks and or for a subsequent de duplication operation.

The stale fingerprint manager performs a verify operation to identify and remove stale fingerprint entries from a fingerprints datastore e.g. primary datastore according to one embodiment. In another embodiment the stale fingerprint manager removes stale fingerprints during a subsequent de duplication operation. A stale entry e.g. stale fingerprint record is an entry that has a fingerprint that corresponds to a data block that has been eliminated deleted or overwritten by the block sharing engine . The fingerprint manager saves context information in the fingerprints entries e.g. entries in the primary FPDS secondary FPDS and any changelogs for each block such as the value of a consistency point counter at the time the block was written to a storage device e.g. disk . The stale fingerprint manager uses the context information to detect and delete stale fingerprint entries from the fingerprints datastore e.g. primary datastore . Entries having higher consistency point counter values are more recent than entries with lower consistency point counter values. In one embodiment the stale fingerprint manager identifies fingerprint entries having the same as other entries but with lower consistency point counter values compared to the other entries as stale fingerprint entries. In another embodiment the stale fingerprint manager identifies fingerprint entries having the same as other entries but with lower consistency point counter values compared to the other entries as stale fingerprint entries. The unidentified entries are stale free fingerprint entries. Complementary to this functionality information on the deleted files and or blocks in the deletion code path is also logged and used to clean up stale entries. Embodiments for identifying and removing stale fingerprint entries are described in greater detail in conjunction with .

At instruction block the method sorts the entries in a fingerprints datastore e.g. fingerprint.0 A for example by fingerprint. Sorting the fingerprints datastore is optional but allows faster identification of duplicate data blocks since the entries for any duplicate data block will reside adjacent to each other in the fingerprints datastore after the sort operation e.g. by fingerprint value is complete. One embodiment of a method sorting fingerprints is described in greater detail in conjunction with .

At instruction block the method determines whether there are any fingerprint entries having identical fingerprint values and identifies these entries as entries that correspond to potential duplicate data blocks. The blocks represented by any entries which have identical fingerprints are considered to be potential duplicate blocks rather than actual duplicates since there is always a possibility that two non identical blocks could have the same fingerprint regardless of the fingerprint scheme being used. If there are entries that have identical fingerprint values instruction block the method compares the data blocks corresponding to these entries to determine whether any of the data blocks are actual duplicate data blocks at instruction block . The method can perform a byte by byte comparison of the potentially duplicate data blocks to determine whether the data blocks are actually identical. In an alternative embodiment instruction block could be eliminated if an approximate verification of comparing fingerprints is deemed sufficient in determining that two blocks are identical.

If there are actual duplicate data blocks the method writes the fingerprint entries that correspond to the actual duplicate data blocks to a second datastore e.g. a secondary file to create a secondary fingerprints datastore e.g. secondary FPDS at instruction block . At instruction block the method overwrites the existing fingerprints datastore e.g. fingerprint.0 A with the entries corresponding to the unique fingerprints to create a primary fingerprints datastore e.g. primary FPDS B . In one embodiment the method sorts the entries in the primary fingerprints datastore B for example by fingerprint at instruction block .

When new data blocks are written or updated in a file system a storage server creates and logs new fingerprint entries into a changelog file A. In one embodiment the changelog A contains information of the same type as the fingerprints datastore e.g. primary FPDS A and secondary FPDS i.e. fingerprint of the block inode number of the file to which the block belongs and the FBN of the block but only for new or modified blocks. The changelog A is sorted when a de duplication start request e.g. start sis command is detected.

The sorted changelog B is merged in memory with the sorted primary FPDS A to identify potential duplicate data blocks. Unlike conventional de duplication solutions the sorted primary FPDS A is significantly smaller by storing only entries having unique fingerprint values. Therefore an in memory merge of the sorted changelog B with a smaller FPDS e.g. primary FPDS A at reference reduces the overall time taken to identify potential duplicate blocks. An in memory merge of data refers to merging data temporary in memory. An on disk merge of data refers to writing merged data to a storage device e.g. disk .

The potential duplicate data blocks are further analyzed to identify the actual duplicate data blocks. The fingerprint entries that correspond to the eliminated actual duplicate data blocks are written to a third datastore e.g. file . The primary FPDS A is overwritten with the fingerprint entries that correspond to the unique fingerprints to create an updated primary FPDS B. Subsequently during a verify operation to identify and remove the fingerprint entries that correspond to the eliminated actual duplicate data blocks stale fingerprint entries the third datastore is merged with the sorted secondary FPDS which is then merged with the updated primary FPDS B to identify and remove stale fingerprint entries. One embodiment of a method for identifying and removing fingerprint entries which correspond to duplicate data blocks using a primary fingerprints datastore and a secondary fingerprints datastore is described in greater detail in conjunction with .

According to one embodiment a fingerprints datastore is divided into multiple datastores such as a primary FPDS and a secondary FPDS. A primary datastore stores a single entry e.g. record for each fingerprint value and thus is smaller than a conventional fingerprints datastore. A secondary fingerprints datastore stores the remaining entries such as the entries that have the same fingerprint value as an entry in the primary FPDS. In one embodiment the secondary FPDS can be a segmented datastore as described in conjunction with and maintained in a sorted order e.g. by . At instruction block the method maintains a primary FPDS e.g. primary FPDS A and a secondary FPDS e.g. secondary FPDS and sorts the primary FPDS A and the secondary FPDS at instruction block .

At instruction block the method detects a de duplication operation start request e.g. sis start command and sorts a changelog e.g. changelog A by fingerprint at step . At instruction block the method merges the sorted changelog B with the sorted primary FPDS A to identify potential duplicate data blocks at instruction block . As noted above unlike conventional de duplication solutions the sorted primary FPDS A is significantly smaller by storing only entries having unique fingerprint values. Therefore merging the sorted changelog B with a smaller FPDS e.g. primary FPDS A reduces the overall time taken to identify potential duplicate blocks.

If there are entries that have identical fingerprint values step the method performs a byte to byte comparison of the data blocks that correspond to the entries that have identical fingerprint values to identify data blocks that are actual duplicate data blocks at instruction block . If there are any data blocks that are actual duplicate data blocks instruction block the method eliminates the actual duplicate data blocks at instruction block . Eliminating the duplicate data blocks can include sharing the remaining instance of each data block that was duplicated and freeing the no longer used duplicate data block s . The method frees the duplicate block or blocks so that only one instance remains of each unique block and shares the remaining instance of the block to the extent possible. The method then updates a reference count file and an active map at instruction block to reflect the newly shared and freed blocks. One embodiment of a method for eliminating a data block such as a duplicate block is described in greater detail in conjunction with .

At instruction block the method writes the fingerprint entries that correspond to the eliminated actual duplicate data blocks to a third datastore e.g. file and overwrites the primary FPDS e.g. primary FPDS A with the fingerprint entries that correspond to the unique fingerprints to create an updated primary FPDS e.g. primary FPDS B at instruction block . In one embodiment the method determines whether the entire primary FPDS A has been examined. If the entire primary FPDS A has not been examined the method returns to instruction block to identify entries that have identical fingerprints until the primary FPDS A has been examined.

When de duplication runs for the first time on a flexible volume with existing data the de duplication module scans the blocks in the flexible volume and creates a fingerprints datastore FPDS which contains a sorted list of all fingerprints for used blocks in the flexible volume according to some embodiments. The FPDS can store an entry e.g. fingerprint record for each data block that is written to the storage.

Conventional de duplication solutions include a single significantly large FPDS that has a flat file structure. With a flat file structure the FPDS is overwritten with every de duplication operation. During every de duplication operation the entries in a changelog are merged with the entries in the FPDS and the old FPDS is overwritten with the merged entries. Therefore traditional implementations incur a significant cost in overwriting the entire FPDS with every de duplication operation irrespective of the size of the changelog. According to certain embodiments a FPDS is improved by organizing the FPDS as a master datastore and datastore segments to avoid overwriting an entire FPDS with every de duplication operation.

In one embodiment the fingerprint manager organizes a FPDS as multiple segments such as a master fingerprints datastore and datastore segments 1 to n to . A master FPDS stores an entry e.g. fingerprint record for each data block that is written to the storage e.g. storage A B in . In one embodiment an entry can include and is not limited to the fingerprint of the block the inode number of the file to which the block belongs and the FBN file block number of the block. A datastore segment contains information of the same type as the fingerprints datastore e.g. master FPDS i.e. fingerprint of the block inode number of the file to which the block belongs and the FBN of the block but only for new and modified data blocks written to the storage e.g. storage A B in . Unlike conventional de duplication solutions the master fingerprints datastore is not overwritten with every de duplication operation but the overwriting is delayed until a verify operation is performed or until a threshold for the number of FPDS segments is reached.

The de duplication engine schedules and triggers operations of the other modules. In particular the de duplication engine triggers operation of the gatherer module which may be done according to a pre specified schedule timing algorithm or in response to a manual input. The de duplication engine detects a request to start a de duplication operation e.g. sis start command and starts a de duplication operation. For example the de duplication engine invokes the gatherer module . In one embodiment where a verify operation is currently in progress the de duplication engine detects a de duplication start request and notify the stale fingerprint manager . One embodiment for invoking a de duplication operation and performing a verify operation in the background is described in detail in conjunction with .

When de duplication runs for the first time the gatherer module identifies each data block that has been written to storage e.g. storage A B in and triggers the fingerprint handler to compute fingerprints for the data blocks and return them to the gatherer module . The fingerprint manager receives the fingerprints from the gatherer module and stores an entry e.g. a fingerprint record for each data block that is written to the storage in a FPDS e.g. master FPDS . The fingerprint manager creates an entry e.g. fingerprint record for each data block that has been written to storage. The fingerprint manager sorts the entries in the FPDS e.g. master FPDS for example by fingerprint value. One embodiment of a sorting process described in greater detail in conjunction with .

The gatherer module also identifies new data blocks that are written or updated in storage e.g. storage A B in and triggers the fingerprint handler to compute fingerprints of the new data blocks and return them to the gatherer module . When new data blocks are written or updated in a file system the fingerprint manager creates and logs an entry e.g. fingerprint record into the changelog . In one embodiment the changelog contains information of the same type as the fingerprints datastore e.g. master FPDS i.e. fingerprint of the block inode number of the file to which the block belongs and the FBN of the block but only for new or modified blocks. The fingerprint manager sorts the entries the changelog for example by fingerprint value.

The fingerprint manager identifies entries in a sorted FPDS that have matching fingerprints to identify potentially duplicate data blocks and eliminate the duplicate data blocks. Eliminating the duplicate data blocks includes sharing the remaining instance of each data block that was duplicated and freeing the no longer used duplicate data block s . For example the fingerprint manager detects a de duplication start request e.g. sis start command and sort the entries in the FPDS to identify potentially duplicate data blocks for and eliminate the duplicate data blocks. The fingerprint manager also detects subsequent de duplication start requests e.g. sis start commands and re executes a sorting process on the entries in a FPDS e.g. master FPDS and the changelog by fingerprint value to identify potential duplicate data blocks.

In some embodiments the fingerprint manager first determines whether the FPDS e.g. master FPDS meets a threshold e.g. number of fingerprint entries and if the FPDS meets a threshold the fingerprint manager writes the fingerprint entries in the sorted changelog to a new datastore segment hereinafter referred to as a fingerprints datastore FPDS segment fingerprints segment or segment e.g. segments . The fingerprint manager sorts the entries in a segment for example by fingerprint value.

The fingerprint manager merges the entries in a sorted FPDS e.g. master FPDS with entries in the existing sorted segment e.g. segment to identify potentially duplicate data blocks by finding entries with matching fingerprints. Typically a fingerprints datastore is overwritten with the merged data with each de duplication operation. Unlike conventional de duplication solutions the master FPDS is not overwritten with every de duplication operation but the overwriting is delayed until a verify operation is performed or until a threshold for the number of FPDS segments is reached.

With each subsequent de duplication operation the fingerprint manager writes the entries in a changelog file to a new FPDS segment e.g. segment until a threshold for a segment count threshold for a number of FPDS segments is reached or a verify operation is to be performed by the stale fingerprint manager .

When the segment count threshold is reached or when the stale fingerprint manager is triggered to perform a verify operation the fingerprint manager merges the fingerprint entries in the sorted changelog the entries in all of the FPDS segments e.g. to A n and the entries in the master FPDS and overwrites the old master FPDS with the merged data to create a new master FPDS in aggregate. The fingerprint manager can use the data in the new master FPDS for a verify operation to identify and remove stale fingerprint entries that correspond to eliminated data blocks and or for a subsequent de duplication operation.

The block sharing engine compares potentially duplicate data blocks identified by the fingerprint manager to each other to identify actual duplicate data blocks. The block sharing engine can also eliminate the actual duplicate data blocks and implement block sharing by calling functions of a file system e.g. file system in . One embodiment of a method for eliminating a data block such as a duplicate block is described in greater detail in conjunction with .

The stale fingerprint manager performs a verify operation to identify and remove stale fingerprint entries from a fingerprints datastore e.g. a new master FPDS in aggregate according to one embodiment. In another embodiment the stale fingerprint manager removes stale fingerprints during a subsequent de duplication operation. A stale entry e.g. stale fingerprint record is an entry that has a fingerprint that corresponds to a data block that has been eliminated deleted or overwritten by the block sharing engine . The stale fingerprint manager detects a request to perform a verify operation. For example the stale fingerprint manage detects a verify operation is triggered when a number of stale entries in a FPDS reaches or exceeds a stale entries threshold. In another example a verify operation is triggered from a CLI. In another example a verify operation is user driven for example by the de duplication module receiving instructions entered by a user via a command line interface.

The fingerprint manager saves context information in the fingerprints entries e.g. entries in the master FPDS all segments to and any changelogs for each block such as the value of a consistency point counter at the time the block was written to a storage device e.g. disk . The stale fingerprint manager can use the context information to detect and delete stale fingerprint entries from the fingerprints datastore e.g. a new master FPDS in aggregate . Entries having higher consistency point counter values are more recent than entries with lower consistency point counter values. The stale fingerprint manager identifies fingerprint entries having the same fingerprint values as other entries but with lower consistency point counter values compared to the other entries as stale fingerprint entries. The unidentified entries are stale free fingerprint entries. Embodiments of a method for identifying and removing stale fingerprint entries is described in greater detail in conjunction with .

With each subsequent de duplication operation the entries in a changelog file C are written to a FPDS segment unless a threshold for a segment count threshold is reached or until a verify operation is to be performed. For example after n de duplication operations there can be n FPDS segments A to A n . During a subsequent de duplication operation e.g. de dupe operation n a changelog C is sorted when a de duplication start request e.g. start sis command is detected. When the threshold for the number of FPDS segments is reached the fingerprint entries in the sorted changelog D the entries in all of the FPDS segments e.g. A to A n and the entries in the master FPDS A are merged on disk thus overwriting the old master FPDS A with the on disk merged data to create a master FPDS B in aggregate. The master FPDS B in aggregate can be used to identify and eliminate duplicate data blocks.

At instruction block according to one embodiment a FPDS is organized and maintained as a segmented datastore such as a master FPDS A and FPDS segments A to A n. A master FPDS A datastore stores an entry e.g. a fingerprint record for each data block that is written to a file system. A datastore segment stores fingerprints entries for new and modified data blocks written to storage e.g. storage A B in .

At instruction block when new data blocks are written or updated in a file system the method creates and logs new fingerprint entries into a changelog file A. At instruction block the method determines whether there is a de duplication start request. For example the method detects a sis start command. If there is not a de duplication start request instruction block the method determines whether there is a request for a verify operation to be performed at instruction block . If there is not a verify operation to be performed instruction block the method returns to instruction block to maintain a sorted master datastore.

If a de duplication start request is detected instruction block the method sorts the entries in the changelog A for example by fingerprint value at instruction block . At instruction block the method determines whether the current number of entries in the master datastore meets a master datastore threshold. In one embodiment the method overwrites the master FPDS A with every de duplication operation until a master datastore threshold is reached. A master datastore threshold can be a user defined threshold and can be stored as a parameter for example in a data store e.g. data store in . For example the master datastore threshold is a comparator e.g. less than less than or equal to greater than greater than or equal to etc. and a number of entries.

At instruction block if the master datastore threshold is reached e.g. a threshold for a number of fingerprint entries in the master datastore the method determines whether the current number of segments meets a segment count threshold at instruction block . The segment count threshold can be comparator e.g. less than less than or equal to greater than greater than or equal to etc. and a number of segments. The segment count threshold can be a user defined threshold and can be stored as a parameter for example in a data store e.g. data store in . For example the segment count threshold is less than or equal to a value of 40.

If the segment count threshold e.g. 40 segments has been met instruction block the method performs an on disk merge of the fingerprint entries in the sorted changelog D the entries in all existing the FPDS segments e.g. A to A n and the entries in the master FPDS A and overwrites the old master FPDS A with the on disk merged data to create a master FPDS B in aggregate at instruction block . The master FPDS B in aggregate can be used to identify and eliminate duplicate data blocks at instruction block .

If the segment count threshold has not been reached instruction block the method writes the fingerprint entries in a sorted changelog to a new datastore segment referred to as a FPDS segment or segment at instruction block . The original master FPDS remains sorted and is maintained as is. In one embodiment where changelogs do not meet a changelog threshold the method appends the fingerprint entries in a sorted changelog to the last FPDS segment if the last segment size is less than a segment size threshold. A changelog threshold can be a user defined threshold and can be stored as a parameter for example in a data store e.g. data store in . A segment can have a maximum segment size e.g. 8 GB 16 GB also referred to as a segment size threshold . Examples of a segment size threshold include and are not limited to a size of a file a number of fingerprints etc. The segment size threshold can be a user defined threshold and can be stored as a parameter for example in a data store e.g. data store in . The segment size threshold can be determined from a change rate e.g. 2 change rate and a size of a volume e.g. 16 TB volume size . For example the segment size threshold is 8 GB.

At instruction block the method sorts the entries in the segment A by fingerprint value and performs an in memory merge of the entries in the segment A with the entries in the sorted master FPDS A at instruction block to identify and eliminate duplicate data blocks at instruction block .

At instruction block the method determines whether there is a request for a verify operation to be performed. If there is not a verify operation to be performed the method returns to instruction block to maintain a sorted master datastore. A verify operation identification and removal of stale fingerprint entries can be automatically triggered when a number of stale entries in a FPDS reaches or exceeds a stale entries threshold. In another example a verify operation is triggered from a CLI. In another example a verify operation is user driven for example by the method detecting instructions entered by a user via a command line interface. If there is a request for a verify operation instruction block the method determines whether all of the segments the changelog and the old master FPDS A have already been merged on disk and the old master FPDS A has been overwritten with the on disk merged data to create a master FPDS B in aggregate at instruction block .

If the a master FPDS in aggregate B has not been created instruction block the method performs an on disk merge of the fingerprint entries in the sorted changelog D the entries in all of the existing FPDS segments e.g. A to A n and the entries in the master FPDS A and overwrites the old master FPDS A with the on disk merged data to create a master FPDS B in aggregate at instruction block . The master FPDS B in aggregate can be used for the verify operation to identify and eliminate stale fingerprint entries at instruction block . Embodiments of methods for identifying and removing stale fingerprint entries is described in greater detail in conjunction with .

Conventional de duplication solutions refer to a data block in a volume using fingerprint entries that contain logical data such as an inode and file block number e.g. . Since these traditional solutions refer to each block logically a fingerprints datastore FPDS needs to store each reference to one physical block. Storing fingerprint entries for each logical block adds overhead to a de duplication operation and does not allow a FPDS to scale easily with increases in shared data in a volume.

According to certain embodiments a FPDS is improved by referencing a data block in a volume uniquely using a virtual volume block number VVBN instead of using . By using a VVBN to refer to a data block a FPDS can more easily scale with increased block sharing. illustrates a block diagram of mapping a data block from a file to a storage device B e.g. disks according to certain embodiments. Mapping illustrates a single block as part of several logical and physical storage containers a file a container file holding a flexible volume an aggregate and a storage device B. Each provides an array of blocks indexed by the appropriate type of block number. The file is indexed by file block number FBN the container file by virtual volume block number VVBN and the aggregate by physical volume block number PVBN . The storage devices e.g. disks are indexed by disk block number DBN .

To translate an FBN to a disk block a file system such as WAFL goes through several steps. At reference the file system uses the file s inode and buffer tree to translate the FBN to a VVBN . At reference the file system translates the VVBN to a PVBN using the container file s inode and buffer tree. At reference RAID translates the PVBN to a DBN . At reference the file system can use an alternative shortened method provided by dual VBNs to bypasses the container map s VVBN to PVBN translation. A file system can store PVBNs in the file s buffer tree to bypass the container map s VVBN to PVBN translation.

Returning to when de duplication runs for the first time on a flexible volume with existing data the de duplication module scans the blocks in the flexible volume and creates a FPDS which contains a sorted list of all fingerprints for used blocks in the flexible volume according to some embodiments. In one embodiment the FPDS stores an entry e.g. fingerprint record for each data block that is written to the storage e.g. storage A B in . One embodiment for dividing a FPDS into multiple parts such as a primary datastore and a secondary datastore is described in greater detail in conjunction with . Another embodiment for organizing a FPDS into segments is described in greater detail in conjunction with .

The de duplication engine schedules and triggers operations of the other modules. In particular the de duplication engine triggers operation of the gatherer module which may be done according to a pre specified schedule timing algorithm or in response to a manual input. The de duplication engine detects a request to start a de duplication operation e.g. sis start command and starts a de duplication operation. For example the de duplication engine invokes the gatherer module . In one embodiment where a verify operation is currently in progress the de duplication engine detects a de duplication start request and notifies the stale fingerprint manager . One embodiment for invoking a de duplication operation and performing a verify operation in the background is described in detail in conjunction with .

The gatherer module identifies each data block that has been written to storage e.g. storage A B in and triggers the fingerprint handler to compute fingerprints e.g. a checksum for the data blocks and return them to the gatherer module . The fingerprint manager receives the fingerprints of the data block that has been written to storage from the gatherer module and creates and stores an entry e.g. fingerprint record for each data block that is written to the storage e.g. storage A B in in a FPDS .

Typically an entry in a FPDS can include and is not limited to the fingerprint of the block fp context information such as the value of a consistency point counter e.g. a generation time stamp cp cnt at the time the block was written to a storage device e.g. disk and logical data such as the inode number of the file to which the block belongs inode and the FBN file block number of the block fbn . According to certain embodiments an entry e.g. fingerprint record is improved by including and is not limited to the fingerprint of the block fp context information such as the value of a consistency point counter e.g. a generation time stamp cp cnt at the time the block was written to a storage device e.g. disk and physical data such as a container file identifier Container FileID and the VVBN virtual volume block number of the block vvbn . The FPDS reduces to a map file that can be indexed by VVBN according to certain embodiments. Instead of using an inode and FBN to refer to a block in a volume the de duplication module can use VVBN to refer to a block .

The gatherer module also identifies new data blocks that are written or updated in storage e.g. storage A B in and triggers the fingerprint handler to compute fingerprints of the new data blocks and return them to the gatherer module . The fingerprint manager creates and stores fingerprint entries for the new data blocks in a changelog file e.g. changelog that is coupled to the de duplication module .

Typically a conventional changelog contains information of the same type as a FPDS i.e. fp inode FBN but only for new or modified blocks. A new or modified data block includes data blocks that written or updated in storage e.g. storage A B in since a last de duplication operation. According to certain embodiments the fingerprint manager creates and stores fingerprint entries in a changelog that include and are not limited to the inode number of the file e.g. inode to which the block belongs the FBN file block number of the block e.g. fbn the VVBN virtual volume block number of the block e.g. vvbn the fingerprint of the block e.g. fp and a generation time stamp e.g. cp cnt .

The fingerprint manager detects a de duplication start request e.g. sis start command and sort the entries in the FPDS and in the changelog by fingerprint value and merge the entries in the changelog with the entries in the FPDS to identify potentially duplicate data blocks.

Unlike conventional de duplication solutions which load a potential duplicate data block using an inode and FBN e.g. sharing engine loads a potential duplicate data block using a VVBN according to certain embodiments. The block sharing engine performs a byte by byte analysis of the loaded potentially duplicate data blocks to identify actual duplicate data blocks. The blocks represented by any entries which have identical fingerprints are considered to be potential duplicate blocks rather than actual duplicates since there is always a possibility that two non identical blocks could have the same fingerprint regardless of the fingerprint scheme being used. The block sharing engine can eliminate the actual duplicate data blocks and implement block sharing by calling functions of a file system e.g. file system in . Eliminating the duplicate data blocks includes sharing the remaining instance of each data block that was duplicated and freeing the no longer used duplicate data block s . One embodiment of a method for eliminating a data block such as a duplicate block is described in greater detail in conjunction with .

In one embodiment the fingerprint manager overwrites the current FPDS with the merged entries to create a new FPDS e.g. fingerprint.next . The new FPDS can be used for a verify operation to identify and remove stale fingerprint entries that correspond to eliminated data blocks and or for a subsequent de duplication operation.

The stale fingerprint manager performs a verify operation to identify and remove stale fingerprint entries from a FPDS according to one embodiment. In another embodiment the stale fingerprint manager removes stale fingerprints during a subsequent de duplication operation. A stale entry e.g. stale fingerprint record is an entry that has a fingerprint that corresponds to a data block that has been eliminated deleted or overwritten by the block sharing engine . The fingerprint manager saves context information in the fingerprints entries for each block such as the value of a consistency point counter e.g. cp cnt at the time the block was written to a storage device e.g. disk .

The stale fingerprint manager sorts the FPDS e.g. fingerprint.next by VVBN and uses the context information e.g. cp cnt to identify stale fingerprint entries in the FPDS . Sorting by VVBN ensures that only the latest copy of VVBN one with the highest cp cnt is retained and all others are removed from the FPDS . Entries having higher consistency point counter values are more recent than entries with lower consistency point counter values. The stale fingerprint manager identifies fingerprint entries having the same VVBN as other entries but with lower consistency point counter values compared to the other entries as stale fingerprint entries. The unidentified entries are stale free fingerprint entries.

The stale fingerprint manager checks the VVBN for the identified entries to check if it is valid. The stale fingerprint manager can examine an active map to ensure a VVBN is valid ensure that a VVBN has not changed . When the stale fingerprint manager determines that a VVBN is not valid it deletes the stale fingerprint entries from the FPDS e.g. fingerprint.next . The stale fingerprint manager also determines whether an entry e.g. record is an entry having logical or physical data. An entry e.g. fingerprint record can include data indicating the type e.g. physical logical of entry. The stale fingerprint manager also checks a refcount for the VVBN to ensure that the VVBN is shared. One embodiment of a method of identifying and removing stale fingerprint entries using VVBNs is described in greater detail in conjunction with . Complementary to this functionality information on the deleted files and or blocks in the deletion code path is also logged and used to clean up stale entries.

A fingerprints datastore A stores an entry e.g. fingerprint record for each data block that is written to storage e.g. storage A B in . According to certain embodiments an entry can include and is not limited to a container file identifier e.g. Container FileID the fingerprint of the block e.g. fp the VVBN virtual volume block number of the block e.g. vvbn and a generation time stamp e.g. cp cnt . The entries in the FPDS A are sorted by fingerprint value.

A changelog A stores fingerprint entries for new or modified blocks. A new or modified data block includes data blocks that written or updated in storage since a last de duplication operation. According to certain embodiments an entry e.g. fingerprint record in a changelog can include and is not limited to the inode number of the file e.g. inode to which the block belongs the FBN file block number of the block e.g. fbn the VVBN virtual volume block number of the block e.g. vvbn the fingerprint of the block e.g. fp and a generation time stamp e.g. cp cnt . The entries in the changelog A are sorted by fingerprint value.

The entries in the sorted changelog B are merged with the entries in the sorted FPDS A. Entries that have identical fingerprint values are identified as entries that correspond to potential duplicate data blocks. The potential duplicate data blocks are loaded. Unlike conventional de duplication solutions which load a potential duplicate data block using an inode and FBN e.g. embodiments load a potential duplicate data block using a VVBN. The potential duplicate data blocks are further analyzed to identify actual duplicate data blocks and the actual duplicate blocks are eliminated. The current FPDS e.g. FPDS A is overwritten with the merged entries to create a new FPDS. The new FPDS can be used for a verify operation to identify and remove stale fingerprint entries that correspond to eliminated data blocks and or for a subsequent de duplication operation.

At instruction block the method maintains a sorted fingerprints data A. The fingerprints datastore A stores an entry e.g. fingerprint record for each data block that is written to storage e.g. storage A B in . According to certain embodiments an exemplary entry can include and is not limited to . The entries can be sorted by fingerprint value for identifying potential duplicate data blocks. At instruction block the method detects a de duplication operation start request e.g. sis start command and sorts the entries in a changelog A for example by fingerprint at instruction block . At instruction block the method merges the entries in the sorted changelog B with the entries in the FPDS A.

At instruction block during the merging process the method identifies potential duplicate data blocks by determining which if any entries in the changelog A have the same fingerprint value as the entries in the FPDS A. The fingerprint entries having the same fingerprint values pertain to potentially duplicate data blocks and these entries can be written to an output datastore e.g. output file . If there are fingerprint entries that have the same fingerprint value instruction block the method loads the potential duplicate data block using VVBN at instruction block and performs a byte by byte analysis to determine whether any of the loaded data blocks are actual duplicate data blocks at instruction block . If there are any actual duplicate data blocks instruction block the actual duplicate data blocks are eliminated at instruction block . Eliminating the duplicate data blocks includes sharing the remaining instance of each data block that was duplicated and freeing the no longer used duplicate data block s . One embodiment of a method for eliminating a data block such as a duplicate block is described in greater detail in conjunction with . At instruction block the method overwrites the existing FPDS A with the merged data to create a new FPDS B. The new FPDS can be used for a verify operation to identify and remove stale fingerprint entries that correspond to eliminated data blocks and or for a subsequent de duplication operation.

During a first phase of a de duplication process a de dupe operation duplicate data blocks are identified and eliminated. The fingerprint entries that correspond to the eliminated duplicate data blocks and remain in a FPDS are referred to as stale fingerprint entries. During a second phase of a de duplication process a verify operation or verify scan stale fingerprint entries are identified and removed from a FPDS.

Conventional implementations of a verify operation include two stages. In stage one a trigger to invoke a verify operation is detected and the entries in the FPDS are first sorted Sort 1 by e.g. order. The verify operation checks whether any of the fingerprint entries are stale and overwrites the existing FPDS with only the stale free entries to create a new stale free FPDS. In stage two the output from stage one is sorted Sort 2 a second time back to its original order such as fingerprint value inode file block number e.g. . One problem with this conventional approach is that it sorts the FPDS twice with each verify operation. The second sort Sort 2 during verify stage two is unnecessary to remove the stale entries. Another problem with the convention approach is that it overwrites the entire FPDS with stale free entries even if the number of stale entries is a small percentage of the FPDS.

One aspect a verify operation optimizes current stale entries removal by reducing the time to sort the fingerprints datastore by recording stale entry information to a separate datastore e.g. stale entries file which would be proportional to size of stale entries in the fingerprints datastore rather than rewriting the entire fingerprints datastore. During a subsequent de duplication operation the entries in the stale entries datastore are merged with the entries in the fingerprints datastore and the stale entries are removed during a de duplication operation when there is a full read write of the entire fingerprints datastore. Thus the second sort of the fingerprints datastore in a conventional solution is eliminated.

The stale fingerprint manager is coupled to a fingerprints datastore FPDS that stores an entry e.g. fingerprint record for each data block that has been written to storage e.g. storage A B in . One embodiment of a FPDS that is divided into multiple parts such as a primary datastore and a secondary datastore is described in greater detail in conjunction with . Another embodiment of a FPDS that is organized into segments is described in greater detail in conjunction with .

In one embodiment an entry can include and is not limited to the fingerprint of the block fp context information such as the value of a consistency point counter e.g. a generation time stamp cp cnt at the time the block was written to a storage device and logical data such as the inode number of the file to which the block belongs inode and the FBN file block number of the block fbn . In another embodiment an entry e.g. fingerprint record can include and is not limited to the fingerprint of the block fp context information such as the value of a consistency point counter e.g. a generation time stamp cp cnt at the time the block was written to a storage device and physical data such as a container file identifier Container FileID and the VVBN virtual volume block number of the block vvbn .

A verify operation stale entries removal operation can be automatically triggered when the number of stale entries in a FPDS reaches or exceeds a stale entries threshold for example when a number of stale fingerprint entries in a FPDS is beyond 20 . The verify trigger detector determines a current number of stale entries in a FPDS and compare the current number to a stale entries threshold. The stale entries threshold can be a user defined threshold stored as a parameter in a data store that is coupled to the stale fingerprint manager . In another example a verify operation is triggered from a CLI. In another example a verify operation is user driven for example by the method detecting instructions entered by a user via a command line interface.

When the verify trigger detector detects a trigger to execute a verify operation the data sorter sorts the entries in the FPDS . In one embodiment the data sorter sorts the entries in the FPDS using logical data and the context information e.g. by such as order . In another embodiment the data sorter sorts the entries in the FPDS using physical data and the context information e.g. by order .

The stale entry identifier determines whether a stale entries datastore e.g. stale entries file exists. When a stale entries datastore does not yet exist the stale entry identifier uses context information that is stored in the FPDS to identify stale fingerprint entries. Fingerprint entries having higher consistency point counter values are more recent than entries with lower consistency point counter values. In one embodiment the stale entry identifier identifies fingerprint entries having the same as other entries but with lower consistency point counter values compared to the other entries as stale fingerprint entries. In another embodiment the stale entry identifier identifies fingerprint entries having the same as other entries but with lower consistency point counter values compared to the other entries as stale fingerprint entries. The unidentified entries are stale free fingerprint entries.

When a stale entries datastore exists the stale entry identifier compares the entries in the FPDS with the entries in the stale entries datastore to identify stale fingerprint entries. The de duplication module identifies fingerprint entries having the same fingerprint values as the fingerprint entries in the stale entries datastore as stale fingerprint entries. The unidentified entries are stale free fingerprint entries.

The stale entry manager creates and updates the stale entries datastore . In one embodiment the stale entry manager creates copies of the identified stale entries in the stale entries datastore . In another embodiment the stale entry manager does not create copies of the stale entries in the stale entries datastore but writes stale entry information for each of the identified stale fingerprint entries to the stale entries datastore . The stale entry information for each entry can include and is not limited to an entry index e.g. record index inode fbn inode generation count and a block generation time stamp. By storing an entry index and entry information rather than an entry itself the size of the stale entries datastore can be reduced. With this optimization the stale entries datastore contains entry indices of the stale fingerprint entries and stale fingerprint manager can use this datastore of indices to remove the stale fingerprint entries from the FPDS .

In one embodiment there is a FPDS A that stores an entry e.g. fingerprint record for each data block that is written to the storage e.g. storage A B in . In another embodiment there is a master FPDS e.g. segment.0 A in and segments e.g. segments A to A n in . In one embodiment an entry can include and is not limited to the fingerprint of the block fp context information such as the value of a consistency point counter e.g. a generation time stamp cp cnt at the time the block was written to a storage device and logical data such as the inode number of the file to which the block belongs inode and the FBN file block number of the block fbn . In another embodiment an entry e.g. fingerprint record can include and is not limited to the fingerprint of the block fp context information such as the value of a consistency point counter e.g. a generation time stamp cp cnt at the time the block was written to a storage device and physical data such as a container file identifier Container FileID and the VVBN virtual volume block number of the block vvbn .

For illustration purposes one embodiment of a verify operation for identifying and removing stale fingerprint entries is described as three stages . During verify stage one a trigger for a verify operation is detected and the entries in the FPDS datastore A are sorted. In one embodiment the entries in the FPDS A are sorted by logical data and the context information e.g. by such as order . In another embodiment the entries in the FPDS A are sorted by physical data and the context information e.g. by order . The stale fingerprint entries are identified from the context information and stale entry information for the stale entries are written to a stale entries datastore A.

During verify stage two 1637 the stale entries in the stale entries datastore are sorted e.g. by entry index . Verify stage three occurs during a subsequent de duplication operation . A next de duplication start request is detected and during verify stage three the entries in the sorted stale entries datastore B are merged in memory with the FPDS A according to one embodiment. In another embodiment during verify stage three the entries in the sorted stale entries datastore B are merged in memory with the master FPDS e.g. segment.0 A in and segments e.g. segments A to A n in . While the data is being merged in memory the entries are compared to identify any entries in the FPDS A or the master FPDS and the segments that correspond to an entry in the stale entries datastore to identify stale entries. The identified stale entries are removed and the FPDS A or master FPDS is overwritten with the stale free entries to create a stale free FPDS B. This stale free FPDS B can be used in performing the de duplication operation to identify and eliminate duplicate blocks.

The de duplication module is coupled to storage e.g. storage A B in storing data blocks of data and generates a fingerprint for each data block in the storage. The de duplication module is coupled to a fingerprints datastore FPDS that stores an entry e.g. fingerprint record for each data block that has been written to storage. In one embodiment the FPDS stores an entry e.g. fingerprint record for each unique fingerprint. One embodiment of a FPDS that is divided into multiple parts such as a primary datastore and a secondary datastore is described in greater detail in conjunction with . In another embodiment there is a master FPDS e.g. segment.0 A in and segments e.g. segments A to A n in .

At instruction block the method detects a trigger to invoke a verify operation. A verify operation can be automatically triggered when the number of stale entries in a FPDS reaches or exceeds a stale entries threshold for example when a number of stale fingerprint entries are beyond 20 . In another example a verify operation is triggered from a CLI. In another example a verify operation is user driven for example by the method detecting instructions entered by a user via a command line interface.

At instruction block the method sorts the entries in the FPDS. In one embodiment the method sorts the entries in the FPDS using logical data and the context information e.g. by such as order . In another embodiment the method sorts the entries in the FPDS using physical data and the context information e.g. by order .

At instruction block the method identifies the stale fingerprint entries from the context information for each entry. Fingerprint entries in the FPDS with higher consistency point counter values e.g. cp cnt are more recent than entries with lower consistency point counter values. In one embodiment the method identifies entries that correspond to freed data blocks using and identifies entries that have the same as other entries but have a lower consistency point counter value compared to the other entries as stale entries. In another embodiment the method identifies entries that correspond to freed data blocks using and identifies entries that have the same as other entries but have a lower consistency point counter value compared to the other entries as stale entries. The unidentified entries are stale free fingerprint entries. Complementary to this functionality information on the deleted files and or blocks in the deletion code path is also logged and used to clean up stale entries.

At instruction block in one embodiment the method copies the stale entries into a stale entries datastore e.g. fingerprint.stale . Each entry in the stale entries datastore can include a segment identifier. In another embodiment the size of the stale entries datastore is minimized by storing information for the stale entries rather than copying the stale entries themselves. Stale entry information for a stale entry can include and is not limited to a entry number e.g. entry index for the stale entry inode file block number generation time stamp etc. By storing entry information instead of a copy of the stale entry an entry size in the stale entries datastore can be reduced from 32 bytes to 24 bytes. In this embodiment the stale entries datastore only contains the entry indexes of the stale fingerprint entries and corresponding entry information e.g. segment identifier rather than the copies of the entries themselves. The stale entries datastore can contain only the entry indexes based on the assumption that the FPDS will not be changed before removing the stale entries using the stale entries datastore. If the FPDS is changed before using the stale entries datastore to remove stale entries from the FPDS the FPDS indexing scheme will not be valid.

During verify stage two at instruction block the method sorts the entries in the stale entries datastore. The entries can be sorted by entry index. At instruction block the method detects a de duplication start request e.g. sis start command . In response to detecting a de dupe start request verify stage three begins and the method determines whether a stale entries datastore exists at instruction block . If there is not a stale entries datastore the method continues to instruction block . For example a verify operation may not have been previously executed to create a stale entries datastore.

If there is a stale entries datastore the method performs an in memory merge of the entries in the sorted stale entries datastore with the FPDS at instruction block according to one embodiment. In another embodiment the method performs an in memory merge of the entries in the sorted stale entries datastore with a master FPDS and all of the segments at instruction block .

At instruction block while the data is being merged in memory the method compares the entries to identify any entries in the FPDS that correspond to an entry in the stale entries datastore to identify the stale fingerprints to be removed from the FPDS according to one embodiment. In another embodiment where a FPDS is organized as a segmented FPDS the method compares the entries in the master FPDS and the entries in all of the FPDS segments with the entries in the stale entries datastore to identify the stale fingerprints to be removed from the FPDS at instruction block . In one embodiment while merging the changelog entries with the FPDS entries each entry from the FPDS is cross checked against the sorted stale entries datastore. If an entry in the FPDS corresponds e.g. matches an entry in the stale entries datastore the method identifies the entry as a stale entry. In another embodiment the stale entries datastore stores an entry index of the stale entries. The FPDS should remain unchanged in order for the indexing scheme in the stale entries datastore to remain valid and prior to the changelog entries merging with the FPDS entries the entry index information in the stale entries datastore is compared to the entries in the FPDS. If there is a match between an entry in the FPDS with the entry index in the stale entries datastore the method identifies the entry as a stale entry.

At instruction block the method removes the stale entries from the FPDS. The method can purge the stale entries. At instruction block the method overwrites the existing FPDS e.g. FPDS or master FPDS with the stale free entries to create a stale free FPDS e.g. stale free master FPDS . At instruction block the method continues with the de duplication operation to identify and eliminate duplicate data blocks using the stale free FPDS.

A de duplication operation is triggered by a de duplication operation start request e.g. sis start command and sorts a changelog A by fingerprint. The de duplication performs an in memory merge of the sorted changelog B with the sorted primary FPDS A to identify and eliminate duplicate data blocks. The de duplication writes the fingerprint entries that correspond to the eliminated duplicate data blocks to a third datastore e.g. datastore and overwrites the primary FPDS A with the fingerprint entries that correspond to the unique data blocks to create an updated primary FPDS B.

A verify operation includes an in memory merge of the entries in the third datastore with the entries in the secondary FPDS A and then with the entries in the primary FPDS B according to one embodiment. In another embodiment the third datastore is a secondary datastore e.g. tempfile.x for each de dupe operation between verify operations. In one embodiment the secondary datastore e.g. tempfile.x is a segmented datastore as described in conjunction with . The secondary datastore can be sorted in order. Later during the verify operation all tempfile.x files can be merged on disk and sorted in order to optimize a verify operation.

The verify operation identifies and removes stale entries from the merged data and writes the remaining stale free entries to a stale free datastore. The verify operation sorts the stale free data by fingerprint and identifies the entries that correspond to duplicate data blocks. The verify operation writes the identified entries to a second datastore to create a updated secondary FPDS B and overwrites the existing primary FPDS B with the fingerprint entries for the unique data blocks to create an updated primary FPDS C.

At instruction block the method detects a trigger to invoke a verify operation. A verify operation can be automatically triggered when the number of stale entries in a FPDS reaches or exceeds a stale entries threshold for example when a number of stale fingerprint entries are beyond 20 . In another example a verify operation is triggered from a CLI. In another example a verify operation is user driven for example by the method detecting instructions entered by a user via a command line interface.

During a previously executed de duplication operation where the method identifies and eliminates duplicate data blocks the fingerprint entries that correspond to the eliminated duplicate data blocks were written to a third datastore. A secondary FPDS contains fingerprints entries that have the same fingerprint value as an entry e.g. record in the primary FPDS. At instruction block the method performs an in memory merge of a third datastore with a secondary FPDS to create an updated secondary FPDS according to one embodiment. In another embodiment the third datastore is a secondary datastore e.g. tempfile.x for each de dupe operation between verify operations. In one embodiment the secondary datastore e.g. tempfile.x is a segmented datastore as described in conjunction with . All tempfile.x files can be merged on disk and sorted in order to optimize a verify operation.

During a previously executed de duplication operation an original primary FPDS is overwritten with fingerprint entries that correspond to the unique data blocks to create an updated primary FPDS. At instruction block the method performs an in memory merge of the entries in the updated primary FPDS with the entries from the in memory merge of the secondary FPDS and third datastore according to one embodiment. In another embodiment the entries in the updated primary FPDS are merged in memory with the entries of the on disk merged tempfile.x files. At instruction block the method removes stale entries from the merged data. The method can identify stale entries using context information such as the value of a consistency point counter at the time the block was written to a storage device.

The method writes the remaining stale free entries to a stale free datastore and sorts the stale free data by fingerprint at instruction block . At instruction block the method identifies the entries that correspond to duplicate data blocks and writes the identified entries to a second datastore to create an updated secondary FPDS. At instruction block the method overwrites the existing primary FPDS with the fingerprint entries for the unique data blocks to create an updated primary FPDS. One embodiment for dividing a FPDS into a primary FPDS and secondary FPDS is described in detail in conjunction with in .

A de duplication module is coupled to a sorted FPDS. The FPDS stores an entry e.g. fingerprint record for each data block that is written to the storage e.g. storage A B in . According to certain embodiments a FPDS is improved by referencing a data block in a volume uniquely using a virtual volume block number VVBN instead of using . By using a VVBN to refer to a data block a FPDS can more easily scale with increased block sharing.

Typically an entry in a FPDS can include and is not limited to the fingerprint of the block fp context information such as the value of a consistency point counter e.g. a generation time stamp cp cnt at the time the block was written to a storage device and logical data such as the inode number of the file to which the block belongs inode and the FBN file block number of the block fbn . According to certain embodiments an entry e.g. fingerprint record can also include physical data such as a container file identifier Container FileID and the VVBN virtual volume block number of the block vvbn . A FPDS is then reduced to a map file that can be indexed by VVBN according to certain embodiments. Instead of or in addition to using an inode and FBN to refer to a block in a volume the method can use VVBN to refer to a block.

At instruction block the method detects a trigger to invoke a verify operation. A verify operation can be automatically triggered when the number of stale entries in a fingerprints datastore FPDS reaches or exceeds a stale entries threshold for example when a number of stale fingerprint entries are beyond 20 . In another example a verify operation is triggered from a CLI. In another example a verify operation is user driven for example by the method detecting instructions entered by a user via a command line interface.

At instruction block the method sorts the entries in the FPDS. The method sorts the entries in the FPDS using by VVBN and the context information e.g. by order . Sorting by VVBN ensures that only the latest copy of the VVBN e.g. one with highest cp cnt is retained in the FPDS.

At instruction block the method identifies the stale fingerprint entries from the context information for each entry. Fingerprint entries in the FPDS with higher consistency point counter values e.g. cp cnt are more recent than entries with lower consistency point counter values. The method identifies entries that correspond to freed data blocks using and identifies entries that have the same as other entries but have a lower consistency point counter e.g. cp cnt value compared to the other entries as stale entries. The unidentified entries are stale free fingerprint entries. Complementary to this functionality information on the deleted files and or blocks in the deletion code path is also logged and used to clean up stale entries.

At instruction block the method removes purges the stale fingerprint entries. At instruction block the method examines a stale free entry to determine whether it has a valid VVBN. The method can examine an active map to ensure a VVBN is valid ensure that a VVBN has not changed . If the method does not confirm that that the VVBN is valid it can delete purge the stale fingerprint entry at instruction block . If the method confirms that the VVBN is valid the method determines whether the fingerprint entry is a logical entry at instruction block . A fingerprint entry can include data indicating the type e.g. physical logical of entry. In one embodiment at instruction block the method also checks a refcount for the VVBN to ensure that the VVBN is shared.

If an entry is not a logical entry instruction block the method writes it to the FPDS as is at instruction block that is as a physical entry. If an entry is a logical entry instruction block the method converts it to a physical entry at instruction block and writes the physical entry to the FPDS at instruction block . At instruction block the method determines whether to validate another stale free entry.

Typically a verify operation stale entries removal operation is a blocking operation that is if a verify operation is executing on a FPDS then no other de duplication sharing operation can run because all de duplication operations should work from a consistent copy of a FPDS. One aspect de duplication makes a verify operation a background job so that if any de duplication operation request is made while any verify operation is executing the de duplication request can be served to help decrease customer response time and to help not lose any space savings due to not being able to run a de duplication operation.

The stale fingerprint manager is coupled to a fingerprints datastore FPDS that stores an entry e.g. fingerprint record for each data block that has been written to storage e.g. storage A B in . One embodiment of a FPDS that is divided into multiple parts such as a primary datastore and a secondary datastore is described in greater detail in conjunction with . Another embodiment of a FPDS that is organized into segments is described in greater detail in conjunction with .

In one embodiment an entry e.g. fingerprint record can include and is not limited to the fingerprint of the block fp context information such as the value of a consistency point counter e.g. a generation time stamp cp cnt at the time the block was written to a storage device and logical data such as the inode number of the file to which the block belongs inode and the FBN file block number of the block fbn . In another embodiment a fingerprint entry can include and is not limited to the fingerprint of the block fp context information such as the value of a consistency point counter e.g. a generation time stamp cp cnt at the time the block was written to a storage device and physical data such as a container file identifier Container FileID and the VVBN virtual volume block number of the block vvbn .

A verify operation stale entries removal operation can be automatically triggered when the number of stale entries in a FPDS reaches or exceeds a stale entries threshold for example when a number of stale fingerprint entries in a FPDS is beyond 20 . The verify trigger detector determines a current number of stale entries in a FPDS for example by examining a stale entries datastore that is stored in a data store that is coupled to the stale fingerprint manager . The verify detector compares the current number of stale entries to a stale entries threshold. The stale entries threshold can be a user defined threshold stored as a parameter in the data store . In another example a verify operation is triggered from a CLI. In another example a verify operation is user driven for example by the verify trigger detector receiving instructions entered by a user via a command line interface.

During a verify stage one when the verify trigger detector detects a trigger to execute a verify operation the verify operation manager executes a verify operation by invoking a data sorter to sort the entries in the FPDS . In one embodiment the data sorter sorts the entries in the FPDS using logical data and the context information e.g. by such as order . In another embodiment the data sorter sorts the entries in the FPDS using physical data and the context information e.g. by order .

The stale entry identifier uses context information that is stored in the FPDS to identify stale fingerprint entries. Fingerprint entries having higher consistency point counter values are more recent than entries with lower consistency point counter values. In one embodiment the stale entry identifier identifies fingerprint entries having the same as other entries but with lower consistency point counter values compared to the other entries as stale fingerprint entries. In another embodiment the stale entry identifier identifies fingerprint entries having the same as other entries but with lower consistency point counter values compared to the other entries as stale fingerprint entries. The unidentified entries are stale free fingerprint entries. In one embodiment the stale entry manager creates a stale entries datastore and stores it in the data store . One embodiment of a stale entry manager creating a stale entries datastore is described in conjunction with .

During verify stage two the data sorter sorts the entries in the stale entries datastore. Subsequently when a de duplication process is invoked the stale entries datastore can be used to remove the stale fingerprint entries from the FPDS .

While a verify operation is executing a de duplication engine that is coupled to the stale fingerprint manager monitors for a de duplication operation start request e.g. sis start command . In one embodiment when the de duplication engine detects a de duplication start request it notifies the verify operation manager and returns a success message to a user in response to the de duplication start request. The de duplication engine adds a message to a queue which is coupled to the de duplication engine for a de duplication job to be performed in response to the de duplication start request. The queue can be a data store.

The verify operation manager receives the notification from the de duplication engine and monitors for a checkpoint creation. A checkpoint is a point in time during execution of a verify operation in which the verify operation manager can pause the verify operation. A checkpoint can be a user defined point in time. A checkpoint creator can be configured to create checkpoints according to a user defined parameter that is stored in the data store . In one embodiment the checkpoint creator creates a first checkpoint during verify stage one for example after the stale entry identifier identifies the stale fingerprint entries. The checkpoint creator can create more than one checkpoint. For example the checkpoint creator creates a second checkpoint during verify stage two after the stale entries datastore is sorted.

When the verify operation manager detects that the checkpoint creator creates a checkpoint the verify operation manager determines whether to suspend a verify operation that is currently executing. The verify operation manager examines the queue which is coupled to the verification operation manager to determine whether there are any pending de duplication jobs to be performed and if so suspend the verify operation. In one embodiment the verify operation manager marks the FPDS as read only stops the verify operation and saves it in its current state to a storage device. The verify operation manager adds a message to the queue for the verify operation job to be resumed and notifies the de duplication engine to invoke the de duplication operation.

The de duplication engine triggers operations of the other modules such as the gatherer module fingerprint handler fingerprint manager and block sharing engine to execute a de duplication operation for identifying and eliminating duplication data blocks. Eliminating the duplicate data blocks includes sharing the remaining instance of each data block that was duplicated and freeing the no longer used duplicate data block s . Embodiments of the modules executing a de duplication operation are described in detail in conjunction with and . Returning to the fingerprint manager merges the entries in the FPDS with entries in a changelog and the block sharing engine identifies and eliminates the duplicate data blocks. In one embodiment the fingerprint manager determines that the FPDS is marked as read only and writes the merged data to a new FPDS to create a shadow copy of the FPDS . Subsequently when a verify operation is complete the shadow copy can be merged with the original FPDS . The de duplication engine clears the message corresponding to the completed de duplication operation from the queue .

The de duplication engine determines whether there is another de duplication job in the queue to be performed. If not the de duplication engine notifies the verify operation manager to resume the suspended verify operation. The verify operation manager receives the notification from the de duplication engine marks the FPDS as read write and restarts the verify job from its saved state from a storage device.

The de duplication module is coupled to storage e.g. storage A B in storing data blocks of data and generates a fingerprint for each data block in the storage. The de duplication module is coupled to a fingerprints datastore FPDS that stores an entry e.g. fingerprint record for each data block that has been written to storage. In one embodiment the FPDS stores an entry e.g. fingerprint record for each unique fingerprint. One embodiment of a FPDS that is divided into multiple parts such as a primary datastore and a secondary datastore is described in greater detail in conjunction with . Another embodiment of a FPDS that is organized into segments is described in greater detail in conjunction with .

A verify operation stale entries removal operation can be automatically triggered when the number of stale entries in a FPDS reaches or exceeds a stale entries threshold for example when a number of stale fingerprint entries in a FPDS is beyond 20 . The method determines a current number of stale entries in a FPDS for example by examining a stale entries datastore that is stored in a data store that is coupled to the de duplication module. The method compares the current number of stale entries to a stale entries threshold. The stale entries threshold can be a user defined threshold stored as a parameter in the data store. In another example a verify operation is triggered from a CLI. In another example a verify operation is user driven for example by the method detecting instructions entered by a user via a command line interface.

At instruction block the method detects a trigger to execute a verify operation and executes the verify operation at instruction block . While a verify operation is executing the method monitors for a de duplication operation start request e.g. sis start command at instruction block . The method determines whether the verify operation is finished at instruction block . If the verify operation is not finished the method determines whether a checkpoint is being created during the verification operation at instruction block . A checkpoint is a point in time during execution of a verify operation in which the method can pause the verify operation. A checkpoint can be a user defined point in time. In one embodiment the method creates a first checkpoint during verify stage one for example after the method identifies the stale fingerprint entries. The method can create more than one checkpoint during a verify operation. For example the method creates a second checkpoint during verify stage two after the method identifies the stale entries.

If a checkpoint is not created instruction block the method determines whether there is a de duplication operation start request e.g. sis start command at instruction block . If there if not a de duplication operation start request e.g. sis start command the method continues to execute the verify operation at instruction block .

When a checkpoint is created instruction block the method determines whether to suspend the verify job that is currently executing at instruction block . The method examines the queue to determine whether there are any pending de duplication jobs to be performed. If there are no pending de duplication jobs the de duplication does not suspend the verify operation and determines whether there is a de duplication start request at instruction block .

When the method detects a de duplication start request e.g. sis start command the method returns a success message to a user at instruction block . A success message can include for example data indicating the de duplication request is successfully received data indicating the de duplication is to be performed etc. At instruction block the method adds a message to a queue which is coupled to the method for a de duplication job to be performed. At instruction block while the verify operation continues to execute the method monitors for when to create a checkpoint. If a checkpoint is not created instruction block the verify operation continues to execute and the method continues to monitor for a checkpoint at instruction block .

When a checkpoint is created instruction block the method determines whether to suspend the verify job that is currently executing at instruction block . If there is a de duplication job that is pending in the queue instruction block in one embodiment the method marks the FPDS as read only at instruction block . In another embodiment the method does not mark the FPDS as read only and continues to stop the verify operation at instruction block . For example in some embodiments a method does not overwrite a complete FPDS. For instance a method only overwrites a segment of FPDS and subsequently all such segments are merged to avoid write cost. Furthermore in some embodiment a verify operation runs on a backup copy of a FPDS ordered or on primary copy of FPDS e.g. primary FPDS which is ordered in de duplication friendly order.

At instruction block the method stops the verify operation and saves the verify job in its current state to a storage device at instruction block . At instruction block the method adds a message to the queue for the verify operation job to be resumed and invokes the de duplication operation at instruction block .

At instruction block the method merges the entries in the FPDS with entries in a changelog and identifies and eliminates the duplicate data blocks at instruction block . In one embodiment method determines that the FPDS is marked as read only at instruction block and writes the merged data to a new FPDS to create a shadow copy of the FPDS at instruction block . Subsequently when a verify operation is complete the shadow copy can be merged with the original FPDS.

The method clears the message corresponding to the completed de duplication operation from the queue. At instruction block the method determines whether there is another de duplication job in the queue to be performed. If not the method resumes the verify operation that is in the queue by restarting the verify job that is saved from a storage device at instruction block . In one embodiment the method marks the FPDS as read write. Upon completion the method clears the message corresponding to the completed verify operation from the queue.

At instruction block the method detects a request to write data to a data block. The flow diagram illustrates an embodiment of computing a fingerprint for a data block that operates concurrently with the operation of writing the blocks to storage e.g. storage A B in . Instructional blocks and are performed concurrently with instructional block . In alternative embodiments computing the fingerprint and writing the fingerprint to the changelog file is not performed concurrently with writing the block to a storage device. At instruction block the fingerprint handler computes a fingerprint for the block. The fingerprint is passed to the fingerprint manager which writes an entry e.g. record for the block in the changelog file at instruction block including but limited to the fingerprint the FBN file block number the inode number of the block and other relevant context information that is specific to this block such as the value of a consistency point counter at the time the block was written to a storage device.

At instruction block the method determines whether the SHARED flag is set for the file which contains the block to be freed. If the SHARED flag is not set meaning that no blocks in the file are shared the process proceeds to instruction block in which the bit corresponding to the block is cleared in the active map. The process then ends. An active map is a bitmap of all data blocks managed by a storage server i.e. one bit per data block. The bit for a given data block is set in the active map if the data block is allocated and cleared if the data block is free to be used. The active map is used during allocation of blocks to determine whether a block is free or not. The active map helps to improve performance by avoiding the need to read the reference count file to identify free blocks.

A reference count file is much larger and therefore takes longer to read than the active map. The reference count file contains an entry e.g. record for each data block maintained by the storage server wherein each entry includes a value REFCOUNT indicating the number of references to that data block. In one embodiment however the active map and the reference count file could be combined into a single file to identify each free block as well as to indicate the number of references to the data block.

If the SHARED flag is set instruction block then at instruction block the process decrements the REFCOUNT value for the block by one in the reference count file. After decrementing the REFCOUNT value the process determines at instruction block whether the REFCOUNT value is zero. If the REFCOUNT value is zero meaning that the block is no longer used the process clears the corresponding bit in the active map and then ends. A data block that is freed can be reused. If the REFCOUNT value is determined to be non zero instruction block the process finishes.

In certain embodiments the system also maintains a change log to identify blocks that are new or modified since the last time a de duplication operation was executed. The change log contains information of the same type as the fingerprints datastore i.e. fingerprint of the block inode number of the file to which the block belongs and the FBN of the block but only for new or modified blocks. From time to time the system then re executes the sorting process of on both the fingerprints datastore and the change log to merge the change log into the fingerprints datastore. In alternative embodiments the system could simply from time to time scan the entire file system compute the fingerprints of all data blocks and eliminate duplicates at essentially the same time.

The particular methods of the de duplication module have been described in terms of computer software with reference to a series of flow diagrams. B B B B B B and relate to computer s in . The methods constitute computer programs made up of computer executable instructions illustrated as blocks acts in . Describing the methods by reference to a flow diagram enables one skilled in the art to develop such programs including such instructions to carry out the methods on suitably configured machines the processor of the machine executing the instructions from computer readable media including memory The computer executable instructions may be written in a computer programming language or may be embodied in firmware logic. If written in a programming language conforming to a recognized standard such instructions can be executed on a variety of hardware platforms and for interface to a variety of operating systems. In addition the present invention is not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings of the invention as described herein. Furthermore it is common in the art to speak of software in one form or another e.g. program procedure process application module logic . . . as taking an action or causing a result. Such expressions are merely a shorthand way of saying that execution of the software by a computer causes the processor of the computer to perform an action or produce a result. It will be appreciated that more or fewer processes may be incorporated into the methods illustrated in B B B B B B and without departing from the scope of the invention and that no particular order is implied by the arrangement of blocks shown and described herein.

