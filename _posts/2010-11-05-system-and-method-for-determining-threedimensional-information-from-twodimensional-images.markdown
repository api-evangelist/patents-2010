---

title: System and method for determining three-dimensional information from two-dimensional images
abstract: A system and method of determining three-dimensional data for an object by performing optical flow analysis to calculate surface profile analysis for a group of monocular component images that vary wavelength distribution, and determining three-dimensional data relating to the object from the component images and the surface profile analysis. Stereo image features are obtained from a single monocular polychromatic raw image or multiple monocular grayscale images having known spectral imaging collection data. Apparatus are described which allow three-dimensional data for an object to be determined from one or more two-dimensional images of the object.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08149268&OS=08149268&RS=08149268
owner: The United States of America as represented by the Secretary of the Army
number: 08149268
owner_city: Washington
owner_country: US
publication_date: 20101105
---
The invention described herein may be manufactured used and licensed by or for the United States Government.

This application is related to U.S. application Ser. No. 12 940 204 entitled SYSTEM AND METHOD FOR DETERMINING THREE DIMENSIONAL INFORMATION FROM PHOTOEMISSION INTENSITY DATA invented by Ronald Meyers and David Rosen filed on even date herewith and hereby incorporated by reference and U.S. application Ser. No. 12 940 228 entitled SYSTEM AND METHOD FOR MEASURING DEPOLARIZATION invented by David Rosen and Ronald Meyers filed on even date herewith and hereby incorporated by reference.

The present invention relates to apparatus and methods for determination of three dimensional data in particular to determining three dimensional data from one or more two dimensional images.

Generally speaking binocular stereo imaging requires at least two viewpoints which may need to be spatially distant from one another for large distances between a viewer and an object.

Photographs taken by satellite can be obtained over the interne revealing a great deal of information in a two dimensional format. However obtaining three dimensional information is useful in such areas as agricultural forecasting environmental monitoring forensics intelligence gathering object detection including detection of camouflaged objects using three dimensional data target acquisition remote mapping and the like.

The unaided eye partially uses photometric stereo methods of depth profiling in addition to binocular stereo methods to acquire depth perception. Realistic presentation of images as perceived by the unaided eye is needed in visualization software and training material which therefore should include photometric stereo information in addition to binocular stereo information. Photometry relates to the measurement of light in terms of its perceived brightness to the human eye as distinguished from radiometry which is the science of measurement of radiant energy including light in terms of absolute power. In photometry the radiant power at each wavelength is weighted by a luminosity function a.k.a. visual sensitivity function that models human brightness sensitivity. Photometric stereo as used herein is a technique in computer vision for estimating the surface normals of objects by observing that object under different lighting conditions. A surface depth profile value can be calculated when using stereo methods.

Methods relating to the determination of depth and or surface depth profile from the optical field are described in J. R. A. Torre o Natural Photometric Stereo Anois do IX Sibgrabi 95 102 October 1996 and J. R. A. Torre o and J. L. Fernandes Matching photometric stereo images J. Opt. Soc. Am. A15 12 2966 2975 1998 both of which are hereby incorporated by reference.

The article entitled Natural Photometric Stereo discusses the human brain s ability to infer shape from the binocular fusion of some kinds of monocular images. Photometric stereo PS images have been observed which are monocular images obtained under different illuminations that produce a vivid impression of depth when viewed under a stereoscope. According to the article the same is true of pairs of images obtained in different spectral bands. The Natural Photometric Stereo discusses employing an optical flow based photometric stereo algorithm a type of colour separated images which have been so produced as to emulate the kinds of records generated by the photosensitive cells in the human retina to obtain depth estimates from them. The Natural Photometric Stereo article speculates on the possibility that a process similar to PS could work on the human visual system. A natural photometric stereo process is postulated in the Natural Photometric Stereo article invoking some physical and biological arguments along with experimental results in support thereof.

In the article entitled Matching photometric stereo images a process of shape estimation is introduced through the matching of photometric stereo images which are monocular images obtained under different illuminations. According to the Matching photometric stereo images article if the illumination directions are not far apart and if the imaged surface is smooth so that a linear approximation to the reflectance map is applicable the disparities produced by the matching process can be related to the depth function of the imaged surface through a differential equation whose approximate solution is can be found. The Matching photometric stereo images article presents a closed form expression for surface depth depending only on the coefficients of the linear reflectance map function. If those coefficients are not available a simple iterative scheme still allows the recovery of depth up to an overall scale factor.

Various articles have been written on the spatial arrangement of objects and depth perception. In the publication by B. K. P. Horn and B. G. Schunk Determining Optical Flow MIT Artificial Intelligence Laboratory 572 0 27 1980 Horn and Schunk article hereby incorporated by reference there is a description of a method for finding optical flow which is defined in the Horn and Schunk article as 

According to the Horn and Schunk article in general optical flow cannot be computed locally since only one independent measurement is available from the image sequence at a point while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented in the Horn and Schunk article which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm used in Horn and Schunk article reportedly can handle image sequences that are quantized rather coarsely in space and time and is reportedly insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image.

In light of the above there exists a need to obtain three dimensional information about an object from existing two dimensional images such as photographic prints without the need to return to a given viewpoint to obtain further information.

A method of determining three dimensional data for an object by performing optical flow analysis to calculate surface profile analysis for a group of monocular component images that vary wavelength distribution and determining three dimensional data relating to the object from the component images and the surface profile analysis. Stereo image features are obtained from a single monocular polychromatic raw image or multiple monocular grayscale images having known spectral imaging collection data. The surface depth profile also termed herein surface height profile of the object relative to an image plane or illumination vectors for the object provides three dimensional object data in a raw image. The raw monocular image is input by scanning a photograph receiving a video signal or receiving the image from an imaging device. The method optionally includes color correction or adjusting the wavelength distribution of the image using a correction factor to refine stereo image quality. The correction factor is optionally correlated with the identity of the imaging device or a recording medium. Image formation data identity of imaging device ambient conditions during imaging and the like are provided with the image and may include a correction factor or allow one to be determined. Color correction may also use the known properties of object s within the image. The three dimensional data is used in an improved method of object recognition by providing a stereo image of the object.

An apparatus for determining a three dimensional data relating to an object from an image of the object includes an image input such as an optical scanner . An image decomposer decomposes the image into a plurality of component images for example an algorithm or plurality of optical filters an optical flow calculator operating on a computer determines optical flow data from the component images and a three dimensional data calculator algorithm operating on the computer calculates the three dimensional data from the optical flow data. The apparatus optionally includes an image corrector such as an optical filter modifying the image input or a software correction using an algorithm. The correction factor used may be retrieved from a memory device such as a database based on image formation data. The apparatus may further comprise an output device such as a visual display or a data communications device.

The present invention has utility as a method and apparatus for extracting three dimensional data from one or more two dimensional images.

Monocular images such as previously recorded photographs are analyzed using methods according to the present invention allowing three dimensional information to be extracted from an image even if it was recorded at a previous time with no intention of recording any three dimensional data. The information extracted does not have to be real time as in some computer vision applications of photometry and need not be used for immediate evaluation of three dimensional shape.

For example stereo images derived from a monocular color photograph such as a picture postcard could be used with this invention for stereo image analysis and stereo map making. A picture postcard is generally not intended as a stereo map of the landscape but the photometric techniques described herein allow extraction of a three dimensional image useful for map making and finds applications in constructing changes in an area as a function of time.

Applications of embodiments of the present invention include agricultural forecasting environmental monitoring forensics intelligence gathering object detection including detection of camouflaged objects using three dimensional data target acquisition remote mapping and the like. For example archived two dimensional color images made with a monocular camera and other equipment not intended for map making can be used to determine three dimensional information about terrain. Photometric stereo approaches can be used at large object distances such as those distances above which binocular stereo can be used with conventional equipment.

Images may include photographs including postcards television images infrared spectral images ultraviolet spectral images x ray spectral images microwave spectral images and other images recorded with no optimization for three dimensional information. The image is analyzed for three dimensional information and illumination information valuable for reasons not foreseen by the individuals originally recording the image.

Image formation associated with an image may be used in an inventive method but is not required. Such image formation data associated with an image illustratively includes imaging device data sensitivity parameters for the film and or imaging device such as a camera used to make the image identity of imaging device type of film and the like imaging condition data time of image such as ambient conditions at the time the image was made solar elevation or other lighting conditions relative solar alignment direction of view distance to objects imaged altitude of imaging device and or objects imaged meteorological conditions such as the presence of cloud or precipitation time of day and the like color correction factors or other data associated with the image.

A surface depth profile is a useful calculated value using stereo methods. The surface depth profile is determined by equations that are presented here. Two different ways that the atmosphere impacts photometric stereo are shown by providing illumination for passive photometric stereo imaging and by attenuating the reflected EMR differently at different wavelengths. Effective angles of illumination for two different wavelengths 400 nm and 700 nm are calculated from published experimental data. The utility of photometric stereo is evaluated at different ground albedos and directions of the sun for the atmospheric conditions of the published data. It is noted that the albedo of an object is a measure of how strongly the object reflects light reflectivity from light sources such as the Sun defined as the ratio of total reflected to incident electromagnetic radiation ranging from 0 dark to 1 bright . The size of the error caused by wavelength dependent scattering is estimated and the validity of the equations present are also discussed for attenuation that is strongly dependent on wavelength.

According to the present invention the surface height profile and illumination information are determined from a two dimensional color image such as film images or digital camera images made with a single view monocular color camera. Other image data inputs for the present invention include multiple monocular images collected that vary collection wavelengths or time of collection thereby extending the present invention to analysis of grayscale infrared ultraviolet and video imagery to glean three dimensional features from monocular sources. In a color image if the object was illuminated from several sources of light at different angles the wavelength distribution of radiances on the object surface depends both on the surface depth profile of the object and on angles of illuminating radiation at the time of the image recording. Photometric techniques are used to determine the surface depth profile of the object and angles of illuminating radiation at the time of the image recording.

Photometric methods are used in the present invention to extract stereo image information from previously recorded monocular color images including images that were recorded with no intention of ever using them to obtain stereo imaging. The variations of height of the surface of an object relative to a background plane are also optionally calculated. This variation is defined as the height above the background plane in the image field. The variation in height referred to synonymously as the surface depth profile of the object is three dimensional information synonymously referred to as stereo information relating to the object. Representative background planes used for height variations include a monocular image taken from an aerial perspective for which the background plane is a generally planar ground surface and often an orthogonal plane relative to the monocular camera or a monocular image taken from the ground in which case the background plane is often a vertically aligned surface.

It is appreciated that while a background plane is most easily extracted as a planar surface modeling a background plane as a contoured surface optionally is used herein to refine stereo image data extracted. The use of a contoured surface as a background plane is well suited for instances where a monocular image background has at least three points amenable to topographical indexing.

The determination of stereo information from a two dimensional image is referred to herein as photometric imaging. Although a single view synonymously monocular camera is not designed to optimize the extraction of stereo image information stereo image information is extracted from color images by mathematical processing according to the present invention.

According to the present invention optical filters or other wavelength discriminating equipment are used to decompose a single color image into multiple component wavelength discriminated images with the component images varying from one component image to another in intensity distribution as a function of wavelength.

Optionally a color correction is made to the raw monocular image or a component image to account for non uniform wavelength sensitivity of the monocular imaging device.

In a preferred embodiment when a band pass optical filter is used for image decomposition to form component images each of the component images obtained from the original raw monocular image corresponds to wavelength distribution associated with the passed bandwidth of the optical filter. These component images derived from decomposition of the original raw monocular color image are used to calculate an optical flow field. An optical flow field is defined as a field in which each point of the optical flow field is assigned a two dimensional vector representing the optical flow. The surface depth profile is then extracted from the optical flow data. A three dimensional image is then constructed using both the mean of the series of component images and the surface depth profile. Image decomposition is also optionally performed with successive monochromatic wavelength sources such as individual laser lines to yield monochromatic component images. With reference data as to time of raw image collection weather conditions and solar spectrum further refinements in three dimensional image extraction and in particular optical field data are obtained from monochromatic component images.

In addition to or instead of the surface depth profile the illumination vector s of an object at the time of the image recordation are also calculated from the raw color monocular image. The illumination field is optionally calculated from the three dimensional image itself or from the two dimensional vector mapping defining the optical field. An illumination field is defined herein as the field where a three dimensional vector is assigned to each point of the field representing the direction of radiation that is illuminating the object at that point. An intensity distribution in terms of the angles describing the illumination is represented for each of the component images derived from the original color monocular image. If narrow band optical filters are used the illumination field at each point of the image corresponds to the direction that electromagnetic radiation is propagating at that wavelength of the image.

In examples of the present invention photometric stereo analysis is applied to an image that has already been recorded using conventional monocular color imaging. The inventive method is readily applied to archival monocular color photographs even if the equipment used to record the photograph was not designed for stereo imaging and even if imaging conditions were not recorded for the raw color image. The term photograph includes photographic prints negatives slides digital analogs thereof infrared images ultraviolet images and the like.

The three dimensional image information assists in the identification of a photographed object. The illumination distribution of the surface of the object can characterize the time of day in an outdoor photography or the indoor lighting for an indoor photograph. Therefore a method according to the present invention can extract historical or forensic information from a color photograph and finds representative applications as diverse as refining agricultural crop growth estimations estimating ice sheet thickness and discerning camouflaged image objects in addition to those applications detailed above.

The decomposition of the color image into component images is achieved in several ways depending on the precision and accuracy required for stereo image reconstruction. For instance a photograph is reimaged with a digital imaging device and a brightness distribution at different wavelengths calculated and assigned based on the type of film used. This facilitates compensating for wavelength bias in the original imaging system film of the pixel array. In instances when one or more objects depicted in a raw image has a known wavelength distribution optionally the known wavelength distribution object is used to compensate for wavelength bias in the original imaging system film of pixel array. An example of an object with a known wavelength distribution is leaves on a tree that has a known optical absorption spectrum.

It is appreciated that a set of component images may be received as image data with the component images varying in collection wavelength distributions. For instance when the color and infrared monocular images or color filtered image sets are collected these images are readily used as input image data and obviate the need for wavelength image decomposition. Optical flow is then determined using image set groups selected from the component images in an analogous method to that used for the component images formed from decomposition of a single image.

Image data for three dimensional shape analysis of the present invention is applied to archived color photographs made with single view film cameras images from a database of digital images and analysis of video information for example color television recordings and other color video recordings made with a single view video camera . Video stills may be analyzed in a similar manner to other images. Time dependent stereo information can be obtained from video recordings including color movies color TV or any other information series of color images that vary as a function of time and independent of classical stereographic alignment of the images to afford depth perception.

The present invention is also amenable to determination of the geometric angular distribution of illumination from archived color images made with single view digital or film cameras color television recordings and other color video images made with a single view video camera. Time dependent illumination distributions can be determined from video recordings that vary as a function of time.

It is appreciated that according to the present invention archived grayscale images recorded with several single view cameras are also operative in situations where the multiple monocular grayscale images which were collected on a media that had different color sensitivities are used as image data input. Each grayscale image collecting camera corresponds to a different distribution of wavelengths which is known if the type of film or type of camera is known even if the individual images are black and white. For example a distant object can be photographed by two viewers close to each other one grayscale camera with sensitivity peaked at yellow wavelengths and the other camera having a sensitivity peaked at green wavelengths are equated to component images as image input per . A plurality of images is obtained each representing a different wavelength distribution of the object.

The present invention is also operative with archived grayscale images recorded while the wavelength of the source is varying with time are used as image data input. Each moment of time corresponds to a different distribution of illumination wavelengths which is known if the type of film or type of camera is known even if the individual images are black and white. An example would be an image made at the same time of day but on days with different atmospheric conditions so that the color of illumination changes. Archived geosynchronous satellite images are representative of such a source of images.

Methods according to the present invention also are operative with images containing an object with a known BRDF Bidirectional Reflectance Distribution Function as an image data input with the BRDF object then being used as a reference to extract information such as stereo image information of other objects in the image illuminating wavelength information camera sensitivity information and atmospheric information.

The wavelength distribution of the image is optionally adjusted using a color correction factor at step . The correction factor may for example be included with or determined from image formation data relating to the formation of the image. The correction factor may be input as part of the color image data in step or determined from other input data. For example an identification of the imaging device used to form the image may be used to retrieve e.g. from a memory device or otherwise generate the correction factor. The correction factor also is optionally generated from the uncorrected wavelength distribution of objects within the image that have a known wavelength distribution such as leaves a known substance such as a rock mineral or ice. Color correction is optional and used as a three dimensional output image refinement.

The color image is decomposed into multiple component images the component images distinguished from one another by different wavelengths being used to form the component images at step . The result of the image decomposition is a plurality of two dimensional component images each component image corresponding to a different wavelength distribution of the original image. The component images are each independently constructed from a wavelength band or a monochromatic wavelength.

The image decomposition of step is achieved in several ways. A digital image may be separated into a plurality of narrow band color images using digital processing. A photograph may be decomposed using narrow band optical filters wavelength discriminating photo detectors or monochromatic wavelengths. It is appreciated that step is optionally performed in concert with the digitization step or the input stage .

The optical flow O.F. for groups such as pairs of component images synonymously referred to as sub images is then calculated at step . In examples such as where a large sequence of component images exists a three dimensional vector is optionally determined for each point in the image field.

The surface depth profile for each point within the image field is calculated at step from the optical field O.F. analysis at step . The stereo three dimensional image information including information from two dimensional image decompositions and surface depth profile is calculated at step . The stereo information determined at step is optionally output as a single or series of stereo images .

The illumination vectors namely the three dimensional vectors describing intensity and direction of flux at every wavelength and every point on the surface of the stereo image are optionally calculated at step and used to generate illumination distribution direction information as an output at step . The process of steps and are optional image refinements to improve resolution and are typically performed if the stereo images are deemed to provide less information than desired.

The image input is a scanner other imaging device such as a digital camera used to image a printed photograph of an object a camera a data link to a camera used to directly image an object a database of images or a video camera. The image input optionally includes an additional data input such as a keyboard wireless receiver or other data input through which image formation data associated with an image is received. Image formation data illustratively includes imaging device data imaging condition data or other data associated with the image.

The color corrector is optionally used to act to correct the color balance of the input image. This is achieved by an automated process for example an algorithm or by manual balancing of pixels. The color corrector optionally is placed before the image input to act as a filter. It is appreciated that the color corrector may be omitted or combined with the image input for example as a software correction to spectral response which is applied by electronic circuitry within the image input.

The image correction input operates in communication with a database of color correction factors from which a correction factor linked to the identity of the imaging device used to provide the image can be retrieved. The image correction input is optional and provides output refinement as desired.

The image decomposer is implemented entirely in software for example using image processing software. The image decomposer is optionally combined with the image input for example allowing input of component images with different spectral responses for example by filtering the image before input. In this case the image decomposer includes a number of color filters used in association with an image input such as a scanner. It is appreciated that an image may be scanned at different wavelength ranges or with a polychromatic band pass set of wavelengths or a monochromatic wavelength.

Any appropriate method can be used to calculate optical flow including that described in B. K. P. Horn and B. G. Schunk Determining Optical Flow 572 0 27 1980 hereby incorporated by reference. Any appropriate method can be used to calculate surface depth profile from the optical field including the methods described in J. R. A. Torre o and J. L. Fernandes Matching photometric stereo images 15 12 2966 2975 1998 and J. R. A. Torre o Natural Photometric Stereo 95 102 October 1996 both of which are hereby incorporated by reference.

Further discussion of optical flow calculations are given below. A three dimensional data image calculator is an algorithm operating on a computer with the data extracted from a monocular view using a sequence of two dimensional images from which optical flow is calculated. The sequence of images can be generated from a single original image by decomposing the image into multiple component images of different wavelength distributions or the sequence of two dimensional images is also readily formed by imaging an object under different conditions as discussed in more detail below.

As used in the following claims optical flow is defined as the apparent velocity of a localized region on the image and can be calculated from two or more component images of different wavelength. These component images can be formed from the decomposition of a single color image a single black and white image or from multiple images obtained under different illumination conditions.

Optical flow data at a coordinate and an image at a particular wavelength are defined by a pair of equations for the two components 

The optical flow at each image point x and y can be calculated using two consecutive images that can be described as brightness value matrices. The brightness element is defined so that its brightness is constant in wavelength. The rate that brightness changes with respect to wavelength is zero which means

The chain rule of differentiation applies to the rate of changing brightness. A continuity condition can be derived from the chain rule and expressed by 

The condition of continuity is a constraint necessary to specify a unique solution to the surface depth. However condition of continuity is insufficient to calculate a unique solution to the optical flow. The perceived direction of optical flow will always be in the direction parallel to the gradient of brightness. If the optical flow component that is parallel to the brightness gradient in the xy plane is designated as right arrow over D where then the continuity condition can be shown to be equivalent to

In a photometric stereo method at least two images are used to calculate the surface depth value designated z of a perceived three dimensional object. The slopes of the surface depth on the perceived object are then defined by p and q where

The components p and q are proportional to Dcircumflex over n Dcircumflex over n which is the magnitude of the optical flow. The component of optical flow right arrow over D that is perpendicular to the brightness gradient right arrow over E cannot be determined without another constraint. Constraints are necessary to determine a unique optical flow which is necessary to calculate a unique surface depth function. Both analytical and numerical algorithms are available for calculating the optical flow.

For example methods of calculating optical flow were developed by Horn and Schunk. See Horn Schunk Determining Optical Flow MIT Artificial Intelligence Laboratory Memo 572 0 27 1980 . The constraint used in this example was the smoothness constraint which limits the motion of the image in a way that the image can be visualized as sliding on the surface of the object being imaged.

The equations for optical flow can solved using numerical methods for example described below. To digitize E D and D integer values are assigned to x y and so x j y i and k where i j k 0 . . . N. The local partial derivatives of right arrow over E are averaged for a cube of adjacent values of i j and k by

The optical flow Dand D can be calculated from the averaged derivatives by an iterative algorithm for example one described by Horn and Schunk. Below the terms Dand Dare reassigned as u and v to match the nomenclature of that article. If

The values of uand vdo not depend on uand v but do depend on their averaged local values and calculated by the following weighted averaging formulas 

If Eis for each k an Nby Nmatrix i.e. i 0 1 2 3 Nand j 0 1 2 3 . . . N then the optical flow at each k is an Nby Nmatrix.

The initial values n 0 of the optical flow components can be chosen as zero 0 although a better initial value may facilitate convergence. The constants and 1 12 were chosen to optimize convergence using a cube of local values determined by the two images at different wavelengths designated by integers k and k 1.

Optical flow techniques can be used to determine three dimensional data from two dimensional image data including archived images.

A preferred embodiment of the present invention allows three dimensional data to be determined from a single view direction an approach which may be termed photometric stereo imaging. In other embodiments several wavelengths at different illumination angles may be collected.

Photometric stereo imaging typically uses natural sources of light such as sunlight or starlight which are usually multi wavelength. Electromagnetic radiation EMR that is scattered from the atmosphere provides different angular distributions for different wavelengths which can be used to acquire stereo information i.e. three dimensional data . A wavelength distribution of directions characterizes EMR sources for photometric stereo imaging.

Both the angular distribution of intensity at different wavelengths and the atmospheric attenuation have a dependence on atmospheric conditions that may impact a photometric imaging application.

The unaided eye partially uses photometric stereo methods of depth profiling in addition to binocular stereo methods to acquire depth perception. Realistic presentation of images as perceived by the unaided eye is needed in visualization software and other applications. A surface depth profile can be determined using the optical flow approach discussed herein.

The atmosphere modifies illumination of an object and directly attenuates the reflected EMR differently at different wavelengths. Effective angles of illumination for two different wavelengths 400 nm and 700 nm can be calculated from published experimental data.

The utility of photometric stereo was evaluated at different ground albedos and directions of the sun for the atmospheric conditions of the published data. The size of the error caused by wavelength dependent scattering is estimated and the validity of the equations present is also discussed for attenuation that is strongly dependent on wavelength.

Illumination angle distributions that change with wavelength have effective viewing angles that can be calculated as functions of wavelength. An example using radiance profiles from atmospherically scattered EMR at wavelengths 400 nm and 700 nm is shown sufficient to calculate effective viewing directions. Although the example shown below is for a specific set of atmospheric conditions variations in angle distributions and the resulting photometric stereo effects likewise vary with other atmospheric conditions.

A method of determining surface depth profile is now described assuming that the images come from two wavelengths illuminating the three dimensional object in two different directions. For example images may correspond to illumination at different times of day. The angular and wavelength distribution in the atmosphere allows EMR electromagnetic radiation scattered in the atmosphere to provide illumination that can be used in photometric imaging.

The surface depth profile of a material in the atmosphere can be calculated using optical flow from the brightness values of two flat images acquired through the atmosphere.

The surface depth profile is extracted from optical quantities other than the angle of the point of view. The surface depth is also optionally extracted from the differential polarization. Polarimetric stereo imaging can also extract stereo information from an object. However most natural sources of EMR are unpolarized and the degree of polarization after emission is de minimus.

Natural EMR usually has a broad wavelength spectrum. Furthermore small increments in wavelength are easier to distinguish than small increments in polarization angle. Therefore photometric stereo imaging is often superior to polarimetric imaging when the polarization background or random noise is large.

The smoothness condition and the continuity condition are together sufficient for specifying the solution to the surface depth profile. The constraint of smoothness can be visualized as the image sliding over the surface of the object being imaged. The constraint on smoothness is mathematically described as the object from one wavelength to the other being transformed by a local rotation by a small angle followed by a translation in a direction perpendicular to the axis of the image which is mathematically expressed in the smoothness transformation where right arrow over and

right arrow over R is the position of a point on the three dimensional object being image the unit vector circumflex over n is a displacement of the two dimensional image perpendicular to the image plane and both V and Dare correction factors that do not directly affect the orthographic projection i.e. the parameters that determine the surface depth profile .of optical flow. The z axis component of the coordinate system origin relative to the image plane determines both Dand V although Dand V are not part of the surface depth profile. To be a true rotation the magnitudes of the two intensities should be equal and can be expressed as right arrow over R right arrow over R right arrow over R which is only valid if right arrow over is small i.e. right arrow over 

The unit vector in the direction of the gradient of S is designated by the normal vector circumflex over n 

The smoothness condition where the image is sliding over the surface of an object as a function of wavelength is mathematically described by the optical flow being orthogonal to the unit vector normal to a surface of the object. A smoothness equation that mathematically describes this condition is right arrow over 0

An analytic solution to the surface depth profile was derived using the smoothness constraint. The equations for finding the surface depth with the constraint for values of xcircumflex over n ycircumflex over n 0circumflex over n is

If V is unknown the analytical solution for z requires as input data A and B. The analytical solution is also sensitive to computational noise and is especially difficult to calculate for 0 Bx Ay.

The equations presented up to the smoothness equation are sufficient to calculate the surface depth profile z if right arrow over known. There are four scalar unknowns A B C V and only three linear equations defined by the transformation so that the position of a point on the object is not completely specified by the two conditions continuity and smoothness described so far. The actual z axis component of a the position point on the object is given by z where where zis an arbitrary real number. The constant zdoes not affect the stereo image shape which is characterized by right arrow over R zcircumflex over n where rather than right arrow over R that contains both object shape and object position information.

The arbitrary constant zdoes specify the values of V D and right arrow over . Therefore V is a function of z. However zcan be chosen so that if z z i.e. z 0 then V 0 which when substituted into the smoothness equation determines the z component of optical flow as a function of the other components. Therefore if zis chosen so that V V z 0 and z

When V and Dare substituted into the smoothness transformation a cross product formula results as expressed by for V z 0 and z

The value of zwhere V 0 characterizes the distance between viewer and object. The vector right arrow over at this zcharacterizes the illumination directions on the object. There are three scalar unknowns and three linear equations equivalent to the cross product shown for this choice of z. If these equations are not singular then there is a unique solution to right arrow over .

The illumination directions are characterized by the rotation vector right arrow over which is determined by the equation of motion for the image where the circumflex over n component is now known. The vector right arrow over that determines effective illumination angles for each wavelength can be calculated after the surface depth profile is specified. The cross product can be expanded out in vector for 

This equation has three unknowns A B C and three independent equations. The solution to right arrow over is given by 

The equation for z for xcircumflex over n ycircumflex over n 0circumflex over n discussed previously is singular for Bx Ay 0 so that the function z does not have to be smoothly differentiable at Bx Ay 0. However the limit as Bx Ay can be found by substituting A B and C by their solution equations in Bx Ay 0. Substitution of the equations for A B C and Dp Dq into Bx Ay 0 results in either E D D 0 or x y. In either case 

A special case of this limit is when E D D 0 and A B under which the surface is flat i.e. lim z 0 at the point designated by xcircumflex over n ycircumflex over n .

A B and C characterize the directions of illumination. The mathematical method of analyzing photometric stereo is capable of extracting both the surface depth profile and the effective angles of illumination but not the distance between object and viewer.

Finite difference equations for z have been developed as part of the present invention from the differential equations described so far and are less sensitive to computational noise than conventional differential equations. The condition of smoothness implies that after digitization there will not be large differences in the change of objects as a function of light wavelength or that a normalization procedure has eliminated large differences.

Digitizing these continuum equations results in finite difference equations with series that converge to the analytical quantities that relate to stereo imaging. The following iterative equation converges to the analytical solution of surface depth profile z. The iteration of step n 0 is calculated using the surface profile series that can be written as 

The zterm can be evaluated only if the local averages of p and q are calculated. The local average derivatives of the surface depth i.e. p and q can be calculated at each step n of the iteration by

The illumination directions are characterized by the rotation vector right arrow over which is determined by the equation of motion for the image where the circumflex over n component is now known. Once x y and z D and Dare known then solutions to A B and C can be calculated from the cross product equation.

Atmospheric calculations can be performed to evaluate photometric stereo imaging under different conditions. Atmospheric conditions affect visualization of stereo images illuminated by the sun or other atmospheric influenced sources. Illumination of an object by two wavelengths at different angles but viewed from a single direction can be used to generate three dimensional stereo of the object using surface depth profiles. Photometric stereo is useful in passive techniques because of the angular distribution of atmospherically scattered radiation. The radiance of electromagnetic radiation EMR scattered by atmospheric particles i.e. the sky radiance does not have a uniform spectrum. The spectrum changes with the direction of the sun relative to the zenith and the direction of view relative to the zenith. This changing spectrum causes some of the three dimensional perspective seen. However one of the most important factors influencing illumination is the angle of the sun that varies during the day.

The effective angle is shown to provide the stereo information can be obtained from two wavelengths in the example shown in . An object tank is imaged under two conditions with the sun on the horizon and at zenith. Imaging occurs at two wavelengths 400 nm and 700 nm. In other examples other wavelengths or sun positions can be used.

The radiance of EMR scattered from the atmosphere is designated L x A t where xis cos where is the polar angle of the view relative to the zenith A is the albedo of the earth s surface and tis the time either when the sun is at the zenith t 0 or the time t 1 that the sun is on the horizon. The radiance scattered from the particles in the atmosphere as a function of angle of the point of view relative to the zenith is published in Driscoll and Vaughan 

An image of an object can be obtained for each wavelength. In this example the viewer in the analysis is facing the object from which the surface depth profile will be determined. In the cases where the sun on the horizon the viewer is facing opposite from the sun i.e. antisolar . The directions of illumination at each wavelength occur at the peaks of L x A t where the EMR is strongest. The peaks of L x A t i.e. angle of illumination are determined by A and t.

The different cases provide different types of stereo information. When the sun is at the zenith t 1 for both A 0 and A 0.8 the antisolar maximum Loccurs at cos 0 for wavelength 700 nm and at cos 0.8 at wavelength 400 nm. Furthermore the intensity at 400 nm and 700 nm are almost equal. Therefore the effective angles from which the object are illuminated is 90 at 700 nm and 37 at wavelength 400 nm. The difference between angles is 53 . The angle between the two effective illumination points is 53 . When the sun is on the horizon t 0 there is more than one peak for each xand A. Since the viewer is looking in the antisolar direction there are broad peaks at 0 and 37 occur for wavelength 400 nm and two peaks 0 and 90 occur at 400 nm. Since the 0 occurs at both wavelengths the stereo perspective is mainly provided at 37 at 400 nm and 90 for 700 nm. Although these are the same angles as in the sun at zenith case the peak at 400 nm is now broader. Therefore the surface depth profile will not be as clearly resolved as for the zenith sun case. For the case where the ground albedo is 0.8 case and the sun is on the horizon i.e. t 1 there is no clearly defined peak radiance for wavelength 400 nm. The radiance at wavelength 700 nm has a well defined peak at 0 but there is no definite illumination direction for 400 nm. Therefore the analysis suggests that the photometric stereo effect won t be effective under this condition.

The atmosphere also attenuates the EMR that is reflected off the object due to scattering and absorption. The attenuation from both scattering and absorption is called extinction. If the extinction does not change from with wavelength then the images won t be substantially different from the zero extinction case. The surface depth profile can be calculated by the methods shown and will be the same as without extinction except for random noise. However extinction is shown to add an error to the surface depth profile if the attenuation varies strongly with wavelength. At long distances the extinction from scattering is strong enough. The error increases with the wavelength derivative of the extinction. If the error is large enough the iterative methods described may not converge and the surface depth profile will be undefined.

Wavelength dependent extinction is shown to cause an error in the surface depth profile if the variation with wavelength is large. This can be easily seen for the special case of a very large difference in extinction coefficients where the intensity at one wavelength is attenuated to zero and the intensity of the other wavelength is still large. Then there is no second direction of illumination and the stereo effect is not significant. In this case the surface depth profile cannot be calculated accurately. Even in a less extreme example there is an error caused by a changing wavelength dependence that can be calculated. Wavelength dependent extinction varies with atmospheric conditions. Atmospheric extinction and scattering can be used under some atmospheric conditions to estimate the distance to a far 1000 m object although extinction can t be used to estimate surface depth profile if the features do not extend a comparable distance 1000 m . Formulas for the error are presented herein.

Extinction caused error is estimated and limits of validity for photometric stereo are analyzed in this section. The surface depth analysis is insensitive to extinction that is totally independent of wavelength. Factors that aren t x or y dependent will not impact the calculation. Therefore these x or y independent factors are normalized here to one. Under the conditions of this analysis the intensity at a viewing point right arrow over R of a brightness element at point right arrow over R is exp where E is the brightness at a brightness element right arrow over R is the position of the brightness element E is the intensity at position right arrow over R is the extinction coefficient and is the wavelength. See also Y. Yu and Jitendra Malik Recovering photometric properties of architectural objects from photographs Proc. 25Ann. Conf. on Computer Graphics and Interactive Techniques 207 217 1998 hereby incorporated by reference.

The optical flow without extinction is where is the component of optical flow in an image plane i.e. an xy plane 

However the optical flow will have an error caused by wavelength dependent extinction. The optical flow with wavelength dependent extinction is defined as 

The direction of the flow is still the direction of the gradient of E and so remains unchanged. Therefore the error introduced into the optical flow by the extinction is the extinction error f

If the extinction coefficient is independent of wavelength then the error is zero right arrow over R 0 . However the larger the change in extinction 

The x and y components of flow are proportional to the optical flow magnitude so that the optical flow modified by extinction are D and D as given by two equations which are

The smoothness constraint no longer applies if the error is large. This can be shown using the equation of motion where the constant zis chosen where V has been minimized i.e. coordinates where V 0 which is equivalent to a rotation of the illumination direction around the right arrow over axis by approximately right arrow over rad for small angles right arrow over 

A visualization analysis using atmospheric scattering can be used to extract stereo information at distances longer than possible with narrow view binocular stereo imaging. The impact of the atmosphere on photometric stereo images occurs both by providing or not providing the illumination in passive imaging and by extinction where each wavelength attenuates differently.

The angular distribution of wavelength sometimes does not significantly change with wavelength such as under dense fog conditions. An estimate of where extinction is significant is made using the common observation that extremely distant objects 1000 meters m under clear conditions have a bluish tint caused by differential attenuation and atmospheric scattering. The atmosphere scatters blue light wavelength 500 nanometers nm more than red light wavelength 600 nm so that with distance the image grows weaker due to extinction while the scattered path radiance increases. This effect called aerial perspective may cause a significant error when the distance is large enough here 1000 m and the wavelengths extend over a wide range here 500 600 nm . A correction for aerial perspective can be made to eliminate this error.

However the correction for aerial perspective can also be used to measure the distance to an object which is equivalent to z. Therefore photometric imaging with atmospheric corrections for extinction could provide distance z and shape e.g. surface depth profile of the object being viewed.

Photometric stereo imaging only requires a sequence of two dimensional images at different wavelengths. These images can be component images from decomposition of a color image into component images of different wavelength ranges or may be images taken under different illumination conditions. Digital imagers can easily be modified to record such data. Mathematical algorithms for extract surface depth profiles and effective angles of illumination can be programmed into image processing software.

Natural light sources especially atmospherically scattered light have angular distributions of wavelength. Natural light is generally only weakly polarized so that photometric stereo is particularly valuable where polarimetric stereo methods are not useful.

For the sun on the horizon the angular distribution of EMR varies with wavelength as often seen by the unaided eye at sunset or sunrise. The two wavelengths are scattered from the atmosphere at distinguishable view angles which is a condition for photometric stereo imaging. The stereo effect is less effective with the sun at zenith high noon because the angular distribution varies with wavelength than when the sun is on the horizon.

The effect of a nonzero albedo was to decrease the effectiveness of the photometric stereo still further. The loss in photometric stereo effect is caused by radiation diffusely reflected from the ground into the atmosphere which scatters some of the EMR back to the ground with an even broader angular distribution than radiation had before ground reflection. Therefore a high albedo therefore creates an angular distribution that varies less with wavelength than a low albedo.

A disparity based algorithm such as described herein for calculation of surface depth profiles can also be used in other situations where stereo information is needed but only one direction viewpoint is available.

The effect of dispersive wavelength dependent extinction is expressed in equations presented here. The error caused by dispersive extinction is proportional to the rate of change 

Photometric stereo imaging can be used to obtain a stereo image from a single view using images or component images decomposed from a single color image of different wavelengths. Surface depth functions are extractable from a single view through the atmosphere using EMR scattered from atmospheric particles in a natural atmosphere. The solution to the example equations used for finding the surface depth use both an equation of continuity constraint and a smoothness constraint. A smoothness constraint can be visualized as the image is sliding on the surface of the object.

The atmosphere affects performance of photometric stereo both by providing natural illumination for passive stereo viewing and by attenuating EMR at different wavelengths. Natural illumination from atmospheric scattering provides the illumination for one approach to passive photometric stereo imaging using images under different illumination conditions because the angular distribution of EMR varies with wavelength. Effective angles can be calculated from published data on downwelling radiance for different times of day. Extinction effects are potentially significant if there is a large change in object brightness as a function of wavelength. Very large changes caused by extinction may invalidate the smoothness condition used in the calculation of surface depth.

Atmospheric effects on stereo images may further be included in visualization software for accurate representation of imaging.

Photometric stereo imaging can be used to extract surface depth profiles and illumination directions. The distance between the viewer and an object may be determined using any suitable technique. The calculation of optical flow from a sequence of two dimensional images generated from a single color image or other images representing different wavelength ranges allows extraction of stereo information.

Photometric techniques can further be used in real time to guide automatic and robotic devices both under natural lighting and in cases where the surroundings are illuminated with a controlled distribution of angles and wavelengths.

As used herein the terminology signal processing circuitry includes a computer processor microprocessor multiprocessor controller mainframe or a plurality of computers processors microprocessors multiprocessors controller or mainframes or equivalents thereof.

As used herein the terminology object may include a thing a person animate or inanimate subject a plurality of objects the ground ground covering e.g. grass a localized piece or pieces of the environment surface physical entity or entities or anything that has photoemission.

The invention is not restricted to the illustrative examples described above. Examples are not intended as limitations on the scope of the invention. Methods apparatus compositions and the like described herein are exemplary and not intended as limitations on the scope of the invention. Changes therein and other uses will occur to those skilled in the art. The scope of the invention is defined by the scope of the claims. Patents patent applications or publications mentioned in this specification are incorporated herein by reference to the same extent as if each individual document was specifically and individually indicated to be incorporated by reference.

