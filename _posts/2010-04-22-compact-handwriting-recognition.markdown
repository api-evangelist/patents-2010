---

title: Compact handwriting recognition
abstract: One or more techniques and/or systems are disclosed for constructing a compact handwriting character classifier. A precision constrained Gaussian model (PCGM) based handwriting classifier is trained by estimating parameters for the PCGM under minimum classification error (MCE) criterion, such as by using a computer-based processor. The estimated parameters of the trained PCGM classifier are compressed using split vector quantization (VQ) (e.g., and in some embodiments, scalar quantization) to compact the handwriting recognizer in computer-based memory.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08369611&OS=08369611&RS=08369611
owner: Microsoft Corporation
number: 08369611
owner_city: Redmond
owner_country: US
publication_date: 20100422
---
Computer based handwriting recognition systems can be used for characters that are written directly onto a touch sensitive input e.g. screen and or for characters scanned from a written document. When characters are written onto a screen for example to be recognized e.g. and digitized it is often referred to as online recognition. East Asian written languages such as Chinese Japanese and Korean can comprise thousands of characters. Recognition systems usually comprise a character classifier that compares the written unknown sample against a trained model. Users of the handwriting recognizers prefer that it yields accurate results however as a number of potential characters increases memory requirements for storing such a classifier also increase.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key factors or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

Currently classifiers often use a modified quadratic discriminant function MQDF model. Typically when using the MQDF model the classifier assumes that the feature vectors of each character class can be modeled by a Gaussian distribution with a mean vector and a full covariance matrix. In order to achieve reasonably high recognition accuracy e.g. an effective classifier a large enough number of leading eigenvectors of the covariance matrix have to be stored. This requires a significant amount of memory to store the relevant model parameters. Further when using the classifier for EA written languages even more memory may be required due to the thousands of potential characters.

Typically handwriting recognizers that yield higher accuracy require more memory. However handwriting recognizers are often implemented on mobile devices. As a result recognition accuracy can be dramatically reduced when implementing an MQDF based recognizer in a computing device such as a mobile device that has limited memory.

One or more techniques and or systems are disclosed that provide for improved accuracy while mitigating an amount of memory used to store the recognizer. That is for example a handwriting recognizer may be constructed that uses less memory while providing improved recognition accuracy by using an alternate to the MQDF approach.

In one embodiment for constructing a compact handwriting character classifier a precision constrained Gaussian model PCGM based handwriting classifier such as for use for EA written languages is trained by estimating parameters for the PCGM under minimum classification error MCE criterion. Further the estimated parameters for the PCGM can then be compressed for storage using split vector quantization VQ thereby compacting the handwriting recognizer for example in memory.

To the accomplishment of the foregoing and related ends the following description and annexed drawings set forth certain illustrative aspects and implementations. These are indicative of but a few of the various ways in which one or more aspects may be employed. Other aspects advantages and novel features of the disclosure will become apparent from the following detailed description when considered in conjunction with the annexed drawings.

The claimed subject matter is now described with reference to the drawings wherein like reference numerals are used to refer to like elements throughout. In the following description for purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the claimed subject matter. It may be evident however that the claimed subject matter may be practiced without these specific details. In other instances structures and devices are shown in block diagram form in order to facilitate describing the claimed subject matter.

As an example a handwritten character may be entered using an input device such as a touch screen digitizer and stylus . A recognizer application resident on the device receives the ink data e.g. comprising the strokes and timing known as trajectory data from which features are extracted from the unknown characters . Typically a trained handwriting recognizer matches the unknown characters features to features of known characters e.g. stored as classification data from training . The recognizer may calculate similarity scores for each known character and a decision rule can select the character class having a highest score for example and output it as a recognized character. Similarly the recognizer may output more than one character depending on the application and settings such as by creating a list of most likely characters.

A method may be devised that facilitates development of a compact recognizer of handwriting. For example because eastern Asian EA written languages such as Chinese Japanese and Korean can comprise thousands of unique characters effective handwriting recognizers typically require large amounts of memory for storage e.g. flash and processing e.g. RAM . Due to this memory limitation mobile handwriting recognizers that have high e.g. effective recognition rates particularly for EA written languages are limited.

As an example in the PCGM the feature vectors of respective character classes Ccan follow a Gaussian distribution e.g. a normal distribution p x C N x where mean has no constraint imposed while precision matrix P lies in a subspace spanned by a set of basis matrices e.g. prototypes which may be denoted as S l 1 . . . L which are shared by the character classes. Consequently in this example the precision matrix Pcan be written as 

In one embodiment in order to train the PCGM based handwriting classifier parameters e.g. settings of the model are estimated. That is for example the model can typically yield observations but the settings are unknown. As an example a set of PCGM parameters can comprise a subset of tied parameters and a subset of untied parameters j 1 . . . M where . . . and M is a number of character classes. In one embodiment a total number of parameters of the PCGMs can be far less than that of other models used for classification e.g. modified quadratic discriminant function MQDF .

In the exemplary embodiment of the parameters are estimated under MCE criterion. That is for example MCE can be used in pattern classification systems where an aim of MCE training is to minimize resulting classification error when attempting to classify a new data set against a statistical model. Here in this embodiment the statistical model used to describe the data is the PCGM and the data set can comprise a set of training data such as labeled known characters from different character classes.

In the exemplary method of at the parameters of the trained PCGM classifier are compressed using split vector quantization VQ . That is for example the estimated parameters for the PCGM are compacted and stored in computer based memory in order to reduce the handwriting recognizer. In this way in this example the reduced total number of parameters of the PCGMs when compared with other models combined with the parameter compression can provide an effective handwriting classifier that has a small enough footprint to be used in mobile devices.

As an example VQ is a lossy compression method based on block coding that uses a fixed to fixed length algorithm to compress data. In one embodiment a Linde Buzo and Gray LBG training sequence can be used to design a VQ for data where the training data comprises source vectors such as from the training data described above. As a further example the split VQ can comprise splitting a set of parameters into subsets then performing the LBG VQ compression on the subsets.

In the exemplary method of having compressed the parameters of the trained PCGM classifier the exemplary method ends at .

For example a discriminant function of a PCGM can be derived from the following log likelihood function for an unknown feature vector x as 

In one embodiment given the discriminant function and decision rule described above the misclassification measure for respective training samples xcan be defined as where

At another training sample is analyzed for example until all of the training samples have been analyzed in the loop .

At for respective misclassification measures from the training samples an empirical loss function is defined. In one embodiment given the misclassification measure described above the following empirical loss function l x can be defined as 

In this embodiment the PCGM parameters can then be estimated by minimizing the empirical loss function starting at . In one embodiment the Quickprop algorithm can be used to minimize the empirical loss function as demonstrated in the acts of the exemplary embodiment . In this embodiment the training can comprise transforming merely mean vectors for the PCGM. In one embodiment starting with maximum likelihood ML trained seed PCGM parameters as initialized at a Quickprop iterative procedure can be used to fine tune merely the mean vectors for the training sample.

At a first derivative of the empirical loss function is calculated such as by calculating the derivative of l X with respect to each e.g. where is the d th element of as follows 

At the mean vector parameter e.g. the d th element of the mean vectors can be updated using the first derivative such as 

At given the updated mean vectors in the previous iteration first and second derivatives of the empirical loss function are calculated for respective mean vectors .

At mean vectors are updated using a combination of first derivative and or second derivative and or a learning rate as where denotes the update act of . In one embodiment if the second derivative is greater than zero and a sign of gradient of the first derivative in the current iteration is different from a sign of the first derivative in the previous iteration that is for example if

In another embodiment if the second derivative is greater than zero and the sign of gradient of the first derivative in the current iteration is the same as the sign of gradient of the first derivative in the previous iteration that is for example if

In one embodiment a size of the update act for the mean can be limited by a specified control parameter.

At a decision is made whether the acts aren repeated T 1 times where Tis a total number of iterations to be performed.

Having transformed the mean vectors for the training samples for the PCGM the estimated PCGM parameters are derived for the trained model.

In one aspect in order to implement a PCGM based recognizer that has been trained under MCE criterion particular parameters are stored for use by the recognizer. As described above one of the limitations of mobile handwriting recognizers particularly those used for EA handwritten characters is that memory of a mobile device is limited. Therefore reducing an amount of parameters stored is facilitated by using the PCGM. Further in this aspect compressing those parameters that are stored will also help reduce memory needs.

In one embodiment a modified version of the discriminant function described above e.g. modified for more efficient evaluation can be used such as 

In this embodiment using this discriminant function three sets of parameters are stored to implement the PCGM based recognizer a set of transformed mean vectors e.g. determined in and constants m c for example comprising a total of D 1 M raw parameters a set of coefficients for example a total of L M raw parameters and a set of prototypes for example a total of D D 1 L 2 raw parameters. As an example if a 4 byte floating point number is used to represent the respective raw parameters a total memory requirement is about 4 D L 1 M 4 D D 1 L 2 bytes. This may translate to about 2.83 MB for a typical system setup of D 128 L 32 M 3000 for example.

To compress the PCGM parameters described in this embodiment a split vector quantization VQ technique can be used. However in one embodiment parameters of the PCGM can also be compressed using scalar quantization. is a series of three flow diagrams and illustrating exemplary embodiments of compressing parameters for a PCGM. The flow diagrams and illustrate exemplary embodiments where the split VQ technique is used to compress the parameters and the flow diagram illustrates an exemplary embodiment where scalar quantization is used to compress parameters.

In the flow diagram the mean vectors m are compressed. At respective transformed mean vectors m R can be split into two or more streams of sub vectors for example uniformly split into QD dimensional sub vectors e.g. D Q D . At the respective sets of sub vectors from a stream can be grouped into clusters in one embodiment using the Linde Buzo Gray LBG algorithm. The LBG algorithm is a vector quantization algorithm used in split VQ to efficiently generate a codebook for the clustered data.

As an example for each q 1 . . . Q the LBG algorithm can be used to group the set of sub vectors m j 1 . . . M into 256 clusters with Euclidean distance as the distortion measure where mis the q th sub vector of m. In one embodiment after clustering the sub vectors the respective clusters comprise a centroid e.g. the mean of the sub vectors in the corresponding cluster . In this embodiment the parameter vector e.g. the mean vector can be represented by multiple indices which respectively correspond to an index of a codeword e.g. centroid to which a corresponding sub vector is quantized e.g. clustered .

For example respective sub vectors m e.g. which have been quantized to clusters can be represented by an index of their nearest centroid e.g. for the cluster to which they have been clustered . Further in this embodiment respective centroids e.g. codewords which can be a D dimensional vector for example may be represented by D4 byte floating point numbers. Additionally in this example the respective index for the codeword can be represented as a single byte unsigned integer. In this example a total of Q M bytes may be used to store the indices and 4 D 256 bytes to store the codebook comprising the codewords e.g. centroids .

Returning to at the index and codebook for the mean vectors that have been compressed can be stored in memory such as to be used by the PCGM based handwriting recognizer. In this way the parameters for the trained PCGM have been compressed in memory such as flash memory on a mobile device and can be used for handwriting recognition. Further in one embodiment the constants ccan also be quantized and compressed in a manner similar to that described above. Further in one embodiment the set of character dependent constants ccan be compressed using scalar quantization. However for example storing the constants cmerely uses several kilobytes so compression may not always be used.

In the flow diagram of coefficients for example can be compressed using the split VQ technique. The flow diagram follows a same sequence of acts as described above which illustrates how the coefficients can be split into sub vectors grouped into clusters and stored in memory as indices and codebooks. As an example the coefficients can be split uniformly . . . into QD dimensional sub vectors e.g. L Q D . Further for each q 1 . . . Q a set of sub vectors j 1 . . . M can be grouped into 256 clusters where is the q th sub vector of . Therefore for example merely Q M 4 L 256 bytes may be needed to store the coefficients.

The flow diagram of illustrates an exemplary embodiment of compressing parameters using scalar quantization. Scalar quantization can approximate a range of values e.g. a set of possible discrete values using a small set of discrete symbols or values. For example the prototypes S . . . S can comprise symmetrical basis matrices for the PCGM. For each prototype S at because they are symmetric merely diagonal and upper diagonal items from the matrix can be stored to represent the prototype.

As an example diagonal items in the basis matrix of the prototype may reflect an auto correlation of elements in the feature vector and can have a dynamic range significantly different from that of upper diagonal items. Therefore at the diagonal items from a set of character dependent symmetrical basis matrices are stored for example each represented by a 4 byte floating point number. Further at the upper diagonal items from the set of character dependent symmetrical basis matrices are compressed using scalar quantization. For example the upper diagonal items can be quantized using an 8 bit scalar quantization. As an example storing the prototypes in this manner utilizes merely D D 1 L 2 256 4 D L 4 bytes of memory. At the index and codebook created by the quantization can be stored in memory.

An alternate method may be devised for constructing a compact handwriting recognizer for example by using compression of parameters and a precision constrained Gaussian model PCGM . is a flow diagram of an exemplary method for creating a compact recognizer. The exemplary method begins at and involves compressing character independent basis parameters of a maximum likelihood ML trained precision constrained Gaussian model PCGM using scalar quantization at .

ML training of statistical models merely provides parameters for the model that will yield data that is more likely to be correct than other parameter values may provide. In other words a mean determined from a sample sub set of all values if it has a normal distribution may be a maximum likelihood estimator for the mean of the set of all the values. In this embodiment the basis parameters which are character independent of the ML trained PCGM are compressed using scalar quantization as described above.

At character dependent mean vector parameters of the ML trained PCGM are transformed by training the PCGM under minimum classification error MCE criterion using the compressed character independent basis matrices parameters. For example as described above in the mean vector parameters of the PCGM are transformed during MCE training. That is the misclassification measure can be defined for respective training samples using a discriminant function of the PCGM and a decision rule and the Quickprop algorithm can be used to transform the mean vectors for the PCGM starting with the ML trained PCGM parameters.

At the transformed mean vectors can be compressed using the split VQ technique as described above for example in . For example the mean vectors can be split into a plurality streams of sub vectors and the sets of sub vectors from each stream can be grouped into clusters. In this example the mean vector can be represented by multiple indices which correspond to an index of a codeword e.g. centroid to which the corresponding sub vector has been quantized e.g. clustered . Further the respective codewords can be organized in codebooks for each stream for example and the codebooks and indices can be stored in computer memory for use as a PCGM based handwriting recognizer.

In one aspect if all the model parameters are updated by MCE training model compression can be performed after completion of MCE training. However in one embodiment because merely the mean vectors are updated in the MCE training the MCE training and the model parameter compression can be combined. In this embodiment character dependent coefficients for the ML trained PCGM can be compressed using split VQ for example such as the coefficients as described above in . Further in one embodiment a set of character dependent constants for the ML trained PCGM can be compressed using scalar quantization such as the prototypes was described above in .

Further the character dependent mean vector parameters of the ML trained PCGM are transformed by training the PCGM under minimum classification error MCE criterion using the compressed character independent basis matrices parameters and character dependent coefficients. For example as shown in the mean vector parameters of the PCGM can be transformed during MCE training. That is the misclassification measure can be defined for respective training samples using a discriminant function of the PCGM and a decision rule and the Quickprop algorithm can be used to transform the mean vectors for the PCGM starting with the ML trained PCGM parameters.

Further in this aspect in one embodiment the MCE training can be used to fine tune the mean vectors where a compressed precision matrix is used to calculate a discriminant function for the PCGM. For example the MCE training can be invoked to fine tune the mean vectors where the compressed precision matrix

Additionally in this embodiment the transformed mean vectors can be compressed using the split VQ technique as described above for example in . For example the mean vectors can be split into a plurality streams of sub vectors and the sets of sub vectors from each stream can be grouped into clusters. In this example the mean vector can be represented by multiple indices which correspond to an index of a codeword e.g. centroid to which the corresponding sub vector has been quantized e.g. clustered . Further the respective codewords can be organized in codebooks for each stream for example and the codebooks and indices can be stored in computer memory for use as a PCGM based handwriting recognizer.

Further in one embodiment the character dependent constants can be updated using values of mean vectors before and after MCE training by c c circumflex over P circumflex over P where and denote values of the mean vector before and after MCE training respectively. They can be further compressed using scalar quantization.

Still another embodiment involves a computer readable medium comprising processor executable instructions configured to implement one or more of the techniques presented herein. An exemplary computer readable medium that may be devised in these ways is illustrated in wherein the implementation comprises a computer readable medium e.g. a CD R DVD R or a platter of a hard disk drive on which is encoded computer readable data . This computer readable data in turn comprises a set of computer instructions configured to operate according to one or more of the principles set forth herein. In one such embodiment the processor executable instructions may be configured to perform a method such as the exemplary method of or exemplary method of for example. Many such computer readable media may be devised by those of ordinary skill in the art that are configured to operate in accordance with the techniques presented herein.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

As used in this application the terms component module system interface and the like are generally intended to refer to a computer related entity either hardware a combination of hardware and software software or software in execution. For example a component may be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer. By way of illustration both an application running on a controller and the controller can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers.

Furthermore the claimed subject matter may be implemented as a method apparatus or article of manufacture using standard programming and or engineering techniques to produce software firmware hardware or any combination thereof to control a computer to implement the disclosed subject matter. The term article of manufacture as used herein is intended to encompass a computer program accessible from any computer readable device carrier or media. Of course those skilled in the art will recognize many modifications may be made to this configuration without departing from the scope or spirit of the claimed subject matter.

Although not required embodiments are described in the general context of computer readable instructions being executed by one or more computing devices. Computer readable instructions may be distributed via computer readable media discussed below . Computer readable instructions may be implemented as program modules such as functions objects Application Programming Interfaces APIs data structures and the like that perform particular tasks or implement particular abstract data types. Typically the functionality of the computer readable instructions may be combined or distributed as desired in various environments.

In other embodiments device may include additional features and or functionality. For example device may also include additional storage e.g. removable and or non removable including but not limited to magnetic storage optical storage and the like. Such additional storage is illustrated in by storage . In one embodiment computer readable instructions to implement one or more embodiments provided herein may be in storage . Storage may also store other computer readable instructions to implement an operating system an application program and the like. Computer readable instructions may be loaded in memory for execution by processing unit for example.

The term computer readable media as used herein includes computer storage media. Computer storage media includes volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions or other data. Memory and storage are examples of computer storage media. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM Digital Versatile Disks DVDs or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by device . Any such computer storage media may be part of device .

Device may also include communication connection s that allows device to communicate with other devices. Communication connection s may include but is not limited to a modem a Network Interface Card NIC an integrated network interface a radio frequency transmitter receiver an infrared port a USB connection or other interfaces for connecting computing device to other computing devices. Communication connection s may include a wired connection or a wireless connection. Communication connection s may transmit and or receive communication media.

The term computer readable media may include communication media. Communication media typically embodies computer readable instructions or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal may include a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.

Device may include input device s such as keyboard mouse pen voice input device touch input device infrared cameras video input devices and or any other input device. Output device s such as one or more displays speakers printers and or any other output device may also be included in device . Input device s and output device s may be connected to device via a wired connection wireless connection or any combination thereof. In one embodiment an input device or an output device from another computing device may be used as input device s or output device s for computing device .

Components of computing device may be connected by various interconnects such as a bus. Such interconnects may include a Peripheral Component Interconnect PCI such as PCI Express a Universal Serial Bus USB firewire IEEE 1394 an optical bus structure and the like. In another embodiment components of computing device may be interconnected by a network. For example memory may be comprised of multiple physical memory units located in different physical locations interconnected by a network.

Those skilled in the art will realize that storage devices utilized to store computer readable instructions may be distributed across a network. For example a computing device accessible via network may store computer readable instructions to implement one or more embodiments provided herein. Computing device may access computing device and download a part or all of the computer readable instructions for execution. Alternatively computing device may download pieces of the computer readable instructions as needed or some instructions may be executed at computing device and some at computing device .

Various operations of embodiments are provided herein. In one embodiment one or more of the operations described may constitute computer readable instructions stored on one or more computer readable media which if executed by a computing device will cause the computing device to perform the operations described. The order in which some or all of the operations are described should not be construed as to imply that these operations are necessarily order dependent. Alternative ordering will be appreciated by one skilled in the art having the benefit of this description. Further it will be understood that not all operations are necessarily present in each embodiment provided herein.

Moreover the word exemplary is used herein to mean serving as an example instance or illustration. Any aspect or design described herein as exemplary is not necessarily to be construed as advantageous over other aspects or designs. Rather use of the word exemplary is intended to present concepts in a concrete fashion. As used in this application the term or is intended to mean an inclusive or rather than an exclusive or . That is unless specified otherwise or clear from context X employs A or B is intended to mean any of the natural inclusive permutations. That is if X employs A X employs B or X employs both A and B then X employs A or B is satisfied under any of the foregoing instances. In addition the articles a and an as used in this application and the appended claims may generally be construed to mean one or more unless specified otherwise or clear from context to be directed to a singular form.

Also although the disclosure has been shown and described with respect to one or more implementations equivalent alterations and modifications will occur to others skilled in the art based upon a reading and understanding of this specification and the annexed drawings. The disclosure includes all such modifications and alterations and is limited only by the scope of the following claims. In particular regard to the various functions performed by the above described components e.g. elements resources etc. the terms used to describe such components are intended to correspond unless otherwise indicated to any component which performs the specified function of the described component e.g. that is functionally equivalent even though not structurally equivalent to the disclosed structure which performs the function in the herein illustrated exemplary implementations of the disclosure. In addition while a particular feature of the disclosure may have been disclosed with respect to only one of several implementations such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for any given or particular application. Furthermore to the extent that the terms includes having has with or variants thereof are used in either the detailed description or the claims such terms are intended to be inclusive in a manner similar to the term comprising. 

