---

title: Method for routing information over a network employing centralized control
abstract: A method and apparatus for centralized control of a network is described. The network includes a number of nodes. The method includes creating a database and storing the database on a master node of the network. The database contains topology information regarding a topology of the network. Each of the nodes is coupled to at least one other of the nodes, with the master node being one of the nodes.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08081582&OS=08081582&RS=08081582
owner: Cisco Technology, Inc.
number: 08081582
owner_city: San Jose
owner_country: US
publication_date: 20100430
---
This application is a divisional of U.S. patent application Ser. No. 09 751 763 entitled A Method for Routing Information over a Network Employing Centralized Control filed Dec. 30 2000 and naming Ali N. Saleh H. Michael Zadikian Zareh Baghdasarian and Vahid Parsi as inventors which issued as U.S. Pat. No. 6 973 023 on Dec. 6 2005 and which in turn is related to U.S. patent application Ser. No. 09 232 397 entitled A METHOD FOR ROUTING INFORMATION OVER A NETWORK filed Jan. 15 1999 and naming Ali N. Saleh H. Michael Zadikian Zareh Baghdasarian and Vahid Parsi as inventors which issued as U.S. Pat. No. 6 856 627 on Feb. 15 2005 and U.S. patent application Ser. No. 09 232 395 entitled A CONFIGURABLE NETWORK ROUTER filed Jan. 15 1999 and naming Ali N. Saleh H. Michael Zadikian Zareh Baghdasarian and Vahid Parsi as inventors which issued as U.S. Pat. No. 6 724 757 on Apr. 20 2004. All of the above identified applications and their disclosures are incorporated herein by reference for all purposes as if completely and fully set forth herein.

This invention relates to the field of information networks and more particularly relates to a protocol for configuring routes over a network.

Today s networks carry vast amounts of information. High bandwidth applications supported by these networks include streaming video streaming audio and large aggregations of voice traffic. In the future these demands are certain to increase. To meet such demands an increasingly popular alternative is the use of lightwave communications carried over fiber optic cables. The use of lightwave communications provides several benefits including high bandwidth ease of installation and capacity for future growth.

The synchronous optical network SONET protocol is among those protocols employing an optical infrastructure. SONET is a physical transmission vehicle capable of transmission speeds in the multi gigabit range and is defined by a set of electrical as well as optical standards. SONET s ability to use currently installed fiber optic cabling coupled with the fact that SONET significantly reduces complexity and equipment functionality requirements gives local and interexchange carriers incentive to employ SONET. Also attractive is the immediate savings in operational cost that this reduction in complexity provides. SONET thus allows the realization of a new generation of high bandwidth services in a more economical manner than previously existed.

SONET networks have traditionally been protected from failures by using topologies that dedicate something on the order of half the network s available bandwidth for protection such as a ring topology. Two approaches in common use today are diverse protection and self healing rings SHR both of which offer relatively fast restoration times with relatively simple control logic but do not scale well for large data networks. This is mostly due to their inefficiency in capacity allocation. Their fast restoration time however makes most failures transparent to the end user which is important in applications such as telephony and other voice communications. The existing schemes rely on 1 plus 1 and 1 for 1 topologies that carry active traffic over two separate fibers line switched or signals path switched and use a protocol Automatic Protection Switching or APS or hardware diverse protection to detect propagate and restore failures.

A SONET network using an SHR topology provides very fast restoration of failed links by using redundant links between the nodes of each ring. Thus each ring actually consists of two rings a ring supporting information transfer in a clockwise direction and a ring supporting information transfer in a counter clockwise direction. The terms east and west are also commonly used in this regard. Each direction employs its own set of fiber optic cables with traffic between nodes assigned a certain direction either clockwise or counter clockwise . If a cable in one of these sub rings is damaged the SONET ring heals itself by changing the direction of information flow from the direction taken by the information transferred over the failed link to the sub ring having information flow in the opposite direction.

The detection of such faults and the restoration of information flow thus occurs very quickly on the order of 10 ms for detection and 50 ms for restoration for most ring implementations. The short restoration time is critical in supporting applications such as current telephone networks that are sensitive to quality of service QoS because such short restoration times prevent old digital terminals and switches from generating red alarms and initiating Carrier Group Alarms CGA . These alarms are undesirable because such alarms usually result in dropped calls causing users down time aggravation. Restoration times that exceed 10 seconds can lead to timeouts at higher protocol layers while those that exceed 1 minute can lead to disastrous results for the entire network. However the price of such quickly restored information flow is the high bandwidth requirements of such systems. By maintaining completely redundant sub rings an SHR topology requires 100 excess bandwidth.

An alternative to the ring topology is the mesh topology. The mesh topology is similar to the point to point topology used in inter networking. Each node in such a network is connected to one or more other nodes. Thus each node is connected to the rest of the network by one or more links. In this manner a path from a first node to a second node uses all or a portion of the capacity of the links between those two nodes.

Networks based on mesh type restoration are inherently more capacity efficient than ring based designs mainly because each network link can potentially provide protection for fiber cuts on several different links. By sharing the capacity between links a SONET network using a mesh topology can provide redundancy for failure restoration at less than 100 of the bandwidth capacity originally required. Such networks are even more efficient when traffic transits several links. However restoration times exhibited by such approaches range from several minutes to several months.

Embodiments of the present invention provide a centralized routing protocol that supports relatively simple provisioning and relatively fast restoration on the order of for example 50 ms while providing relatively efficient bandwidth usage i.e. minimizing excess bandwidth requirements for restoration on the order of less than 100 redundant capacity and preferably less than 50 redundant capacity . Such a centralized routing protocol is in certain embodiments easily scaled to accommodate increasing bandwidth requirements.

According to embodiments of the present invention an apparatus and method are described for configuring routes over a network. Such a method embodied in a protocol according to embodiments of the present invention provides several advantages including relatively fast restoration on the order of for example 50 ms and relatively efficient bandwidth usage i.e. on the order of less than 100 redundant capacity and preferably less than 50 redundant capacity .

In one embodiment of the present invention a network is described. The network includes a master node. The network includes a number of nodes and each of the nodes is coupled to at least one other of the nodes with the master node being one of the nodes. The master node maintains topology information regarding a topology of the network.

In one aspect of the embodiment the network includes a backup node which is one of the nodes of the network. Such a backup node maintains a redundant copy of the topology information.

In another embodiment of the present invention a method for centralized control of a network is described. The network includes a number of nodes. The method includes creating a database and storing the database on a master node of the network. The database contains topology information regarding a topology of the network. Each of the nodes is coupled to at least one other of the nodes with the master node being one of the nodes.

In one aspect of the embodiment the method further includes retrieving backup topology information from a backup node with the backup node is one of the nodes. The backup node maintains a redundant copy of the topology information as the backup topology information. Moreover the master node and the backup node can be maintain synchronization of the database and the backup topology information.

The foregoing is a summary and thus contains by necessity simplifications generalizations and omissions of detail consequently those skilled in the art will appreciate that the summary is illustrative only and is not intended to be in any way limiting. Other aspects inventive features and advantages of the present invention as defined solely by the claims will become apparent in the non limiting detailed description set forth below.

The following is intended to provide a detailed description of an example of the invention and should not be taken to be limiting of the invention itself. Rather any number of variations may fall within the scope of the invention which is defined in the claims following the description.

To limit the size of the topology database and the scope of broadcast packets networks employing an embodiment of the protocol described herein can be divided into smaller logical groups referred to herein as zones. Each zone runs a separate copy of the topology distribution algorithm and nodes within each zone are only required to maintain information about their own zone. There is no need for a zone s topology to be known outside its boundaries and nodes within a zone need not be aware of the network s topology external to their respective zones.

Nodes that attach to multiple zones are referred to herein as border nodes. Border nodes are required to maintain a separate topological database also called link state or connectivity database for each of the zones they attach to. Border nodes use the connectivity database s for intra zone routing. Border nodes are also required to maintain a separate database that describes the connectivity of the zones themselves. This database which is called the network database is used for inter zone routing. The network database describes the topology of a special zone referred to herein as the backbone which is always assigned an identifier ID of 0. The backbone has all the characteristics of a zone. There is no need for a backbone s topology to be known outside the backbone and its border nodes need not be aware of the topologies of other zones.

A network is referred to herein as flat if the network consists of a single zone i.e. zone 0 or the backbone zone . Conversely a network is referred to herein as hierarchical if the network contains two or more zones not including the backbone. The resulting multi level hierarchy i.e. nodes and one or more zones provides the following benefits 

1. The size of the link state database maintained by each network node is reduced which allows the protocol to scale well for large networks.

3. Different sections of a long route i.e. one spanning multiple zones can be computed separately and in parallel speeding the calculations.

4. Restricting routing to be within a zone prevents database corruption in one zone from affecting the intra zone routing capability of other zones because routing within a zone is based solely on information maintained within the zone.

As noted the protocol routes information at two different levels inter zone and intra zone. The former is only used when the source and destination nodes of a virtual path are located in different zones. Inter zone routing supports path restoration on an end to end basis from the source of the virtual path to the destination by isolating failures between zones. In the latter case the border nodes in each transit zone originate and terminate the path restoration request on behalf of the virtual path s source and destination nodes. A border node that assumes the role of a source or destination node during the path restoration activity is referred to herein as a proxy source destination node. Such nodes are responsible for originating terminating the restore path request RPR with their own zones. Proxy nodes are also required to communicate with border nodes in other zones to establish an inter zone path for the VP.

In one embodiment every node in a network employing the protocol is assigned a globally unique 16 bit ID referred to herein as the node ID. A node ID is divided into two parts zone ID and node address. Logically each node ID is a pair zone ID node address where the zone ID identifies a zone within the network and the node address identifies a node within that zone. To minimize overhead the protocol defines three types of node IDs each with a different size zone ID field although a different number of zone types can be employed. The network provider selects which packet type to use based on the desired network architecture.

Type 0 IDs work well for networks that contain a small number of large zones e.g. less than about 4 zones . Type 2 IDs are well suited for networks that contain a large number of small zones e.g. more than about 15 . Type 1 IDs provide a good compromise between zone size and number of available zones which makes a type 1 node ID a good choice for networks that contain an average number of medium size zones e.g. between about 4 and about 15 . When zones being described herein are in a network the node IDs of the nodes in a zone may be delineated as two decimal numbers separated by a period e.g. ZoneID.NodeAddress .

Once a network topology has been defined the protocol allows the user to configure one or more end to end connections that can span multiple nodes and zones. This operation is referred to herein as provisioning. Each set of physical connections that are provisioned creates an end to end connection between the two end nodes that supports a virtual point to point link referred to herein as a virtual path or VP . The resulting VP has an associated capacity and an operational state among other attributes. The end points of a VP can be configured to have a master slave relationship. The terms source and destination are also used herein in referring to the two end nodes. In such a relationship the node with a numerically lower node ID assumes the role of the master or source node while the other assumes the role of the slave or destination node. The protocol defines a convention in which the source node initiates recovery under the control of a master node described subsequently and that the destination node simply waits for a message from the source node informing the source node of the VP s new path although the opposite convention could easily be employed.

VPs are also assigned a priority level which determines their relative priority within the network. This quality of service QoS parameter is used during failure recovery procedures to determine which VPs are first to be restored. Four QoS levels 0 3 are nominally defined in the protocol with 0 being the lowest although a larger or smaller number of QoS levels can be used. Provisioning is discussed in greater detail subsequently herein.

In one embodiment network nodes use a protocol such as that referred to herein as the Hello Protocol in order to establish and maintain neighbor relationships and to learn and distribute link state information throughout the network. The protocol relies on the periodic exchange of bi directional packets Hello packets between neighbors. During the adjacency establishment phase of the protocol which involves the exchange of INIT packets nodes learn information about their neighbors such as that listed in Table 1.

During normal protocol operation each node constructs a structure known as a Link State Advertisement LSA which contains a list of the node s neighbors links the capacity of those links the quality of service available on over links one or more costs associated with each of the links and other pertinent information. The node that constructs the LSA is called the originating node. Normally the originating node is the only node allowed to modify its contents except for the HOP COUNT field which is not included in the checksum and so may be modified by other nodes . The originating node retransmits the LSA when the LSA s contents change. The LSA is sent in a special Hello packet that contains not only the node s own LSA in its advertisement but also ones received from other nodes. The structure field definitions and related information are illustrated subsequently in and described in the corresponding discussion. Each node stores the most recently generated instance of an LSA in its database. The list of stored LSAs gives the node a complete topological map of the network. The topology database maintained by a given node is therefore nothing more than a list of the most recent LSAs generated by its peers and received in Hello packets.

In the case of a stable network the majority of transmitted Hello packets are empty i.e. contain no topology information because only altered LSAs are included in the Hello messages. Packets containing no changes no LSAs are referred to herein as null Hello packets. The Hello protocol requires neighbors to exchange null Hello packets periodically. The HelloInterval parameter defines the duration of this period. Such packets ensure that the two neighbors are alive and that the link that connects them is operational.

An INIT message is the first protocol transaction conducted between adjacent nodes and is performed upon network startup or when a node is added to a pre existing network. An INIT message is used by adjacent nodes to initialize and exchange adjacency parameters. The packet contains parameters that identify the neighbor the node ID of the sending node its link bandwidth both total and available on a QoS3 QoSn basis and its configured Hello protocol parameters. The structure field definitions and related information are illustrated subsequently in and described in the text corresponding thereto.

In systems that provide two or more QoS levels varying amounts of link bandwidth may be set aside for the exclusive use of services requiring a given QoS. For example a certain amount of link bandwidth may be reserved for QoS3 connections. This guarantees that a given amount of link bandwidth will be available for use by these high priority services. The remaining link bandwidth would then be available for use by all QoS levels 0 3 . The Hello parameters include the HelloInterval and HelloDeadInterval parameters. The HelloInterval is the number of seconds between transmissions of Hello packets. A zero in this field indicates that this parameter hasn t been configured on the sending node and that the neighbor should use its own configured interval. If both nodes send a zero in this field then a default value e.g. 7 seconds preferably used. The HelloDeadInterval is the number of seconds the sending node will wait before declaring a silent neighbor down. A zero in this field indicates that this parameter hasn t been configured on the sending node and that the neighbor should use its own configured value. If both nodes send a zero in this field then a default value e.g. 30 seconds should be used. The successful receipt and processing of an INIT packet causes a START event to be sent to the Hello State machine as is described subsequently.

Once adjacency between two neighbors has been established the nodes periodically exchange Hello packets. The interval between these transmissions is a configurable parameter that can be different for each link and for each direction. Nodes are expected to use the HelloInterval parameters specified in their neighbor s Hello message. A neighbor is considered dead if no Hello message is received from the neighbor within the HelloDeadInterval period also a configurable parameter that can be link and direction specific .

In one embodiment nodes in a network continuously receive Hello messages on each of their links and save the most recent LSAs from each message. Each LSA contains among other things an LSID indicating which instance of the given LSA has been received and a HOP COUNT. The HOP COUNT specifies the distance as a number of hops between the originating node and the receiving node. The originating node always sets this field of 0 when the LSA is created. The HOP COUNT field is incremented by one for each hop from node to node traversed by the LSA instance. The HOP COUNT field is set to zero by the originating node and is incremented by one on every hop of the flooding procedure. The ID field is initialized to FIRST LSID during node start up and is incremented every time a new instance of the LSA is created by the originating node. The initial ID is only used once by each originating node. Preferably an LSA carrying such an ID is always accepted as most recent. This approach allows old instances of an LSA to be quickly flushed from the network when the originating node is restarted.

During normal network operation the originating node of an LSA transmits LS update messages when the node detects activity that results in a change in its LSA. The node sets the HOP COUNT field of the LSA to 0 and the LSID field to the LSID of the previous instance plus 1. Wraparound may be avoided by using a sufficiently large LSID e.g. 32 bits . When another node receives the update message the node records the LSA in its database and schedules the LSA for transmission to its own neighbors. The HOP COUNT field is incremented by one node and transmitted to the neighboring nodes. Likewise when the nodes downstream of the current node receive an update message with a HOP COUNT of H they transmit their own update message to all of their neighbors with a HOP COUNT of H 1 which represents the distance in hops to the originating node. This continues until the update message either reaches a node that has a newer instance of the LSA in its database or the hop count field reaches MAX HOPS.

Those skilled in the art will recognize the boundaries between and order of operations in this and the other flow diagrams described herein are merely illustrative and alternative embodiments may merge operations impose an alternative decomposition of functionality of operations or re order the operations presented therein. For example the operations discussed herein may be decomposed into sub operations to be executed as multiple computer processes. Moreover alternative embodiments may combine multiple instances of particular operation or sub operations. Furthermore those skilled in the art will recognize that the operations described in this exemplary embodiment are for illustration only. Operations may be combined or the functionality of the operations may be distributed in additional operations in accordance with the invention.

The operations referred to herein may be modules or portions of modules e.g. software firmware or hardware modules . For example although the described embodiment is generally discussed in terms of software modules and or manually entered user commands such actions may be embodied in the structure of circuitry that implements such functionality such as the micro code of a complex instruction set computer CISC firmware programmed into programmable or erasable programmable devices e.g. EPROMs the configuration of a field programmable gate array FPGA the design of a gate array or full custom application specific integrated circuit ASIC or the like.

The software modules discussed herein may include modules coded in a high level programming language e.g. the C programming language script batch or other executable files or combinations and or portions of such files. While it is appreciated that operations discussed herein may consist of directly entered commands by a computer system user or by steps executed by application specific hardware modules the preferred embodiment includes steps executed by software modules. The functionality of steps referred to herein may correspond to the functionality of modules or portions of modules. The software modules may include a computer program or subroutines thereof encoded on computer readable media.

Each of the blocks of may thus be executed by a module e.g. a software module or a portion of a module or a computer system user. Thus the above described method the operations thereof and modules therefor may be executed on a computer system configured to execute the operations of the method and or may be executed from computer readable media. The method may be embodied in a machine readable and or computer readable medium for configuring a computer system to execute the method. Thus the software modules may be stored within and or transmitted to a computer system memory to configure the computer system to perform the functions of the module.

Those software modules may therefore be received e.g. from one or more computer readable media by the various hardware modules of a router such as that described in the Patent Application entitled A CONFIGURABLE NETWORK ROUTER having A. Saleh H. M. Zadikian J. C. Adler Z. Baghdasarian and V. Parsi as inventors as previously incorporated by reference herein. The computer readable media may be permanently removably or remotely coupled to the given hardware module. The computer readable media may non exclusively include for example any number of the following magnetic storage media including disk and tape storage media optical storage media such as compact disk media e.g. CD ROM CD R etc. and digital video disk storage media nonvolatile memory storage memory including semiconductor based memory units such as FLASH memory EEPROM EPROM ROM or application specific integrated circuits volatile storage media including registers buffers or caches main memory RAM etc. and data transmission media including computer network point to point telecommunication and carrier wave transmission media. In a UNIX based embodiment the software modules may be embodied in a file which may be a device a terminal a local or remote file a socket a network connection a signal or other expedient of communication or state change. Other new and various types of computer readable media may be used to store and or transmit the software modules discussed herein.

The LSA of the inactive node propagates throughout the network until the hop count reaches MAX HOPS. Various versions of the GET LSA request are generated by nodes along the path each with a varying number of requested LSA entries. An entry is removed from the request when the request reaches a node that has an instance of the requested LSA that meets the criteria of list B.

All database exchanges are expected to be reliable using the above method because received LSA s must be individually acknowledged. The acknowledgment packet contains a mask that has a 1 in all bit positions that correspond to LSA s that were received without any errors. The low order bit corresponds to the first LSA received in the request while the high order bit corresponds to the last LSA. Upon receiving the response the sender verifies the checksum of all LSA s in its database that have a corresponding 0 bit in the response. The sender then retransmits all LSA s with a valid checksum and ages out all others. An incorrect checksum indicates that the contents of the given LSA has changed while being held in the node s database. This is usually the result of a memory problem. Each node is thus required to verify the checksum of all LSA s in its database periodically.

The LS checksum is provided to ensure the integrity of LSA contents. As noted the LS checksum is used to detect data corruption of an LSA. This corruption can occur while the advertisement is being transmitted while the advertisement is being held in a node s database or at other points in the networking equipment. The checksum can be formed by any one of a number of methods known to those of skill in the art such as by treating the LSA as a sequence of 16 bit integers adding them together using one s complement arithmetic and then taking the one s complement of the result. Preferably the checksum doesn t include the LSA s HOP COUNT field in order to allow other nodes to modify the HOP COUNT without having to update the checksum field. In such a scenario only the originating node is allowed to modify the contents of an LSA except for those two fields including its checksum. This simplifies the detection and tracking of data corruption.

Specific instances of an LSA are identified by the LSA s ID field the LSID. The LSID makes possible the detection of old and duplicate LSAs. Similar to sequence numbers the space created by the ID is circular a sequence number starts at some value FIRST LSID increases to some maximum value FIRST LSID 1 and then goes back to FIRST LSID 1. Preferably the initial value is only used once during the lifetime of the LSA which helps flush old instances of the LSA quickly from the network when the originating node is restarted. Given a large enough LSID wrap around will never occur in a practical sense. For example using a 32 bit LSID and a MinLSInterval of 5 seconds wrap around takes on the order of 680 years.

LSIDs must be such that two LSIDs can be compared and the greater or lesser of the two identified or a failure of the comparison indicated. Given two LSIDs x and y x is considered to be less than y if either 2and or 2and is true. The comparison fails if the two LSIDs differ by more than 2. Sending Receiving and Verifying LSAs

For each new LSA in the link state database step then the following steps are taken. If the LSA is new several actions are performed. For each node in the neighbor list step the state of the neighboring node is determined. If the state of the neighboring node is set to a value of less than ACTIVE that node is skipped steps and . If the state of the neighboring node is set to a value of at least ACTIVE and if the LSA was received from this neighbor step the given neighbor is again skipped step . If the LSA was not received from this neighbor step the LSA is added to the list of LSAs that are waiting to be sent by adding the LSA to this neighbor s LSAsToBeSent list step . Once all LSAs have been processed step requests are sent out. This is accomplished by stepping through the list of LSAs to be sent steps and . Once all the LSAs have been sent the process is complete.

Otherwise the node s link state database is searched to find the current LSA step and if not found the current LSA is written into the database step . If the current LSA is found in the link state database the current LSA and the LSA in the database are compared to determine if they were sent from the same node step . If the LSAs were from the same node the LSA is installed in the database step . If the LSAs were not from the same node the current LSA is compared to the existing LSA to determine which of the two is more recent step . The process for determining which of the two LSAs is more recent is discussed in detail below in reference to . If the LSA stored in the database is the more recent of the two the LSA received is simply discarded step . If the LSA in the database is less recent than the received LSA the new LSA is installed in the database overwriting the existing LSA step . Regardless of the outcome of this analysis the LSA is then acknowledged by sending back an appropriate response to the node having transmitted the Hello message step .

The basic flooding mechanism in which each packet is sent to all active neighbors except the one from which the packet was received can result in an exponential number of copies of each packet. This is referred to herein as a broadcast storm. The severity of broadcast storms can be limited by one or more of the following optimizations 

Every node establishes adjacency with all of its neighbors. The adjacencies are used to exchange Hello packets with and to determine the status of the neighbors. Each adjacency is represented by a neighbor data structure that contains information pertinent to the relationship with that neighbor. The fields described in Table 2 support such a relationship.

Preferably a node maintains a list of neighbors and their respective states locally. A node can detect the states of is neighbors using a set of neighbor states such as the following 

HSM includes a Down state an INIT Sent state a ONE WAY state an EXCHANGE state an ACTIVE state and an INIT Received state . HSM transitions between these states in response to a START transition IACK RECEIVED transitions and INIT RECEIVED transitions and and an EXCHANGE DONE transition in the manner described in Table 3. It should be noted that the Disabled state mentioned in Table 3 is merely a fictional state representing a non existent neighbor and so is not shown in for the sake of clarity. Table 3 shows state changes their causing events and resulting actions.

After the successful exchange of INIT packets the two neighbors enter the Exchange State. Exchange is a transitional state that allows both nodes to synchronize their databases before entering the Active State. Database synchronization involves exchange of one or more Hello packets that transfer the contents of one node s database to the other. A node should not send a Hello request while its awaiting the acknowledgment of another. The exchange may be made more reliable by causing each request to be transmitted repeatedly until a valid acknowledgment is received from the adjacent node.

When a Hello packet arrives at a node the Hello packet is processed as previously described. Specifically the node compares each LSA contained in the packet to the copy the node currently has in its own database. If the received copy is more recent then the node s own or advertises a better hop count the received copy is written into the database possibly replacing the current copy. The exchange process is normally considered completed when each node has received and acknowledged a null Hello request from its neighbor. The nodes then enter the Active State with fully synchronized databases which contain the most recent copies of all LSAs known to both neighbors.

A sample exchange using the Hello protocol is described in Table 4. In the following exchange node 1 has four LSAs in its database while node 2 has none.

Another example is the exchange described in Table 5. In the following exchange node 1 has four LSAs 1 through 4 in its database and node 2 has 7 3 and 5 through 10 . Additionally node 2 has a more recent copy of LSA in its database than node 1.

At the end of the exchange both nodes will have the most recent copy of all 10 LSAs 1 through 10 in their databases.

For each VP that is to be configured or as also referred to herein provisioned a physical path needs to be selected and configured. VPs may be provisioned statically or dynamically. For example a user can identify the nodes through which the VP will pass and manually configure each node to support the given VP. Using a method according to the present invention this is done using a centralized technique via a master node. The selection of nodes may be based on any number of criteria such as QoS latency cost and the like. Alternatively the VP may be provisioned dynamically using any one of a number of methods such as a shortest path first technique or a distributed technique e.g. as described herein . An example of a distributed technique is the restoration method described subsequently herein.

In one embodiment of networks herein failures are detected using the mechanisms provided by the underlying physical network. For example when using a SONET network a fiber cut on a given link results in a loss of signal LOS condition at the nodes connected by that link. The LOS condition propagated an Alarm Indication Signal AIS downstream and Remote Defect Indication RDI upstream if the path still exists and an LOS defect locally. Later the defect is upgraded to a failure 2.5 seconds later which causes an alarm to be sent to the Operations System OS per Bellcore s recommendations in GR 253 GR 253 Synchronous Optical Network SONET Transport Systems Common Generic Criteria Issue 2 Bellcore December 1995 included herein by reference in its entirety and for all purposes . Preferably when using SONET the handling of the LOS condition follows Bellcore s recommendations in GR 253 which allows nodes to inter operate and co exist with other network equipment NE in the same network. The mesh restoration protocol is invoked as soon as the LOS defect is detected by the line card which occurs 3 ms following the failure a requirement under GR 253 .

The arrival of the AIS at the downstream node causes that downstream node to send a similar alarm to its downstream neighbor and for that node to send an AIS to its own downstream neighbor. This continues from node to node until the AIS finally reaches the source node of the affected VP or a proxy border node if the source node is located in a different zone. In the latter case the border node restores the VP on behalf of the source node. Under GR 253 each node is allowed a maximum of 125 microseconds to forward the AIS downstream which quickly propagates failures toward the source node.

Once a node has detected a failure on one of its links either through a local LOS defect or a received AIS indication the node scans its VP table looking for entries that have the failed link in their path. When the node finds one the node releases all link bandwidth used by the VP. Then if the node is a VP s source node or a proxy border node the VP s state is changed to RESTORING and the VP placed on a list of VPs to be restored. Otherwise if the node isn t the source node or a proxy border node the state of the VP is changed to DOWN and a timer is started to delete the VP from the database if a corresponding restore path request isn t received from the origin node within a certain timeout period. The VP list that was created in the previous step is ordered by quality of service QoS which ensures that VPs with a higher QoS setting are restored first. Each entry in the list contains among other things the ID of the VP its source and destination nodes configured QoS level and required bandwidth.

For each VP on the list the node then sends an RPR to all eligible neighbors in order to restore the given VP. The network will of course attempt to restore all failed VPs. Neighbor eligibility is determined by the state of the neighbor available link bandwidth current zone topology location of the Target node and other parameters. One method for determining the eligibility of a particular neighbor follows 

Due to the way RPR messages are forwarded by tandem nodes and the unconditional and periodic retransmission of such messages by origin nodes multiple instances of the same request are not uncommon even multiple copies of each instance circulating the network at any given time. To minimize the amount of broadcast traffic generated by the protocol and aid tandem nodes in allocating bandwidth fairly for competing RPRs tandem nodes preferably execute a sequence such as that described subsequently.

The term same instance as used below refers to messages that carry the same VP ID origin node ID and hop count and are received from the same tandem node usually the same input link assuming only one link between nodes . Any two messages that meet the above criteria are guaranteed to have been sent by the same origin node over the same link to restore the same VP and to have traversed the same path. The terms copy of an instance or more simply copy are used herein to refer to a retransmission of a given instance. Normally tandem nodes select the first instance they receive since in most but not all cases as the first RPR received normally represents the quickest path to the origin node. A method for making such a determination was described in reference to . Because such information must be stored for numerous RPRs a standard data structure is defined under this protocol.

The Restore Path Request Entry RPRE is a data structure that maintains information about a specific instance of a RPRE packet. Tandem nodes use the structure to store information about the request which helps them identify and reject other instances of the request and allows them to correlate received responses with forwarded requests. Table 6 lists an example of the fields that are preferably present in an RPRE.

When an RPR packet arrives at a tandem node a decision is made as to which neighbor should receive a copy of the request. The choice of neighbors is related to variables such as link capacity and distance. Specifically a particular neighbor is selected to receive a copy of the packet if 

Processing of RPRs begins at step in which the target node s ID is compared to the local node s ID. If the local node s ID is equal to the target node s ID the local node is the target of the RPR and must process the RPR as such. This is illustrated in as step and is the subject of the flow diagram illustrated in . If the local node is not the target node the RPR s HOP COUNT is compared to MAX HOP in order to determine if the HOP COUNT has exceed or will exceed the maximum number of hops allowable step . If this is the case a negative acknowledgment NAK with a Flush indicator is then sent back to the originating node step . If the HOP COUNT is still within acceptable limits the node then determines whether this is the first instance of the RPR having been received step . If this is the case a Restore Path Request Entry RPRE is created for the request step . This is done by creating the RPRE and setting the RPRE s fields including starting a time to live TTL or deletion timer in the following manner 

The ID of the input link is then added to the path in the RPRE e.g. Path PathIndex LinkID step . Next the local node determines whether the target node is a direct neighbor step . If the target node is not a direct neighbor of the local node a copy of the modified RPR is sent to all eligible neighbors step . The PendingReplies and SentTo Fields of the corresponding RPRE are also updated accordingly at this time. If the target node is a direct neighbor of the local node the RPR is sent only to the target node step . In either case the RPRE corresponding to the given RPR is then updated step .

If this is not the first instance of the RPR received by the local node the local node then attempts to determine whether this might be a different instance of the RPR step . A request is considered to be a different instance if the RPR 

If this is simply a different instance of the RPR and another instance of the same RPR has been processed and accepted by this node a NAK Wrong Instance is sent to the originating neighbor step . The response follows the reverse of the path carried in the request. No broadcasting is therefore necessary in such a case. If a similar instance of the RPR has been processed and accepted by this node step the local node determines whether a Terminate NAK has been received for this RPR step . If a Terminate NAK has been received for this RPR the RPR is rejected by sending a Terminate response to the originating neighbor step . If a Terminate NAK was not received for this RPR the new sequence number is recorded step and a copy of the RPR is forwarded to all eligible neighbors that have not sent a Flush response to the local node for the same instance of this RPR step . This may include nodes that weren t previously considered by this node due to conflicts with other VPs but does not include nodes from which a Flush response has already been received for the same instance of this RPR. The local node should then save the number of sent requests in the PendingReplies field of the corresponding RPRE. The term eligible neighbors refers to all adjacent nodes that are connected through links that meet the link eligibility requirements previously described. Preferably bandwidth is allocated only once for each request so that subsequent transmissions of the request do not consume any bandwidth.

Note that the bandwidth allocated for a given RPR is released differently depending on the type of response received by the node and the setting of the Flush and Terminate indicators in its header. Table 7 shows the action taken by a tandem node when the tandem node receives a restore path response from one of its neighbors.

If the VP specified in the RPR terminates at this node i.e. this node is indeed the target node the target node determines whether an RPRE exists for the RPR received step . If an RPRE already exists for this RPR the existing RPRE is updated e.g. the RPRE s LastSequenceNumber field is updated step and the RPRE deletion timer is restarted step . If no RPRE exists for this RPR in the target node i.e. if this is the first copy of the instance received an RPRE is created step pertinent information from the RPR is copied into the RPRE step the bandwidth requested in the RPR is allocated on the input link by the target node step and an RPRE deletion timer is started step . In either case once the RPRE is either updated or created a checksum is computed for the RPR step and written into the checksum field of the RPR step . The RPR is then returned as a positive response to the origin node step . The local target node then starts its own matrix configuration. It will be noted that the RPRE created is not strictly necessary but makes the processing of RPRs consistent across nodes.

If the sending node is listed in the RPRE the RPR sequence number is analyzed to determine whether or not the RPR sequence number is valid step . As with the previous steps if the RPR contains an invalid sequence number e.g. doesn t fall between FirstSequenceNumber and LastSequence Number inclusive the RPR response is ignored step . If the RPR sequence number is valid the receiving node determines whether Flush or Terminate in the RPR response step . If neither of these is specified the RPR response sequence number is compared to that stored in the last sequence field of the RPR step . If the RPR response sequence number does not match that found in the last sequence field of the RPRE the RPR response is again ignored step . If the RPR response sequence number matches that found in the RPRE or a Flush or Terminate was specified in the RPR the input link on which the RPR response was received is compared to that listed in the RPR response path field e.g. Response.Path Response.Pathlndex InputLinkID step . If the input link is consistent with information in the RPR the next hop information in the RPR is checked for consistency e.g. Response.Path Response.Pathlndex 1 RPRE.ReceivedFrom step . If either of the proceeding two tests are failed the RPR response is again ignored step .

If a Terminate was specified in the RPR response step the bandwidth on all links over which the RPR was forwarded is freed step and the Terminate and Flush bits from the RPR response are saved in the RPRE step . If a Terminate was not specified in the RPR response bandwidth is freed only on the input link i.e. the link from which the response was received step the Terminate and Flush bits are saved in the RPRE step and the Flush bit of the RPR is cleared step . If a Terminate was not specified in the RPR Pending Replies field in the RPRE is decremented step . If this field remains non zero after being decremented the process completes. If Pending Replies is equal to zero at this point or a Terminate was not specified in the RPR the RPR is sent to the node specified in the RPR s Received From field i.e. the node that sent the corresponding request step . Next the bandwidth allocated on the link to the node specified in the RPR s Received From field is released step and an RPR deletion timer is started step .

With regard to matrix configuration the protocol pipelines such activity with the forwarding of RPRs in order to minimize the impact of matrix configuration overhead on the time required for restoration. While the response is making its way from node N to node N node N is busy configuring its matrix. In most cases by the time the response reaches the origin node all nodes along the path have already configured their matrices.

The Terminate indicator prevents bad instances of an RPR from circulating around the network for extended periods of time. The Terminate indicator is propagated all the way back to the originating node and prevents the originating node and all other nodes along the path from sending or forwarding other copies of the corresponding RPR instance.

Terminating RPR Packets are processed as follows. The RPR continues along the path until any one of the following four conditions is encountered 

Further optimizations of the protocol can easily be envisioned by one of skill in the art and are intended to be within the scope of this specification. For example in one embodiment a mechanism is defined to further reduce the amount of broadcast traffic generated for any given VP. In order to prevent an upstream neighbor from sending the same instance of an RPR every T milliseconds a tandem node can immediately return a no commit positive response to that neighbor which prevents that neighbor from sending further copies of the instance. The response simply acknowledges the receipt of the request and doesn t commit the sender to any of the requested resources. Preferably however the sender of the positive response periodically transmits the acknowledged request until a valid response is received from its downstream neighbor s . This mechanism implements a piece wise or hop by hop acknowledgment strategy that limits the scope of retransmitted packets to a region that gets progressively smaller as the request gets closer to its target node.

However it is prudent to provide some optimizations for efficiently handling errors. Communication protocols often handle link errors by starting a timer after every transmission and if a valid response isn t received within the timeout period the message is retransmitted. If a response isn t received after a certain number of retransmission the sender generates a local error and disables the connection. The timeout period is usually a configurable parameter but in some cases the timeout period is computed dynamically and continuously by the two end points. The simplest form of this uses some multiple of the average round trip time as a timeout period while others use complex mathematical formulas to determine this value. Depending on the distance between the two nodes the speed of link that connects them and the latency of the equipment along the path the timeout period can range anywhere from millisecond to seconds.

The above strategy is not the preferred method of handling link errors. This is because the fast restoration times required dictates that 2 way end to end communication be carried out in less than 50 ms. A drawback of the above described solution is the time wasted while waiting for an acknowledgment to come back from the receiving node. A safe timeout period for a 2000 mile span for instance is over 35 ms which doesn t leave enough time for a retransmission in case of an error.

This problem is addressed in one embodiment by taking advantage of the multiple communication channels i.e. OC 48 s that exist between nodes to 

Network is flat meaning that all nodes belong to the same zone zone 0 or the backbone zone. This also implies that Node IDs and Node Addresses are one and the same and that the upper three bits of the Node ID address are always zeroes using the aforementioned node ID configuration. Table 8 shows link information for network . Source nodes are listed in the first column and the destination nodes are listed in the first row of Table 8. The second row of Table 8 lists the link ID L the available bandwidth B and distance D associated with each of the links. In this example no other metrics e.g. QoS are used in provisioning the VPs listed subsequently.

Table 9A shows a list of exemplary configured VPs and Table 9B shows the path selected for each VP by a shortest path algorithm such as that described herein. The algorithm allows a number of metrics e.g. distance cost delay and the like to be considered during the path selection process which makes it possible to route VPs based on user preference. Here the QoS metric is used to determine which VP has priority.

Routes are computed using a QoS based shortest path algorithm. The route selection process relies on configured metrics and an up to date view of network topology to find the shortest paths for configured VPs. The topology database contains information about all network nodes their links and available capacity. All node IDs are assigned by the user and must be globally unique. This gives the user control over the master slave relationship between nodes. Duplicate IDs are detected by the network during adjacency establishment. All nodes found with a duplicate ID are disabled by the protocol and an appropriate alarm is generated to notify the network operations center of the problem so that proper action can be taken.

For any given hop count 1 through LastHop Path ultimately contains the best route from R to all other nodes in the network. To find the shortest path in terms of hops not distance from R to n row n of the array is searched until an entry with a cost not equal to MAX COST is found. To find the least cost path between R and n regardless of the hop count entries 1 through LastHop of row n are scanned and the entry with the lowest cost selected.

Protocol messages or packets preferably begin with a standard header to facilitate their processing. Such a header preferably contains the information necessary to determine the type origin destination and identity of the packet. Normally the header is then followed by some sort of command specific data e.g. zero or more bytes of information .

The protocol can be configured to use a number of different commands. For example seven commands may be used with room in the header for 9 more. Table 11 lists those commands and provides a brief description of each with detailed description of the individual commands following.

The initialization or INIT packet shown in is used by adjacent nodes to initialize and exchange adjacency parameters. The packet contains parameters that identify the neighbor its link bandwidth both total and available and its configured Hello protocol parameters. The INIT packet is normally the first protocol packet exchanged by adjacent nodes. As noted previously the successful receipt and processing of the INIT packet causes a START event to be sent to the Hello State machine. The field definitions appear in Table 12.

Hello packets are sent periodically by nodes in order to maintain neighbor relationships and to acquire and propagate topology information throughout the network. The interval between Hello packets is agreed upon during adjacency initialization. Link state information is included in the packet in several situations such as when the database at the sending nodes changes either due to provisioning activity port failure or recent updates received from one or more originating nodes. Preferably only modified LS entries are included in the advertisement. A null Hello packet also sent periodically is one that has a zero in its LSCount field and contains no LSAs. Furthermore it should be noted that a QoSn VP is allowed to use any bandwidth reserved for QoS levels through n. Table 13 describes the fields that appear first in the Hello packet. These fields appear only once.

The restore path packet is sent by source nodes or proxy border nodes to obtain an end to end path for a VP. The packet is usually sent during failure recovery procedures but can also be used for provisioning new VPs. The node sending the RPR is called the origin or source node. The node that terminates the request is called the target or destination node. A Restore Path instance is uniquely identified by its origin and target nodes and VP ID. Multiple copies of the same restore path instance are identified by the unique sequence number assigned to each of them. Only the sequence number need be unique across multiple copies of the same instance of a restore path packet. Table 17 provides the definitions for the fields shown in .

The CP packet is sent by source nodes or proxy border nodes to obtain an end to end path for a VP. The node sending the CP is called the origin or source node. The node that terminates the request is called the target or destination node. A CP instance is uniquely identified by its origin and target nodes and VP ID. Multiple copies of the same CP instance are identified by the unique sequence number assigned to each of them. Only the sequence number need be unique across multiple copies of the same instance of a restore path packet. Table 18 provides the definitions for the fields shown in .

The Delete Path packet is used to delete an existing path and releases the existing path s allocated link resources. The Delete Path packet can use the same packet format as the Restore Path packet. The originating node is responsible for initializing the Path PathLength and Checksum fields to the packet which should include the full path of the VP being deleted. The originating node also sets PathIndex to zero. Tandem nodes should release link resources allocated for the VP after they have received a valid response from the target node. The target node should set the PathIndex field to zero prior to computing the checksum of packet.

The TestPath packet is used to test the integrity of an existing virtual path. The TestPath packet uses the same packet format as the RestorePath packet. The originating node is responsible for initializing the Path PathLength and Checksum fields of the packet which should include the full path of the span being tested. The originating node also sets PathIndex to zero. The target node should set the PathIndex field to zero prior to computing the checksum of packet. The TestPath packet may be configured to test functionality or may test a path based on criteria chosen by the user such as latency error rate and the like.

The Link Down packet is used by slave nodes to inform the master node of link failures when master nodes are present in the network. This message is provided for instances in which the alarms associated with such failures AIS and RDI do not reach the master node.

An extension of the preceding approach is the use of a centralized network management technique according to an embodiment of the present invention. Centralized control can be employed in which a central network node is elected to handle routing and provisioning tasks such as provisioning connections in a network according to embodiments of the present invention.

In a network employing a centralized method according to embodiments of the present invention the network s nodes are preferably configured in groups of three or more nodes and interconnected in a mesh configuration in a manner such as that described previously with regard to . The nodes form a single distributed system that can span thousands of miles and supports tens of thousands of connections. For such a system to work properly and reliably there should be a well defined interface between the nodes and an agreed upon control hierarchy. Such an interface and hierarchy can be implemented as outlined previously or may be implemented using a centralized method.

Using a centralized method one node is designated the master node within each network. This master node is responsible for all path discovery implementation assurance and restoration activities. A second node preferably one that is geographically diverse from the master node is assigned the role of the backup node. The backup node is responsible for closely monitoring the master node and is always ready to take over the master node s responsibilities should the master node fail. The user can also designate one or more nodes as standby nodes. Such nodes act as a second line of defense against failures on the master and backup nodes. In the case where both the master and backup nodes experience failures the remaining standby node with the highest priority assumes the role of the backup node should the then current master node fail.

If the master is not the same step and the node ID is numerically lower than that of the previous master step then the following process occurs. A warning message indicating that multiple master nodes are operating multiple masters is logged step . The contents of the message are copied into a local structure overwriting those of the previous message step . This is the point to which the process jumps if the IAM MASTER message received was not the first IAM MASTER message received. The hop count field of the message is then incremented step . This is the point to which the process jumps if the hop count and source node of the IAM MASTER message received was the same as a previous IAM MASTER message. If the hop count doesn t exceed the maximum allowed step the IAM MASTER message is forwarded to all immediate neighbors step .

Regardless of the hop count the system controller then waits for a specified time e.g. 25 ms hop count and sends a positive reply to the master step . The reply carries information about the node such as 

Finally the version numbers of all local executable images are compared with those available on the master node and make a list of all images that need to be updated step . The list of images created in step above is then used by the node to update its local copy of the executable images. This may be accomplished for example by initiating one or more File Transfer Protocol FTP sessions with the master node. FTP which is a TCP IP application is an efficient file transfer protocol that is readily available on most TCP IP hosts.

At the end of the sequence illustrated in the master node should have a list of all network resources and all nodes should have the most recent version of their executable images. The acquired list contains information about whether a given node has routing capabilities i.e. has a working route processor module. Such nodes automatically assume the role of a standby node unless they are specifically configured otherwise. The master node assigns the role of the backup node to one of the standby nodes according to Table 19. Once a backup node has been selected for the network the master node sends the backup node copies of various databases e.g. the databases containing information regarding the network s topology the topology database and information regarding the virtual paths carried by the network if necessary and a copy of the resource list the dynamic database that contains information regarding resources also referred to herein as the resource database or run time database .

The master node is assumed to have the most up to date copy of the database which is also referred to herein as the authoritative or primary copy. The backup and standby nodes should have a minor copy of the authoritative database but this is not assumed unless the authoritative copy is no longer available due to damage or master node failure. Preferably each version of the database is preferably assigned a unique serial number that allows different versions of the database to be uniquely identified. Such a serial number is normally higher on more recent versions of the same database simplifying the location and identification of the most recent copy. Serial numbers can be assigned for example by the master node and in order to implement the preceding paradigm incremented when the authoritative copy is modified. Changes to secondary copies of the database by nodes other than the master node are not usually allowed. When this occurs however only the version number of the database should be changed and not the database s serial number. This allows independent branches of the authoritative copy to be easily tracked and merged. In most cases however only the authoritative copy of the database is modified by management agents. Other secondary copies are treated as read only by their respective nodes.

During network startup the master backup and standby nodes participate in a database synchronization activity that results in a single authoritative copy of the database s . As depicted in the sequence starts with the master node sending a message referred to herein as a GET DATABASE INFO message to the backup node step . Upon receiving the message step the backup node sends back a reply containing the serial and version numbers of the backup node s database s step . The master node uses this information to determine whether a copy of the master node s authoritative database s should be sent to that backup node. If the numbers match those of the authoritative database s step the backup node is assumed to be up to date and no action is taken by the master node step . If however either number differs from that of the authoritative database s step then a copy of the affected database s are sent to the backup node step .

The next action performed is the master node s sending a copy of the resource list the dynamic or run time database as noted to the backup node step . The embedded hierarchy of the resource list is preferably maintained when such a transfer occurs. Once the backup node has been updated the backup node in turn sends copies of its database s e.g. topology database and dynamic database to all standby nodes found in the resource list step . The sequence is similar to the one described above and so will not be repeated here. Once all nodes have been synchronized they remain synchronized by messages that inform them of any changes made to the database s e.g. LSA updates for topology changes and a CreatePath packet for changes to VPs step . Furthermore all user initiated changes regardless of where such changes are entered are handled through the master node which also updates the backup and standby nodes before committing the changes to the database s .

The process begins with the system controller sending a START MULTIPLE message to the route processor step which causes the route processor to enter batch processing mode where configuration requests are deleted until an END MULTIPLE message is received. The system controller then retrieves the next connection record from the database step . This should be the highest priority connection of all such remaining connections.

The system controller sends an ADD CONNECTION message to the route processor step . The message includes information about the connection such as the following which may be obtained from the corresponding database record 

Upon receiving the ADD CONNECTION request the route processor computes the shortest path route for the connection taking into consideration the QoS bandwidth and any other parameters specified step . This may be accomplished for example using a method such as the QSPF method described earlier herein and described more fully in patent application Ser. No. 09 478 235 filed Jan. 4 2000 and entitled A Method For Path Selection In A Network having A. Saleh as inventor which is hereby incorporated by reference in its entirety and for all purposes. If the route lookup attempt is successful step the route processor then updates the input output maps of all affected nodes and sends a positive reply to the system controller step . A positive reply from the route processor changes the state of the connection to MAPPED step . If the route lookup attempt fails for any reason step the route processor sends back a negative response that carries a reason code explaining the cause of the failure step . A negative response changes the state of the connection to FAILED step and causes an error message to be generated step .

The system controller continues until all provisioned connections have been processed step . The system controller then sends an END MULTIPLE request to the route processor step which causes the route processor to send all input output maps to their respective nodes. The route processor then sends a copy of those maps to the system controller step which in turn sends a copy to the backup node step .

Upon receiving the response from the route processor step the system controller does one of two things depending on the result of the operation step . A positive response causes the connection to be removed from the database step and an update message to be sent to the backup node step . Once the backup node has acknowledged the receipt of the information step the master mode sends a positive response to the original sender of the request step . A negative response from the route processor causes the master node to reject the request by returning a negative response to the requestor step .

While the above processing is performed the requesting node s await the master node s response step and continue to do so unless some reason for reconsidering the transaction exists e.g. a timeout condition occurs step . Thus the connectivity change is not committed until a positive response is received from the master node steps and with a negative response resulting in the connectivity change being abandoned. In certain embodiments of the present invention changing connections is merely a combination of adding and dropping the appropriate connections across various links. Within the master node several actions are performed in determining the viability of a connectivity change and maintaining topology information in the face of such changes as previously discussed.

While particular embodiments of the present invention have been shown and described it will be obvious to those skilled in the art that based upon the teachings herein changes and modifications may be made without departing from this invention and its broader aspects and therefore the appended claims are to encompass within their scope all such changes and modifications as are within the true spirit and scope of this invention. Furthermore it is to be understood that the invention is solely defined by the appended claims.

