---

title: Dispatching conflicting data changes
abstract: A method for distributing one or more conflicting data changes. The method includes extracting data changes from the source site log, and applying the data changes to the destination site. The method then includes identifying one or more constraints for each data change and generating a hash value for each identified constraint. The method then dispatches the data changes into a plurality of streams based on the hash values. The hash values are used to identify conflicting data changes and determine how the data changes should be sent through the plurality of streams such that data changes are applied in parallel in the plurality of streams without violating any causal consistency constraints. The method then includes committing data changes in the plurality of streams to the destination site in a single transaction, creating a consistent transactional view.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08756192&OS=08756192&RS=08756192
owner: Microsoft Corporation
number: 08756192
owner_city: Redmond
owner_country: US
publication_date: 20100107
---
Replication is the ability to track dispatch and apply data changes from one database to another. In a Relational Database Management System RDBMS data changes in a database are naturally logged for recovery purposes. System metadata for replication determines what data are being changed and the changes that are to be replicated. The data to be replicated may include the whole database entire tables selected rows from tables specified columns from tables or stored procedures. The data changes in the log may then be marked for replication. The data changes may be split into consecutive log ranges such that each log range of data changes are replicated to the destination site in one batch. In conventional transactional replication data changes are typically dispatched from a source site and applied at a destination site through a single connection or stream. While a single stream ensures that the data changes are applied at the destination site in the same order as the source site to maintain transactional consistency there are throughput limitations that can occur due to this method.

In order to avoid these throughput limitations replication is performed using multiple streams i.e. multi streaming such that the data changes are applied in parallel at the destination site. This parallelism leads to higher throughput and performance for the system as a whole. When performing replication using a multi streaming approach the data changes within a batch are dispatched into multiple distinct streams based on a predefined hashing key. Consequently the data changes that are dispatched in different streams may be applied at the destination site in a different order from which they were performed at the source site. Applying data changes out of order may violate one or more causal consistency constraints among conflicting data changes. Examples of causal consistency constraints include primary key constraints unique constraints and foreign key constraints.

Described herein are implementations of various technologies for dispatching conflicting data changes into multiple streams. Conflicting data changes may refer to two or more data changes that involve the same causal consistency constraints and would violate these causal consistency constraints if they were applied out of order at the destination site. In one implementation a computer application may receive a log from a source site that includes a number of data changes and the order of the corresponding data changes that may be applied at a destination site. The computer application may then extract all of the data changes from the log. After extracting the data changes from the log the computer application may generate a set of hash values such that each constraint in each data change has a hash value.

In generating the set of hash values the computer application may use a hash function to create a hash value for each constraint. The computer application may then concatenate all of the hash values for one data change into a binary string and associate this binary string with its corresponding data change from the log.

In one implementation the source site and the destination site may be connected to each other by multiple streams. Initially if all of the streams connecting the source site to the destination site are empty the computer application may dispatch non conflicting data changes evenly into multiple streams following the order as indicated in the log. After the data changes have been dispatched but before the data changes have been committed at the destination site the computer application may maintain a hash table to keep track of the data changes that have been dispatched. Each entry in the hash table may include a constraint hash value a constraint identification ID and the ID of the stream in which the data change has been dispatched.

In one implementation before dispatching a subsequent data change the computer application may search the hash table to determine whether an entry in the hash table has the same constraint ID and the same constraint hash value i.e. constraint pair of the subsequent data change. If the computer application does not find a matching entry in the hash table the computer application may dispatch the subsequent data change into a stream such that the data changes are evenly distributed into the multiple streams. When the subsequent data change is dispatched the computer application may add the subsequent data change s constraint ID constraint hash value and stream ID as a new entry into the hash table.

If however the computer application finds a matching entry in the hash table the computer application may determine that the subsequent data change and the data change related to the constraint ID and the constraint hash value in the matching entry are conflicting. The computer application may then dispatch the subsequent data change into the same stream where the matching entry s data change was dispatched to ensure that the two data changes are performed in their original relative order.

In one implementation the subsequent data change may conflict with more than one entry in the hash table. In this case the computer application may dispatch conflicting data changes into different streams but the computer application may throttle the stream of the subsequent data change to avoid violating any constraint. By throttling the stream of the subsequent data change the computer application may ensure that the prior data changes occur before the subsequent data change such that the data changes are performed at the destination in the correct order. In any of the implementations described above all of the data changes dispatched into various streams in one batch may be delivered to the destination site in a single transaction and therefore applied together.

The above referenced summary section is provided to introduce a selection of concepts in a simplified form that are further described below in the detailed description section. The summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter. Furthermore the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure.

In general one or more implementations described herein are directed to dispatching conflicting data changes into multiple streams. Various techniques for dispatching conflicting data changes into multiple streams will be described in more detail with reference to .

Implementations of various technologies described herein may be operational with numerous general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with the various technologies described herein include but are not limited to personal computers server computers hand held or laptop devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and the like.

The various technologies described herein may be implemented in the general context of computer executable instructions such as program modules being executed by a computer. Generally program modules include routines programs objects components data structures etc. that performs particular tasks or implement particular abstract data types. The various technologies described herein may also be implemented in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network e.g. by hardwired links wireless links or combinations thereof. In a distributed computing environment program modules may be located in both local and remote computer storage media including memory storage devices.

The computing system may include a central processing unit CPU a system memory and a system bus that couples various system components including the system memory to the CPU . Although only one CPU is illustrated in it should be understood that in some implementations the computing system may include more than one CPU. The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. By way of example and not limitation such architectures include Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnect PCI bus also known as Mezzanine bus. The system memory may include a read only memory ROM and a random access memory RAM . A basic input output system BIOS containing the basic routines that help transfer information between elements within the computing system such as during start up may be stored in the ROM .

The computing system may further include a hard disk drive for reading from and writing to a hard disk a magnetic disk drive for reading from and writing to a removable magnetic disk and an optical disk drive for reading from and writing to a removable optical disk such as a CD ROM or other optical media. The hard disk drive the magnetic disk drive and the optical disk drive may be connected to the system bus by a hard disk drive interface a magnetic disk drive interface and an optical drive interface respectively. The drives and their associated computer readable media may provide nonvolatile storage of computer readable instructions data structures program modules and other data for the computing system .

Although the computing system is described herein as having a hard disk a removable magnetic disk and a removable optical disk it should be appreciated by those skilled in the art that the computing system may also include other types of computer readable media that may be accessed by a computer. For example such computer readable media may include computer storage media and communication media. Computer storage media may include volatile and non volatile and removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media may further include RAM ROM erasable programmable read only memory EPROM electrically erasable programmable read only memory EEPROM flash memory or other solid state memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by the computing system . Communication media may embody computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and may include any information delivery media. The term modulated data signal may mean a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media may include wired media such as a wired network or direct wired connection and wireless media such as acoustic RF infrared and other wireless media. Combinations of any of the above may also be included within the scope of computer readable media.

A number of program modules may be stored on the hard disk magnetic disk optical disk ROM or RAM including an operating system one or more application programs a replication application program data and a database system . The operating system may be any suitable operating system that may control the operation of a networked personal or server computer such as Windows XP Mac OS X Unix variants e.g. Linux and BSD and the like. The replication application will be described in more detail with reference to in the paragraphs below.

A user may enter commands and information into the computing system through input devices such as a keyboard and pointing device . Other input devices may include a microphone joystick game pad satellite dish scanner or the like. These and other input devices may be connected to the CPU through a serial port interface coupled to system bus but may be connected by other interfaces such as a parallel port game port or a universal serial bus USB . A monitor or other type of display device may also be connected to system bus via an interface such as a video adapter . In addition to the monitor the computing system may further include other peripheral output devices such as speakers and printers.

Further the computing system may operate in a networked environment using logical connections to one or more remote computers . The logical connections may be any connection that is commonplace in offices enterprise wide computer networks intranets and the Internet such as local area network LAN and a wide area network WAN . When using a LAN networking environment the computing system may be connected to the local network through a network interface or adapter . When used in a WAN networking environment the computing system may include a modem wireless router or other means for establishing communication over a wide area network such as the Internet. The modem which may be internal or external may be connected to the system bus via the serial port interface . In a networked environment program modules depicted relative to the computing system or portions thereof may be stored in a remote memory storage device . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

It should be understood that the various technologies described herein may be implemented in connection with hardware software or a combination of both. Thus various technologies or certain aspects or portions thereof may take the form of program code i.e. instructions embodied in tangible media such as floppy diskettes CD ROMs hard drives or any other machine readable storage medium wherein when the program code is loaded into and executed by a machine such as a computer the machine becomes an apparatus for practicing the various technologies. In the case of program code execution on programmable computers the computing device may include a processor a storage medium readable by the processor including volatile and non volatile memory and or storage elements at least one input device and at least one output device. One or more programs that may implement or utilize the various technologies described herein may use an application programming interface API reusable controls and the like. Such programs may be implemented in a high level procedural or object oriented programming language to communicate with a computer system. However the program s may be implemented in assembly or machine language if desired. In any case the language may be a compiled or interpreted language and combined with hardware implementations.

The distributor may manage the flow of data through the replication system . The distributor may contain a distribution database that tracks the changes to publications that need to be replicated from publishers . In one implementation the publisher may act as its own distributor . In another implementation a remote database server may be used to hold the distribution database.

The subscriber may be a server or database that receives data from the publisher or any other database. A subscription is the group of data that a server or database on the subscriber will receive. The subscription may include one or more publications. Further the subscriptions may be push or pull subscriptions. A push subscription is a subscription when the publishing server will periodically push transactions out to the subscribing server or database. A pull subscription is subscription when the subscribing server will periodically connect to the distribution database and pull information.

In a conventional replication system data changes are typically applied through a single connection between the distributor and the subscriber . While the single connection may ensure that the transactions will be applied at the subscriber in the exact order as the transactions occur on the publisher there may be performance losses with this method because the subscriber may be under utilized especially when requests are from a heavily loaded publisher .

The replication system in may overcome some of the limitations of the conventional transactional replication system by utilizing multiple streams at the subscriber to apply the data changes. In one implementation the distributor may store the data changes it received from the publisher in a command queue. The distributor may then read and sort through the data changes and send the data changes to any of the streams that may connect the distributor to the subscriber .

As mentioned above when sending data changes to different streams the data changes may be applied at the subscriber in a different order from which they were stored in the command queue. In one implementation applying data changes out of order may violate one or more causal consistency constraints among conflicting data changes. Descriptions of causal consistency constraints such as a primary key constraint a unique constraint and a foreign key constraint are provided below.

In one example of a constraint violation a table tab may have a primary key k on column k of the table tab . The data changes may include deleting a row with the primary key k and then inserting the primary key k again on the table tab. Accordingly the data changes may be as follows 

In another example of a constraint violation a table tab may have a primary key on column k and a unique key constraint on column x. For example the data changes may include updating the primary key of row k . . . x . . . from k to k. In this example the update data change is equivalent to a delete data change followed by an insert data change that is a delete from tab where k k data change should be executed before a insert tab values k . . . x . . . data change is executed as shown below.

In yet another example of a constraint violation a table tab may have a primary key on column k and a foreign key column x. The foreign key column x in table tab refers to the primary key column x of a table tab. In the data changes provided below a row x . . . is inserted into the table tab and then a row k . . . x . . . is inserted into table tab. 

At step the replication application may receive a log from a source site that includes all of the data changes and the order of the data changes that may be applied at a destination site. The log may include the logical data changes performed at the source site. In one implementation the source site may be the publisher and the destination site may be the subscriber . As such the replication application may be stored in the distributor .

At step the replication application may extract all of the data changes from the log received at step . After extracting the data changes from the log at step the replication application may generate a hash value for each constraint in each data change extracted from the log. In one implementation a table marked for replication may have a primary key constraint a unique constraint a foreign key constraint or combinations thereof. As such for an insert or delete data change the replication application may generate one or more hash values for each constraint using a hash algorithm. The hash algorithm may be configured to generate the same hash value for two or more identical constraints. For example if a first data change e.g. delete k from column k in table 1 has a primary key constraint on column k of table 1 the replication application may generate a hash value for the primary key constraint with respect to the column k and table 1. Then if a second data change e.g. insert k into column k of table 1 also has a primary key constraint on column k of table 1 the replication application may generate an identical hash value for the second data change as it did for the first data change. In this manner the replication application may be able to determine whether two or more data changes are conflicting by identifying whether the data changes have the same hash values. In order to generate the hash values all column values of the primary key constraint the unique constraint and the foreign key constraint may be logged into the computer system .

In one implementation the replication application may generate hash values for an update data change as follows. If no primary key columns are modified the replication application may generate one hash value for the primary key constraint. If however a column of the primary key constraint is modified the replication application may log both pre and post images of all column values of the primary key constraint. The replication application may generate a first hash value for the pre image and a second hash value for the post image of the modified primary key constraint.

If a column of a unique constraint is modified the replication application may log both pre and post images of all column values of the unique constraint in the computer system . The replication application may then generate a first hash value for the pre image and a second hash value for the post image of this unique constraint.

If a column of a foreign key constraint is modified the replication application may log both pre and post images of all column values of the foreign key constraint in the computer system . The replication application may then generate a first hash value for the pre image and a second hash value for the post image of this foreign key constraint.

At step the replication application may concatenate all of the hash values generated for a single data change into a binary string. The replication application may then associate the binary string with the single data change. In one implementation the replication application may concatenate the hash values such that the first part of the binary string may include the number of primary key constraint hash values the number of unique constraint hash values and the number of foreign key constraint hash values that were generated for the corresponding data change. The second part of the binary string may include an identification number ID of the table to which the data change is directed and the hash values corresponding to the column values of the primary key constraints. The third part of the binary string may include an ID of the unique constraint and the hash values corresponding to the column values of the unique constraint. In one implementation the binary string may not include a unique constraint if the data change does not have a unique constraint. The fourth part of the binary string may include the ID of the table being referenced by a foreign key constraint and the hash values corresponding to the column values of the foreign key constraint. In one implementation the binary string may not include a foreign key constraint if the data change does not have a foreign key constraint.

At step the replication application may determine whether the hash values generated at step exist in a hash table. The hash table may include a record of the data changes that may have been dispatched into one or more streams. In one implementation each entry in the hash table may include a constraint hash value a constraint ID and the ID of the stream in which the data change has been dispatched. For a primary key constraint the constraint ID may be the ID of the table to which the data change is directed. For a unique constraint the constraint ID may be the ID of the unique constraint. For a foreign key the constraint ID may be the ID of table which this foreign key refers to. A table may have one primary key constraint zero or more unique constraints and zero or more foreign key constraints.

In one implementation the hash table may be organized such that every constraint of a dispatched but not yet committed data change may have an entry in the hash table. The entry of the hash table may include a constraint pair i.e. constraint hash value constraint ID where the constraint ID is the ID of the table to which the data change is directed the unique constraint or the foreign key referenced table. The constraint hash value may include the hash value of the corresponding primary key column values unique constraint column values or foreign key column values. Each entry in the hash table may also contain an ID of the stream where the corresponding data change is dispatched. In one implementation all of the entries in the same bucket of the hash table may be double linked together. As such the entries which are hashed into the same bucket may be chained together by double links i.e. each entry has two pointers one pointing to its next entry and the other pointing to its previous entry . In another implementation all of the entries in the hash table for the same stream may be linked into a stack. Each stream may have a pointer pointing to the top of this stack.

Referring back to step if the replication application does not find a matching entry in the hash table the replication application may proceed to step .

At step the replication application may dispatch the data change into multiple streams following the order as indicated in the log. In one implementation the replication application may dispatch the data change into a stream by hashing the hash value of the primary key into the stream ID. By hashing the hash value of the primary key into the stream ID the replication application may evenly distribute data changes into multiple streams. In one implementation in order to evenly distribute data changes into multiple streams the replication application may use a mod operation to map the generated hash value to a stream ID. For example in a replication system that has 64 streams if the replication application generates a hash value of 123 for a particular data change then the replication application may map particular data change to the stream ID that is equal to 59 i.e. 123 64 59 . After the current data change is dispatched the replication application may add the current data change s constraint ID constraint hash value and stream ID into the hash table as a new entry.

Referring back to step if the replication application finds a matching entry in the hash table the replication application may determine that the data change in the matching entry and the current data change being evaluated are conflicting data changes. In one implementation a matching entry may have the same constraint ID and the same constraint hash value as the data change being dispatched. After determining that the data changes are conflicting the replication application may proceed to step .

At step the replication application may dispatch the current data change being evaluated into the same stream where the matching entry s data change was dispatched to ensure that the data changes are performed in their original order. By dispatching the current data change into the same stream where the matching entry s data change was dispatched the replication application may ensure that no causal consistency constraint violations will occur when the data changes are applied to the destination site. In one implementation each data change may be dispatched into a stream then applied to the destination site and then committed at the destination site.

After the current data change is dispatched the replication application may add the current data change s constraint ID constraint hash value and stream ID into the hash table as a new entry if the current data change s constraint value does not exist in any of the entries of the hash table. For example the current data change may have two constraint hash values such that the first constraint hash value is listed in at least one entry in the hash table and the second constraint hash value is not listed in the hash table. Here the replication application may not add a new entry to the hash table for the first constraint hash value because the first constraint hash value is already listed in one entry in the hash table. However the replication application may add a new entry to the hash table for the second constraint hash value because the second constraint hash value does not exist in the hash table.

Dispatching a data change may include dispatching or transmitting the data change to a stream i.e. putting a data change into a buffer for this stream . Applying the data change may include executing the buffered data change at the destination before the associated transaction has been committed. Committing the data change includes committing the associated transaction of the data change.

In one implementation the current data change may conflict with more than one entry in the hash table. In this case among the streams that have the conflicting data changes the replication application may throttle the stream containing the least number of data changes and immediately apply and commit data changes of other streams. After applying and committing the data changes of the other streams the replication application may resume the throttled stream and dispatch the current data change into this stream. By throttling the stream of the current data change the replication application may ensure that the prior conflicting data changes may be applied at the destination before the current data change such that the data changes are performed at the destination in the correct order. In another implementation the replication application may delay or throttle the dispatch of the current data change until the data changes of all the matching hash table entries have been committed.

For example suppose a table tab has a primary key on column k and a foreign key column x referencing the primary key column x of the table tab. The source site may perform the following data changes 

In one implementation when a batch of data changes is replicated the replication application either commits all of the data changes in the batch at the destination site or none of the data changes in the batch. In this manner the replication application ensures that the data at the destination site conforms to the data at the source site. Accordingly when replicating a batch of data changes using multi streaming the replication application may invoke a dispatcher thread to start a root transaction. In one implementation the dispatcher thread may dispatch the data changes into a buffer of a stream . Each stream may have a fixed buffer size that may hold data changes which have been dispatched to a corresponding stream but not yet applied at the destination site. After the dispatcher thread dispatches the data changes into the buffer of the stream and before the data changes are applied at the destination site each stream may invoke a worker thread that may enlist a nested transaction related to the data changes in the buffer into the root transaction. Here the root transaction may include multiple nested transactions such that all the nested transactions of the same root transaction may either all be committed or all be aborted. As such it is not possible for some nested transactions to be committed while others are aborted.

In one implementation a nested transaction may use locks when applying the data changes. Two different nested transactions of the same root transaction may use these locks to indicate that the two different nested transactions are not compatible with each other. For instance after a first nested transaction has performed a data change it may hold a lock on the piece of data related to the data change. Then if a second nested transaction tries to make a conflicting data change on the same piece of data the latter data change i.e. by the second nested transaction may be blocked until the prior nested transaction commits. Once the prior nested transaction commits the prior nested transaction s locks may be transferred to the parent root transaction. As such the latter data change may no longer conflict with the prior data change and the latter nested transaction may resume. In one implementation if the latter nested transaction is aborted all the nested transactions of the same root transaction including any previously committed nested transactions may also be aborted.

In the previous example i.e. delete from tab where k k data change is dispatched into Stream a constraint pair table tab has value k may already exists in the hash table. Here if the insert tab values k . . . x . . . data change is dispatched into Stream a constraint pair table tab hash value x may be inserted into the hash table as a new entry and associated with Stream. In order to ensure that the insert tab values x . . . data change which has been applied via Stream is visible to Stream the replication application may commit the nested transaction on Stream. After committing the nested transaction on Stream the replication application may transfer the locks of the nested transaction to the parent i.e. the root transaction . As such the insert tab values k . . . x . . . data change on Stream will not be blocked by these transferred locks.

In one implementation a data change being dispatched to a stream may first be appended to the stream s buffer. After a stream s buffer becomes full data changes in this buffer may then be applied to the destination site and these data changes may then be removed from the buffer. In one implementation the worker thread may apply data changes multiple times before committing its nested transaction. When a worker thread commits its nested transaction the locks of this nested transaction are transferred to the parent i.e. the root transaction . After this transfer any later conflicting data changes will not be blocked by these locks.

After a worker thread commits its nested transaction for its stream with all previous data changes in the stream thus being committed the replication application may then remove from the hash table all the entries for those data changes which are linked in a stack. In one implementation the hash table may include entries which are hashed into the same bucket and may then be chained together by double links. The double links in the bucket of the hash table may be used to efficiently remove an entry from the bucket. After the entries are removed from the hash table the worker thread may start a new nested transaction and enlist it into the root transaction. Subsequent data changes dispatched into this worker thread s stream may be wrapped into this new nested transaction.

In one implementation all log records of parallel nested transactions of the same root transaction may be linked into one single chain. This single chain may ensure that the nested transactions may be undone correctly in case of abort. Any failure of a nested transaction may abort all prior committed nested transactions of the same root transaction.

Deadlocks may occur when the replication application applies conflicting data changes to the destination site in multiple streams. In one implementation deadlocks may not exist among data changes themselves given that 1 each data change modifies only one row 2 any two conflicting data changes involving the same constraint pair are not scheduled simultaneously i.e. one data change does not start until the prior conflicting data change has finished and 3 for any two conflicting data changes either they are dispatched into the same nested transaction thus the same stream or the prior conflicting data change should be committed by a prior nested transaction before the latter data change starts.

However deadlocks may exist due to a lock escalation at the destination site or due to different lock granules being used between the source site and the destination site e.g. row locks are disabled at the destination site but enabled at the source site . Here the replication application may detect such types of deadlocks and abort the whole root transaction. The replication application may then try to re apply the whole batch using a different hash function of hashing primary key hash values into a stream ID to reduce the possibility of running into the same deadlock again. If the size of the hash table becomes a concern the replication application may increase the frequency of commit by committing a nested transaction after all current data changes in the buffer are applied. Once a worker thread commits its nested transaction entries for the data changes included in this nested transaction may then be removed from the hash table. In one implementation more frequent commits may also reduce the possibility of deadlocks.

In another implementation the replication application may reduce the number of commits by eliminating the commit which occurs when multiple matching entries are located in the hash table before dispatching the data change. Here all the data changes in a batch may need to be pre processed to identify clusters of conflicting data changes. Any two conflicting data changes may then be categorized into the same cluster. Data changes of the same cluster may then be dispatched into the same stream. Different clusters of data changes may be dispatched into the same stream and committed in the same nested transaction. This would reduce the total number of nested transactions but the pre processing of the data changes may require an extra round of reading data changes which may degrade performance.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

