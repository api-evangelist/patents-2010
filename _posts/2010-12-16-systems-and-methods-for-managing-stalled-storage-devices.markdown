---

title: Systems and methods for managing stalled storage devices
abstract: Embodiments relate to systems and methods for managing stalled storage devices of a storage system. In one embodiment, a method for managing access to storage devices includes determining that a first storage device, which stores a first resource, is stalled and transitioning the first storage device to a stalled state. The method also includes receiving an access request for at least a portion of the first resource while the first storage device is in the stalled state and attempting to provide access to a representation of the portion of the first resource from at least a second storage device that is not in a stalled state. In another embodiment, a method of managing access requests by a thread for a resource stored on a storage device includes initializing a thread access level for an access request by a thread for the resource. The method also includes determining whether the storage device, which has a device access level, is accessible based at least in part on the thread access level and the device access level and selecting a thread operation based at least in part on the determination of whether the storage device is accessible. The thread operation may be selected from attempting the thread access request if the device is accessible and determining whether to restart the thread access request if the device is not accessible.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07971021&OS=07971021&RS=07971021
owner: EMC Corporation
number: 07971021
owner_city: Hopkinton
owner_country: US
publication_date: 20101216
---
This application is a continuation of U.S. patent application Ser. No. 12 057 302 filed Mar. 27 2008 entitled SYSTEMS AND METHODS FOR MANAGING STALLED STORAGE DEVICES which is hereby incorporated by reference herein in its entirety. U.S. patent application Ser. No. 12 057 302 was filed on the same day as the following U.S. Patent applications U.S. patent application Ser. No. 12 057 298 entitled SYSTEMS AND METHODS FOR MANAGING STALLED STORAGE DEVICES published as U.S. Patent Application Publication No. 2009 0248975 U.S. patent application Ser. No. 12 057 321 entitled SYSTEMS AND METHODS FOR A READ ONLY MODE FOR A PORTION OF A STORAGE SYSTEM published as U.S. Patent Application Publication No. 2009 0248765 and U.S. patent application Ser. No. 12 057 303 entitled SYSTEMS AND METHODS FOR A READ ONLY MODE FOR A PORTION OF A STORAGE SYSTEM published as U.S. Patent Application Publication No. 2009 0248756 all of which are hereby incorporated by reference herein in their entirety.

A portion of the disclosure of this patent document includes material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure as it appears in the Patent and Trademark Office patent file or records but otherwise reserves all copyrights whatsoever.

Embodiments disclosed herein relate generally to systems and methods for managing stalled storage devices in a storage system.

The amount of data stored on digital computing systems has increased dramatically in recent years. Accordingly users have become increasingly reliant on a storage system associated with these computing systems to safely store this data. The storage system may include one or more storage devices such as for example one or more hard disk drives. On occasion a storage device may experience a problem that for example causes the storage device to respond relatively slowly to access requests for data stored on the device. While the storage device is experiencing the problem the storage device may appear for example to other storage devices and to processes on the computing system to have stalled. In some cases for example the problem may cause the storage device to fail. In other cases for example the storage device may be able to recover from the problem and return to normal operation. In some computing system implementations a stalled storage device may adversely affect the performance of computing system.

Because of the foregoing challenges and limitations there is a need to provide systems and methods for managing stalled storage devices. In various embodiments the disclosed systems and methods are generally applicable to a storage system comprising one or more storage devices and in certain embodiments to a clustered storage system comprising a plurality of storage nodes.

An embodiment of a method for managing access to storage devices is described. The method comprises determining that a first storage device is stalled and transitioning the first storage device to a stalled state. A first resource is stored on the first storage device. The method further comprises receiving an access request for at least a portion of the first resource while the first storage device is in the stalled state and attempting to provide access to a representation of the portion of the first resource from at least a second storage device that is not in a stalled state.

In another embodiment a computer readable medium is described. Executable instructions are stored on the computer readable medium that when executed by a processor cause the processor to perform a method for managing access to storage devices. The method for managing access to storage devices comprises determining that a first storage device is stalled and transitioning the first storage device to a stalled state. A first resource is stored on the first storage device. The method further comprises receiving an access request for at least a portion of the first resource while the first storage device is in the stalled state and attempting to provide access to a representation of the portion of the first resource from at least a second storage device that is not in a stalled state.

In another embodiment a system for managing storage on storage devices is described. The system comprises a first storage device that is configured to store a first resource and a second storage device that is configured to store a representation of at least a portion of the first resource. The system also comprises a software module that is configured to determine that the first storage device is stalled transition the first storage device to a stalled state receive an access request for the at least a portion of the first resource while the first storage device is in the stalled state and attempt to provide access to the representation of the portion of the first resource from at least the second storage device.

In another embodiment a method of managing access requests by a thread for a resource stored on a storage device is described. The method comprises initializing a thread access level for an access request by a thread for a resource stored on a storage device. The storage device has a device access level. The method further comprises determining whether the storage device is accessible based at least in part on the thread access level and the device access level and selecting a thread operation based at least in part on the determination of whether the storage device is accessible. The thread operation may be selected from attempting the thread access request if the device is accessible and determining whether to restart the thread access request if the device is not accessible.

In another embodiment a computer readable medium is described. Executable instructions are stored on the computer readable medium that when executed by a processor cause the processor to perform a method of managing access requests by a thread for a resource stored on a storage device. The method of managing access requests comprises initializing a thread access level for an access request by a thread for a resource stored on a storage device. The storage device has a device access level. The method further comprises determining whether the storage device is accessible based at least in part on the thread access level and the device access level and selecting a thread operation based at least in part on the determination of whether the storage device is accessible. The thread operation may be selected from attempting the thread access request if the device is accessible and determining whether to restart the thread access request if the device is not accessible.

In another embodiment a system of managing access requests by a thread for a resource stored on a storage device is described. The system comprises a storage device for storing a resource and a software module. The software module is configured to initialize a thread access level for an access request by a thread for a resource stored on a storage device. The storage device has a device access level. The software module is further configured to determine whether the storage device is accessible based at least in part on the thread access level and the device access level and to select a thread operation based at least in part on whether the storage device is determined to be accessible. The thread operation may be selected from attempting the thread access request if the device is accessible and determining whether to restart the thread access request if the device is not accessible.

For purposes of this summary certain aspects advantages and novel features are described herein. It is to be understood that not necessarily all such advantages may be achieved in accordance with any particular embodiment. Thus for example those skilled in the art will recognize that the systems and methods may be embodied or carried out in a manner that achieves one advantage or group of advantages as taught herein without necessarily achieving other advantages as may be taught or suggested herein. Furthermore embodiments may include several novel features no single one of which is solely responsible for the embodiment s desirable attributes or which is essential to practicing the systems and methods described herein. Additionally in any method or process disclosed herein the acts or operations of the method or process may be performed in any suitable sequence and are not necessarily limited to any particular disclosed sequence.

These and other features will now be described with reference to the drawings summarized above. The drawings and the associated descriptions are provided to illustrate embodiments and not to limit the scope of the invention. Throughout the drawings reference numbers may be reused to indicate correspondence between referenced elements. In addition the first digit of each reference number generally indicates the figure in which the element first appears.

Systems and methods which represent one embodiment of an example application of the invention will now be described with reference to the drawings. Variations to the systems and methods which represent other embodiments will also be described.

For purposes of illustration some embodiments will be described in the context of a file system which may be a distributed file system. The present invention is not limited by the type of environment in which the systems and methods are used however and systems and methods may be used in other environments such as for example other file systems other distributed systems the Internet the World Wide Web a private network for a hospital a broadcast network for a government agency and an internal network for a corporate enterprise an Intranet a local area network a wide area network a wired network a wireless network and so forth. Some of the figures and descriptions however relate to an embodiment of the invention wherein the environment is that of a distributed file system. It is also recognized that in other embodiments the systems and methods may be implemented as a single module and or implemented in conjunction with a variety of other modules and the like. Moreover the specific implementations described herein are set forth in order to illustrate and not to limit the invention. The scope of the invention is defined by the appended claims and their equivalents.

One example of a distributed file system in which embodiments of systems and methods described herein may be implemented is described in U.S. patent application Ser. No. 10 007 003 entitled SYSTEMS AND METHODS FOR PROVIDING A DISTRIBUTED FILE SYSTEM UTILIZING METADATA TO TRACK INFORMATION ABOUT DATA STORED THROUGHOUT THE SYSTEM filed Nov. 9 2001 which claims priority to Application No. 60 309 803 filed Aug. 3 2001 U.S. Pat. No. 7 146 524 entitled SYSTEMS AND METHODS FOR PROVIDING A DISTRIBUTED FILE SYSTEM INCORPORATING A VIRTUAL HOT SPARE filed Oct. 25 2002 and U.S. patent application Ser. No. 10 714 326 entitled SYSTEMS AND METHODS FOR RESTRIPING FILES IN A DISTRIBUTED FILE SYSTEM filed Nov. 14 2003 which claims priority to Application No. 60 426 464 filed Nov. 14 2002 all of which are hereby incorporated by reference herein in their entirety.

For purposes of illustration some embodiments will also be described with reference to updating data structures in a file system using information stored in related data structures of the file system. Embodiments of a file system capable of updating data structures with information stored in related data structures of a file system are disclosed in U.S. patent application Ser. No. 11 255 337 titled SYSTEMS AND METHODS FOR ACCESSING AND UPDATING DISTRIBUTED DATA and is hereby incorporated by reference in its entirety.

For purposes of illustration embodiments of the disclosed system and methods will be described in the context of access to resources stored on a storage system. Embodiments of the systems and methods may be utilized for access to the broadest range of resources such as for example data metadata graphics files sound or video files images databases spreadsheets software programs and or processes. Resources may be utilized in environments such as for example computer systems network systems storage systems file systems telecommunications systems library systems inventory systems reservation systems retail systems on line systems financial systems and the like.

In various embodiments the resources are stored on a storage system that comprises one or more storage devices. Embodiments of the storage system may utilize any suitable type storage devices including for example semiconductor storage magnetic storage and or optical storage. For example resources may be stored on one or more of the following a hard disk drive partitions of a hard disk drive multiple hard disks local and or remote disks redundant mirrored and or striped disks SCSI devices RAID disk systems clustered storage systems and so forth. Additionally storage devices may include magnetic tapes floppy disks cartridges and optical disks such as CD ROMs or DVDs. Furthermore resources may be stored on storage devices such as volatile or nonvolatile memory comprising for example dynamic random access memory DRAM static random access memory SRAM non volatile random access memory NVRAM or read only memory ROM .

In some of the example embodiments described herein the resources will be discussed in terms of data stored on the storage system. In many cases the data will be organized in a file system such as for example a distributed file system . In certain embodiments in addition to storing data files on the storage devices the storage system may also store protection data associated with the data files. For example in some embodiments the protection data may comprise parity data and or error correction data which can be used to reconstruct portions of data that have been corrupted and or cannot be accessed. In other embodiments the protection data may comprise mirrored copies of the data. In still other embodiments data striping across multiple storage devices may be used. Other data protection methods may be utilized and in some embodiments more than one data protection method can be used to provide a high degree of reliability and recoverability for the data.

In one embodiment the communication medium comprises a Transmission Control Protocol TCP connection. In other embodiments the communication medium includes the World Wide Web WWW a Socket Direct Protocol SDP connection over Infiniband gigabit Ethernet a local area network a wide area network a wireless network a wired network a serial connection Internet Protocol IP over FibreChannel proprietary communication links connection based datagrams or streams and or connection based protocols. In some embodiments a storage node is a local area network that communicates with other storage nodes through a wide area network.

In certain embodiments the storage system comprises a clustered storage system the storage nodes and comprise nodes of the clustered storage system and the communication medium comprises a high performance low latency network such as for example gigabit Ethernet or Infiniband. The clustered storage system may comprise 1 2 3 6 10 24 48 100 or more nodes. In certain embodiments the clustered storage system may comprise a distributed file system and may use operating system software such as for example OneFS operating system software available from Isilon Systems Inc. Seattle Wash. .

In the example storage system illustrated in each node comprises respectively a state management module and a driver configured to communicate device specific commands to the storage devices . In certain embodiments each state management module communicates bi directionally with the driver via a bus or suitable communication subsystem.

The state management modules may comprise one or more processors and may include memory for example NVRAM flash memory cache memory and or a hard disk drive . In certain embodiments each state management module is configured to execute processes for i managing requests for the resources associated with its respective node and or for ii managing requests for resources associated with remote nodes. In some embodiments the state management modules may be used to maintain information relating to the nodes and or the storage devices currently available to the system . For example one or more of the state management modules may communicate to other nodes via the communication medium information relating to which storage nodes and or storage drives are currently present in the system their current operational state for example available or unavailable and or how much space is available on each node and or storage device. As an illustrative example the state management module of node A may be configured to send a notification to the state management module of the node B and the state management module of the node C when a storage device associated with node A for example the hard disk drive becomes unavailable when the storage device becomes available again and or when the storage device becomes permanently unavailable. As will be described other notifications may be used in certain embodiments of the system . Accordingly the system advantageously may direct access requests for data to the storage nodes and or storage devices that are currently available and that store the data and or the protection data associated with the data .

In certain embodiments one or more nodes called an accelerator node may include a state management module but not a driver and its associated storage device. In certain such embodiments accelerator node s may be used to improve the performance of the system by executing some or all of the processes for managing requests for resources that are stored on the nodes having storage devices. For example in some embodiments an accelerator node rather than a storage node may be used for system operations such as quota management load balancing snapshots backup and restore operations and so forth.

In certain embodiments the state management modules may be configured to use one or more protocols for coordinating activities among multiple nodes and or systems. For example embodiments of a protocol for coordinating activities among nodes are disclosed in U.S. patent application Ser. No. 11 262 306 entitled NON BLOCKING COMMIT PROTOCOL SYSTEMS AND METHODS filed Oct. 28 2005 which claims priority to U.S. Provisional Appl. No. 60 623 843 entitled NON BLOCKING COMMIT PROTOCOL SYSTEMS AND METHODS filed Oct. 29 2004 and U.S. patent application Ser. No. 11 449 153 entitled NON BLOCKING COMMIT PROTOCOL SYSTEMS AND METHODS filed Jun. 8 2006 all of which are hereby incorporated herein by reference in their entirety.

In the example storage system schematically illustrated in the state management modules and may be configured to execute one or more program modules that carry out embodiments of the methods described herein. The word module refers to logic embodied in hardware or firmware or to a collection of software instructions possibly having entry and exit points written in a programming language such as for example C or C . A software module may be compiled and linked into an executable program installed in a dynamically linked library or may be written in an interpreted programming language such as for example BASIC Perl or Python. It will be appreciated that software modules may be callable from other modules or from themselves and or may be invoked in response to detected events or interrupts. Software instructions may be embedded in firmware such as an EPROM. It will be further appreciated that hardware modules may be comprised of connected logic units such as gates and flip flops and or may be comprised of programmable units such as programmable gate arrays or processors. The modules described herein are preferably implemented as software modules but may be represented in hardware or firmware. Moreover although in some embodiments a module may be separately compiled in other embodiments a module may represent a subset of instructions of a separately compiled program and may not have an interface available to other logical program units. The modules may be stored in any type of computer readable medium or other computer storage device. All of the methods and processes described herein may be embodied in and partially and or fully automated via software code modules executed by one or more general purpose computers or processors. The software code modules may be stored in any type of computer readable medium or other computer storage device. Some or all of the methods and processes may alternatively be embodied in specialized computer hardware.

In certain embodiments one or more of the state management modules and may comprise a general purpose computer using one or more microprocessors such as for example a Pentium processor a Pentium II processor a Pentium Pro processor a Pentium IV processor an x86 processor an 8051 processor a MIPS processor a Power PC processor a SPARC processor an Alpha processor and so forth. In other embodiments one or more of the state management modules and may comprises a special purpose computer comprising one or more integrated circuits such as application specific integrated circuits ASICs field programmable gate arrays FPGAs and so forth.

The storage system may be configured to operate with a variety of one or more operating systems that perform standard operating system functions such as accessing opening reading writing and closing a file. It is recognized that other operating systems may be used such as for example Microsoft Windows 3.X Microsoft Windows 98 Microsoft Windows 2000 Microsoft Windows NT Microsoft Windows Vista Microsoft Windows CE Microsoft Windows ME Palm Pilot OS Apple MacOS Disk Operating System DOS UNIX IRIX Solaris SunOS FreeBSD Linux IBM OS 2 operating systems and so forth.

During normal operations the example storage system shown in will handle numerous access requests for resources stored on the system . Access requests can include for example read requests and write requests for the data and or protection data stored on the system . Storage devices may be able to respond to an access request on a relatively short time such as a few milliseconds for fast hard disk drives . However on occasion a storage device may experience a problem that causes the response to the access request to take longer than normal. The problem may be temporary and the device may soon return to normal operation or the problem may be permanent and the device may need to be removed from the storage system.

A storage device may respond slowly for a variety of reasons. For example as described above embodiments of the storage system may include one or more hard disk drives. An attempt to access for example read or write data on a hard disk drive may fail due to for example a mechanical imperfection on a disk platter for example an oil spot a speck of dust a pit caused by a crash of the read write head and so forth physical shock and or vibrations electrical and or magnetic interference and so forth. Many commercially available hard disk drives have a controller that automatically employs a sequence of techniques to recover from data access errors for example seek errors and or read errors . For example if a read error is detected the disk controller may first attempt to reconstruct the data using an error correction code ECC stored on the disk. If the controller is unable to reconstruct the data using the ECC the controller may wait for the disk platter to spin around and then the controller may retry the read. The controller may attempt several such retries and if the retries fail the controller in some embodiments may then invoke more advanced error correction algorithms that are more complex and take additional processing time. Finally if the advanced error correction algorithms fail the controller may return an error message for example an ECC error to the calling process. For many hard drives this automatic sequence of error correction techniques may take a considerable period of time for example from seconds to tens of seconds during which the response of the hard disk to access requests from the other devices nodes and or processes in the system will appear to be slow. 

The ability of a hard disk drive controller to provide automatic error correction is generally considered to be beneficial in computing environments where a single hard disk drive stores data without data protection schemes such as parity data striping or mirroring for example a personal computer . However storage system embodiments that implement data protection methods may be able to recover and or reconstruct data stored on a slow device by accessing protection data for example using parity data and or a mirrored copy on other storage devices in the system. Such storage system embodiments may be able to recover and or reconstruct the data more rapidly than waiting for the slow device to perform its automatic error correction protocol. Accordingly it may be advantageous for embodiments of the storage system to detect slow devices and redirect I O requests for data on the slow device to other storage nodes and or storage devices.

Moreover in some storage system embodiments a slowdown caused by one slow storage device and or node may cascade to other storage devices and or nodes thereby degrading the performance of the storage system as a whole. For example a system wide resource such as a vnode lock may be held on a data file portions of which are stored on the slow drive. Processes on other nodes attempting to access the locked data file will have to wait until the slow drive releases the lock before the other processes can access the data file. These other processes are effectively deadlocked by the slow drive. Additionally processes on other nodes will be slowed as they attempt to access data stored on the slow drive. Therefore in some embodiments a single slow drive disadvantageously may cause a global slowdown affecting all the storage devices in the storage system.

Storage devices at times may exhibit other types of non normal behavior additionally or alternatively to responding slowly to access requests for resources. The term stalled is used herein to describe storage devices exhibiting a broad range of non normal behaviors including but not limited to responding slowly and or erratically to access requests returning hardware firmware and or software errors such as for example ECC errors and so forth.

Therefore it may be advantageous for embodiments of a storage system to implement systems and methods for managing stalled storage devices. Such embodiments may provide more reliable and rapid access to resources stored on the system as well as avoiding or at least reducing the likelihood of a global slowdown caused by one or more stalled storage devices.

In this example of the method in state the system determines whether a storage device is stalled. For example in some embodiments the system performs one or more diagnostic tests further described below to determine whether the device is responding slowly compared to an expectation for normal response. If the device fails at least one of the one or more diagnostic tests the method proceeds to state in which the system transitions the device to a stalled state. For example in some embodiments the system performs a group change in which a message is communicated to other nodes for example node B and node C that the hard disk drive is going to enter the stalled state. The system additionally may redirect pending requests for data stored on the hard disk drive to other storage devices that may store a mirrored copy of the requested data and or parity data permitting reconstruction of the requested data . Once the group change is complete nodes on the system know that the drive is stalled and future access requests for data stored on the drive are routed to other nodes and or devices.

In this example the method continues in state in which the system evaluates whether the stalled device should remain in the stalled state or be transitioned to another state an example state model will be described below with reference to . For example in certain embodiments the system runs one or more evaluation tests to determine the nature and or extent of the problem on the stalled drive. The evaluation tests may include testing and or repair of the sectors of the drive that caused the slow response. In some embodiments if the system determines that the device is once again functioning normally the system may transition the device from the stalled state back to a normal operational state. However if the system determines that the device is not functioning normally the system may transition the device from the stalled state to a failed state.

Further examples of embodiments of the method for managing access to storage devices will now be described in the illustrative context of access to resources stored on the storage system . In these illustrative examples the access requests are input output I O requests for data stored on a storage device for example the hard disk drive on node A .

As described above stalled devices include storage devices that respond slowly to access requests for data stored on the device. In some embodiments of the method in state the system determines whether a device has stalled by measuring an I O response time for the device and comparing the measured I O response time to an expected I O response time for the device.

The time t may be measured in a variety of ways in various embodiments. For example the time t may be measured relative to a kernel s system time a real time clock a time stamp counter TSC a scheduling clock using sets of time and so forth. For a given clock the time may be measured to within a time resolution. For example in some embodiments of a scheduling clock the time resolution is about 10 ms. The time resolution of the clock may introduce round off errors into the measurement of a time period. For example if a request initiates just after a clock tick and completes just before the next clock tick the measured I O response time will be zero since the request was performed entirely within one clock tick. In contrast a very short request that straddles a clock tick for example initiates just before the clock tick and completes just after the clock tick will have a measured I O response time approximately equal to the time resolution of the clock. Accordingly it may be advantageous for embodiments of the disclosed systems and methods to take account of the time resolution of the measurements as a potential lower limit for the accuracy and precision of the I O response times.

In certain embodiments of the systems and methods for managing stalled storage devices I O response time measurements for example t t t and t are used at least in part in determining whether a particular storage device has stalled. There are a variety of factors that may influence measured I O response times in various embodiments of the storage system . In some embodiments time measurements may be affected by kernel load. For example in some implementations I O response time measurements are clocked in interrupt threads whose execution can be delayed by kernel activities. In some embodiments of the system the kernel load may cause time measurement delays of up to about 500 ms. Therefore I O response times of about 500 ms may in some cases indicate a high kernel load rather than a slow drive.

Other factors influencing time measurements depend on the type of storage device used by the system. In embodiments of storage systems using hard disk drives the time taken for an I O operation to complete includes seek time the time needed to move a read write head into position on the disk and media transfer time the time needed to transfer the data to the disk . Seek times can vary widely and may account for most of the completion time for the I O operation. However not every I O request causes a significant seek. For example to improve performance a storage system may attempt to allocate blocks of a data file near each other on the disk to reduce seek time during a sequential I O operation. Also file fragmentation may result in numerous small I O requests to regions of the disk located near each other so that seek time for a fragmented file may not in some cases be excessive. Although the media transfer time generally is much shorter than the seek time longer media transfers may comprise a larger number of disk sectors which may increase the likelihood of encountering a corrupted sector thereby causing a delay due to the disk s error correction protocol .

Disk caching may also have an influence on time measurements. For example in some embodiments writes are acknowledged to the system when the data is stored on the disk s cache rather than when the data is actually written to the storage medium. Therefore the measurement of the I O response time for a particular write operation may occur at a significantly later time than when the write was acknowledged. Read operations may be cached but in embodiments in which the cache size is sufficiently large and logical block addressing LBA is used read caching may not impact time measurements significantly.

As discussed above certain embodiments of the disclosed systems and methods have the capability to evaluate whether a stalled device should remain in the stalled state or be transitioned to some other state. In certain such embodiments common timing protocols are used both for determining whether a device is stalled and for determining whether a device is no longer stalled. One possible advantage of these embodiments is that it is less likely that a storage device will be transitioned back and forth between a stalled state and a non stalled state simply due to differences in the timing protocols for entering and exiting the stalled state.

Embodiments of the disclosed systems and methods may use a variety of techniques for determining whether a storage device and or a node has stalled. For example in the case of slow devices certain embodiments utilize one or more diagnostic tests or heuristics to determine whether the storage device response time is sufficiently slow to trigger the transition to the stalled state. Certain such embodiments advantageously may take into account time measurement factors described above such as for example clock time resolution kernel load disk seek time disk caching and so forth. Example diagnostic tests will now be described in the illustrative context of data I Os to a hard disk drive. In certain embodiments the storage system may utilize some or all of the following diagnostic tests. If a storage device fails one or more diagnostic tests the storage system identifies the device as stalled.

In this example diagnostic test a storage device that does not complete any I O operations within a first threshold time Tis considered to be stalled. For example with reference to the example shown in if any of the measured I O response times t t t or tare greater than or equal to Tthen the drive fails diagnostic test 1 and is considered to be stalled. In some embodiments a running timer is started whenever an I O response measurement starts so that the drive can be determined to be stalled if the running time T without waiting for the slow I O to complete which may never occur . In some embodiments a single I O failure or error such as an ECC error will cause the device to fail diagnostic test 1.

A storage device that is lightly loaded receives relatively infrequent I O requests. If an I O request is slow and fails diagnostic test 1 as further described below the device may be transitioned to the stalled state and evaluation procedures may be performed. If the evaluation procedures cause the device to transition out of the stalled state before the next I O request is received then this embodiment of managing access to stalled devices may do relatively little for system performance. However the evaluation procedures may identify and cause the repair of disk sectors likely to lead to future ECC errors which may reduce the likelihood of slow access on future I Os.

The choice of the first time threshold Tmay depend on one or more factors which include system dependent and or independent factors. For example the first time threshold Tmay be chosen to be longer than timing uncertainties due to clock time resolution about 10 ms in some embodiments kernel load about 500 ms in some embodiments and cache flushes on write operations. A possible advantage of such embodiments is a reduced likelihood that timing uncertainties will cause a healthy device to fail the diagnostic test 1 a false positive . In some embodiments the first threshold Tis short enough to catch disk drive error corrections such as retries. For example in certain disk drive embodiments the retry error correction sequence takes about 1 second. Therefore in certain embodiments the first time threshold Tis set to be about 900 ms so that the drive is identified as stalled before the retry error correction sequence returns a read error to the calling process. A threshold time Tof about 900 ms is longer than the 10 ms time resolution and the 500 ms kernel load uncertainty applicable to some system embodiments and will reduce the likelihood of false positives in these systems. In other embodiments a different threshold time Tmay be used for example 100 ms 250 ms 325 ms 400 ms 500 ms 750 ms 1120 ms 5000 ms and so forth. In certain embodiments the time threshold Tmay depend on time of day. For example the time threshold Tmay be longer during daytime than nighttime or vice versa .

In other embodiments the time threshold Tmay depend on one or a combination of other factors including a retry time for access requests to the device a seek time for the storage device an input output rate of the storage device a kernel delay a system load a load of the storage device a cache flush time and so forth. In some embodiments the storage system may monitor certain system parameters for example kernel load device load and so forth and dynamically adjust the threshold Tbased on current values of the system parameters.

In this example diagnostic test a storage device that experiences multiple slow I Os within a past time period is considered to be stalled. In this example test an I O is considered to be slow if its measured I O response time is longer than a second time threshold T which may be different from the first time threshold T. For example in embodiments using both diagnostic test 1 and diagnostic test 2 the second time threshold Tshould be shorter than the first time threshold T otherwise a single slow I O request will cause diagnostic test 1 to fail before diagnostic test 2 requiring multiple slow I Os is ever evaluated.

In some embodiments if a number of slow I Os is above a number threshold then the device fails diagnostic test 2. In other embodiments if a frequency of slow I Os for example number per unit time interval is above a frequency threshold then the device fails diagnostic test 2. The number and or the frequency may be determined by a weighted average over past slow I Os. The weighting may be a decaying function of the elapsed time since the slow I O occurred so that recently occurring slow I Os are weighted more heavily than slow I Os in the distant past. The elapsed time decay may be a linear decay an exponential decay a geometric decay or some other suitable time decay. The elapsed time decay may represent a time window in which events occurring outside the time window are given low or no weight. The elapsed time delay may include a combination of the above example time delays as well as others. Many variations are possible. An elapsed time decay is not used in some embodiments for example distant events are weighted the same as more recent events .

As an example of an embodiment of diagnostic test 2 the system accumulates a weighted number of slow I Os that occur in the last total number of I Os. If the weighted number of slow I Os equals or exceeds a number threshold then the device fails the example diagnostic test. In some embodiments the I Os accumulated for the example diagnostic test include read requests and not write requests. In some such embodiments read requests that return certain access errors are not included in the test for example if the read is to a bad sector on an ECC list . In one embodiment a device fails the example diagnostic test if the weighted number of slow I Os is greater than or equal to 200 of the last 2000 I Os.

In certain embodiments a weighted average may be used to determine for example the weighted number of slow I Os a weighted I O response time and so forth. In certain such embodiments the weighting may represent a time decay. For example in some embodiments the decay comprises multiplying the value of a quantity denoted by x by a decay factor for every subsequent sample denoted by an index n . The value x measured at sample n is represented as x. In some embodiments the value at the next sample decays to x D 1 D where D is a decay coefficient greater than one. At the following sample the value decays to x D 1 D . For each subsequent sample the value is multiplied by an additional factor of D 1 D . Thus the value at the j th subsequent sample will be equal to x D 1 D . In this example embodiment the value xdecays according to a geometric progression. This decay is also equivalent to an exponential decay with an exponential decay coefficient equal to ln D D 1 where ln represents the natural logarithm. For this example decay embodiment a value will decay to one half its original value in a half life given by ln 2 ln D ln D 1 . For example in some embodiments the decay coefficient D 2000 and the half life is 1386 samples. A relatively small value for the decay coefficient D results in a more rapid decay than a relatively larger decay coefficient. In some embodiments the decay coefficient D may be non constant and may depend on one or more factors including for example the value x the sample n a clock time and so forth. In some embodiments the decay coefficient D may be dynamically adjusted based on current values of system operating parameters for example kernel load device load and so forth .

As described above in some embodiments the diagnostic test 2 may use a weighted sum and or average of various values. In some of these embodiments a weighted sum Sof the last N values may be calculated as

The choice of the second time threshold Tmay depend on one or more factors which may be system dependent. For example the second time threshold Tmay be selected to be longer than timing uncertainties due to clock time resolution. The threshold Tmay be chosen to be longer than the full stroke seek time of a hard drive for example about 20 ms for some hard drives . Smaller values for Tmay cause more false positives due to kernel load delays. The value of Tmay be chosen based on device load. For example in some embodiments a smaller value is used for heavily loaded devices high rate of I Os and a lower value is used for lightly loaded devices low rate of I Os .

The choice of the number and or frequency threshold and or the weighting functions used to determine these thresholds may depend on one or more factors which may be system dependent. For example a number frequency threshold that is too low may cause diagnostic test 2 to fail more often than is desirable for example based on the processing costs of entering exiting the stalled state . A number frequency threshold that is too high may cause diagnostic test 2 to fail too infrequently and may allow a number of problematic I O events to go undetected.

One possible advantage of diagnostic test 2 is that it may be less sensitive than diagnostic test 1 to timing delays caused by kernel load because a single slow I O caused by kernel delay will not cause the device to fail diagnostic test 2 but could cause the device to fail diagnostic test 1 . There may be other differences between diagnostic tests 1 and 2 in various embodiments. For example in one embodiment utilizing both diagnostic test 1 and diagnostic test 2 a single I O failure or error will cause diagnostic test 1 to fail. Therefore if a drive fails diagnostic test 2 but not diagnostic test 1 then each of the multiple I Os causing test 2 to fail was slow compared to T but successful otherwise test 1 would have failed . Accordingly by suitably choosing the time thresholds T Tand the number or frequency threshold the diagnostic tests can be tailored to diagnose different types of device problems.

In certain embodiments the parameters of diagnostic test 2 for example T the number frequency thresholds the weighting functions and so forth may be dynamically adjusted based on current values of system operating parameters for example kernel load device load and so forth .

In this example diagnostic test a storage device that has been continuously busy for a third time threshold Tbut has not completed a suitable number of I Os during the time Tis considered to be stalled. A storage device is considered to be busy if there is at least one access request to the drive that has initiated but not yet completed. Diagnostic test 3 tends to aggregate the performance of multiple I Os allowing the device to have a higher level of performance at the expense of latency. The third time threshold Tmay be different from Tand or T. In some embodiments the third time threshold Tis much longer than Tand T. For example Tis about 30 seconds in one embodiment. In such embodiments diagnostic test 3 tends to gauge trends in device I O better than diagnostic test 1 or 2.

An advantage of this example is that diagnostic test 3 requires the device to be continuously busy which reduces the likelihood that round off errors will dominate time measurements. For example by measuring a number of I Os together in a continuous time period T the time resolution for example 10 ms in some embodiments is applied once to a larger time difference.

The choice of the third time threshold Tmay depend on one or more factors which may be system dependent or independent. For example the time Tmay be chosen to better represent device I O performance than the single slow I O time threshold T. The time Tmay be chosen to be sufficiently long so that a backlog of write cache data if present at the start of the busy time period will not cause the test to fail. The time Tmay be chosen to be significantly longer than the time delay caused by kernel load. In other embodiments the time threshold Tmay depend on one or a combination of other factors including a time resolution of a timing clock a retry time for access requests to the device a seek time for the storage device an input output rate of the storage device a kernel delay a system load a load of the storage device a cache flush time and so forth.

Diagnostic test 3 is evaluated if a storage device is continuously busy for the time period T and test 3 fails if a sufficient number of I Os in this time period are uncompleted. Therefore for test 3 to be evaluated the I O demand on the drive must be sufficiently high that the device is continuously busy for the time T. Accordingly larger values of Tare suitable for drives expected to have a high I O demand. For example if the value of Tis large and the I O demand is low the device will never be continuously busy for diagnostic test 3 ever to come into play. In certain embodiments the number of uncompleted I Os and the third threshold Tare based at least in part on access patterns of the devices in the system .

In certain embodiments the parameters of diagnostic test 3 for example Tand or the number of completed I Os may be dynamically adjusted based on current values of system operating parameters for example I O demand .

In this example a fourth time threshold is determined according to a dynamically adjustable set of I O times and expectations for outstanding I O requests. The fourth time threshold Tmay be dynamically adjusted as I Os complete.

In this example the expected behavior for outstanding write requests to a device is that the device will be able to complete the request in a time period denoted by max write . The time period max write may depend on properties of the device and is about 20 ms in some embodiments. In some embodiments of the system in contrast to reads which are acknowledged promptly there may be uncertainty over the time when the write is completely written to the storage medium. For example a caller may be notified of the completion of a write request when the write is cached on the device. Therefore in this example the fourth threshold time Tincludes an allowance for an assumed number of pending writes.

In some implementations of this example timeout model the number of pending writes tends to increase with each write and one or more heuristics may be used to reduce the expected number of outstanding writes in the time T. For example some of these heuristics include i reducing the number to 0 after completion of a cache flush ii limiting the number to be below a threshold which may be based on known device parameters iii if the device is idle reduce the number by a factor idle time assumed idle delay max write which may be rounded down so that the remainder is counted against the next request iv if the time for a read completion is greater than max read reduce the number by completion time max read max write which may be rounded up and v if the time for a write completion is greater than max write reduce the number by completion time max write which may be rounded up.

In various embodiments the storage system may provide one or more operational states for storage devices and or nodes. In certain embodiments the state management modules of the respective nodes use the current operational states of the storage devices and or nodes to determine how to allocate storage on the system where to direct access requests and so forth. For example in one example embodiment the operational states include an UP state a STALLED state and a DOWN state. During normal operations the device is in the UP state. If the system determines that the device has stalled see state in the system transitions the device to the STALLED state see state in . The system may then run evaluation tests on the stalled drive see state in . If the device passes the evaluation tests the system transitions the device from the STALLED state back to the UP state. If the device fails the evaluation tests the system transitions the device from the STALLED state to the DOWN state in which for example no further access requests are directed to the DOWN device. In some embodiments the system may restripe data away from the device in the DOWN state. If the DOWN device cannot be repaired the device may be removed from the system in certain embodiments. In other embodiments of the storage system additional and or different operational states may be used. An example of another embodiment of operational states in a storage system will now be described.

In the example shown in some of the states include a designation soft failed SF or not soft failed NSF . In this example a device may be soft failed if an error occurs on the device that is not related to a resource access request to the device for example a non ECC error . In some embodiments a device may be soft failed by a user or system manager. One possible advantage of system embodiments using the SF NSF designation is that the system provide operational states that distinguish between access request problems for example ECC errors and non access request problems for example non ECC errors . The example states shown in also reference restriping data from a device. In certain embodiments restriping takes data and or metadata stored on one device and redistributes the data among other devices and or nodes of the storage system . The restriping process may be used for example when one of the devices and or nodes experiences some type of failure such that the missing data may be regenerated and then restored on the system . The restriping process may also be used when one or more devices and or nodes are added to or removed from the storage system such that data may be added to the new devices nodes or redistributed to other devices nodes. Further examples of embodiments of a restriping process are provided in the above incorporated U.S. patent application Ser. No. 10 714 326 entitled SYSTEMS AND METHODS FOR RESTRIPING FILES IN A DISTRIBUTED FILE SYSTEM filed Nov. 14 2003 which claims priority to Application No. 60 426 464 filed Nov. 14 2002.

In certain embodiments the example state diagram shown in includes the following states for storage devices and or nodes of the example storage system shown in .

In this embodiment of a state model for devices a STALLED SF state is not provided. Therefore the STALLED NSF state will be referred to simply as the STALLED state. Other embodiments may provide a STALLED SF state.

The example state diagram shown in provides various transitions among the operational states. In the following the transitions will be described with reference to devices in the storage system . As noted above in other embodiments the operational states and transitions shown in may apply to devices nodes or both.

Examples of some of the transitions not involving the STALLED state will now be described for an embodiment of the storage system . When a device is added to the system the drive is brought into service in the UP NSF state. A device transitions from UP to DOWN without change of its NSF or SF designation when the device is unmounted or when the system fails to detect the device. The device may transition from the UP state to the DOWN SF state when a non ECC I O error occurs. For example in some embodiments the device may make the transition from the UP state to the DOWN SF state when a non ECC read error occurs or when a write error occurs one or both of which may indicate that the device has been disconnected from its controller . A device may transition from DOWN to UP when the device is mounted or when the system re detects the device after a detection failure. A user may request a device transition between the DOWN NSF state and the DOWN SF state. A DOWN NSF device prevents automatic initiation of restriping. Devices in the DOWN SF state initiate restriping in the absence of DOWN NSF devices . If a device transitions to the UP SF state restriping is automatically initiated unless there are DOWN NSF devices in the system . A device in the UP or DOWN state transitions to the DEAD state when it is to be removed from the system for example by user intervention . A device in the DEAD UP SF or DOWN SF state transitions to the GONE state after the restriping process removes all references to the device from the system . Additional and or different transitions may be allowed in other embodiments of the system .

Certain embodiments of the storage system include the STALLED state shown in for devices and or nodes that have stalled. Examples of transitions involving the STALLED state will be described for an embodiment of the storage system . A device in the UP NSF state may transition to the STALLED state if the device fails one or more of the diagnostic tests described above for example the device is responding slowly . In the example state diagram shown in this transition is marked timeout to indicate that in some embodiments the transition is performed if an access request times out. In some embodiments a user may request a device transition to the STALLED state for testing debugging purposes. A device in the STALLED state may transition to the UP NSF state if evaluation of the device indicates that the device is again responding normally. In this transition is marked recovery to indicate that in some embodiments the device has recovered or been repaired. In some embodiments this transition may be made by user request as part of device testing debugging. A device in the STALLED state may transition to the UP SF state if for example the device remains in the STALLED state for too long and or if the device has been in the STALLED state too many times during a past time period. In some embodiments the device can transition from the STALLED state to the UP SF state for any of the reasons that a device transitions from the UP NSF state to the UP SF state. In the example shown in this transition is marked failure to indicate that recovery and or repair attempts for example sector repair have failed to return the device to a normal operative state. A device may transition from the STALLED state to the DOWN NSF state through for example user intervention to suspend a stalled drive. A device may transition from the STALLED state to the DEAD state if the device is to be removed from the system .

In certain embodiments the storage system stores and or implements an embodiment of the following example procedure to abort and then restart pending I O requests to the device transitioning to the STALLED state. In this example embodiment the storage system s I O request structure which may be generally similar to the UNIX bio structure for block I O operations includes an abortable bit which is unset for example 0 by default. When the abortable bit is unset in an I O request to a device in the STALLED state the state management module will not abort the I O request and will send the request to the associated device driver. However when the abortable bit is set for example 1 on an I O request to a device in the STALLED state the state management module will return the request to the calling thread with an ESLOW error rather than directing the request to the device driver. In some embodiments the system s I O request structure provides the abortable bit for read requests but not for write requests because write requests to stalled devices may not slow the system as much as read requests to stalled devices. A possible advantage of providing an I O request structure with the abortable bit is that I O requests to a STALLED device can be aborted and redirected to non STALLED devices.

In the embodiment of the method shown in in state the abortable bit is set for pending I O requests to the stalled device and in state the pending abortable I O requests are aborted. Aborted I O requests are retuned to their calling threads with an ESLOW error code. For example if node B sends a request to node A for data stored on the stalled drive the state management module of node A returns the request to node B with an ESLOW error rather than sending the I O request to the driver which handles the low level access to the hard disk drive . In some embodiments I O requests in a request queue in a driver are aborted and I O requests that have been sent to the device controller are not aborted.

Each of the state management modules and has a data structure that includes information on the current operational state of the nodes and or devices of the system . In state the system initiates a group change in which the current operational state for the nodes and devices of the system is updated. For example in the system embodiment of the state management module may broadcast messages to the nodes and to notify these nodes that the drive is going to enter the STALLED state. The state management modules and update their respective data structures to reflect that the drive is in the STALLED state. Before the group change is complete it is possible that abortable I O requests may continue to be issued to the stalled drive . However in state the state management module will return such access requests with an ESLOW error code to the calling threads.

When the group change is complete all the nodes of the storage system have information on the current operational states for example UP NSF STALLED DOWN NSF and so forth of the nodes devices in the system . In state aborted I O requests are restarted as will be further described below and the system can direct data access requests to currently available devices for example devices in the UP NSF state rather than to devices in the STALLED state. Accordingly embodiments of the storage system that implement an embodiment of the system and methods disclosed herein advantageously can reduce the likelihood that future access requests will be made to slow and or erratic drives which may reduce the likelihood of a global slowdown of the system .

In certain embodiments in addition or as an alternative to the methods described herein the storage system may store and or implement various methods for managing devices that are unavailable for I O. For example in some implementations a device may be unavailable because the device has been transitioned to a DOWN state or a STALLED state and or because of an accidental disconnection an irregularity in a power supply an abnormal temperature an excessive duty cycle aberrant behavior and so forth. Therefore certain such embodiments of the storage system may store and or implement embodiments of one or more methods described in for example U.S. patent application Ser. No. 11 643 725 filed Dec. 21 2006 entitled SYSTEMS AND METHODS FOR MANAGING UNAVAILABLE STORAGE DEVICES which is hereby incorporated by reference herein in its entirety.

As described above in certain embodiments of the method shown in in state the system evaluates whether a device in the STALLED state should remain in the STALLED state or be transitioned to another state such as for example the states described with reference to .

In the following illustrative example the hard disk drive of node A of the embodiment of the storage system has been transitioned to the STALLED state. To evaluate the STALLED drive in this example the state management module forks off a background evaluation process to test the drive . In various embodiments the evaluation process may send various types of I O requests to the driver for data on the drive . The evaluation process may repeat one or more evaluation tests on the drive . In some embodiments each repeated test is delayed to reduce consumption of system CPU. If the drive passes one or more of the evaluation tests the system considers the drive recovered and the state management module transitions the drive back to the UP NSF state. In certain embodiments garbage collection may be initiated to collect any blocks that were orphaned while the drive was in the STALLED state.

In some system embodiments if the drive repeatedly fails the evaluation tests the state management module soft fails the drive and transitions it to the UP SF state. In other embodiments the state management module transitions the drive to the UP SF state if the drive remains in the STALLED state for more than a threshold time period. When a device is soft failed and in the UP SF state the system may utilize other types of diagnostic tests to determine the severity of problems with the device.

In various embodiments of the system a variety of evaluation tests may be used to determine if a stalled device has recovered and should be returned to the UP NSF state. For example evaluation tests may include testing and or repair of the stalled device. In certain embodiments where the stalled device is a hard disk drive the evaluation tests may perform some of the following actions.

As described above in transitioning a device to a STALLED state embodiments of the storage system may perform a group change operation so that nodes and or devices in the storage system have knowledge of the current operational state of the other nodes and or devices in the system . Certain embodiments of the system implement procedures to preferentially direct I O requests to normally operating devices for example devices in the UP NSF state rather than to slow or soft failed devices for example devices in the STALLED and or UP SF states . However in some cases the requested data may not be obtainable by accessing just the normally operating devices. For example if a client requests data that is stored on a stalled and or soft failed device the normally operating devices may not have a complete mirrored copy of the data and or may not have sufficient parity data to reconstruct the requested data. In such cases rather than returning an error to the client certain embodiments may permit limited access to stalled and or soft failed devices to obtain enough data to fulfill the client s request.

Accordingly certain such embodiments may implement a multi step I O restart mechanism. For example on a first I O attempt the I O request is directed to normally operating devices but not stalled or soft failed devices. If the first I O attempt fails a second I O attempt is directed to normally operating devices and stalled devices. If the second I O attempt fails a third I O attempt is directed to normally operating devices stalled devices and soft failed devices. In some embodiments if the failure of the first I O attempt was not due to the inability to obtain the data from a stalled device the second I O attempt is skipped and the restart mechanism goes directly to the third I O attempt because the requested data is likely on a soft failed rather than a stalled device .

Certain embodiments of the storage system store and or implement the multi step I O restart mechanism by defining i operational states ii capabilities and iii access levels for devices and or nodes of the system . Operational states have been described above with reference to the example shown in . In the following illustrative example storage devices and nodes may be in any of the operational states shown in with the exception that the STALLED state is not defined for nodes. In other embodiments a STALLED state may be used for nodes.

Capabilities for a device and or node include the I O operations that can be performed on the device and or node. In the following illustrative example the capabilities include whether the device node is readable READ or writeable WRITE . In other embodiments the capabilities may include whether new blocks may be allocated to and or freed from the device node whether data should be restriped away from the device node and so forth. Capabilities may be defined for any type of I O operation used by the system .

Access levels for a device and or node are defined to reflect a minimum access level an I O request thread must have for a particular capability of the device node. For example in some embodiments if the access level of an I O request thread exceeds the minimum access level for the device node then the I O thread is permitted to have access to the device node. In such a case the device node is accessible to the I O request thread. If the access level of the I O request thread is below the minimum access level for the device node then the device node is inaccessible and the I O request fails. In the following illustrative example the following access levels are used.

In some embodiments an enumeration is used to represent the access levels. For example NORMAL 0 READ STALLED 10 MODIFY STALLED 20 READ SOFTAIL 30 and NEVER hex infinity. A similar enumeration may be used for the access levels of I O request threads. In some embodiments a simple comparison of the I O thread s access level and the device node s access level determines whether the device node is available for the I O operation of the I O request thread.

In certain embodiments device and or node operational states and capabilities are reevaluated whenever there is a group change. For example the state management modules for each of the nodes in the example system may use a data structure for example an array to store information for the states and capabilities of each of the nodes devices of the system . Accordingly such embodiments of the system can readily determine the following types of information i determine the capability of a node device pair ii determine a list of all nodes that have a given capability and or iii determine a list of all devices in a given node that have a given capability. Also in some embodiments the access level of a node supersedes the access level of a device on that node. For example if a node is out of service for repair the system may assign the node an access level of NEVER. An I O request to that node will always fail even if some or all of the individual devices of the node have access levels of NORMAL.

Each capability access level pair maps to a certain set of device states and node states. Table 1 is an example of this mapping for this illustrative embodiment using the operational states shown in .

Each node state and drive state has a minimum access level for each capability. Tables 2 and 3 show the minimum access levels for reads and writes for nodes and drives respectively for this example embodiment. As can be seen from Tables 1 3 in this example embodiment writes are not permitted on soft failed devices or soft failed nodes.

As described above for any capability a device has a minimum access level for that capability see for example Table 2 for nodes and Table 3 for devices . In the example method the minimum access level for the storage medium is associated with the variable device level. The I O request thread is associated with two access level variables current level and desired level. The variables device level current level and desired level are used to store one of the access levels NORMAL READ STALLED MODIFY STALLED READ SOFTFAIL or NEVER.

In the example embodiment of the method shown in in state the system initializes the current level to be NORMAL and the desired level to be NEVER. The example method continues in state to determine whether the device is accessible to the thread. In this embodiment the device is accessible if the thread s current level is greater than or equal to the device s device level. If the device is accessible the method continues in state and the thread s I O request is attempted by the device. If the device is not accessible to the thread the method continues in state in which the desired level of the thread is set. In this example if the thread s desired level is less than the thread s current level or the device s device level is less than the thread s desired level then the thread s desired level is set to the device s device level.

In this example the method continues in state and determines whether the thread s I O request operation succeeded. For example if the device was accessible to the thread state the thread operation was attempted state and may have succeeded or failed on the device. If the device was inaccessible to the thread state the thread operation nonetheless could have been successful because the I O request was fulfilled by another device of the storage system . In state if the thread operation succeeded the method ends. If the thread operation did not succeed the method continues in state in which the system determines whether the operation can be restarted. In this embodiment a restart is possible if the thread s current level is less than the thread s desired level and the thread s desired level is not equal to NEVER. If a restart is possible the method continues in state in which current level is set equal to desired level and then the method returns to state to determine if the device is accessible. For example a group change may have occurred and the device s access level may have changed. If a restart is not possible the method continues in state in which an error is returned to the caller.

While certain embodiments of the invention have been described these embodiments have been presented by way of example only and are not intended to limit the scope of the present invention. For example certain illustrative embodiments of the disclosed systems and methods have been described with reference to managing access to stalled devices such as hard disk drives. The disclosed systems and methods are not limited by the illustrative examples. For example in other embodiments the disclosed systems and methods may be applied to managing access to stalled nodes on a clustered storage system. Many variations are contemplated.

Embodiments of the disclosed systems and methods may be used and or implemented with local and or remote devices components and or modules. The term remote may include devices components and or modules not stored locally for example not accessible via a local bus. Thus a remote device may include a device which is physically located in the same room and connected via a device such as a switch or a local area network. In other situations a remote device may also be located in a separate geographic area such as for example in a different location country and so forth.

