---

title: Method and apparatus for providing highly-scalable network storage for well-gridded objects
abstract: An apparatus comprising a plurality of storage nodes comprising a plurality of corresponding storage disks and configured to store data in a distributed manner between the storage disks that achieves a Redundant Array of Independent Disks-0 (RAID0) like performance based on positioning information and without indexing the distributed data. A network component comprising a storage disk configured to maintain a plurality of physical files for different user data that are mapped to different volumes, wherein the volumes are distributed between the storage disk and a second storage disk based on a RAID0 like data distribution scheme without being indexed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08996803&OS=08996803&RS=08996803
owner: Futurewei Technologies, Inc.
number: 08996803
owner_city: Plano
owner_country: US
publication_date: 20101210
---
The present application claims priority to U.S. Provisional Patent Application No. 61 361 247 filed Jul. 2 2010 by Guangyu Shi et al. and entitled Method and Apparatus for Providing Highly Scalable Network Storage for Well Gridded Objects which is incorporated herein by reference as if reproduced in its entirety.

Cloud storage is a model of network online storage where data is stored on multiple virtual servers generally hosted by third parties rather than being hosted on dedicated servers. A customer that requires data hosting services can buy or lease storage capacity from cloud storage hosting providers that operate large data centers. The data center operators provide resources in the background referred to as virtual resources according to the requirements of the customer and expose them as storage pools which customers can use to store files or data objects. The resource may be distributed across multiple servers. Typically cloud storage services may be accessed through a web service application programming interface API or through a Web based user interface. In some cloud storage systems service providers provide block based storage services to customers or users. Since a provider typically owns the infrastructure of the storage service it is beneficial for the provider to pursue efficient and highly scalable design and implementation for providing the storage service to customers. Thus one important design and implementation objective for providers is to maximize throughput and scalability such as in the case of relatively large systems.

In one embodiment the disclosure includes an apparatus comprising a plurality of storage nodes comprising a plurality of corresponding storage disks and configured to store data in a distributed manner between the storage disks that achieves a Redundant Array of Independent Disks 0 RAID0 like performance based on positioning information and without indexing the distributed data.

In another embodiment the disclosure includes a network component comprising a storage disk configured to maintain a plurality of physical files for different user data that are mapped to different volumes wherein the volumes are distributed between the storage disk and a second storage disk based on a RAID0 like data distribution scheme without being indexed.

In a third aspect the disclosure includes a method comprising receiving a request to write data to a storage system wherein the request comprises positioning information mapping the positioning information into a data volume data block and data sector combination hashing the data volume data block and data sector combination placing a first strip of the data at a location in the storage system based on the hash and distributing one or more second strips of the data at one or more subsequent locations in the storage system that are separated from the location by an offset.

These and other features will be more clearly understood from the following detailed description taken in conjunction with the accompanying drawings and claims.

It should be understood at the outset that although an illustrative implementation of one or more embodiments are provided below the disclosed systems and or methods may be implemented using any number of techniques whether currently known or in existence. The disclosure should in no way be limited to the illustrative implementations drawings and techniques illustrated below including the exemplary designs and implementations illustrated and described herein but may be modified within the scope of the appended claims along with their full scope of equivalents.

Disclosed herein is a system and method for providing a block based storage system with improved efficiency and scalability in comparison to other cloud storage systems. The design and implementation of the block based storage system may be suitable for handling file systems that are maintained on a plurality of storage units and thus provides better efficiency and scalability than other local hard disk drive HDD based systems. The block based storage system may scale the storage services efficiently with increasing user demand and without the need for substantially large data volume redistribution among different serving or storage nodes e.g. in a network. The block based storage system may be managed with relatively less operational requirements and thus less operational expense OPEX in comparison to other storage systems. The system scalability may be improved using a self managed distributed hash table DHT based method which may leverage the topology or hierarchy of the physical infrastructure of the storage system and the characteristics of target storage layout. The system throughput efficiency may also be improved using a data distribution mechanism which may achieve RAID0 like performance in a relatively highly distributed and highly scalable manner. Additionally a file layout may be used to improve the HDD or storage unit read write efficiency.

As shown in each case may comprise multiple disks . For instance a case may correspond to a shelf cabinet that comprises a stack of disks . The disks and the cases correspond to two hierarchical levels of the storage system topology which may comprise other levels not shown that provide a hierarchical storage nodes cluster architecture. For instance a plurality of cases may be part of a unit such as a unit of multiple cabinets and a plurality of units may be part of a container. A plurality of containers may also be part of a data center. The different storage levels of the storage system topology may correspond to or may be accessed and maintained by a plurality of storage nodes for example in a network. For instance each of the cases may correspond to a storage node. Alternatively a plurality of cases or less than all of the disks in the case may correspond to a single storage node.

The storage services provided by the storage system topology may include a Virtual Block Service VBS which is a block based storage service. In the case of well gridded object architecture such as in the storage system topology the VBS may support APIs or API functions similar to put start position key data and get start position key number of items . The get method above may be supported due to the well gridded object architecture for example where data objects may be distributed about proportionally over a plurality of storage nodes. In the put method the size of data may be used to define or indicate a data object.

When a user or customer requests to read and or write some files the storage system topology supported file system may redirect such request to the VBS using an Advanced Technology Attachment ATA over Ethernet AoE for instance to access the storage system devices over an Ethernet network. Using the AoE protocol to access the storage system devices in a VBS may be optional and other protocols or schemes may be used instead.

In a VBS system the storage system devices may be accessed using positioning information such as . The Block data and Sector data sizes may be fixed and the Volume size may be expanded according to user s demand. On the user side a local file system may manage the mapping of user visible positioning file information such as to the appropriate addressing information for placement of the file s data in the VBS system. Specifically the user s local file system may be configured to manage mapping files into locations in the data Volume e.g. in the storage target node s or disk s which may be divided into a grid of blocks and sectors within blocks of known and fixed sizes.

To address the scalability issue of the storage system a self managed DHT scheme may be used to leverage or take advantage of the system s physical infrastructure topology such as the storage system topology and other special characteristics that may be related to the target storage layout. For example in the case of the storage system topology it may be difficult to achieve non blocking Input Output I O over each pair of disks without substantial investment in the switching and routing infrastructure for the system. To avoid such cost a two layered DHT may be used which may handle an inter case level e.g. between the disks and an intra case level e.g. between the cases . The two layer DHT may comprise a first DHT for handling the inter case level which may be self managed or maintained by a storage target or node associated with each case . The two layer DHT may also comprise a second DHT for handling the intra case level which may be distributed between multiple storage nodes that correspond to different cases .

The DHT may be a decentralized distributed system among different storage nodes that provides a lookup service similar to a hash table. The DHT may comprise a plurality of key value pairs which may be used by the storage nodes to retrieve a value associated with a given key. The mapping from keys to values in the DHT may be distributed among the storage nodes where a change in the set of storage nodes may not substantially change the DHT entries. As such the DHT may scale to substantially large number of storage nodes and handle the arrivals departures and or failures of storage nodes.

To address the throughput efficiency a suitable data distributor may be used. The data distributor may be configured to achieve a RAID0 like performance in a distributed manner between the storage nodes . The data distributor may also be configured to determine how to split and distribute the data of one volume to a number of storage nodes to achieve a relatively high throughput. Typically existing DHT schemes may use hash functions such as Secure Hash Algorithm SHA 1 to randomize the data distribution. In this case the output key resulting after performing the function hash may be evenly distributed without ordering data blocks and or sectors e.g. according to a block number and or a sector number. In implementing such distribution scheme without ordering blocks and or sectors the data placement and retrieval operations may consume a substantial amount of network bandwidth and cause a lot of random access to each storage node.

One alternative for data distribution is using the Cassandra s order preserved partitioner method. However in a relatively large scale data center this method may lead to storing one specific volume on one or two storage nodes which may result in low performance when users attempt random read write access. Such method may negatively impact volume access e.g. uneven volume distribution by multiple applications or a single application that comprises multiple threads. The negative impact on volume access may increase as the level of concurrency and volume increases.

The hash function F may be used to generate a random number that indicates a random position on a DHT ID space. The DHT may be cyclical or have algebraic cyclical properties recurring values where ID values may belong to a range of cyclical or recurring values a ring . The data distribution scheme may comprise a DHT cluster that corresponds to the DHT ID space. The DHT cluster may comprise a plurality of nodes e.g. storage or host nodes that may be arranged logically in a ring configuration also referred to herein as a cluster node configuration NC . The nodes may correspond to the ID values in the DHT ID space.

A strip placement function Node may be used to map a strip of a volume to a host node as follows Node strip number where G is a magic step function strip number s is a function of a block id and the number of blocks that may fit in a strip or block id plus sector id and the number of sectors that may fit in a strip NC is the node configuration and Node is the strip placement function. The strip placement function Node may provide the target or host node for a strip of the volume. The magic step function may start from F V id and make a step size d around the ring of nodes such as Range . The Node may be a function of node configuration NC a hash function F that selects the starting position of a volume e.g. Volume V a step size d and a strip number s function.

The data distribution scheme may introduce a system parallelism parameter N that mimics the number of disks in a RAID0 system where one volume may be distributed and stored on N disks. As such the data distribution scheme may store a plurality of strips of one volume on N nodes. The strip number for each strip may depend on the block id and N where multiple blocks may fit into one strip. This relationship between the strip number the block id and N may be described as strip number block 

The strip number of the first strip of the volume may be set to zero Strip 0 the strip number of the second strip of the volume may be set to one Strip 1 and strip numbers of subsequent strips may be set similarly. As described above each strip number may be mapped to a node using the Node function. For example the Strip 1 s hosting node may be denoted by Node 1 . For purposes of brevity the magic step function G may be dropped from the expression above and hence the target or host node may be denoted as Node strip number .

The number of host nodes N for a volume N the step size or offset d and the number of blocks in each strip s may be defined in different ways. These parameters may be defined for a volume as a fixed header to a first strip Strip 0. Alternatively the parameters may be dynamically assigned as a configuration file for a volume at a node where the first strip may be expected for example using Node V id F d 0 NC . The second approach may be more attractive since the second approach may separate the volume stripping meta data from the volume s actual data. The function Node V id F d 0 NC to obtain the first strip may be independent of d which may be used when calculating the host nodes from the nodes except the first node that is designated for the first strip Strip 0 .

In one strip placement algorithm the number of nodes N that may be designated for the volume s different strips may be fixed. For example N is equal to four in . The offset d may also be fixed for example d Range F N where N and Range F are fixed. The strip placement algorithm may also use a cyclic pattern e.g. using the strip placement functions above to distribute the blocks within a volume to appropriate target strips on host nodes. As such a plurality of strips may be mapped on one or more nodes . For example the first node labeled 0 may store Strip 0 and Strip 4 a second node labeled 2 at offset d may store Strip 1 and Strip 5 a third node labeled 4 may store Strip 2 and Strip 6 and a fourth node labeled 6 may store Strip 3 and Strip 7.

In another strip placement algorithm an offset d value may be varied and the Range F may or may not be divisible by d. Thus N may be varied for different data volumes to maintain an about equal storage consumption or data distribution on each strip hosting node. Other algorithms or combinations of the two algorithms above or some of their features may also be used for volume strip placement on the nodes .

The data distribution scheme may determine the host node for a combination for requested data using relatively simple calculations as described above. As such the user may fetch multiple strips in different nodes if needed in a parallel fetching scheme where the strips may be obtained at about the same time from the different nodes . A local file system at the user environment may perform the parallel fetching scheme for instance for a volume that comprises a plurality files which may be accessed by a multi threaded application. The data distribution scheme may place or access the distributed data over the host nodes without using an index for data placement by taking advantage of the representation of a volume as a group of fixed sized and well gridded blocks and sectors.

The distribution scheme and the DHT scheme that matches the storage system topology may be applied to any well gridded objects for example in the case of a VBS where the objects may be treated as volumes of distributed data. In well gridded object architectures the schemes above may allow data distribution without indexing which may improve object placement and retrieval and avoid unnecessary index calculation and manipulation. Unlike other DHT based VBS storage systems or storage systems based on target storage objects that use indexing the DHT and data distribution schemes above may use logical indexing by implementing the hashing functions above based on the objects and other parameters to substantially improve storage system performance. The data distribution scheme may be used with different DHT schemes that may be currently available. For example additional functions or plug ins may be added to some existing DHT schemes to implement the DHT and data distribution schemes above.

The size of each physical file may be fixed for example equal to about 10 Megabytes MB . The size of the physical file e.g. data file may be determined according to the number of data blocks in the physical file which may correspond to the number of strips within a data volume strip number or s. The fixed data file size may simplify operations and improve sequential access efficiency in the node storing strips e.g. HDDs . The blocks may be placed sequentially according to the corresponding block IDs inside each physical file . The block IDs may be distributed by a cyclic policy among the node storing strips . The sectors may also be placed logically in a sequential manner inside each block and may be ordered by corresponding sector IDs. The sequential placement of blocks and sectors in the file layout may lead to a RAID0 like performance and increase the storage system throughput linearly. Further the physical files may be created according to a Lazy Creation scheme where a physical file may not be created until one sector of that file is accessed for example by a write operation to that sector from a user.

At block the data volume data block and data sector combination may be hashed. At block a first strip of the data may be placed or fetched at a location in the storage system based on hashing. For instance the data volume data block and data sector information may be hashed using the hashing functions and parameters described in the data distribution scheme to obtain a location to begin storing the data. The data volume data block and data sector and the hashing value e.g. location may be saved as a value and key pair respectively in an entry in a DHT which may be stored locally at the storage node associated with the location. At block one or more second strips of the data may be stored or retrieved at one or more subsequent locations in the storage system that are separated by an offset. The data strips may be distributed in a cyclical manner between the storage nodes as described above. The data may be distributed in different nodes in a plurality of strips that comprise a sequential order of blocks and sectors as indicated by the positioning information. Thus the data or file may be accessed or retrieved in the order indicated by the corresponding strips blocks and sectors in the nodes. The method may then end.

The network components described above may be implemented on any general purpose network component such as a computer or network component with sufficient processing power memory resources and network throughput capability to handle the necessary workload placed upon it. illustrates a typical general purpose network component suitable for implementing one or more embodiments of the components disclosed herein. The network component includes a processor which may be referred to as a central processor unit or CPU that is in communication with memory devices including secondary storage read only memory ROM RAM I O devices and network connectivity devices . The processor may be implemented as one or more CPU chips or may be part of one or more application specific integrated circuits ASICs .

The secondary storage is typically comprised of one or more disk drives or tape drives and is used for non volatile storage of data and as an over flow data storage device if RAM is not large enough to hold all working data. Secondary storage may be used to store programs that are loaded into RAM when such programs are selected for execution. The ROM is used to store instructions and perhaps data that are read during program execution. ROM is a non volatile memory device that typically has a small memory capacity relative to the larger memory capacity of secondary storage . The RAM is used to store volatile data and perhaps to store instructions. Access to both ROM and RAM is typically faster than to secondary storage .

At least one embodiment is disclosed and variations combinations and or modifications of the embodiment s and or features of the embodiment s made by a person having ordinary skill in the art are within the scope of the disclosure. Alternative embodiments that result from combining integrating and or omitting features of the embodiment s are also within the scope of the disclosure. Where numerical ranges or limitations are expressly stated such express ranges or limitations should be understood to include iterative ranges or limitations of like magnitude falling within the expressly stated ranges or limitations e.g. from about 1 to about 10 includes 2 3 4 etc. greater than 0.10 includes 0.11 0.12 0.13 etc. . For example whenever a numerical range with a lower limit R and an upper limit R is disclosed any number falling within the range is specifically disclosed. In particular the following numbers within the range are specifically disclosed R R k R R wherein k is a variable ranging from 1 percent to 100 percent with a 1 percent increment i.e. k is 1 percent 2 percent 3 percent 4 percent 7 percent . . . 70 percent 71 percent 72 percent . . . 95 percent 96 percent 97 percent 98 percent 99 percent or 100 percent. Moreover any numerical range defined by two R numbers as defined in the above is also specifically disclosed. Use of the term optionally with respect to any element of a claim means that the element is required or alternatively the element is not required both alternatives being within the scope of the claim. Use of broader terms such as comprises includes and having should be understood to provide support for narrower terms such as consisting of consisting essentially of and comprised substantially of. Accordingly the scope of protection is not limited by the description set out above but is defined by the claims that follow that scope including all equivalents of the subject matter of the claims. Each and every claim is incorporated as further disclosure into the specification and the claims are embodiment s of the present disclosure. The discussion of a reference in the disclosure is not an admission that it is prior art especially any reference that has a publication date after the priority date of this application. The disclosure of all patents patent applications and publications cited in the disclosure are hereby incorporated by reference to the extent that they provide exemplary procedural or other details supplementary to the disclosure.

While several embodiments have been provided in the present disclosure it should be understood that the disclosed systems and methods might be embodied in many other specific forms without departing from the spirit or scope of the present disclosure. The present examples are to be considered as illustrative and not restrictive and the intention is not to be limited to the details given herein. For example the various elements or components may be combined or integrated in another system or certain features may be omitted or not implemented.

In addition techniques systems subsystems and methods described and illustrated in the various embodiments as discrete or separate may be combined or integrated with other systems modules techniques or methods without departing from the scope of the present disclosure. Other items shown or discussed as coupled or directly coupled or communicating with each other may be indirectly coupled or communicating through some interface device or intermediate component whether electrically mechanically or otherwise. Other examples of changes substitutions and alterations are ascertainable by one skilled in the art and could be made without departing from the spirit and scope disclosed herein.

