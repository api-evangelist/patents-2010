---

title: System and method for providing high availability data
abstract: A computer-implemented data processing system and method writes a first plurality of copies of a data set at a first plurality of hosts and reads a second plurality of copies of the data set at a second plurality of hosts. The first and second pluralities of copies may be overlapping and the first and second pluralities of hosts may be overlapping. A hashing function may be used to select the first and second pluralities of hosts. Version histories for each of the first copies of the data set may also be written at the first plurality of hosts and read at the second plurality of hosts. The version histories for the second copies of the data set may be compared and causal between the second copies of the data set may be evaluated based on the version histories for the second copies of the data set.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09223841&OS=09223841&RS=09223841
owner: Amazon Technologies, Inc.
number: 09223841
owner_city: Reno
owner_country: US
publication_date: 20100426
---
Enterprise computing environments often need to access data relating to a particular business application. In order to avoid a single point of failure data is often stored at multiple hosts at different locations e.g. different locations within a given data center different data centers and so on . Thus for example if a particular data set becomes unavailable from one host e.g. due to host failure due to a network partition or other network failure and so on a client process may access the data at another host. The individual hosts may not be highly available but the combination of the individual hosts provides a more highly available solution.

When storing the same data at multiple locations a problem that is encountered is maintaining consistency between the various copies of the data. The state of the data set as it exists at one host may not be consistent with the state of the data set as it exists at the other host. For example if a client process has made changes to a data set at one host and the data set then becomes unavailable from that host the changes that have been made in the copy of the data set at that host may be lost at least temporarily. A recent version of the data set may be obtained from another host. However if the client process starts operating on the data set from the other host a further problem arises in that two versions of the data set may potentially be created each with changes that are not reflected in the other data set.

Accordingly an on going need exists for systems and methods that are capable of providing highly available data. It should be appreciated that although certain features and advantages are discussed the teachings herein may also be applied to achieve systems and methods that do not necessarily achieve any of these features and advantages.

An exemplary embodiment relates to a computer implemented data processing method comprising writing a first plurality of copies of a data set at a first plurality of hosts and reading a second plurality of copies of the data set at a second plurality of hosts. In an exemplary embodiment an arrangement is used for writing and reading the data set in which the first and second pluralities of hosts need not be entirely overlapping. In another exemplary embodiment version histories are also written and read at the hosts and are used to evaluate causal relationships between the data sets after the reading occurs. In another exemplary embodiment a hashing arrangement is used to select the hosts where the writing and reading of the data sets occurs.

It should be understood that the detailed description and specific examples while indicating preferred embodiments of the present invention are given by way of illustration and not limitation. Many modifications and changes within the scope of the present invention may be made without departing from the spirit thereof and the invention includes all such modifications.

Referring to a data processing system according to an exemplary embodiment is shown. Data processing system includes user computers communication network and a network services system . User computers may access network services system via communication network . Network services system includes network interface a data set service and one or more other services . The network interface receives data from and provides data to the user via communication network . For example the network interface may provide the users computers with access to data sets maintained by the data set service as well as to other data generated and or maintained by the other services .

Data set service includes a data storage system which may store the data sets. The data states may change over time based on user interaction and or based on other changes in system . Herein the term data set refers to any data that may change over time. For example each data set may include one or more items that may be added removed and or modified from the data set. Data storage system is configured to store information in a highly available manner so that in the event of a system fault e.g. host failure network failure and so on the data sets remain available with a high level of consistency as discussed below. In an exemplary embodiment the data storage system is implemented using a Berkeley database transaction data storage system.

Referring now also to provides another example of a data processing system . In the example of network services system is a merchant website system and the network interface is a network shopping interface . Merchant website system may for example be implemented in a distributed computing environment comprising thousands of hosts or more. Merchant website system may provide a merchant website e.g. an on line retail website accessible to a user operating a user computer to shop for items e.g. goods services subscriptions etc. . In such an embodiment network shopping interface may provide users with graphical and or text data on the website to facilitate the display and or sale of items. The data provided to users may include item information such as pricing dimensions availability items currently selected for purchase and so on. Merchant shopping interface may also be configured to receive data from user such as data indicating items the user is interested in data needed to complete a transaction and so forth.

In the example of data set service is shown to be a shopping cart data service that maintains lists of items selected for purchase or possible purchase by users of the website. In such an example each data set may be a shopping cart related to a specific customer. The data set may include item identification information for items in the shopping cart item information for items that a user may have selected but not yet purchased quantity information of items in the shopping cart and so on. The shopping cart data service may be accessed through a shopping cart service which may comprise other business logic associated with shopping carts. The website system may publish web pages for users of the website that include all or a portion of the data set e.g. a webpage showing all or a portion of a user s shopping cart. In other example embodiments the data sets may comprise other data that may be collected by website system based on the interaction of a user or for the convenience of the visitor or to facilitate operation of the website. For example the data set service may also maintain data sets relating to specific entities e.g. data sets relating to different users of a website different sessions on the website different transactions conducted on the website different items offered by the website different categories of items offered by the website different advertisements published on the website different pages of the website and so on . As will also be appreciated although shows a website system the data processing system may be used in other applications.

Referring again to data set service may be used both in connection with local processes and remote processes. In the context of remote processes read and write requests for data set service may be received from a remote process by way of communication network . For example the network services system may offer services that are accessible to remote processes through an application programming interface API across the Internet. Such service requests may be made by third parties for example to assist in the operation of their own data processing systems.

Referring now to construction and operation of the data set service is shown in greater detail. As shown in the data set service may comprise a plurality of hosts . Herein the term plurality means two or more. For example the data set service may comprise tens hundreds or thousands of hosts or more. In an exemplary embodiment each host is functionally equivalent e.g. executes the same code or executes related versions of the same code . As will be described below the data set storage system may be distributed across the hosts such that each host stores a portion of the data sets. Each host stores a subset of the data of the key value pairs and the system attempts to maintain N replicas of each data set where N is a positive integer representing the replication factor or the number of times to replicate the data set . The value N is configurable and affects both the durability availability and consistency of data. If there are S physical hosts in the system then the overall system comprises S N physical hosts although the smaller S the lower the total system availability and each host stores approximately

As shown in to store data received from a client process e.g. one of the services the data set service receives a write request from the client process step and then responds by writing the data at multiple hosts step . For purposes of this application the term client process refers to any program logic that may request data sets from any other program logic e.g. herein from the data set service . In an exemplary embodiment the data is written at multiple hosts based on preference lists as described below. After the data is written a response is sent to the client process confirming that the write operation has been performed step . Exemplary write operations are described in greater detail in connection with and .

As shown in to provide data to a client process the data set service receives a read request from the client process step and then responds by reading the data at multiple hosts step . After the data is read a response is sent to the client process confirming that the read operation has been performed and including the requested data step . Exemplary read operations are described in greater detail in connection with .

With regard to when all relevant network connections and hosts are healthy e.g. available and responsive the hosts involved in the read operation typically provide consistent data. However when one or more of the network connections or hosts is troubled or failed the hosts may provide different versions of the same data set. Thus as shown in after the data sets are received at the client process step the data sets may be reconciled step . The reconciled data set may then be transmitted to the data service for storage step . As described in greater detail below the existence of inconsistent versions of the same data set may be detected using a data versioning arrangement. The data versioning arrangement may also be used by version reconciliation logic provided as part of or in connection with client process as shown in to reconcile the inconsistent versions. An exemplary data versioning arrangement is described in greater detail below in connection with .

Referring to in an exemplary embodiment data set service includes a mechanism to partition data sets over hosts in system . In an exemplary embodiment described below a consistent hashing arrangement may be used to store data sets such that data is spread relatively evenly over the hosts . In other embodiments other data partitioning arrangements may be used.

Referring first to in an exemplary embodiment in order to access the data stored by the data set service e.g. via a read operation or a write operation client processes transmit data requests that include a key for the data set to which each request refers. For example in the context of a shopping cart application the key may be generated based on the user ID of the user to whom the shopping cart is related e.g. the user ID may be used as the key . The keys may be any data value that is associated with a data set and that is suitable for use as an input to a hash function. As shown in the key is applied to hash function which in turn generates a hash value h as a function of the key. In an exemplary embodiment the hash function achieves an approximately uniform spread of hash values over a hash range. In the illustrated embodiment the values are shown to be spread over the hash range 0 2 however any number of hash values or effectively any size hash range may be used.

Upon becoming active participants in the data set service each host is assigned a set of positions over the hash range. For purposes of explanation it is assumed in the remainder of the discussion herein that there are five hosts which implement the data set service shown as host A host B host C host D and host E. It will be appreciated that in practice data set service may be implemented by tens hundreds or thousands of hosts or more.

Referring to shows the manner in which responsibility for a read operation or a write operation is assigned to a particular host based on a hash value. Each host is responsible for read write operations in connection with hash values extending from its own position in the hash range to the position of the previous host . For example if hosts A B C D and E are positioned at hash values h h h h and h respectively then host B is responsible for the range of hash values h

The arrangement shown in results in each host being responsible for the region of the ring between it and its predecessor host on the ring . For example host B is responsible for the portion of the ring between it and host A. If a host enters or leaves it only affects the responsibilities of its immediate successor on the ring all other hosts are unaffected. This is shown in in which the addition of a host F impacts the responsibilities of its immediate successor on the ring host B but not the responsibilities of other hosts such as host A. Thus individual hosts may be added or removed without a total remapping of the partitioning of data sets to hosts thereby promoting incremental scalability.

Referring now to the hashing arrangement of may be used to support data replication. In rather than the data set being assigned merely to the immediate successor host on the ring the data set is assigned to the first N successor hosts . As described below data set service may operate to ensure that there are N replicas of the data among the hosts and each host is responsible for the region of the ring between itself and its Nth predecessor.

As shown in in such an arrangement each key has a preference list of hosts which is the order that each of the hosts is first encountered while moving clockwise around the ring from the hash value generated based on the key. The preference list represents the preferred order of hosts used for accessing e.g. reading or writing a data set. When all hosts are healthy the top N hosts in the preference list store the data set. If a particular host happens to fail or if there is a network partition the data set may temporarily be stored at a host that is lower ranked in the preference list . If multiple hosts fail then the data set may be stored at multiple lower ranked hosts in the preference list . With N 3 a client process accessing a data set associated with key kreads or writes to hosts A B and D and then E and then C if any earlier hosts are unavailable in that order as can be seen by moving clockwise around ring from the position of key k. A client process accessing a data set associated with key kreads or writes to hosts B D E and then C and then A if any earlier hosts are unavailable in that order as can be seen by moving clockwise around ring from the hash position of key k. As indicated above the value N is a configurable value thus more hosts may be added to system to permit more replication of data sets. Accordingly the level of availability of data sets is configurable and may be made as high as desired using the appropriate number of hosts .

When all hosts are available successive operations on the same data set access the same set of N hosts and are therefore consistent i.e. an operation accesses the same data that was read written by the previous operation on the same key . When there are network or host failures successive operations to the same data set may access different sets of hosts however the operations may still be consistent as long as there is some overlap in the sets of hosts that are accessed. For example a first operation on key kmay access hosts A B and D. Later if host B is unavailable a second operation on kmay access hosts A D and E. Thus by accessing available hosts that are highest in the preference list minor changes in the availability of hosts from operation to operation do not negatively affect consistency because subsequent accesses may involve overlapping hosts. The availability of at least N hosts must change during two successive operations in order for there to be no overlap between the host sets resulting in an inconsistency . As indicated above the value N is a configurable value accordingly a probabilistic guarantee of consistency is configurable and may be made as high as desired. This includes probabilistic guarantees of both global consistency the system response reflects the absolute latest change made to the data and subjective consistency the system s response reflects the latest changes made by the client making the current request .

In an exemplary embodiment client operations on data sets may be serviced at multiple locations e.g. servers . Furthermore successive operations on the same data set may be serviced by different servers. In an exemplary embodiment in order to access the hosts that store a given data set a server stores information regarding the host positions in the hash space in order to compute the preference list as well as the availability of hosts in order to select the N available hosts that are highest in the preference list . In the presence of network or host failures different servers may store different information regarding the availability of hosts. In the presence of hosts joining or leaving the system different servers may store different information regarding the set positions in the hash space. For example server X may not be aware that host A has joined the data set service . Hence in servicing an operation on a data set with key k server X may access hosts B D and E. Another server Y may already be aware of both host A and the hash positions of host A. Based on this information when servicing a subsequent operation on key kserver Y may access hosts A B and D. Thus by accessing available hosts that are highest in the preference list the probability of accessing at least one host during write and read operations is increased. Accordingly minor differences in information regarding host availability and hash positions from server to server do not negatively impact consistency during successive operations. As indicated above this probabilistic guarantee of consistency is determined by the value of N.

In an exemplary embodiment the preference list may be implemented by way of operation of hashing function e.g. without being separately stored . In another exemplary embodiment the preference list may be stored. As will be appreciated other factors may be taken into account when constructing the preference list . The preference list may be manually or automatically constructed to take into account such factors. For example in order to further improve availability and durability preference list may be constructed so as to include hosts in the same preference list which have a relatively low probability of correlated failure. For example if system is distributed over multiple networks there may be sets of hosts that are unlikely to fail together. Hence system can maximize availability and durability by choosing the N hosts for the N replicas of a data set such that they have low correlated failures. Likewise low failure correlation may also exist where hosts are running on different hardware using different program logic implementations running in geographically diverse areas and combinations thereof. For example when moving clockwise around ring a set of rules may be applied to assess whether an encountered host meets any additional criteria that are desired to be considered. If the encountered host does not meet the additional criteria the search for an available host may continue onward around ring until a host is encountered that does meet the additional criteria.

Other arrangements may also be used to achieve geographic diversity. For example rather than using a single ring a tiered ring arrangement may be used. For example a two tiered or ring of rings arrangement may be used in which multiple bottom level rings each have a position on a top level ring and each have responsibility for a range of hash values on the top level ring akin to the manner in which hosts each have responsibility for a range of hash values on the ring as described above. Each bottom level ring may for example correspond to a set of hosts located a different geographic location. Within the set of hosts at each location each host may be assigned responsibility for a range of hash values on the respective bottom level ring. As will be appreciated more than two tiers of rings may also be used e.g. additional levels of rings may be used to specify areas within a data center particular racks of hosts within a data center and so on .

Referring to the hosts may be assigned to multiple positions on the ring in order to promote load balancing that is to avoid non uniform data and load distribution that may otherwise be created by a random position assignment of each host on ring . Thus in hosts A B C D and E are assigned multiple positions on ring . This multiple positioning tends to reduce the variance in the number of data sets assigned to each host because the increased number of random placements on ring tends to cause the number of data sets assigned to each host to converge on an average value. Thus assigning more positions to each host on the ring improves load balancing. In an exemplary embodiment only the first instance of each encountered host is placed in the preference list . In the case of key kthe first host with a larger position which the corresponding data set is assigned to is host A. With N 4 a process accessing a data set associated with key kreads or writes to hosts A B C and D. The preference list for key kis different than above due to the hosts having multiple positions on ring and due to the hosts being encountered in a different order. In the case of key k the first host with a larger position which the corresponding data set is assigned to is host B. A client process accessing a data set associated with key kreads or writes to hosts B C D and A in that order. In other example embodiments multiple instances of each encountered host may be placed in the preference list e.g. in order to retry a host that was previously unavailable.

Assigning hosts multiple positions on ring also facilitates usage of heterogeneous hardware that is more powerful hosts may be assigned more positions on ring and less powerful hosts may be assigned fewer positions on ring . For example in host E has fewer positions than any other host and thus is assumed to be a less powerful host. As will be appreciated a range of hosts may be used each being more or less powerful than other hosts . The number of positions assigned to a particular host may be a function of the relative power of the particular host .

Additionally if a sufficient number of positions assigned to each host then each host may have a successor predecessor relationship with each of the other hosts . Accordingly if one of the hosts becomes unavailable or is decommissioned the load handled by the decommissioned host may be approximately evenly dispersed across the remaining available hosts without losing data availability. Likewise when a host becomes available again or a new host is added to data set service the newly available host may offload a roughly equivalent amount of load from each of the other available hosts .

Referring now to A B and A B exemplary read and write operations are shown. The read write operations may be invoked by a service request made to data set service by client processes . Upon receiving the service request the data set service performs the requested operation and provides a response to the client process .

At data set service one of the hosts is responsible for coordinating the read or write request. The host responsible for coordinating the read or write request is referred to herein as the coordinator. In an exemplary embodiment the coordinator is the first host listed in the preference list and coordinating the read or write request includes performing a local read or write operation. For example the service request may initially be received by another host and that host may make a decision to forward the service request to the host which serves as the coordinator e.g. the top host in the preference list . In another exemplary embodiment the coordinator may be another host such as a host that is not on the preference list and coordinating the read or write request does not include performing a local read or write operation. For example the coordinator may be a host which happens to initially receive the read or write request but which does not happen to be near the top of the preference list and which does not make a decision to forward the service request to a host which is near the top of the preference list . For purposes of providing an example it is assumed herein that the coordinator is the first host listed in the preference list .

In an exemplary embodiment as described above read and write operations may access the first N healthy hosts in preference list skipping over hosts that are potentially down or inaccessible. When all hosts are healthy the top N hosts in the preference list of a key may be accessed. When there are host failures or network partitions hosts that are further down in the preference list may be accessed instead thereby maintaining high availability.

Referring first to an example write operation is shown. In a write request for version Vis received by host A from client process either directly or indirectly as described above . Assuming the distribution of hosts on ring as shown in then the preference list for key kis P A B C D E. Host A is the coordinator and in this example performs the write operation locally step . Host A then copies the new version Vto the remaining N 1 highest ranked reachable hosts hosts B and C e.g. if N 3 which then also perform the write operation and store additional copies step .

When the data set is stored in addition to the data itself the key associated with the data and a vector clock are also stored. The key permits the data set to be identified later. The vector clock is used for data versioning to capture causality relations between different versions of the same data set and comprises a list of host ID counter pairs associated with the versions of data sets. Data versioning through the use of vector clocks is discussed in greater detail below in connection with .

In hosts B and C report back to host A whether the write operation was successful and host A responds to client process confirming whether the write operation was successful step . In exemplary embodiment in order for a write operation to be considered successful the write operation must be successful at W hosts where W is a configurable value and W N. Thus for example if N 3 and W 2 a write operation is considered successful if it is successful at two hosts even if the write operation was attempted at three hosts . It may be noted that if the write operation is successful one or more of the hosts copies of the data set may still eventually migrate to the top N hosts in the preference lists as described in greater detail below. Thus even if a write operation is not considered successful according to the test set forth above eventual consistency of the data set at the top N hosts may still be achieved.

Referring to an example write operation with data hand off is shown. Data hand off is a mechanism that attempts to migrate data to the N highest ranked hosts in the preference list for a data set. For example as described above in general the coordinator attempts to send the data to the top N hosts in the preference list . However if one or more of the hosts is down the coordinator sends the data to hosts further down the preference list . The preference list provides a well defined sequence of hosts that will participate in write operations and in read operations and the data hand off mechanism is used to migrate the data back to the N highest ranked hosts in the preference list .

Thus as shown in host A receives a write request for version Vas in . Host A then performs the write operation and attempts to copy the new version to the remaining N highest ranked reachable hosts hosts B and C. In the illustrated example host C has temporarily failed and thus a write at host D is attempted. The data written at host D may be tagged with a hint suggesting which host should have received and written the data e.g. host C so that at some later time host D may forward the data to host C. In when host C is healthy a data hand off is made and the data is copied back to host C. The data is thus migrated back to host C which is one of the N highest ranked hosts in the preference list .

In an exemplary embodiment related techniques may be used to restore lost copies of data sets. For example when hosts enter or leave and there is a corresponding change in the preference lists which may cause data to become misplaced. For example a host added to system will displace the rankings of other hosts in preference lists . In such situations to implement a data hand off pairs of hosts may periodically perform a comparison of ranges they share in common and then perform necessary data transfers to reconcile any differences detected during the comparison. For example a host the sender holding a range of keys for which it is not one of the top N hosts may choose any one of the top N hosts at random the receiver . As another example the host may choose a host in the top N hosts that is unlikely to have the data for example because the host recently joined the data set service . The two hosts may then proceed with a low level database comparison within that range and the sender may forward any data sets that are more recent than what the receiver is storing to reconcile any differences that are detected by the comparison. The data may migrate to at least one host in the preference list and then be propagated to remaining hosts in the preference list . For example the propagation to the remaining hosts may be implemented by comparing data sets stored at pairs of hosts that are within the top N hosts in the preference lists for some set of keys. In an exemplary embodiment Merkle trees may be used to efficiently find set differences between the data stored at two hosts. For example a Merkle tree may be used in which each node of the tree contains a summary or hash value computed over the data in its subtree and in which the leaves contain hashes of one or more data values e.g. keys versions and clocks . Differences in the contents of the trees may be found by recursing down branches along which the data summaries hash values differ. To improve the efficiency of the comparison the Merkle tree may be encoded using a Bloom filter.

Using the above described mechanisms the data set service makes an ongoing attempt to dynamically migrate copies of the most recent versions of data sets to the top N hosts in their preference lists . Thus even though copies of the most recent version of a dataset may initially be copied at hosts which are lower in its preference list or may for another reason become lost at one of the top N hosts the copies eventually migrate back to the top N hosts in the preference lists resulting in eventual consistency of the data set at the top N hosts.

Referring to an example read operation performed using preference list is shown. In a read request is received by host A from client process either directly or indirectly as described above step . Host A coordinates the read operation by requesting data from B and C in parallel to doing a local read. Hosts B and C perform the requested read operation. In host A receives read results from hosts B and C step and provides a response to client process step .

When receiving a read request a coordinator may request all existing versions of data for that key from the N highest ranked reachable hosts in the preference list for that key and then wait for R responses before returning the result to the client process where R is the number of hosts that needed to participate in a successful read operation . In the example of the value R is set equal to three.

Like the value W the value R is configurable. For example if R 1 then once host A responds with a successful read the data from that read is returned to the client process for use. As another example if R 2 then data may not be returned until reads are performed on both hosts A and B. Upon performing the two reads system realizes that the data is the same version and return the same data as when R 1. As yet another example if R 3 then data may not be returned until reads were performed on hosts A B and C.

The values R and W may be configured to be less than N to provide consistent and high performance. Setting the values R and W such that R W N yields a quorum like system in which there is a configurably high probability of overlap between the set of hosts that participate in read and write operations. The higher N is set the more likely the system is to have availability and durability because the chances that at least one replica exists is high. On the other hand it may be noted that data need not be written to and read from the same set of hosts . For example a data set may be written to hosts which are further down on the preference list migrated through data hand off to hosts that are higher on the preference list and then ultimately read from the hosts that are higher on the preference list . Eventual consistency of the data set at the top N hosts in the preference list is achieved. In another exemplary embodiment R and W may be configured to be much smaller than N e.g. such that R W

In an exemplary embodiment the application programming interface for the data set service may be configured as follows. For example the commands may have the following form write Key Value Context ResultCode. read Key ValueList Context ResultCode where Key is an unbounded sequence of bytes Value is an object comprising data an unbounded sequence of bytes and metadata a read only arbitrary extensible data set containing information about the value including the last time the value was written diagnostic and debugging information and so on ValueList is a list of values Context is opaque object used internally by the storage system to track vector clock state for the read modify write cycle and ResultCode is a code indication whether a read or write operation was successful.

The write operation changes the value identified by the key to the value specified by the Value parameter unless the Context is stale meaning that an intervening write has already occurred on that key. In an exemplary embodiment the client process restarts the read modify write cycle optimistic locking . In another exemplary embodiment the client process may permit the write operation to continue in which case there may be conflicting versions of the data set. The read operation performs a lookup in the data set service for value s associated with the key. Any and all values that are successfully read are returned in the ValueList. An opaque Context object is returned for use in a subsequent update operation. If multiple values are returned the client process is expected to perform a reconciliation operation for all of the values. If a subsequent update is performed using the returned Context the assumption is that the updated value is a represents a reconciliation of all values returned in the value list plus any additional changes to the value if any .

As will be appreciated a greater or lesser level of complexity in the application programming interface may be used. For example in an exemplary embodiment the Value object may include a type parameter that permits information concerning how long data should be maintained to be specified e.g. so that old abandoned data may eventually be deleted.

In another exemplary embodiment a key may be used that is divided into two parts partition key object key . In such an embodiment the partition key may be hashed to generate the preference list for the key as described above for the key parameter. Two data sets sharing the same partition key may therefore have the same preference list and hence with very high probability their respective copies of data sets would reside on the same set of hosts . Such a scheme allows accessing several data sets together as an optimization since the same set of hosts is in the top N hosts of the preference lists for all the keys that share a partition key. For example in the merchant website example of it may be desirable to store all data sets that relate to a particular user e.g. shopping cart profile credit card information and so on on the same set of hosts . By using the same partition key for each of these data sets the data sets are stored on the same set of hosts . The partition key object key combination uniquely identifies each individual data set for the user. Another optimization made possible by this arrangement is range queries on keys sharing a partition key. For example such range queries may be used to iterate through all object keys for a given partition key by accessing a single host that is in the top N of the preference list for that partition key.

In another exemplary embodiment a type parameter may be added to the write command e.g. write Key Value Context Type ResultCode so that a client process may specify the type of data that is being written. The data set service may be configured to delete data a certain amount of time after it is last accessed e.g. in order to reclaim storage space when data is no longer needed . The time allowed before deletion may be based on the type of data. The type may also be used to decide the number of copies of the data that the data set service should store e.g. on the basis that some types of data may be more critical than others .

In another exemplary embodiment a read context may also be passed as input to the read command e.g. read Key Context ValueList Context ResultCode . In such an embodiment the read context passed as input to the read command may be obtained as a result of a previous read. By passing it back as input to a read operation a client process may indicate interest in retrieving the specific version of the data set that was accessed during the previous read operation. As will be appreciated other variations on the application programming interface are also possible.

Referring to an exemplary data versioning arrangement is discussed. As previously indicated in order to provide high availability the data set service permits multiple versions of the same data to be present at the same time on different hosts . An ongoing attempt is made to migrate copies of the most recent versions of data sets to the top N hosts in their preference lists however this process is not instantaneous. Before the migration occurs copies of older versions of a data set may be in existence at various hosts in its preference list even at hosts that are at or near the top of the preference list . Thus for example one host may have one version reflecting temporarily lost old changes and another host may have another version reflecting new changes made while the old changes are unavailable.

In an exemplary embodiment it is desirable to be able to determine whether two copies of the same data set are different versions of the data set and have differences relative to each other. It is also desirable to be able to assess those differences such that it is possible to distinguish situations in which two versions have an ancestor descendant relationship with each other e.g. one version is merely outdated and has been incorporated into the other version from situations in which two versions are in conflict e.g. each version contains data that is not reflected in the other version .

In an exemplary embodiment a version history is stored with each copy of a data set. For example the version history may be stored in the form of vector clocks which capture causality relations between different versions of the same data set. The vector clocks may concisely store enough information about the version history of the data set to permit a determination whether two versions are in conflict. In an exemplary embodiment the vector clock comprises a list of host ID counter pairs associated with the versions of data sets. The host ID value indicates the host that coordinated the write operation. The counter value indicates the number of times that host has written to the data set. The counter value encodes causality information for a data version that is a summary of what changes preceded that version.

When trying to determine whether two versions of a data set have a causal ordering and hence one can be forgotten or are on parallel branches and hence need reconciliation it is enough to examine their vector clocks. If one has greater or equal counter values for all the host IDs in the other s vector clock then the former is a descendant of the latter and the latter can be forgotten. Thus the vector clocks permit client processes to reconcile multiple versions of the same data in order to collapse multiple branches of data evolution back into one.

In the coordinator is one of the N highest ranked reachable hosts in the preference list . As indicated above the coordinator may also be a host that is not one of the N highest ranked reachable hosts in the preference list . In such an example when receiving a write request the coordinator may choose one of the N highest ranked reachable hosts in the preference list for that key to generate a vector clock for the new version and store the new version locally. The coordinator may then send the new version along with the new vector clock to the remaining N highest ranked reachable hosts as previously described.

At step the same client process updates data version Vusing host A. The host A which coordinates the write copies the clock of the previous version and increases the counter value associated with host A to two and creates the vector clock for data version V. Again host A forwards the data version Vand its associated vector clock A to hosts B and C for local write operations and store additional copies. Version Vdescends from version Vand therefore over writes version V however there may be replicas of version Vlingering at host partitions that have not yet seen version V.

At step the same process updates data version Vusing a host B to coordinate the request. For example host A may be unavailable. Since a new host B coordinates the update a new vector clock entry is created associated with this host B with a counter value of one. Data set service stores data version Vand the associated clock A B . The vector clock for data version Vmay also be stored if desired in order to maintain version history or to allow more complex reconciliations to be performed. After step a host that is aware of version V but not of version V may receive version Vand the associated vector clock. The host can determine by comparing the respective clocks A and A B of version Vand version Vthat version Vcausally precedes version Vand hence that it was meant to be overwritten by version V. If on the other hand a different sequence of events has occurred and a vector clock for data version Vhas less than or equal counters for all of the hosts in the clock of version V then version Vis an ancestor of version Vand can be removed.

At step a different client process reads version Vand tries to update it using host C. For example hosts A and B may be unavailable. In the present example it is assumed that host C was not involved in the write operation of step and is not aware of version V. Since a new host C coordinates the update a new vector clock entry is created associated with this host C with a counter value of one. Data set service stores data version Vand the associated clock A C . After step a host that is aware of version Vor version Vcould determine upon receiving version Vand the associated vector clock that version Vand version Vare over written by the new data and can be removed.

At step a client process reads both version Vand version V. For example the read operation may be coordinated by host A and may also involve hosts B and C. Host A obtains its own copy of the data set with vector clock A the copy of the data set from host B with vector clock A B and the copy of the data set from host C with vector clock A C . The context of the read is a summary of the clocks of version Vand version V namely A B C . Host A will find that there is no causal relation between version Vand version Vbecause from an examination of the vector clocks there are changes in each of version Vand version Vthat are not reflected in the other. The versions Vand Vare then reconciled.

In an exemplary embodiment the data set service host A in this example provides the multiple versions to client process and or version reconciliation logic associated with client process which in turn decides how to perform the reconciliation. This arrangement permits any business logic that is used to perform the reconciliation to be stored or associated with the client process rather than with the data set service . Although client process and version reconciliation logic are shown as being separate it will be appreciated that client process and version reconciliation logic may be provided in integral fashion. In another exemplary embodiment the version reconciliation logic may be provided with the data set service . The multiple versions may be reconciled by for example using a default ordering on the versions to decide which one to keep by merging the different versions to produce a single reconciled version by performing an analysis of the data and determining how to treat discrepancies on a discrepancy by discrepancy basis and so on. As will appreciated different approaches may be more optimal in different situations depending on the application.

At step a write request is received from client process . Host A coordinates the write and updates the corresponding counter value in the vector clock. The updated version may also include other changes implemented by client process unrelated to the reconciliation operation. New version Vwill have the vector clock A B C .

It may be noted that at step host A updates the counter number to A B C regardless whether any changes are implemented to the data set in addition to the reconciliation. No single version exists with the vector clock A B C so updating the counter in the vector clock distinguishes the parent clock from the new clock. Additionally increasing the counter is desirable because multiple client processes may attempt to reconcile at the same time e.g. using different hosts to coordinate but arrive at different results e.g. because of different merge logic because they added changes as well as reconciling and so on . If the counter is not updated the different merge attempts may be assigned the same clock i.e. A B C and hence be indistinguishable from each other.

In an exemplary embodiment rather than comprising only host ID counter pairs the vector clock comprises a number of additional values and has the following form Vector Clock The host ID is a unique identifier for a host and the counter parameter encodes the causality information for a data version and corresponding to the host ID counter pair described previously. In an exemplary embodiment the combination of the parameters operates in the manner described previously with regard to the host ID alone. That is a host is considered a different host i.e. no causal relation between different versions of a data set may be implied if any one of the three parameters is different.

In an exemplary embodiment hosts do not write vector clocks synchronously to disk. Hence the potential exists that a host may forget the sequence numbers it generated for each key and consequently reuse the sequence numbers thereby compromising the consistency of the vector clocks. When the risk of forgetting e.g. after host failure is identified a host updates its parameter so that for all future vector clocks it generates for any key it appears to be an entirely different host. Thus incrementing the parameter upon rebooting the host permits vector clocks generated prior to failure to be distinguished from vector clocks generated after rebooting. As will be appreciated the counter for each vector clock is monotonically increasing in an unbounded fashion. In an exemplary embodiment in order to avoid unbounded counter numbers each host is periodically forced to choose a new unique identity e.g. by incrementing the parameter. For example a host be assigned a new unique identity after rebooting thereby also zeroing the parameter. This causes the highest possible counter value to be bounded by the number of writes that a single host can coordinate before changing identity. In another exemplary embodiment an identity change may be triggered automatically in a host if one or more of its counter values reaches a predetermined threshold value.

The parameter may be used to track a key generation counter. In an exemplary embodiment after data hand off hosts delete any data that was obtained. This saves storage capacity for hosts that are lower down on the preference list . At the same time the hosts maintain the parameter which is incremented after data hand off thereby preventing any causal relationship being assumed the next time the host is asked to perform a write operation. For example if host D coordinates a write operation for version of a data set having a vector clock A D performs a data hand off and later is asked to coordinate another write operation for a version of the data set having a vector clock A it would be inappropriate for the updated data set to have a vector clock A D . By assigning a new value in this situation the host is made to appear like a new host thereby avoiding the appearance of causality between the two versions. In an exemplary embodiment each host maintains a separate per key and remembers the key generation for every key for which it generated a vector clock since it last changed identity e.g. changed or updated its . Likewise each host may also remember the last parameter used in a vector clock for the key since either the corresponding parameter or parameter was updated.

The parameter may be used to monitor the age of the data set and entries in its vector clock. In some applications it is desirable to delete data if the data exceeds a predetermined age. For example in a shopping cart application it may be desirable to delete a shopping cart that has gone abandoned for a period of days weeks months or years and so on. The time stamp may be used to support the deletion of data sets in this manner. Additionally the time stamp may also be used for vector clock truncation. As will be appreciated as the length of the list of different hosts or same hosts with different or parameters that have coordinated a write operation in connection with a data set increases the length of the vector clock for that data set increases i.e. because the length of the list of host ID counter pairs contained in the vector clock increases . Accordingly using the time stamp vector clocks that have aged by a predetermined amount may be deleted or truncated.

In other exemplary embodiments rather than using vector clocks other version history mechanisms may be used to track the changes in data sets. For example hash histories may also be used. Herein the term version history refers to any data structure that may be used to track changes in a data set over time i.e. to track that changes exist not necessarily to track the nature of the changes . As may be appreciated different version history mechanisms may provide different tradeoffs in terms of disk space usage bandwidth maintaining consistency when deleting old versions speed and ease in detecting causal precedence and so on. In an exemplary embodiment a version history mechanism is used which permits the detection of causal precedence or the absence thereof previously referred to as a conflict between two or more copies of a data set. The version history mechanism may be used to allow version conflicts to occur availability without the loss of data and to facilitate maintaining consistency as data migrates to hosts that are highest in preference lists.

The invention is described above with reference to drawings. These drawings illustrate certain details of specific embodiments that implement the systems methods and programs of the present invention. However describing the invention with drawings should not be construed as imposing on the invention any limitations that may be present in the drawings. The present invention contemplates methods systems and program products on any machine readable media for accomplishing its operations. The embodiments of the present invention may be implemented using an existing computer processor or by a special purpose computer processor incorporated for this or another purpose or by a hardwired system.

As noted above embodiments within the scope of the present invention include program products comprising machine readable media for carrying or having machine executable instructions or data structures stored thereon. Such machine readable media can be any available media which can be accessed by a general purpose or special purpose computer or other machine with a processor. By way of example such machine readable media can comprise RAM ROM EPROM EEPROM CD ROM or other optical disk storage magnetic disk storage or other magnetic storage devices or any other medium which can be used to carry or store desired program code in the form of machine executable instructions or data structures and which can be accessed by a general purpose or special purpose computer or other machine with a processor. When information is transferred or provided over a network or another communications connection either hardwired wireless or a combination of hardwired or wireless to a machine the machine properly views the connection as a machine readable medium. Thus any such connection is properly termed a machine readable medium. Combinations of the above are also included within the scope of machine readable media. Machine executable instructions comprise for example instructions and data which cause a general purpose computer special purpose computer or special purpose processing machines to perform a certain function or group of functions.

Embodiments of the invention are described in the general context of method steps which may be implemented in one embodiment by a program product including machine executable instructions such as program code for example in the form of program modules executed by machines in networked environments. Generally program modules include routines programs objects components data structures etc. that perform particular tasks or implement particular abstract data types. Machine executable instructions associated data structures and program modules represent examples of program code for executing steps of the methods disclosed herein. The particular sequence of such executable instructions or associated data structures represent examples of corresponding acts for implementing the functions described in such steps.

Embodiments of the present invention may be practiced in a networked environment using logical connections to one or more remote computers having processors. Logical connections may include a local area network LAN and a wide area network WAN that are presented here by way of example and not limitation. Such networking environments are commonplace in office wide or enterprise wide computer networks intranets and the Internet and may use a wide variety of different communication protocols. Those skilled in the art will appreciate that such network computing environments will typically encompass many types of computer system configurations including personal computers hand held devices multi processor systems microprocessor based or programmable consumer electronics network PCs servers minicomputers mainframe computers and the like. Accordingly the user computers depicted in may include but are not limited to desktop computers laptop computers set top boxes personal digital assistants cellular telephones media players web pads tablets etc. Embodiments of the invention may also be practiced in distributed computing environments where tasks are performed by local and remote processing devices that are linked either by hardwired links wireless links or by a combination of hardwired or wireless links through a communications network. In a distributed computing environment program modules may be located in both local and remote memory storage devices.

An exemplary system for implementing the overall system or portions of the invention might include a general purpose computing device in the form of a computer including a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit. The system memory may include read only memory ROM and random access memory RAM . The computer may also include a magnetic hard disk drive for reading from and writing to a magnetic hard disk a magnetic disk drive for reading from or writing to a removable magnetic disk and an optical disk drive for reading from or writing to a removable optical disk such as a CD ROM or other optical media. The drives and their associated machine readable media provide nonvolatile storage of machine executable instructions data structures program modules and other data for the computer.

It should be noted that although the flowcharts provided herein show a specific order of method steps it is understood that the order of these steps may differ from what is depicted. Also two or more steps may be performed concurrently or with partial concurrence. Such variation will depend on the software and hardware systems chosen and on designer choice. It is understood that all such variations are within the scope of the invention. Likewise software and web implementations of the present invention could be accomplished with standard programming techniques with rule based logic and other logic to accomplish the various database searching steps correlation steps comparison steps and decision steps. It should also be noted that the word engine as used herein and in the claims is intended to encompass implementations using one or more lines of software code and or hardware implementations and or equipment for receiving manual inputs. Components such as engines interfaces databases browsers and so on may be in communication with each other either because such components are provided in integral fashion because they are in communication with each other through a communication link such as a network and or for other reasons.

The foregoing description of embodiments of the invention have been presented for purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise form disclosed and modifications and variations are possible in light of the above teachings or may be acquired from practice of the invention. The embodiments were chosen and described in order to explain the principles of the invention and its practical application to enable one skilled in the art to utilize the invention in various embodiments and with various modifications as are suited to the particular use contemplated.

