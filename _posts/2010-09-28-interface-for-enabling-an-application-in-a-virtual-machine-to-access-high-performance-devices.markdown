---

title: Interface for enabling an application in a virtual machine to access high performance devices
abstract: A high-performance device interface (HPDI) provides flexible and high-performance access by applications residing in a Virtual Machine (VM) to high-performance devices. The technique enables VM applications to use a single interface even when multiple device drivers exist, and can pass data efficiently between a VM application and a front-end device driver (a device driver implemented in a VM, such as may be used in a paravirtualization environment). It improves overall performance of a VM by reducing the copying of data during communications between a VM application and a front-end device driver, which reduces processor and memory usage.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09135043&OS=09135043&RS=09135043
owner: NetApp, Inc.
number: 09135043
owner_city: Sunnyvale
owner_country: US
publication_date: 20100928
---
At least one embodiment of the present invention pertains to virtualization systems and more particularly to an interface for enabling an application in a virtual machine to access high performance devices.

Virtualization is commonly used today to improve the performance and utilization of multi core multi processor computer systems. In a virtualization environment multiple virtual machines share the same physical hardware such as processors cores memory and input output I O devices. A software layer called a hypervisor typically provides the virtualization i.e. virtualization of physical processors memory and peripheral devices. This technique thereby enables the sharing of hardware by multiple virtual machines.

A virtual machine can provide a complete system platform which supports the execution of a complete operating system. One of the advantages of virtual machine environments is that multiple operating systems which may or may not be of the same type can coexist on the same physical platform. In addition a virtual machine can have an architecture that differs from that of the physical platform in which is implemented.

One problem often associated with virtualization environments is that there is no interface by which an application in a virtual machine can access high performance high speed devices at their native or optimum speed. In this context high performance devices may include for example network communication adapters such as Ethernet adapters and small computer system interface SCSI devices such as disk drives. This is in contrast with for example integrated drive electronics IDE devices which in comparison to the aforementioned devices generally are relatively low performance devices.

The cause of this problem is that the application does not have direct access to the device driver software within the virtual machine there are several layers of software between them. This architecture has the advantage of enabling device specific details to be hidden from the user space. However because the application and the device driver may not be able to map each other s address spaces they may not be able to share data with each other. In that case communication of data between the application and the device driver involves making several copies of the data as it propagates through the various software layers since the application and the device driver do not have any context about each other yet data consistency needs to be preserved . Consequently this process undesirably consumes additional CPU cycles to propagate the data through the various layers. Further due to this copying the data that reaches the destination software layer i.e. the application or the device driver depending on the direction of communication often ends up on a completely different memory page from where it started at the source software layer i.e. the device driver or the application which is an inefficient use of memory.

In a particular implementation a common problem associated with virtualization environments that provide device access via paravirtualization PV is a lack of flexibility when using PV interfaces. PV is a technique in which a virtual machine does not necessarily simulate hardware but instead or in addition it offers a special application programming interface API that can only be used by modifying the guest application. In PV the guest application is aware of the hypervisor whereas in full virtualization the guest application is not aware of the hypervisor.

A PV interface is an idealized device interface that allows an application in a virtual machine to better access underlying devices. However in PV different hypervisors require different device drivers in the virtual machines these device drivers within the virtual machines are called front end device drivers . In order for the application to derive optimum performance from the PV device the application might need to be modified. If the application is using an operating system API to access the PV device the API might need to be modified instead. Effectively therefore custom modifications are needed inside the guest virtual machine to leverage the high performance device.

This summary is provided to introduce in a simplified form certain concepts that are further described in the Detailed Description below. This summary is not intended to identify essential features of the claimed subject matter or to limit the scope of the claimed subject matter.

The technique introduced here includes a high performance device interface HPDI that provides flexible and high performance access to applications residing in a Virtual Machine VM to high performance devices. The technique enables VM applications to use a single interface even when multiple device drivers exist i.e. physical device drivers and or PV device drivers and can pass data efficiently between a VM application and a front end device driver. It improves overall performance of a VM by reducing the copying of data e.g. by using zero copy data sharing during communications between a VM application and a front end device driver which reduces CPU and memory usage.

In certain embodiments the technique includes the HPDI which may be an API or a set of APIs between one or more VM applications and one or more VM device drivers as well as associated techniques in which a VM application leverages the HPDI to achieve high performance when communicating with a device.

In certain embodiments two APIs are made available to a VM application for communicating with a device through the HPDI Send and Receive. Send is used by the application when it initiates data transfer. For example when issuing an I O operation to a disk or other type of storage device Send can be used to write data to or read data from the storage device. Receive allows a physical device to present data to the VM application asynchronously. For example the VM application may need to receive networking packets via a network communication adapter at any time without having requested them. Receive may be used for this purpose.

The HPDI also provides an abstract programming interface for a VM application. Accordingly rather than having to implement Send and Receive for multiple device drivers as might otherwise be necessary to obtain optimum performance the VM application only needs to support the one HPDI. As such the HPDI is device driver independent. This means the VM application need not be aware of the specific device driver s being used. Essentially the HPDI provides an abstraction layer that can provide high performance without the common detrimental side effects of software layering data copies increased latency etc. the VM application only needs to communicate with the HPDI.

Moreover in embodiments associated with virtualization environments that support paravirtualization PV the VM application can be made hypervisor independent increasing portability and flexibility for application developers. In certain embodiments therefore the HPDI operates logically on top of a paravirtualized device interface that operates between a hypervisor and a VM which provides efficient device access to the physical devices.

Other aspects of the technique will be apparent from the accompanying figures and detailed description.

In one embodiment the system implements the Xen virtualization environment i.e. using the Xen hypervisor in which case the management virtual machine A may be a Xen domain 0 dom0 and each user virtual machine B may be a Xen domain U domU . As another example the virtualization environment could be based on Microsoft Hyper V technology.

The illustrated system implements paravirtualization i.e. there is a paravirtualization PV interface which enables communication between management virtual machine A and the user virtual machine B. Accordingly the user virtual machine B includes a front end device driver FEDD which communicates with a back end device driver BEDD in the management virtual machine A via the PV interface . In conjunction with the PV interface the system implements a ring structured memory which is shared between the user virtual machine B and the management virtual machine A as part of shared memory . Note however that the technique introduced here does not necessarily have to be implemented with paravirtualization.

An application in a user virtual machine B needs to communicate with a physical device within the host system. Accordingly the technique introduced here provides a high performance device interface HPDI within a user virtual machine B to facilitate such communication. Details of the HPDI are discussed below. The purpose and nature of the application are not germane to the technique being introduced here however it is noted that one possible embodiment of the application is a data module of a network storage server as described below.

It is assumed here only for purposes of facilitating description that the illustrated system implements paravirtualization similar to that used by Xen and Microsoft Hyper V. Note that in other embodiments a different virtualization system environment may be employed. Note that the terms domain and virtual machine are used interchangeably in this description. Xen provides several components for inter domain communication to facilitate paravirtualization 1 a grant references mechanism 2 a shared input output I O ring and 3 an Event Channel mechanism. The grant references mechanism enables explicit sharing of memory pages between domains virtual machines . Each domain has its own grant table not shown and each entry in the grant table is identified by a grant reference which is an index into the table. A grant reference refers to a shared memory page which can be passed between domains i.e. between a user virtual machine and the management virtual machine. The grant reference mechanism is dynamic and provides for two types of page sharing read only and read write.

The shared I O ring is a shared bi directional producer consumer ring structure that allows communication between domains such as virtual machines A and B. The device drivers use the shared I O ring to send and receive I O requests and responses across domains. The shared I O ring is established in a region of memory that is shared between two domains. Therefore as part of an initial handshake mechanism that takes place between the FEDD and BEDD the grant reference information for the shared memory page is also exchanged.

Notification of requests responses on the shared I O ring happens via event channel interrupts between the two domains. Each ring request corresponds to an unit of I O between the domains raw data required for the I O read or write are represented via the corresponding data page s grant reference in the ring request. Thus all data transfers between the FEDD and BEDD can happen via an efficient zero copy shared memory mechanism.

The Event Channels mechanism is an asynchronous event notification mechanism that emulates hardware interrupts to the guest. An Event Channel module not shown can be provided inside each domain to acquire a physical interrupt request IRQ line within the domain and to register that IRQ line with the hypervisor . Collaborating modules in the domains e.g. ScsiFront and ScsiBack establish an Event Channel containing virtual interrupts between them for future asynchronous notification. The Event Channel module uses bitmaps for the virtual interrupts in a shared memory page established during initialization along side the shared I O ring between the domains. When a virtual interrupt needs to be sent by a module e.g. ScsiFront an Event Channel API along with the corresponding Event Channel handle is called. The API sets the appropriate bit in the shared memory region and makes a hypervisor call to generate an interrupt to the peer. The hypervisor knows the physical IRQ line occupied by the Event Channel module in the peer and triggers that interrupt in the peer. At the receipt of an interrupt the Event Channel module checks the bitmap to identify the appropriate virtual interrupt.

In addition to these mechanisms Xen provides XenStore a hierarchical management data store which is mainly used in the control path during the device discovery stage in initialization. The memory pages it uses are shared between domains such that Dom0 the management domain exports a device tree of devices that are available to DomUs user domains . A DomU will traverse this tree when it desires to run a device.

The domains A B can interact with XenStore using for example the XenBus API which is the interface provided by XenBus the abstract paravirtualization bus of Xen. The XenBus API enables paravirtualized device drivers to receive notifications and information from XenStore and to write data back to it. Therefore any paravirtualization child device of XenBus has three components to be fully functional 1 a shared memory page for the I O ring the actual ring request contents are specific to the protocol between the front end and back end 2 an Event Channel for signaling activity on the shared I O ring and 3 a XenStore entry for the device containing information about the device.

In one embodiment the HPDI is an API. Referring still to the functionality of the HPDI is two fold 1 to provide a single interface for VM applications to use even when multiple FEDDs exist in a virtual machine and 2 to pass data efficiently between a VM application and a FEDD. To that end the HPDI includes two APIs Send and Receive for a VM application to use.

Send is invoked by the application and can be used for storage device I O or networking I O. In both cases the application sends a Send request to the HPDI either passing it data or requesting it. Note that even if the application requests data it is still uses the Send interface. A Send Callback function is used by the HPDI to communicate a success or error notification back to the application.

Receive allows a physical device to present data to the application asynchronously. For example the application may need to receive networking packets via a network communication adapter at any time without having requested them. A Receive Callback function is used by the HPDI to communicate a success or error notification back to the application .

The opcode indicates whether this particular instance invocation of the Send function is to perform a read or a write. In the case of a write the scatter gather list indicates the source memory locations where the data to be written resides in the case of a read it includes the destination memory locations to which the read data are to be written. Note that in another embodiment a simple pointer to a memory region could be used instead of a scatter gather list. The length value indicates the length of the data to be written or read. The context information can be any information that uniquely identifies this particular invocation of the Send function to distinguish it from other instances of the Send function . The context information can be in the form of for example a pointer to a unique context structure called application specific context information ACI that describes the operation. Each operation from the application corresponds to one or many operations of the FEDD. In each such FEDD operation the corresponding ACI is stored.

Next at step the HPDI responds to Send command by creating an internal log entry for this operation which includes the pointer to the Send Callback function and the ACI. The HPDI also queries the FEDD to determine whether sufficient space is available in the shared ring buffer . If sufficient space is not available then the HPDI buffers the command and parameters step until sufficient space is available.

When sufficient space is available in the shared ring buffer at step the HPDI sends the command and its parameters to the FEDD and then returns control to the application . Next at step the FEDD uses the above described grant reference mechanism to communicate with the BEDD in the management virtual machine using the PV interface . In the case of a write operation this communication involves sending the data to be written to the BEDD . In the case of a read operation it involves sending the pointers to the memory pages that have been pre allocated by the application to receive data from the device.

At step the BEDD sends an IRQ to the FEDD when in the case of a write the data has been successfully written to the physical device or in the case of a read the data has been placed by the BEDD into the shared ring buffer . The FEDD responds to the IRQ at by invoking a callback function in the HPDI HPDI callback this is separate from the Send Callback that the application previously registered with the HPDI . At step inside the HPDI callback the HPDI examines the FEDD operation that completed and retrieves the corresponding ACI that was stored in the FEDD operation. The HPDI uses that ACI to retrieve the corresponding log entry and derives the Send Callback function from it. Finally at step the HPDI executes the Send Callback function which performs application specific function s which includes or refers to the appropriate ACI for this invocation of the Send function. In the case of read the notification will also include the read data or a pointer to the read data.

Note that for explicit read devices devices where the exact location to be read and the buffers into which the data is to be read into are specified a prion the Send function of the HPDI enables both writing and reading data from the device. SCSI devices are a common example of this type of device. Other disk and block devices follow a similar model.

For non explicit read devices devices where data is received at any point in time asynchronously by the device the VM application need not make an explicit read to receive data from the device. Network devices are examples of non explicit read devices. Network packets containing data in general can be received at any time on any interface for any application. Further processing is performed on the packet to assign it to the right application. In at least some embodiments only non explicit read devices use the Receive function of HPDI.

For non explicit read devices such as network devices a network packet could be received asynchronously for the VM application at any point in time. The Receive API of the HPDI is provided to accommodate such devices. Using this API the VM application pre allocates memory buffers and passes them to the HPDI . In a PV environment the HPDI can share the memory buffers with the BEDD via the FEDD . As network packets are received for the VM application the BEDD deposits network packets obtained from the device directly into these buffers. Thus data is available to the VM application without further copies being required along the path.

For each Receive invocation along with the receive buffer application specific receive context information ARCI is allocated by the application and shared with the HDPI . In addition for each invocation a Receive CallBack function is provided to the HDPI . The HDPI will execute the Receive CallBack when data is placed into the specific receive buffer step .

In one embodiment for PV environments for each receive buffer passed to HDPI via the Receive API the HDPI creates an appropriate FEDD operation to share the buffer with BEDD and queues it on the shared PV ring infrastructure for the BEDD to pick up. The corresponding ARCI information is stored in the FEDD operation. As can be seen this model provides end to end zero copy between the physical device via BEDD followed by FEDD to the VM application .

Referring now to when a data packet is subsequently received by a physical device e.g. by a network adapter over a network at step the BEDD at step has already placed the data into one of the previously allocated shared buffers and sends an IRQ to the FEDD . At step the FEDD responds to the IRQ by invoking an internal callback function of HPDI . Inside the callback at step the HPDI identifies the FEDD operation that has completed and retrieves the ARCI from the FEDD operation. From the ARCI at step the HPDI is able to lookup the appropriate Receive Callback routine registered by the VM application for this invocation of the Receive operation and executes it. The Receive CallBack performs one or more application specific operations on the received data and passes the data up the application stack at step . This process is repeated for all received packets.

Note that the pool of receive buffers provided to the HDPI get depleted as new packets are received from the BEDD . Therefore as packets are received new receive buffers need to be shared afresh with the HDPI by using more invocations of the Receive API.

The above described HPDI can function as a single interface by which one or more applications can communicate with multiple device drivers in a given virtual machine. This scenario is illustrated in . In the HPDI provides an application or multiple applications with access to multiple paravirtualized FEDDs through N in the user virtual machine B. The multiple FEDDs through N may be of different types and may correspond to different hypervisors through N. Alternatively or additionally the multiple FEDDs through N may correspond to multiple different BEDDs through N in the management virtual machine. These BEDDs in turn may be used to access different physical devices through N as shown which may be of different types.

The above described technique can be implemented in many different processing environments. One example of such an environment is a network storage system such as illustrated in . In a network storage server is coupled to a storage subsystem that includes non volatile mass storage devices and to a set of clients through an interconnect . The interconnect may be for example a local area network LAN wide area network WAN metropolitan area network MAN global area network such as the Internet a Fibre Channel fabric or any combination of such interconnects. Each of the clients may be for example a conventional personal computer PC server class computer workstation handheld computing communication device or the like.

The storage server manages storage of data in the storage subsystem on behalf of the clients . For example the storage server receives and responds to various read and write requests from the clients directed to data stored in or to be stored in the storage subsystem . The mass storage devices in the storage subsystem can be for example conventional magnetic or optical disks or tape drives alternatively they can be non volatile solid state memory such as flash memory or solid state drives SSDs or a combination of the aforementioned types of devices. The mass storage devices can be organized as a Redundant Array of Inexpensive Devices RAID in which case the storage server accesses the storage subsystem using one or more well known RAID protocols. The storage server may include a storage operating system not shown i.e. a functional module which controls most of the operations of the storage server including servicing client initiated data access requests.

The storage server may be a file level server such as used in a NAS environment a block level storage server such as used in a SAN environment or it may be capable of providing both file level and block level data access. Further although the storage server is illustrated as a single unit in it can have a distributed architecture such as shown in . For example a storage server can include a physically separate network module e.g. N module and data module e.g. D module which may communicate with each other over an external interconnect . To control and perform the above described operations a storage server may contain a storage operating system of which the N module and D module may be components.

A storage server can be implemented as virtual storage server for example as a user virtual machine in a virtualization environment such as described above. In that case an application within such user virtual machine may be for example the storage operating system of a storage server or one or more elements of the storage operating system. illustrates an example of such a storage operating system.

In the embodiment shown in the storage operating system includes several modules or layers . These layers include a storage manager which is the core functional element of the storage operating system . The storage manager imposes a structure e.g. a hierarchy on the data stored by the storage server and services read and write requests from clients.

To allow the storage server to communicate over a network e.g. with clients the storage operating system also includes a multiprotocol layer and a network access layer which operate logically under the storage manager . The multiprotocol layer implements various higher level network protocols such as Network File System NFS Common Internet File System CIFS Hypertext Transfer Protocol HTTP Internet small computer system interface iSCSI and or backup mirroring protocols. The network access layer includes one or more network drivers that implement one or more lower level protocols to communicate over the network such as Ethernet Internet Protocol IP Transport Control Protocol Internet Protocol TCP IP Fibre Channel Protocol FCP and or User Datagram Protocol Internet Protocol UDP IP .

To allow the storage server to communicate with a local storage subsystem the storage operating system includes a RAID layer and an associated storage driver layer logically under the storage manager . The RAID layer implements a higher level RAID algorithm such as RAID 0 RAID 1 RAID 4 RAID 5 or RAID 6. The storage driver layer implements a lower level storage device access protocol such as Fibre Channel Protocol FCP or small computer system interface SCSI . Also shown is the data path between the clients and storage devices.

In the illustrated embodiment the storage manager the RAID layer and the storage drivers are functionally grouped into a set of code and or circuitry i.e. a D module which is responsible for data management functions whereas the multiprotocol layer and the network access layer are functionally grouped into a separate set of code and or circuitry i.e. the N module which is responsible for network communications. In this embodiment the N module and the D module communicate through a special purpose communication link and protocol .

In certain embodiments the above mentioned application is the D module of a storage operating system of a virtual network storage server such as described above. In other embodiments the application is the RAID module of a storage operating system. In still other embodiments the application is one or more other components of the storage operating system or it is the entire storage operating system. In still further embodiments the application is a completely different type of application.

The processors may be or include the CPUs of the processing system and thus control the overall operation of the processing system . In certain embodiments the processor s accomplish this by executing software or firmware stored in memory such as memory . Each processor may be or may include one or more programmable general purpose or special purpose microprocessors digital signal processors DSPs programmable controllers application specific integrated circuits ASICs programmable logic devices PLDs or the like or a combination of such devices.

The memory is or includes the main memory working memory of the processing system . The memory represents any form of random access memory RAM read only memory ROM flash memory as discussed above or the like or a combination of such devices. In use the memory may contain among other things software and or firmware code and data for use in implementing a virtualization environment one or more virtual machines and the HPDI described above.

Also connected to the processors through the interconnect are a network adapter and a storage adapter . The network adapter provides the processing system with the ability to communicate with remote devices such as clients over a network and may be for example an Ethernet adapter or Fibre Channel adapter. The storage adapter allows the processing system to access its associated storage subsystem and may be for example a Fibre Channel adapter or a SCSI adapter.

The techniques introduced above can be implemented by programmable circuitry programmed configured by software and or firmware or entirely by special purpose circuitry or by a combination of such forms. Such special purpose circuitry if any can be in the form of for example one or more application specific integrated circuits ASICs programmable logic devices PLDs field programmable gate arrays FPGAs etc.

Software and or firmware to implement the techniques introduced here may be stored on a machine readable storage medium and may be executed by one or more general purpose or special purpose programmable microprocessors. A machine readable medium as the term is used herein includes any mechanism that can store information in a form accessible by a machine a machine may be for example a computer network device cellular phone personal digital assistant PDA manufacturing tool any device with one or more processors etc. . For example a machine accessible medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. etc.

The techniques introduced above can be implemented by programmable circuitry programmed configured by software and or firmware or entirely by special purpose circuitry or by a combination of such forms. Such special purpose circuitry if any can be in the form of for example one or more application specific integrated circuits ASICs programmable logic devices PLDs field programmable gate arrays FPGAs etc.

Software or firmware to implement the techniques introduced here may be stored on a machine readable storage medium and may be executed by one or more general purpose or special purpose programmable microprocessors. A machine readable medium as the term is used herein includes any mechanism that can store information in a form accessible by a machine a machine may be for example a computer network device cellular phone personal digital assistant PDA manufacturing tool any device with one or more processors etc. . For example a machine accessible medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. etc.

References in this specification to an embodiment one embodiment or the like mean that the particular feature structure or characteristic being described is included in at least one embodiment of the present invention. Occurrences of such phrases in this specification do not necessarily all refer to the same embodiment. On the other hand different embodiments may not be mutually exclusive either.

Although the present invention has been described with reference to specific exemplary embodiments it will be recognized that the invention is not limited to the embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense.

