---

title: Gate-level logic simulator using multiple processor architectures
abstract: Techniques for simulating operation of a connectivity level description of an integrated circuit design are provided, for example, to simulate logic elements expressed through a netlist description. The techniques utilize a host processor selectively partitioning and optimizing the descriptions of the integrated circuit design for efficient simulation on a parallel processor, more particularly a SIMD processor. The description may be segmented into cluster groups, for example macro-gates, formed of logic elements, where the cluster groups are sized for parallel simulation on the parallel processor. Simulation may occur in an oblivious as well as event-driven manner, depending on the implementation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08738349&OS=08738349&RS=08738349
owner: The Regents of the University of Michigan
number: 08738349
owner_city: Ann Arbor
owner_country: US
publication_date: 20100421
---
The present application claims the benefit of U.S. Provisional Application No. 61 326 212 entitled Gate Level Logic Simulator Using Multiprocessor Architectures filed on Apr. 20 2010 which is hereby incorporated by reference herein in its entirety.

This invention was made with government support under Contract No. HR0011 07 3 0002 awarded by Defense Advanced Research Projects Agency DARPA . The United States Government has certain rights in this invention.

This disclosure generally relates to simulation of integrated circuits for validation and more particularly to oblivious and event based techniques for simulating integrated circuit descriptions in multiple processor architectures.

Logic simulation is a central aspect of the integrated circuit IC development process and serves as the primary tool for verifying a wide range of aspects in a design. Foremost among these aspects is the correctness of a design s functional behavior both as a behavioral description of the system as well as a structural gate level description. Most industry design flows invest the largest fraction of their time and resources precisely on this task in an attempt to provide the best possible guarantee that the system satisfies its original functional specification. Often large server farms comprising thousands of machines are employed for months at a time to execute billions of cycles of simulation. Much of this time is consumed in simulation of gate level netlists which involves large netlists at a fairly low level description comprising many components that must be simulated. Overall the simulation and verification of an integrated circuit design is one of the most time consuming tasks in the entire development process and the performance limitations of logic simulators are one its main reasons. The consequences are poor design coverage delayed product releases and bugs that escape into silicon.

Logic simulation entails evaluating the response of an IC design over time when subjected to a set of input stimuli typically selected by the designer to be representative of practical use situations. For most designs synchronous the response of the logic simulation is computed once for each cycle of simulated execution. Modern logic simulator implementations read in a design description then compile the description to produce machine code emulating the same functionality as the design s primitives. Simulation finds application in many aspects of a design development process including functional validation power and timing estimation and checking of equivalence among different circuit representation. Gate level netlists must be simulated for most of these applications.

Particularly problematic for logic simulators however is that simulation of structural netlists is a notoriously time consuming process yet essential to determine that if a synthesized design matches the initial design specifications and behavioral description. As circuit designs increase in size and features offered they increase in complexity. As a result there is an increasing need for improved performance by logic simulators for IC design.

The present application describes techniques for logic simulation of integrated circuit IC designs using techniques that selectively partition and optimize descriptions of the IC design for more efficient simulation. The partitioning may be applied to netlist descriptions of an IC design and optimized for operation on an external processor. Once the netlist has been partitioned and optimized it may be simulated either in an oblivious manner or in an event driven manner. For the former the simulation may proceed by computing the output values of logic component after logic component until the entire IC design is simulated. For the latter the simulation may be performed by computing the output values only of those logic components or groups of logic components for which certain event based triggers have occurred for instance a change in the input values so that most of those components whose output cannot experience a change in value do not contribute to the computation time of simulation.

The logic simulator may be executed using a host processor and a general SIMD class processor. In some examples the logic simulator is executed using a host processor and a general purpose graphics processor unit GP GPU device. The logic simulator is able to leverage massive parallelism in suitable processor configurations to achieve large performance improvements over even the fastest modern commercial logic simulators.

In some examples the logic simulator is a gate level concurrent simulator that partitions a netlist register transfer level RTL or other connectivity based description of an IC optimizes that description and then maps it to a processor for simulation. In some examples the mapping process includes clustering and gate balancing processes which are optimized for the simulation processor.

In some examples the logic simulator is optimized for integrated circuits with large structural netlists. By leveraging the parallelism offered by various types of processor architectures in particular GP GPUs the simulator is able to leverage netlist clustering and balancing algorithms tuned for the target architecture.

In some examples the logic simulator functions as an event driven simulator so that only a fraction of a netlist s gates are simulated each cycle. In such examples the logic simulator is to have minimal overhead for run time event scheduling as this is an intrinsically sequential process while the simulator still needs to maintain a massively parallel computation environment most of the time. Thus in some examples the logic simulator is a hybrid simulator where clusters of gates called macro gates are simulated in an oblivious fashion while the scheduling of individual cluster groups is organized in an event driven fashion.

In an embodiment a method for logic simulation of a connectivity level description of an integrated circuit where the connectivity level description comprises a plurality of logic elements comprises in a first processor clustering logic elements into cluster groups each comprising at least one of the plurality of logic elements where the cluster groups are sized such that during a simulation cycle each cluster group is capable of being simulated on a different processor block of a second processor wherein the processing units of the second processor are capable of simultaneous operation and in the second processor simultaneously simulating a plurality of the cluster groups each being simulated on a different one of the processing units.

In some such examples the connectivity level description is a netlist and the logic elements are logic gates within the netlist.

Furthermore some of these examples further comprise separating the cluster groups into one of a plurality of layers from a lowest level layer to a highest level layer.

In some examples each cluster group is defined by a gap height which is the number of logic levels in a longest logic chain within the cluster group and a lid width which is the number of logical output values provided by the cluster group after the simulation cycle. Some of these examples further comprise adjusting the respective gap height and lid width for each cluster group such that the cluster group is capable of simulation on one of the processing units.

In another embodiment a logic simulator for simulating a connectivity level description of an integrated circuit where the connectivity level description comprises a plurality of logic elements comprises a parallel processor having a plurality processing units capable of simultaneous execution by the parallel processor and a host processor configured to cluster logic elements into cluster groups each cluster group comprising at least one of the plurality of logic elements where the cluster groups are sized such that each cluster group is capable of being simulated on one of the processing units of the parallel processor and map each cluster group for simulation by one of the plurality of processing units.

In some examples the parallel processor comprises a SIMD general purpose graphics processing unit GP GPU architecture or compute unified device architecture CUDA architecture. In some of these examples the parallel processor comprises a plurality of cores each having a plurality of the processing units. In some examples the parallel processor comprises a device memory connected to each of the plurality of cores and a shared memory wherein the shared memory is configured for simultaneous access by each of the processing units within one of the cores during a processing cycle.

The present application describes techniques for logic simulation of integrated circuit IC designs using partitioning and optimization techniques that may be initiated under different conditions including from event based triggers. The logic simulators may be executed on general SIMD class processors. In some examples however the logic simulator is executed on a general purpose graphics processing unit GP GPU for example that allows software applications to run parallel processes across different functional pipelines. More specifically in some examples a particular class of GP GPU architectures are used namely a compute unified device architecture CUDA which is a parallel computing architecture provided by NVIDIA Corporation of Santa Clara Calif. in various processors. These processor configurations are provided by example not limitation. In each of these processor types as well as other applicable architectures the logic simulator may leverage massive parallelism to achieve large performance improvements in IC circuit simulation and testing. In some examples the logic simulator is optimized for integrated circuits with large structural netlists. By leveraging the parallelism offered by SIMD GP GPUs CUDAs etc. a logic simulator is capable of applying a netlist balancing process tuned for the target architecture.

In some examples the logic simulator may be implemented as a gate level concurrent simulator having a design compilation process that partitions a netlist register transfer level RTL or other connectivity based description of an IC. The simulator may optimize that netlist etc. and then map gates thereof to the processor architecture. To achieve improvements in performance the simulator clusters the gates for simulation and balances these clusters for simulation in a parallel processing environment. Thus in some examples the particular clustering and gate balancing is optimized for more efficient utilization depending on the processor architecture.

In some examples the logic simulator may be implemented as an oblivious simulator that evaluates each gate during each simulation cycle. In other examples the logic simulator is an event driven simulator that only evaluates a gate in response to particular events such as if a change occurs at a gate s input nets. Oblivious simulators can offer very simple scheduling static data structures and better data locality. While event driven simulators rely upon a dynamic analysis of which gates must be scheduled for re evaluation.

In some event driven designs the logic simulator is configured such that only a fraction of a netlist s gates are simulated each cycle. The logic simulator may for example be designed to use minimal overhead for run time event scheduling as this is an intrinsically sequential process while the simulator still needs to maintain a massively parallel computation environment most of the time. The clustering and balancing therefore may be hybridized with an event trigger based solution. For example the logic simulator may cluster gates e.g. into macro gates that are simulated in an oblivious fashion but the scheduling of individual cluster groups is organized in an event driven manner.

The host processor and the SIMD unit may be connected via a direct memory access DMA which is provided by way of illustration as any suitable wired or wireless communication channel between the processors may be used. In an implementation computationally intensive parallel code operations of the simulator are offloaded to the processing unit for execution. Typically the processor functions as a host processor and is a single core processor or a multi core processor with only a few cores. In the illustrated example the processor has four cores . The SIMD unit in contrast is to have many cores many more than the host processor . These cores may be hardware or software based. In some examples they each reflect a different processor executing a dedicated thread execution block where in some of these examples the cores are distributed over different processors. As used herein core refers to a grouping of processing units. A SIMD processor for example may comprise multiple cores each comprising multiple processing units each processing unit capable of executing a different thread and in parallel.

The processor is able to map code onto these cores for parallel execution. Twenty four cores are shown by way of example though not all are labeled with a reference number . In most examples the SIMD unit will have much larger numbers of cores that may be segmented into different processors. Example SIMD processors may have 80 96 192 240 or 320 cores although the processors are not limited in the number of cores. Each core functions as a processor block. By way of example not limitation these cores may be implemented with 16 bit 64 bit or 128 bit architectures further in some examples they may each include dedicated floating point processors. The logic simulator is able to take advantage of any number of cores on the SIMD unit . Thus to allow for a massive parallel execution of netlist simulation the SIMD unit is not limited to a particular number of cores.

The GP GPU processor includes a plurality of cores each having processing units delineated in hardware. The processing units can execute multiple threads concurrently e.g. up to 512 or more simultaneous threads at the same time. In some examples each core executes blocks of threads all running the identical kernel code. As a part of the GP GPU architecture a fast shared memory is available to each core and more particularly to each processing unit of that core . In some examples this shared memory is accessible within 1 clock cycle. A device memory is also provided on the GP GPU and would typically be sized between 256 MB to 2 GB depending on the application although larger memories may be used. The device memory is accessible by all the cores but with a clock with a latency typically larger than that of the shared memory for example a latency of 300 to 400 clock cycles. This latency may be masked by time interleaving thread execution within a core for example while a group of threads is executing on local data others are suspended waiting for their data to be transferred from the device memory. In the illustrated example all execution takes place on the SIMD unit or the GP GPU while the host processor serves only to invoke the beginning execution of a thread batch and waits for completion of one or more cycles of netlist simulation.

In some examples the operation may have a hybrid nature in which the logic simulator not only applies an oblivious simulation approach to clustering e.g. to simulate every logic gate in the design at every simulation cycle but also an event driven approach that limits the number of cluster groups being simulated at any given time. Thus while the oblivious simulation for clustering has the advantage of uniform control flow to prevent superfluous computation of gates whose inputs did not change the event driven simulation can determine which subset of the gates should be simulated each cycle e.g. by only simulating those gates whose input values have changed.

Three factors may guide the process of macro gate formation as shown in . i For macro gates that are used in an event driven simulation at a coarse granularity compared to individual gates the time required to simulate a certain macro gate should be substantially larger than the overhead to decide which macro gate to activate. ii The microprocessors in the GPU communicate through slower device memory and as such the cores are able to execute independent of each other. This is assured if the macro gates that are simulated in parallel can be simulated independently of each other. Thus the macro gate formulation process may achieve this goal by duplicating small portions of logic among the macro gates thereby eliminating the need for communication. iii Cyclic dependencies between macro gates may be avoided in order to simulate each macro gate at most once per cycle implying that the netlist can be levelized at the granularity of macro gates as well.

In the illustrated example the clustering operation partitions the macro gates into layers each layer encompassing a fixed number of netlist levels. The macro gates may then be defined by selecting a set of nets at the output boundary of a layer and then including the cone of influence extending from those output nets backward to the layer s input nets the tapering shape is that of a trapezoid. Three layers are shown in the illustrated example each layering having two levels of gate logic. However any number of layers and levels larger or smaller may be used. The number of levels within each layer is termed the gap height and corresponds to the height of the macro gate i.e. the number of logic levels in the longest logic chain within a macro gate.

In the operation some logic gates have been assigned to more than one macro gate. Logic gate for example has been duplicated into two macro gates and for computation of respective output nets.

There are several possible policies for selecting the nets whose cones of influence will be clustered by the process into a single macro gate. To minimize duplication for example the logic simulator can attempt to cluster those nets whose cones of influence have a maximum number of gates in common. The number of output nets used to generate each macro gate is a variable parameter termed the lid. In some examples the logic simulator executing the process exemplified in may select the value for the lid parameter so that the number of logic gates in all macro gates is approximately the same.

Next the host processor extracts the combinational logic elements e.g. AND OR NOT XOR etc. from the netlist at block for the purpose of setting up an initial netlist to be mapped for simulation. At block the host processor then partitions the netlist into cluster groups e.g. macro gates such as which are logic blocks appropriately sized to fit within the constraints of the SIMD architecture e.g. with the constraints of a GP GPU or CUDA architecture. The block for example may prepare preliminary or rough cluster groupings based on size estimates quickly computed on the fly. At block the CPU then balances these preliminary cluster groupings through an optimization process restructuring cluster groups to improve compute efficiency during simulation. The block for example may perform a SIMD independent clustering of netlist gates whereas the block may perform a SIMD processor specific optimization of the generated cluster groups to form balanced macro gates that simulate efficiently on the SIMD unit e.g. the GP GPU . In some examples the block balances the clusters to maximize efficiency although maximization is not required. Finally all the required data structures are compiled into the SIMD kernel and transferred to the SIMD device. The optimized macro gates partitioned design is then simulated at block .

The block may be implemented through any number of synthesis techniques. The process generally uses a gate level netlist as the input where this input is either a synthesized version of a design under verification or a behavioral description to which a synthesis step is applied. Such synthesis may or may not be optimized for time power area or any other metric.

In operation the block may extract the combinational portion of the gate level netlist and map it to the GP GPU creating data structures to represent the gates as well as their input and outputs. An example illustration of an extracted combination portion of a netlist is shown in . A combinational portion of a netlist topology contains a plurality of logic gates having plurality of corresponding inputs from a lower level region and a plurality of outputs to a higher level region. These reflect the structures required for simulation of the portion . The block may establish a value matrix which stores the intermediate net values for simulation of the portion . In the illustrated example there is a one to one correspondence between a row of intermediate values and the various levels of logic on the netlist . As a thread is executing during simulation the GP GPU may be able to store and retrieve intermediate net values using the local shared memory . In the illustrated example each net of the matrix requires 2 bits of storage in a 4 valued simulator where all these values may be stored in the faster shared memory . The shared memory may also store gate type truth tables consulted for the evaluation of each gate.

All other data structures associated with the simulation may reside in the device memory . That is in the illustrated example input buffer values may be stored in the device memory along with output buffer values of the entire netlist simulation. The netlist topology information may be stored there as well. Thus data such as the netlist topology information that is required often may be stored in the device memory . However in the example described data to be shared among threads is stored locally.

Generally speaking for cone partitioning the netlist is viewed as a set of logic cones one for each of the netlist s outputs. The block transmits preliminary partition data to a block where each identified cone by block includes all the gates that contribute to the evaluation of the cone output. The block analyzes these cones and assigns them to macro gates i.e. cluster groups . The process for example may operate one macro gate at a time as illustrated or may define multiple macro gates at once.

Due to the lack of inter cluster communication capability each macro gate is to include one or more cones of logic and each cone is fully contained within a macro gate. As a result once a macro gate has been completely simulated by the process one or more output values have been computed and can be stored directly into an output buffer vector. In forming the cones into macro gates the block may produce cone overlap which necessarily requires that some gates are duplicated because they belong to multiple cones. The incidence of this extra computation is small in practice. In execution the block may assign one cone of logic to the cluster group from block and add additional cones to this cluster group until memory resources have been exhausted. Various criteria for adding cones may be used such as the maximal number of overlapping gates where for example a second logic cone is selected so that it is the cone that overlaps the most with the first cone. In other words the block may create all the cones of logic per the partitioning approach e.g. by taking each output wire from a layer and finding all the gates that contribute to compute the value of that wire. The block then is able to pack together several of these cones to make a macro gate. How many cones may be chosen is based on how much shared memory which is fast to access is available on the platform. One way to fit more cones into a macro gate is to assign cones with a large number of gates in common. For instance if cone A is comprised of 20 gates and cone B is comprised of 25 gates and cones A and B have 15 gates in common then the block could assign these two cones to the same macro gate and generated a macro gate comprising 20 25 15 30 gates a reduction of 15 gates from the potential .

The block determines macro gate sizing. To do this two macro gate parameters are considered the gap and the lid which collectively control the granularity at which an event driven mechanism operates. The block may determine gap and lid values for the macro gates by evaluating a range of candidate value pairs. For each candidate pair the block may assess several metrics number of macro gates number of monitored nets size of macro gates and or activation rate. The activation rate may be obtained by a mock up of the simulation on a micro testbench for example. Once the candidate gap lid pairs have been determined the block may then select the locally optimal values for the gap and lid pair.

The boundaries for the range of gap values considered may be derived from the number of monitored nets generated e.g. considering only gap values for which no more than 50 of the total nets are monitored. In practice small gap values tend to generate many monitored nets while large gap values trigger high activation rates. For determination of the lid value the block may bound the analysis by estimating how many macro gates will be created at each layer striving to run all the macro gates concurrently. Such determination may be based on the number of cores e.g. on the particular GPU architecture being used. For a CUDA architecture that includes 14 microprocessors and a CUDA scheduler that allows at most three thread blocks in concurrent execution on the same core the block may thus consider lid values that generate no more than 14 3 42 macro gates per layer.

In other examples the clustering of cones used to form macro gates may be based on an activity profile. Any macro gate containing a frequently activated logic gate will result in the entire macro gate being simulated. As such the block may seek to consolidate logic cones with frequently activated logic gates into the same macro gate. show an example of clustering based on estimated activity profile e.g. activation rates where the shading of a cone indicates its activation frequency.

Profiling may be used to estimate activity profiles for example first simulating a micro testbench 10 000 cycles long on a segmented circuit using the default clustering policy. During this simulation the activation rates of each logic cone in each layer may be aggregated. The activation frequency of a cone is defined as the maximum of the activation frequency of all of its input wires because that cone will need to be simulated if any of the input nets undergoes a value change. However the input nets which form the base of the cone are part of monitored nets. Hence the activation frequency of all cones can be computed by recording the activation frequency of the monitored nets. Once these are computed the segmentation process is performed again but cones are added to macro gates based on activation frequency rather than simply by logic sharing.

The block determines the value of gap and lid but these do not necessarily have to be constant during an entire clustering process. In fact from an activity stand point the gap parameter may vary for example with lower level gates being the most frequently active. To address this an annihilation ratio of a logic cone may be defined as the ratio of the activation frequency at the apex of the cone and the maximum of the activation frequencies of the wires at the base of the cone. In an example the block reaches the ideal gap value when the number of logic levels in a cone results in a sufficiently low value of this annihilation ratio for all the cones in a layer. In fact the number of levels needed may vary from cone to cone such that the block can locally vary values of a gap with a macro gate being composed of a few cones reaching the desired value of annihilation ratio within the same number of levels and the lid being a byproduct thereof. For example the gap value may vary such that within a macro gate different gap heights are present instead of the purely trapezoidal shape in the example of . Across the lid the lower gap heights may be formed within the macro gate.

In block determines if additional cluster groups are to be formed and if so control is returned to block . If not the clustering process is completed when all gates have been mapped to a set of cluster groups preferably by minimizing logic overlap while satisfying the constraints of shared memory resources.

Returning to the block performs balancing on the macro gates provided by process . This balancing may seek to minimize the critical execution path of thread blocks macro gates on the GP GPU . For example the block may consider each defined macro gate individually and optimize the scheduling of each gate simulation so that the number of logic levels is minimized. The simulation latency of a single cycle is limited by the macro gate with the most logic levels since each additional level requires another access to the slower device memory. Considering the number of logics levels gap and the number of concurrent threads simulating distinct gates lid the block balances these within the constraints of the SIMD UNIT architecture. With the GP GPU it is desired to have a maximum of 256 concurrent threads. Although more or fewer concurrent threads may be executed depending on the processor architecture.

The cluster balancing of block may reshape the natural triangular macro gates to a rectangular shape with a 256 wide base. Macro gates tend to be triangular in visual depiction because they are a collection of cones of logic which are usually triangular where a wide set of inputs computes one output through several stages of gates. An example clustering operation is illustrated in . The original macro gate has a base width of 3 160 gates and a height of 67 levels of logic where most of the deeper levels require significantly less than 3 000 threads. In SIMD based processors including GP GPU and CUDA simulation on the macro gate would required 3 160 simultaneous threads at Level and the same number of occupied threads until all 67 levels of simulated code has been completed even though after each successive logic level fewer threads will be functions. The block reshapes the macro gate execution scenario into a more efficient rectangular shape scenario with a base width of 256 gates i.e. 256 simultaneous threads that are used consistently through all logic levels. In the illustrated example the balanced macro gate executes over a larger cluster height 81 versus 67 than would have occurred prior to balancing. But the tradeoff results in substantially larger efficiency of operation as fewer gates are required to simulate the macro gate 256 versus 3160 .

The balancing process of may include for example setting a macro gate width parameter to W i.e. W gates can be simulated concurrently in each level . Then sorting the gates in a macro gate by level where gates at Level are first. Fill W slots at each level starting from Level until all gates are mapped to a slot to complete the balancing.

In the illustrated example of after the balancing at the block the process will have generated a finite number of macro gates optimized them and generated all the support data structures necessary for the kernel code to simulate all gates in a netlist with a high level of parallelism on the GP GPU . At this point cluster data for the macro gates and kernel code can be transferred to the GP GPU or other SIMD unit more generally and simulated cycle by cycle at block .

Simulation begins with the host processor transferring the kernel code and data structures to the SIMD unit e.g. the GP GPU . In some examples the SIMD unit takes over without the assistance of the CPU and schedules the defined macro gates for parallel execution. When all macro gates have executed one simulation cycle is complete and the SIMD unit may return control to the host processor which may read the primary outputs evaluate the testbench set primary inputs and invoke the next cycle. In other examples the SIMD unit may perform multiple consecutive simulation cycles before communicating with the host processor and returning control to the host processor for evaluating inputs and the testbench. Whether communication between the SIMD unit and the host processor occurs after each cycle or more infrequently may be determined by the testbench.

In some examples macro gate execution on the SIMD UNIT proceeds in three phases scattering logic evaluation and gathering. During scattering the macro gate s primary input data is retrieved from the device memory and copied to the value matrix of stored in the local shared memory . Next logic evaluation progresses when each thread begins execution. The threads each simulating one gate of the macro gate retrieve the relevant portion of the netlist from the device memory as well as gate truth tables and net matrices from the local shared memory. With this information the threads evaluate their gates by consulting the truth table. Computed results are copied from the value matrix to the output buffer vectors in the device memory . Finally the threads synchronize after simulating their respective gates and the process is repeated for all the subsequent logic levels in the macro gate. shows an example of macro gate execution for the sample netlist of . Six simultaneous simulation threads are shown Thread Thread corresponding to the six inputs required at Level. At Level Thread Thread are used to each simulate a corresponding Level gate producing gate output values n n and n respectively. The process continues through Level and Level with the outputs for each thread synchronized after each level execution.

The simulation performed by the block may be carried out directly on the SIMD UNIT e.g. the GP GPU . The overall simulation may alternate between execution of all active macro gates in a layer followed by observing the value changes in the monitored nets of the next layer to determine which macro gates will be activated over the next level. Each individual macro gate is simulated by a single thread block in an oblivious fashion. For simulation the block may include two kinds of parallelism first multiple independent macro gates are simulated by independent thread blocks possibly executing in parallel on different multi processors within the SIMD UNIT and second the gates at the same level inside a macro gate are simulated in parallel by different threads.

Each macro gate corresponds to one thread block and each core executes multiple thread blocks. Three concurrent thread blocks in a core for example requires assigning 3 macro gates per core. The cores can execute more or fewer macro gates. For example the simulation block can allocate one macro gate per core which allows for a larger macro gate. Generally speaking however memory access latency is better optimized with multiple thread blocks assigned per core. In some examples an internal scheduler in the GP GPU is responsible for determining which core will compute which thread blocks hence the scheduling of macro gates after they have been marked for simulation is internal.

The simulation block may in some examples simulate the macro gates using an event based triggering mechanism. illustrates an example event driven simulation operation . The illustration shows a layered structure of macro gates and monitored nets and . The illustration also shows how only activated macro gates from the pool of macro gates at each layer are scheduled for execution.

In some examples the simulation block is executed through two kernels alternatively or simultaneously executing on the SIMD UNIT . Operating within the SIMD unit a simulation kernel simulates all active macro gates in a layer. Also operating in the SIMD unit a scheduling kernel evaluates the array of monitored nets associated with the simulated layer to determine which macro gates should be activated in the next layer.

In the illustrated example a single layer is simulated each simulation phase where all layers layer layer are simulated during a single simulation cycle. During phase of a simulation cycle macro gates and are active and under simulation because both have been scheduled by the scheduling kernel as shown. The scheduling kernel monitors the resulting output nets from phase and determines which macro gate inputs have changed as a result. In this example that is macro gate only. The kernel performs a synchronization to begin phase and activates the macro gate for simulation by the kernel . During phase therefore only macro gate is simulated not the layer macro gate formed by the block . For layer two overlapping macro gates have been formed. In the illustrated example at the end of phase the kernel determines from the monitored nets that only the inputs for macro gate have changed. Therefore only that macro gate is activated for simulation during phase. During the next simulation cycle the kernel may simulate a different set of the macro gates as determined by the scheduler .

In an implementation of the logic simulation in which an array of monitored nets are mapped to unique locations if a macro gate simulation modifies the value of any of these nets the corresponding mapped locations are tagged. Additionally each macro gate may have a corresponding sensitivity list where all the input nets triggering its activation are tagged. In such examples to determine if any input change has occurred a bit wise AND operation between the monitored nets array and a macro gate s sensitivity list may be performed e.g. by the block . If a macro gate input has changed the macro gate is activated for simulation during the next simulation cycle. If no input has changed then the macro gate will not be activated for simulation during the next cycle.

With reference to an example description of data storage is now provided. It will be appreciated that the techniques may be implemented in other data storage configurations. Each individual macro gate may be simulated by a thread block corresponding to the processors and each thread within that block simulates one logic gate one level at a time. The threads inside the thread blocks synchronize after each level so as to finish writing all the outputs of the gates of a previous level before the gates of next level are simulated. Truth tables for the gates in the technology library are mapped to the shared memory because of their frequent access.

The intermediate net values outputs of internal gates may be stored in the shared memory because they are accessed by several gates and are the most frequently accessed values. The macro gate topology may be stored in the device memory from which each single thread fetches their corresponding gate type and connectivity information. The logic gates may be stored in a regular fashion like a matrix where the location of each gate corresponds to the position of their output net in the balanced macro gate. The logic gates may also correspond to the layout of the nets in the shared memory thus creating the scope of very regular execution suited for the SIMD UNIT . Each thread in the block may fetch the information corresponding to a gate which contains locations of input nets and which logic function this gate should perform. However the balanced macro gate from the block is a regular structure meaning that all such fetches are contiguous and may be coalesced to have minimum number of device memory loads. The input nets are read from the shared memory and the truth table access determines the desired output which is then written to the shared memory . At the end of simulating a macro gate the produced outputs of the macro gate which are actually monitored nets are transferred to their device memory location for value change detection.

At least some of the various blocks operations and techniques described above may be implemented utilizing hardware a processor executing firmware instructions a processor executing software instructions or any combination thereof. When implemented utilizing a processor executing software or firmware instructions the software or firmware instructions may be stored on any suitable non transitory computer readable medium such as on a magnetic disk an optical disk or other storage medium in a RAM or ROM or flash memory processor hard disk drive optical disk drive tape drive etc. The software or firmware instructions may include machine readable instructions that when executed by the processor cause the processor to perform various acts.

When implemented in hardware the hardware may comprise one or more of discrete components an integrated circuit an application specific integrated circuit ASIC CPU etc.

While the present invention has been described with reference to specific examples which are intended to be illustrative only and not to be limiting of the invention it will be apparent to those of ordinary skill in the art that changes additions and or deletions may be made to the disclosed embodiments without departing from the spirit and scope of the invention.

The foregoing description is given for clearness of understanding only and no unnecessary limitations should be understood therefrom as modifications within the scope of the invention may be apparent to those having ordinary skill in the art.

