---

title: Control selection approximation
abstract: A method includes displaying a user interface of an application on a device's touch-sensitive display. The user interface includes a plurality of regions, including a respective region at a respective hierarchy level. The respective region has two or more child regions at a hierarchy level below the respective hierarchy level. The method includes detecting a first contact at a location that corresponds to the respective region and that does not correspond to any of the two or more child regions. When the application is configured to process the first contact, not in conjunction with the respective region, but in conjunction with at least one child region of the two or more child regions, the method includes identifying a respective child region in accordance with positions of the child regions relative to the location, and processing the first contact in conjunction with the identified respective child region using the application.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08552999&OS=08552999&RS=08552999
owner: Apple Inc.
number: 08552999
owner_city: Cupertino
owner_country: US
publication_date: 20100928
---
This application claims priority to U.S. Provisional Application Ser. No. 61 354 666 filed Jun. 14 2010 entitled Control Selection Approximation which is incorporated herein by reference in its entirety.

This relates to the following applications 1 U.S. Patent Application No. 12 892 851 filed Sep. 28 2010 entitled Control Selection Approximation 2 U.S. patent application Ser. No. 12 789 695 filed May 28 2010 entitled Gesture Recognizers with Delegates for Controlling and Modifying Gesture Recognition which in turn claims priority to U.S. Provisional Application No. 61 298 531 filed Jan. 26 2010 entitled Gesture Recognizers with Delegates for Controlling and Modifying Gesture Recognition and 3 U.S. patent application Ser. No. 12 566 660 filed Sep. 24 2009 entitled Event Recognition now U.S. Pat. No. 8 285 499 which in turn claims priority to U.S. Provisional Patent Application No. 61 210 332 filed on Mar. 16 2009 entitled Event Recognition which are incorporated herein by reference in their entirety.

This relates generally to user interface processing including but not limited to apparatuses and methods for recognizing touch inputs.

The use of touch sensitive surfaces as input devices for computers and other electronic computing devices has increased significantly in recent years. Exemplary touch sensitive surfaces include touch pads and touch screen displays. Such surfaces are widely used to select and or manipulate user interface objects on a display.

Touch inputs are commonly based on finger contacts. However the size of fingers or fingertips may make it challenging to accurately select user interface objects that are designed and sized for different user interface methods e.g. traditional mouse based inputs . In addition user interface objects may be sized small for various reasons e.g. provide more screen display real estate such that more information can be displayed in a single view . Furthermore people with temporary or permanent disability handicap or ailments e.g. reduced visual perception and or reduced motor skills may have difficulty precisely selecting and or manipulating user interface objects.

Thus it would be desirable to have a comprehensive framework or mechanism for recognizing touch based gestures and events as well as gestures and events from other input sources even if the touch based gestures and events are detected outside intended user interface objects.

To address the aforementioned drawbacks in accordance with some embodiments a method is performed at an electronic device having a touch sensitive display. The method includes displaying on the touch sensitive display a user interface of an application. The displayed user interface includes a plurality of regions that are arranged in multiple hierarchical levels. The plurality of regions includes a respective region at a respective hierarchy level. The respective region has two or more child regions at a hierarchy level below the respective hierarchy level. The method also includes detecting on the touch sensitive display a first contact at a first location that corresponds to the respective region and that does not correspond to any of the two or more child regions of the respective region. The method includes in response to detecting the first contact determining whether the application is configured to process the first contact in conjunction with the respective region. The method further includes when the application is not configured to process the first contact in conjunction with the respective region determining whether the application is configured to process the first contact in conjunction with at least one child region of the two or more child regions of the respective region. The method includes when the application is configured to process the first contact in conjunction with at least one child region of the two or more child regions of the respective region identifying a respective child region of the two or more child regions of the respective region in accordance with positions of the child regions relative to the first location and processing the first contact in conjunction with the identified respective child region using the application.

In accordance with some embodiments a method is performed at an electronic device having a touch sensitive display. The method includes displaying on the touch sensitive display a web page or other document including a plurality of activatable user interface objects. The method also includes detecting on the touch sensitive display a first contact at a first location that corresponds to the displayed web page. The method includes in response to detecting the first contact at the first location determining whether the first location corresponds to at least one of the activatable user interface objects. The method further includes when the first location does not correspond to at least one of the activatable user interface objects identifying a user interface object if any that best satisfies a predefined rule with respect to the first location and performing an action corresponding to the identified user interface object.

In accordance with some embodiments an electronic device includes a touch sensitive display one or more processors for executing programs and memory storing one or more programs to be executed by the one or more processors. The one or more programs include instructions executed by the one or more processors so as to perform any of the aforementioned methods.

In accordance with some embodiments a computer readable storage medium stores one or more programs configured for execution by one or more processors in an electronic device. The one or more programs include instructions for performing any of the aforementioned methods.

In accordance with some embodiments an electronic device includes a touch sensitive display one or more processors and memory storing one or more programs for execution by the one or more processors. The one or more programs include instructions that when executed by the one or more processors cause the electronic device to display on the touch sensitive display a user interface of an application. The displayed user interface includes a plurality of regions that are arranged in multiple hierarchical levels. The plurality of regions includes a respective region at a respective hierarchy level. The respective region has two or more child regions at a hierarchy level below the respective hierarchy level. The one or more programs also include instructions that when executed by the one or more processors cause the electronic device to detect on the touch sensitive display a first contact at a first location that corresponds to the respective region and that does not correspond to any of the two or more child regions of the respective region. The one or more programs include instructions that when executed by the one or more processors cause the electronic device to in response to detecting the first contact determine whether the application is configured to process the first contact in conjunction with the respective region. The one or more programs include instructions that when executed by the one or more processors cause the electronic device to when the application is not configured to process the first contact in conjunction with the respective region determine whether the application is configured to process the first contact in conjunction with at least one child region of the two or more child regions of the respective region. The one or more programs further include instructions that when executed by the one or more processors cause the electronic device to when the application is configured to process the first contact in conjunction with at least one child region of the two or more child regions of the respective region identify a respective child region of the two or more child regions of the respective region in accordance with positions of the child regions relative to the first location and process the first contact in conjunction with the identified respective child region using the application.

In accordance with some embodiments a computer readable storage medium stores one or more programs for execution by one or more processors in an electronic device with a touch sensitive display. The one or more programs include instructions that when executed by the one or more processors cause the electronic device to provide for display on the touch sensitive display a user interface of an application. The displayed user interface includes a plurality of regions that are arranged in multiple hierarchical levels. The plurality of regions includes a respective region at a respective hierarchy level. The respective region has two or more child regions at a hierarchy level below the respective hierarchy level. The one or more programs also include instructions that when executed by the one or more processors cause the electronic device to in response to detection of a first contact on the touch sensitive display determine whether the application is configured to process the first contact in conjunction with the respective region. The first contact at a first location corresponds to the respective region and does not correspond to any of the two or more child regions of the respective region. The one or more programs include instructions that when executed by the one or more processors cause the electronic device to when the application is not configured to process the first contact in conjunction with the respective region determine whether the application is configured to process the first contact in conjunction with at least one child region of the two or more child regions of the respective region. The one or more programs further include instructions that when executed by the one or more processors cause the electronic device to when the application is configured to process the first contact in conjunction with at least one child region of the two or more child regions of the respective region identify a respective child region of the two or more child regions of the respective region in accordance with positions of the child regions relative to the first location and process the first contact in conjunction with the identified respective child region using the application.

In accordance with some embodiments an electronic device includes a touch sensitive display one or more processors and memory storing one or more programs for execution by the one or more processors. The one or more programs include instructions that when executed by the one or more processors cause the electronic device to display on the touch sensitive display a web page or other document including a plurality of activatable user interface objects and detect on the touch sensitive display a first contact at a first location that corresponds to the displayed web page. The one or more programs also include instructions that when executed by the one or more processors cause the electronic device to in response to detecting the first contact at the first location determine whether the first location corresponds to at least one of the activatable user interface objects. The one or more programs further include instructions that when executed by the one or more processors cause the electronic device to when the first location does not correspond to at least one of the activatable user interface objects identify a user interface object if any that best satisfies a proximity criterion with respect to the first location and perform an action corresponding to the identified user interface object.

In accordance with some embodiments a computer readable storage medium stores one or more programs for execution by one or more processors in an electronic device with a touch sensitive display. The one or more programs include instructions that when executed by the one or more processors cause the electronic device to display on the touch sensitive display a web page or other document including a plurality of activatable user interface objects and detect on the touch sensitive display a first contact at a first location that corresponds to the displayed web page. The one or more programs also include instructions that when executed by the one or more processors cause the electronic device to in response to detecting the first contact at the first location determine whether the first location corresponds to at least one of the activatable user interface objects. The one or more programs further include instructions that when executed by the one or more processors cause the electronic device to when the first location does not correspond to at least one of the activatable user interface objects identify a user interface object if any that best satisfies a proximity criterion with respect to the first location and perform an action corresponding to the identified user interface object.

Reference will now be made in detail to embodiments examples of which are illustrated in the accompanying drawings. In the following detailed description numerous specific details are set forth in order to provide a thorough understanding of the present invention. However it will be apparent to one of ordinary skill in the art that the present invention may be practiced without these specific details. In other instances well known methods procedures components circuits and networks have not been described in detail so as not to unnecessarily obscure aspects of the embodiments.

It will also be understood that although the terms first second etc. may be used herein to describe various elements these elements should not be limited by these terms. These terms are only used to distinguish one element from another. For example a first contact could be termed a second contact and similarly a second contact could be termed a first contact without departing from the scope of the present invention. The first contact and the second contact are both contacts but they are not the same contact.

The terminology used in the description of the invention herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. As used in the description of the invention and the appended claims the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will also be understood that the term and or as used herein refers to and encompasses any and all possible combinations of one or more of the associated listed items. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

As used herein the term if may be construed to mean when or upon or in response to determining or in response to detecting depending on the context. Similarly the phrase if it is determined or if a stated condition or event is detected may be construed to mean upon determining or in response to determining or upon detecting the stated condition or event or in response to detecting the stated condition or event depending on the context.

As used herein the term event refers to an input detected by one or more sensors of the device. In particular the term event includes a touch on a touch sensitive surface. An event comprises one or more sub events. Sub events typically refer to changes to an event e.g. a touch down touch move and lift off of the touch can be sub events . Sub events in the sequence of one or more sub events can include many forms including without limitation key presses key press holds key press releases button presses button press holds button press releases joystick movements mouse movements mouse button presses mouse button releases pen stylus touches pen stylus movements pen stylus releases finger contacts finger movements finger lift offs oral instructions detected eye movements biometric inputs and detected physiological changes in a user among others. Since an event may comprise a single sub event e.g. a finger contact on a touch sensitive display the term sub event as used herein also refers to an event.

As used herein the terms event recognizer and gesture recognizer are used interchangeably to refer to a recognizer that can recognize a gesture. In some embodiments an event recognizer can recognize other events e.g. motion of the device .

When using touch based gestures to control an application running in a device having a touch sensitive surface touches have both temporal and spatial aspects. The temporal aspect called a phase indicates when a touch has just begun whether it is moving or stationary and when it ends that is when the finger is lifted from the screen. A spatial aspect of a touch is the set of views or user interface regions in which the touch occurs. The views or regions in which a touch is detected may correspond to programmatic levels within a view hierarchy. For example the lowest level view in which a touch is detected is called the hit view and the set of events that are recognized as proper inputs may be determined based at least in part on the hit view of the initial touch that begins a touch based gesture.

Electronic device includes a touch screen display e.g. touch sensitive display . In some embodiments user interface may include an on screen keyboard not depicted that is used by a user to interact with electronic devices . In some embodiments electronic device also includes one or more input devices e.g. keyboard mouse trackball microphone physical button s touchpad etc. . In some embodiments touch sensitive display has the ability to detect two or more distinct concurrent or partially concurrent touches and in these embodiments display is sometimes herein called a multitouch display or multitouch sensitive display.

When performing a touch based gesture on touch sensitive display of electronic device the user generates a sequence of sub events that are processed by one or more CPUs of electronic device . In some embodiments one or more CPUs of electronic device process the sequence of sub events to recognize events.

Electronic device typically includes one or more single or multi core processing units CPU or CPUs as well as one or more network or other communications interfaces respectively. Electronic device includes memory and one or more communication buses respectively for interconnecting these components. Communication buses may include circuitry sometimes called a chipset that interconnects and controls communications between system components not depicted herein . Memory may include high speed random access memory such as DRAM SRAM DDR RAM or other random access solid state memory devices and may include non volatile memory such as one or more magnetic disk storage devices optical disk storage devices flash memory devices or other non volatile solid state storage devices. Memory may optionally include one or more storage devices remotely located from the CPU s . Memory or alternately the non volatile memory device s within memory comprise a computer readable storage medium. In some embodiments memory or the non volatile memory device s within memory comprises a non transitory computer readable storage medium. In some embodiments memory or the computer readable storage medium of memory stores the following programs modules and data structures or a subset thereof 

Each of the above identified elements may be stored in one or more of the previously mentioned memory devices. Each of the above identified module application or system elements corresponds to a set of instructions for performing functions described herein. The set of instructions can be executed by one or more processors e.g. one or more CPUs . The above identified modules or programs i.e. sets of instructions need not be implemented as separate software programs procedures or modules and thus various subsets of these modules may be combined or otherwise rearranged in various embodiments. In some embodiments memory may store a subset of the modules and data structures identified above. Furthermore memory may store additional modules and data structures not described above.

A driver or a set of drivers communicates with hardware . Drivers can receive and process input data received from hardware . Core Operating System OS can communicate with driver s . Core OS can process raw input data received from driver s . In some embodiments drivers can be considered to be a part of core OS .

A set of OS application programming interfaces OS APIs are software procedures that communicate with core OS . In some embodiments APIs are included in the device s operating system but at a level above core OS . APIs are designed for use by applications running on the electronic devices or apparatuses discussed herein. User interface UI APIs can utilize OS APIs . Application software applications running on the device can utilize UI APIs in order to communicate with the user. UI APIs can in turn communicate with lower level elements ultimately communicating with various user interface hardware e.g. multitouch display .

While each layer input output processing stack can utilize the layer underneath it that is not always required. For example in some embodiments applications can occasionally communicate with OS APIs . In general layers at or above OS API layer may not directly access Core OS driver s or hardware as these layers are considered private. Applications in layer and UI API usually direct calls to the OS API which in turn accesses the layers Core OS driver s and hardware .

Stated in another way one or more hardware elements of electronic device and software running on the device such as for example drivers core OS operating system operating system API software and Application and User Interface API software detect input events which may correspond to sub events in a gesture at one or more of the input device s and or a touch sensitive display and generate or update various data structures stored in memory of device used by a set of currently active event recognizers to determine whether and when the input events correspond to an event to be delivered to application . Embodiments of event recognition methodologies apparatus and computer program products are described in more detail below.

In this example each subordinate view includes lower level subordinate views or a child region of the respective child region . In other examples the number of view levels in the hierarchy may differ in different branches of the hierarchy with one or more subordinate views having lower level subordinate views and one or more other subordinate views may not have any such lower level subordinate views. Continuing with the example shown in search results panel view contains a respective subordinate view subordinate to panel for each search result. Here this example shows one search result in a subordinate view called maps view . Search field view includes a subordinate view herein called clear contents icon view which clears the contents of a search field in the search field view when a user performs a particular action e.g. a single touch or tap gesture on the clear contents icon in view . Home row view includes subordinate views and which respectively correspond to a contacts application an email application a web browser and an iPod music interface.

Stated differently search result panel view is a parent region of maps view and outermost view is a parent region of search result panel view . At the same time search result panel view is a child region of outermost view and maps view is a child region of search result panel view . Similarly search field view and home row view are also child regions of outermost view .

In a touch sub event is represented in outermost view . Given the location of touch sub event over both the search results panel and maps view the touch sub event is also represented over those views as and respectively. Actively involved views of the touch sub event include all views that correspond to the location of touch sub event e.g. views search results panel maps view and outermost view . A hit view is the lowest level view among the actively involved views which in this example is maps view . Additional information regarding sub event delivery and actively involved views is provided below with reference to .

Views and corresponding programmatic levels can be nested. In other words a view can include other views. Consequently the software element s e.g. event recognizers associated with a first view can include or be linked to one or more software elements associated with views within the first view. While some views can be associated with applications others can be associated with high level OS elements such as graphical user interfaces window managers etc.

In application includes a plurality of views e.g. and through N in view hierarchy . In this example Hit View includes a plurality of child views A B and C and a plurality of user interface objects text through text in the child views A through C . Hit View 1 also called a parent view of Hit View includes a plurality of child views D and E and a plurality of user interface objects text text text text and text in the plurality of child views D and E . Although a respective child view may match the size of a user interface object that is included in the respective child view in this example each child view is larger than a respective user interface object it includes e.g. child view A is larger than text . Such view or region can be used as a hit region for the respective user interface object it includes such that any function associated with the respective user interface object is activated when a contact is detected at a location corresponding to the hit region even if the contact is detected outside the respective user interface object. In some embodiments respective child views in a subset of child views are larger than corresponding respective user interface objects while remaining child views match the size of corresponding respective user interface objects.

When application is configured to process contact in conjunction with Hit View contact is processed in conjunction with Hit View using application . When application is not configured to process contact in conjunction with Hit View e.g. Hit View does not have any gesture recognizers or there is no gesture recognizer attached to Hit View that is configured to recognize or try to recognize contact other views are evaluated to determine whether application can process contact in conjunction with any other view . For example child views A through C of Hit View are first evaluated. In this example Hit View does not have any gesture recognizer that is configured to recognize or try to recognize contact because the location of the contact does not overlap the hit region of any user interface object in Hit View .

When application is configured to process contact in conjunction with one of child views A through C e.g. when only child view C of child views A through C has a gesture recognizer that is configured to recognize or try to recognize contact contact is processed in conjunction with that child view using the application. When application is configured to process contact in conjunction with two or more of the child views A through C e.g. when each of the child views A through C has a gesture recognizer that is configured to recognize or try to recognize contact one of the two or more child views is selected to process contact . Typically a child view that is closest to contact is selected e.g. view B including text .

When application is not configured to process contact in conjunction with any of the child views A through C of Hit View a parent view e.g. of Hit View is evaluated. When application is configured to process contact in conjunction with parent view also called Hit View 1 contact is processed in conjunction with parent view using application . If the application is not configured to process contact in conjunction with parent view other child views of parent view i.e. child views of parent view other than Hit View such as child view D and E are evaluated. Hit View is excluded from the child views of parent view because that view Hit View was already evaluated with respect to contact prior to contact being evaluated with respect to Hit View .

Similar to processing contact using one or more child views of Hit View when application is configured to process contact with one of child views D through E contact is processed in conjunction with the one child view using the application. When application is configured to process contact in conjunction with two or more of the child views e.g. both D and E one of the two or more child views is selected to process contact . Typically a child view that is closest to contact is selected e.g. view D including text . Typically once a respective view is selected for processing a contact the contact is bound to that view meaning that all event processing for the contact and any subsequent portions of a gesture or event until the event associated with the contact is completed.

When application is not configured to process contact in conjunction with any of the child views D and E of parent view child views not shown of the child views D and E are evaluated followed by evaluation of a parent view e.g. grandparent view of the parent view and then child views of the grandparent view. Such evaluations are repeated through the view hierarchy until all lower views of highest view N are evaluated.

In some embodiments when application is not configured to process contact in conjunction with any view in view hierarchy contact is ignored. In other embodiments a predefined action e.g. a default action such as a selection of a default user interface object is performed.

Although not shown in in some embodiments Hit View includes a single child view that includes a single user interface object e.g. a button . When application is not configured to process a contact on Hit View in conjunction with Hit View but is configured to process the contact in conjunction with the single child view of Hit View the contact is processed in conjunction with the single child view using application .

Similar to illustrates that application includes a plurality of views e.g. A B and through N in view hierarchy displayed on a touch sensitive display. In this example parent view Hit View 1 includes two toolbars A and B . Hit View A that corresponds to contact includes a first toolbar toolbar and includes a plurality of buttons or icons on the toolbar through in a plurality of views F through H . In this example a respective child view matches the size and shape of a respective user interface object included in it. View B includes a second toolbar toolbar and a plurality of buttons through in a plurality of views I through K .

When application is not configured to process contact in conjunction with any of child views F through H parent view and subsequently view B are evaluated followed by evaluation of child views I through K of view B. If application is configured to process contact in conjunction of any of child views I through K contact may activate a function of application that corresponds to one of the buttons on toolbar e.g. button D button E or button F even though contact was detected on toolbar.

Similar to illustrates that application includes a plurality of views e.g. and through N in view hierarchy displayed on a touch sensitive display. In this example Hit View includes two user interface objects of a different size e.g. image and button .

When a respective view includes user interface objects of different sizes and or shapes or child views of different sizes and or shapes there are multiple ways to identify a user interface object that is closest to a contact. In some embodiments a centroid to centroid distance which is a distance from a centroid e.g. of a contact e.g. to a centroid of a user interface object is used to determine a closest user interface object. In some embodiments a shortest distance which is a distance from a first point on a user interface object that is closest to the centroid e.g. of the contact e.g. is used. In some embodiments a predefined number of points e.g. eight points are used to simplify distance calculation. For example eight points top left corner top right corner bottom left corner bottom right corner middle of the top edge middle of the bottom edge middle of the left edge and middle of the right edge can be used to calculate eight distances per candidate user interface object and a shortest distance of the eight distances is selected for each candidate user interface object. In some embodiments a distance from a contact to a user interface object comprises a weighted average of distances from the contact to a predetermined number e.g. three of closest points of a respective child view or user interface object. For example the three closest points of a respective child view or object may be selected from among nine predefined points including the eight points mentioned above and the centroid of the object or other center point in the objects interior . In some embodiments different weights are used for each of the predetermined number of closest points. For example when the predetermined number of closest points is three the weights can be 0.5 for the closest point 0.3 for the second closest point and 0.2 for the third closest point weighted distance 0.5d 0.3d 0.2d where d dand dare the distances from the first point to the closest second closest and third closest points .

With respect to although the foregoing has been described for identifying a closest user interface object analogous methods can be used to identify a closest view or region.

In some embodiments at least one view in view hierarchy includes a portion of a web page or other document. In some embodiments views in view hierarchy include different portions of a web page or other document.

The device displays on the touch sensitive display a user interface of an application the displayed user interface including a plurality of regions e.g. views in that are arranged in multiple hierarchical levels e.g. view hierarchy . The plurality of regions includes a respective region e.g. Hit View at a respective hierarchy level. The respective region has two or more child regions at a hierarchy level below the respective hierarchy level e.g. views A B and C .

The device detects on the touch sensitive display a first contact e.g. at a first location that corresponds to the respective region and that does not correspond to any of the two or more child regions of the respective region. In Hit View is the lowest level hit view as there are no lower level views that hit or overlap with contact . The first location is typically the centroid of the first contact region but may alternatively be determined from the first contact region in accordance with any appropriate methodology for assigning a specific location to a contact.

In some embodiments respective child regions of the two or more child regions have respective hit regions at least one of which is different from the corresponding child region e.g. text is a child region of view and view A is the hit region of text . Detecting that the first contact corresponds to the respective region includes determining that the first location does not correspond to any of the hit regions of the respective region s child regions. In some user interfaces a respective hit region includes and is larger than a corresponding child region e.g. view A includes and is larger than text . A user interface object or region can have a hit region of a different size e.g. larger smaller and or skewed from the user interface object or region to facilitate selection of particular objects or regions of the user interface. On the other hand the hit region of a respective user interface object or region can be coextensive with have the same size shape and position as the user interface object or region.

Typically in order for the first contact to correspond to the respective region the first location is located outside any of child regions of the respective region e.g. contact is located outside any of child views A through C .

In response to detecting the first contact the device determines whether the application is configured to process the first contact in conjunction with the respective region e.g. the device determines whether Hit View includes a gesture recognizer that is configured to recognize or try to recognize contact .

In some embodiments the application is deemed to be configured to process the first contact in conjunction with the respective region when the respective region includes a user interface object having at least one property in a predefined set of properties. For example application in is deemed to be configured to process the first contact e.g. in conjunction with view A when a user interface object e.g. text in that view A has a predefined approximation property which enables object selection or activation even when a user contact does not overlap the object . In some embodiments the application is deemed to be configured to process the first contact in conjunction with the respective region when the respective region itself has at least one property in a predefined set of properties. For example application in would be deemed to be configured to process the first contact e.g. in conjunction with view if view were to have a text property specifying text to be spoken when the view is selected and an accessibility option for speaking such text is enabled.

In some embodiments a respective region is deemed to be configured to process the first contact when the respective region is the region for a user interface object of a type in a predefined set of user interface object types e.g. button checkbox combobox link list box menu button pop up button radio button slider and text field . In some embodiments the predefined set of user interface object types also includes one or more of disclosure triangle heading image static text and text area.

In some embodiments the application is deemed to be configured to process the first contact in conjunction with the respective region when the respective region includes a user interface object that is configured to be activated in response to a gesture that includes the first contact at the location of that first contact . For example application in is deemed to be configured to process the first contact e.g. in conjunction with view A when view A includes a user interface object e.g. text that is configured to be activated in response to a gesture that includes the first contact. Non limiting examples of a user interface object that is not configured to be activated in response to a gesture that includes the first contact include 1 a static user interface object e.g. a label for displaying text or graphics without any assigned function and 2 a user interface object that is configured to be activated in response to a different gesture e.g. a two finger contact . An example of a user interface region that is not configured to be activated in response to a gesture that includes the first contact is Hit View because the first contact does not overlap any selectable activatable object in Hit View . Furthermore the application is deemed to be configured to process the first contact in conjunction with the respective region when the respective region itself has a user interface object that is configured to be activated in response to a gesture that includes the first contact.

In some embodiments the application is deemed to be configured to process the first contact in conjunction with the respective region when at least one event recognizer is associated with the respective region. Event recognizers or gesture recognizers are described in detail with respect to . More typical are embodiments in which an application is deemed to be configured to process the first contact in conjunction with the respective region when an event recognizer configured to recognize a gesture that includes the first contact is associated with the respective region. Conversely when a particular type of contact e.g. a one finger contact is detected and a respective region or a view does not include an event recognizer that is configured to recognize a gesture beginning with that type of contact the respective region is not deemed to be configured to process the first contact in conjunction with the respective region.

When the application is configured to process the first contact in conjunction with the respective region the device processes the first contact in conjunction with the respective region using the application. For example the device activates a function associated with the respective region e.g. if the respective region includes a save button the device initiates saving of a corresponding content or document . In another example if the respective region includes a hyperlink e.g. a uniform resource locator the device initiates opening a resource e.g. a web page corresponding to the hyperlink.

In some embodiments processing the first contact in conjunction with the respective region using the application includes presenting confirmation information. In some embodiments presenting conformation information includes providing affordance. For example the device may visually distinguish e.g. bolding flashing enlarging and or changing colors text corresponding to the respective region e.g. text included in the respective region a name of a function corresponding to the respective region or embedded text corresponding to the respective region such as help text . Presenting confirmation information can include non visual signals. For example the device may use a text to speech software to convert the text corresponding to the respective region into an audible speech signal or may play a sound clip corresponding to the respective region. In another example the device may convert the text corresponding to the respective region into Braille symbols for presentation using a Braille device not shown .

In some embodiments processing the first contact in conjunction with the respective region using the application includes receiving a confirmation gesture. For example after presenting the confirmation information the device receives a confirmation gesture e.g. a single tap gesture a double tap gesture or a tap and hold gesture located anywhere on the touch sensitive display of the device or within a specific region on the touch sensitive display such as bottom half of the touch sensitive display .

When the application is not configured to process the first contact in conjunction with the respective region in the device performs at least a subset of the following operations e.g. operations and operations included therein and in some embodiments operations and or .

The device determines in whether the application is configured to process the first contact in conjunction with at least one child region of the two or more child regions of the respective region e.g. in the device determines whether application is configured to process contact in conjunction with any of views A B and C .

When the application is configured to process the first contact in conjunction with at least one child region of the two or more child regions of the respective region the device performs the following operations e.g. operations and and operations included therein .

The device identifies a respective child region of the two or more child regions of the respective region in accordance with positions of the child regions relative to the first location. For example the device identifies child view B of child views A through C as a closest child view in accordance with positions of child views A through C relative to contact .

In some embodiments identifying the respective child region operation of the two or more child regions of the respective region in accordance with positions of the child regions relative to the first location includes identifying the child region of the two or more child regions of the respective region that best satisfies a proximity criterion with respect to the first location. For example the device identifies a child region that is closest to the first location based on the proximity criterion. The proximity criterion is based on at least one of the centroid to centroid distance the shortest distance between a user interface object or a child view to the first location a shortest distance to the first location based on a predetermined number of points for each child view and a shortest distance to the first location based on a weighted average of distances from a predetermined number of points for each child view .

In some embodiments identifying the respective child region operation of the two or more child regions of the respective region in accordance with positions of the child regions relative to the first location includes identifying the child region of the two or more child regions of the respective region that best satisfies a proximity criterion with respect to the first location and confirming that the identified child region satisfies a maximum distance limit with respect to the first location. For example if the distance from the identified child view to the first location exceeds the maximum distance limit the identified child region is ignored. In some embodiments when none of the child regions are within maximum distance limit of the contact of the first location the device ignores the contact at the first location. In other embodiments the device continues to evaluate other regions including a parent region of the respective region.

In some user interfaces one or more child regions of the two or more child regions has a predefined approximation property and at least one other child region does not have the predefined approximation property. Identifying the respective child region includes identifying a respective child region of the one or more child regions of the respective region that have the predefined approximation property in accordance with positions of the child regions relative to the first location. For example at least a subset of the child regions can be configured to have a predefined approximation property. The predefined approximation property indicates whether a corresponding region is eligible for activation selection even when a contact is detected at a location that does not correspond to the region. The predefined approximation property can be located within properties e.g. in in each of the subset of the child regions. When the predefined approximation property is used the device identifies a child region among child regions that have the predefined approximation property.

When an appropriate region of the user interface e.g. the identified child region has been identified the device processes the first contact in conjunction with that region e.g. the identified respective child region using the application.

In some embodiments the plurality of regions comprises a plurality of hierarchically arranged views corresponding to the regions. Each of the plurality of the views typically has at least one event recognizer associated with the respective view. Processing the first contact in conjunction with the identified respective child region using the application comprises generating an event or sub event when the contact corresponds to the first portion of a gesture or event to be processed by at least one event recognizer of the view corresponding to the identified respective child region.

In some embodiments the plurality of regions comprises a plurality of hierarchically arranged views corresponding to the regions. Processing the first contact in conjunction with the identified respective child region using the application comprises processing the first contact as a contact in the identified respective child region. In some embodiments processing the first contact as a contact in the identified respective child region includes generating a new event or sub event as explained above at a location that corresponds to the identified respective child region. For example in when the device processes contact as a contact in child view B the device generates a new event that indicates a virtual contact at a location that corresponds to child view B.

In some user interfaces the respective region has a parent region e.g. Hit View 1 at a hierarchy level above the respective hierarchy level. Optionally in this situation when the application is not configured to process the first contact in conjunction with at least one child region of the two or more child regions of the respective region and the application is configured to process the first contact in conjunction with the parent region the device processes the first contact in conjunction with the parent region using the application. For example in if application is not configured to process contact in conjunction with any of child views A through C the device processes contact in conjunction with Hit View 1 if the application is configured to process contact in conjunction with Hit View 1 .

In some embodiments when the application is not configured to process the first contact in conjunction with at least one child region of the two or more child regions of the respective region the device processes the first contact as a contact in the parent region. In some embodiments processing the first contact as a contact in the parent region includes generating a new event or sub event as explained above at a location that corresponds to the parent region. For example in when the device processes contact as a contact in Hit View 1 the device generates a new event that indicates a virtual contact at a location that correspond to Hit View 1 .

In some user interfaces the parent region has one or more child regions at a hierarchy level at the respective hierarchy level and when the application is not configured to process the first contact in conjunction with the parent region the device processes the first contact in conjunction with at least one of the one or more child regions of the parent region based on a condition that the application is configured to process the first contact in conjunction with at least one of the one or more child regions of the parent region.

Note that details of the processes described above with respect to method are also applicable in an analogous manner to method described below. For brevity these details are not repeated below.

The device displays on the touch sensitive display a web page or other document including a plurality of activatable user interface objects e.g. in one or more views in view hierarchy include a web page or other document in some embodiments and the views also include user interface objects and . In some embodiments all user interface objects are activatable. In some embodiments user interface objects with certain user interface object properties e.g. hidden or static user interface objects are not activatable while other user interface objects are activatable. In some embodiments user interface objects of certain types e.g. labels are not activatable. Optionally some or all of the objects in the displayed document may be generated by an embedded application embedded in the document as opposed to being statically defined by HTML or XML instructions e.g. tags or elements in the document. On the other hand some or all of the objects in the displayed document may defined by HTML or XML instructions in the document.

The device detects on the touch sensitive display a first contact at a first location that corresponds to the displayed web page e.g. contact .

In response to detecting the first contact at the first location the device determines whether the first location corresponds to at least one of the activatable user interface objects e.g. in contact does not correspond to any of the user interface objects .

In some embodiments when the first location corresponds to one of the activatable user interface objects the device performs an action corresponding to an activatable user interface object that corresponds to the first location e.g. the hit region of the object overlaps the first location .

When the first location does not correspond to at least one of the activatable user interface objects the device performs the following operations e.g. and and in some embodiments operations included therein .

The device identifies a user interface object if any that best satisfies a predefined rule with respect to the first location. In some embodiments the predefined rule requires identifying a closest activatable user interface object and the device identifies a closest user interface object. In some embodiments the predefined rule requires identifying a closest activatable user interface object within a maximum distance limit and the device identifies a closest activatable user interface object if there is one. However the device may not identify any activatable user interface object if none of the activatable user interface objects are within the maximum distance limit.

In some embodiments the predefined rule includes a proximity criterion with respect to the first location. As discussed above the proximity criterion is based on at least one of the object centroid to first location distance the shortest distance from a user interface object to the first location the shortest distance from a user interface object to the first location based on a predetermined number of points for each activatable user interface object e.g. the closest point of eight candidate points four corners four edge midpoints of the object and the shortest weighted average distance from a user interface object to the first location where the weighted average distance to the first location from a user interface object is based on a predetermined number e.g. 3 of points of nine candidate points four corners four edge midpoints and the centroid of the user interface object.

In some embodiments detecting the first contact includes determining a speed and direction of the first contact at the first location and the predefined rule is based on a weighted combination of a plurality of factors including the proximity criterion with respect to the first location and a direction and speed of movement of the first contact. For example in although image is a closest activatable user interface object to contact the device may identify button as a user interface object that best satisfies a predefined rule when contact is moving toward button at a sufficient speed. Similarly when contact is moving away from a particular button the direction of movement of the contact away from the button is taken into account when applying the predefined rule operation which reduces the likelihood of identifying that button as the object that best satisfies the predefined rule.

In some embodiments identifying the user interface object includes determining respective distances from the first location to at least a subset of the activatable user interface objects e.g. in the device determines respective distances from contact to image and button in order to identify a user interface object that best satisfies the predefined rule.

In some embodiments each of the respective distances comprises a weighted average of distances from the first location to a plurality of points of a respective activatable user interface object. For example a distance between contact and image can be a weighted average of distances from the first location to a predetermined number e.g. three of points of image e.g. a weighted average of a distance from contact centroid to point a distance from contact centroid to point and a distance from contact centroid to point . In some embodiments a shortest distance is more heavily weighted than a next shortest distance e.g. the distance from contact centroid to point is more heavily weighted than the distance from contact centroid to point .

The device performs an action corresponding to the identified user interface object. The action performed typically depends on one or more characteristics of the identified user interface object. For example when the identified user interface object is image in the device performs an action corresponding to image e.g. it changes the image displayed in or directs the device to display a web page associated with image etc. .

In some embodiments the device determines on the touch sensitive display a respective location of a respective activatable user interface object in the plurality of activatable user interface objects. For example when rendering a web page the device determines respective locations of respective activatable user interface objects e.g. hyperlinks in the web page and uses the respective locations in determining whether a contact overlaps with one of the user interface objects e.g. operation and or identifying a user interface object that best satisfies a predefined rule e.g. operation .

The operations described above with reference to and may be implemented in accessibility module in application software in particular browser application and or embedded application . In addition to or instead of accessibility module application software and or embedded application the operations described above may be implemented by components depicted in .

Event sorter receives event information and determines the application and application view of application to which to deliver the event information. Event sorter includes event monitor and event dispatcher module . In some embodiments application includes application internal state which indicates the current application view s displayed on touch sensitive display when the application is active or executing. In some embodiments device global internal state is used by event sorter to determine which application s is are currently active and application internal state is used by event sorter to determine application views to which to deliver event information.

In some embodiments application internal state includes additional information such as one or more of resume information to be used when application resumes execution user interface state information that indicates information being displayed or that is ready for display by application which in turn may be controlled by an embedded application in document a state queue for enabling the user to go back to a prior state or view of application and a redo undo queue of previous actions taken by the user.

Event monitor receives event information from touch sensitive display and or input devices . Event information includes information about an event e.g. a user touch on touch sensitive display as part of a multi touch gesture or a motion of device and or a sub event e.g. a movement of a touch across touch sensitive display . For example event information for a touch event includes one or more of a location and time stamp of a touch. Similarly event information for a swipe event includes two or more of a location time stamp direction and speed of a swipe. Touch sensitive display and input devices transmit event information and sub event information to event monitor either directly or through a peripherals interface which retrieves and stores event information. In some embodiments the electronic device also includes one or more sensors not shown such as proximity sensor accelerometer s gyroscopes microphone and video camera and the sensors transmit information event and sub event information to event monitor .

In some embodiments event sorter also includes a hit view determination module and or an active event recognizer determination module .

Hit view determination module provides software procedures for determining where a sub event has taken place within one or more views when touch sensitive display displays more than one view. Views are made up of controls and other elements e.g. user interface objects that a user can see on the display.

Another aspect of the user interface associated with an application is a set of views sometimes herein called application views or user interface windows in which information is displayed and touch based gestures occur. The application views of a respective application in which a touch is detected may correspond to programmatic levels within a programmatic or view hierarchy of the application. For example the lowest level view in which a touch is detected may be called the hit view and the set of events that are recognized as proper inputs may be determined based at least in part on the hit view of the initial touch that begins a touch based gesture.

Hit view determination module receives information related to sub events of a touch based gesture. When an application has multiple views organized in a hierarchy hit view determination module identifies a hit view as the lowest view in the hierarchy which should handle the sub event. In most circumstances the hit view is the lowest level view in which an initiating sub event occurs i.e. the first sub event in the sequence of sub events that form an event or potential event . Once the hit view is identified by the hit view determination module the hit view typically receives all sub events related to the same touch or input source for which it was identified as the hit view.

Active event recognizer determination module determines which view within a view hierarchy should receive a particular sequence of sub events. In some embodiments active event recognizer determination module determines that a child region or a parent region of the hit view or any other region in the view hierarchy related to the hit view should receive a particular sequence of sub events when the hit view does not have an event recognizer that is configured to recognize a particular sequence of sub events. Similarly active event recognizer determination module repeats the determining operation until active event recognizer determination module identifies a region that has an event recognizer that is configured to recognize a particular sequence of sub events.

Event dispatcher module dispatches the event information to an event recognizer e.g. event recognizer . In embodiments including active event recognizer determination module event dispatcher module delivers the event information to an event recognizer determined by active event recognizer determination module . In some embodiments event dispatcher module stores in an event queue the event information which is retrieved by a respective event receiver module .

In some embodiments operating system includes event sorter . Alternatively application includes event sorter . In yet other embodiments event sorter is a stand alone module or a part of another module stored in memory such as contact motion module .

In some embodiments application includes a plurality of event handlers and one or more application views each of which includes instructions for handling touch events that occur within a respective view of the application s user interface. Each application view of the application includes one or more event recognizers . Typically a respective application view includes a plurality of event recognizers . In other embodiments one or more of event recognizers are part of a separate module such as a user interface kit not shown or a higher level object from which application inherits methods and other properties. In some embodiments a respective event handler includes one or more of data updater object updater GUI updater and or event data received from event sorter . Event handler may utilize or call data updater object updater or GUI updater to update the application internal state . Alternatively one or more of the application views includes one or more respective event handlers . Also in some embodiments one or more of data updater object updater and GUI updater are included in a respective application view .

A respective event recognizer receives event information e.g. event data from event sorter and identifies an event from the event information. Event recognizer includes event receiver and event comparator . In some embodiments event recognizer also includes at least a subset of metadata and event delivery instructions which may include sub event delivery instructions .

Event receiver receives event information from event sorter . The event information includes information about a sub event for example a touch or a touch movement. Depending on the sub event the event information also includes additional information such as location of the sub event. When the sub event concerns motion of a touch the event information may also include speed and direction of the sub event. In some embodiments events include rotation of the device from one orientation to another e.g. from a portrait orientation to a landscape orientation or vice versa and the event information includes corresponding information about the current orientation also called device attitude of the device.

Event comparator compares the event information to predefined event or sub event definitions and based on the comparison determines an event or sub event or determines or updates the state of an event or sub event. In some embodiments event comparator includes event definitions . Event definitions contain definitions of events e.g. predefined sequences of sub events for example event event and others. In some embodiments sub events in an event include for example touch begin touch end touch movement touch cancellation and multiple touching. In one example the definition for event is a double tap on a displayed object. The double tap for example comprises a first touch touch begin on the displayed object for a predetermined phase a first lift off touch end for a predetermined phase a second touch touch begin on the displayed object for a predetermined phase and a second lift off touch end for a predetermined phase. In another example the definition for event is a dragging on a displayed object. The dragging for example comprises a touch or contact on the displayed object for a predetermined phase a movement of the touch across touch sensitive display and lift off of the touch touch end . In some embodiments the event also includes information for one or more associated event handlers .

In some embodiments event definition includes a definition of an event for a respective user interface object. In some embodiments event comparator performs a hit test to determine which user interface object is associated with a sub event. For example in an application view in which three user interface objects are displayed on touch sensitive display when a touch is detected on touch sensitive display event comparator performs a hit test to determine which of the three user interface objects is associated with the touch sub event . If each displayed object is associated with a respective event handler the event comparator uses the result of the hit test to determine which event handler should be activated. For example event comparator selects an event handler associated with the sub event and the object triggering the hit test.

In some embodiments the definition for a respective event also includes delayed actions that delay delivery of the event information until after it has been determined whether the sequence of sub events does or does not correspond to the event recognizer s event type.

When a respective event recognizer determines that the series of sub events do not match any of the events in event definitions the respective event recognizer enters an event impossible event failed or event ended state after which it disregards subsequent sub events of the touch based gesture. In this situation other event recognizers if any that remain active for the hit view continue to track and process sub events of an ongoing touch based gesture.

In some embodiments a respective event recognizer includes metadata with configurable properties flags and or lists that indicate how the event delivery system should perform sub event delivery to actively involved event recognizers. In some embodiments metadata includes configurable properties flags and or lists that indicate how event recognizers may interact with one another. In some embodiments metadata includes configurable properties flags and or lists that indicate whether sub events are delivered to varying levels in the view or programmatic hierarchy.

In some embodiments a respective event recognizer activates event handler associated with an event when one or more particular sub events of an event are recognized. In some embodiments a respective event recognizer delivers event information associated with the event to event handler . Activating an event handler is distinct from sending and deferred sending sub events to a respective hit view. In some embodiments event recognizer throws a flag associated with the recognized event and event handler associated with the flag catches the flag and performs a predefined process.

In some embodiments event delivery instructions include sub event delivery instructions that deliver event information about a sub event without activating an event handler. Instead the sub event delivery instructions deliver event information to event handlers associated with the series of sub events or to actively involved views. Event handlers associated with the series of sub events or with actively involved views receive the event information and perform a predetermined process.

In some embodiments event handler s includes or has access to data updater object updater and GUI updater . In some embodiments data updater object updater and GUI updater are included in a single module of a respective application or application view . In other embodiments they are included in two or more software modules. In some embodiments data updater creates and updates data used in application . In some embodiments object updater creates and updates objects used in application . GUI updater updates the GUI.

It shall be understood that the foregoing discussion regarding event handling of user touches on touch sensitive displays also applies to other forms of user inputs to operate electronic devices with input devices not all of which are initiated on touch screens e.g. coordinating mouse movement and mouse button presses with or without single or multiple keyboard presses or holds user movements such as taps drags scrolls etc. on touch pads pen stylus inputs movement of the device oral instructions detected eye movements biometric inputs and or any combination thereof which may be utilized as inputs corresponding to events and or sub events which define an event to be recognized.

The foregoing description for purpose of explanation has been described with reference to specific embodiments. However the illustrative discussions above are not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications to thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated.

