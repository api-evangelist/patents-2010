---

title: Automatic speech recognition learning using categorization and selective incorporation of user-initiated corrections
abstract: An automatic speech recognition system recognizes user changes to dictated text and infers whether such changes result from the user changing his/her mind, or whether such changes are a result of a recognition error. If a recognition error is detected, the system uses the type of user correction to modify itself to reduce the chance that such recognition error will occur again. Accordingly, the system and methods provide for significant speech recognition learning with little or no additional user interaction.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08280733&OS=08280733&RS=08280733
owner: Microsoft Corporation
number: 08280733
owner_city: Redmond
owner_country: US
publication_date: 20100917
---
The present application is a continuation of and claims priority of U.S. patent application Ser. No. 10 761 451 filed Jan. 20 2004 the content of which is hereby incorporated by reference in its entirety.

The present invention relates to computer speech recognition and more particularly to training a computer speech recognition system.

The rapid and accurate recognition of human speech by a computer system has been a long sought goal by developers of computer systems. The benefits that would result from such a computer speech recognition CSR system are substantial. For example rather than typing a document into a computer system a person could simply speak the words of the document and the CSR system would recognize the words and store the letters of each word as if the words had been typed. Since people generally can speak faster than type efficiency would be improved. Also people would no longer need to learn how to type. Computers could also be used in many applications where their use is currently impracticable because a person s hands are occupied with tasks other than typing.

Typical CSR systems recognize words by comparing a spoken utterance to a model of each word in a vocabulary. The word whose model best matches the utterance is recognized as the spoken word. A CSR system may model each word as a sequence of phonemes that compose the word. To recognize an utterance the CSR system identifies a word sequence the phonemes of which best match the utterance. These phonemes may however not exactly correspond to the phonemes that compose a word. Thus CSR systems typically use a probability analysis to determine which word most closely corresponds to the identified phonemes.

When recognizing an utterance a CSR system converts the analog signal representing the utterance to a more useable form for further processing. The CSR system first converts the analog signal into a digital form. The CSR system then applies a signal processing technique such as fast fourier transforms FFT linear predictive coding LPC or filter banks to the digital form to extract an appropriate parametric representation of the utterance. A commonly used representation is a feature vector with FFT or LPC coefficients that represent the frequency and or energy bands of the utterance at various intervals referred to as frames . The intervals can be short or long based on the computational capacity of the computer system and the desired accuracy of the recognition process. Typical intervals may be in the range of 10 milliseconds. That is the CSR system would generate a feature vector for every 10 milliseconds of the utterance. Each frame is typically 25 ms long. Therefore a 25 ms long frame is generated every 10 ms. There is an overlap between successive frames.

To facilitate the processing of the feature vectors each feature vector is quantized into one of a limited number e.g. 256 of quantization vectors. That is the CSR system defines a number of quantization vectors that are selected to represent typical or average ranges of feature vectors. The CSR system then compares each feature vector to each of the quantization vectors and selects the quantization vector that most closely resembles the feature vector to represent the feature vector. Each quantization vector is uniquely identified by a number e.g. between 1 and 256 which is referred to as a codeword. When a feature vector is represented as a quantization vector there is a loss of information because many different feature vectors map to the same quantization vector. To ensure that this information loss will not seriously impact recognition CSR systems may define thousands or millions of quantization vectors. The amount of storage needed to store the definition of such a large number of quantization vectors can be considerable. Thus to reduce the amount of storage needed CSR systems segment feature vectors and quantize each segment into one of a small number e.g. 256 quantization vectors. Thus each feature vector is represented by a quantization vector identified by a codeword for each segment. For simplicity of explanation a CSR system that does not segment a feature vector and thus has only one codeword per feature vector or frame is described.

As discussed above a spoken utterance often does not exactly correspond to a model of a word. The difficulty in finding an exact correspondence is due to the great variation in speech that is not completely and accurately captured by the word models. These variations result from for example the accent of the speaker the speed and pitch at which a person speaks the current health e.g. with a cold of the speaker the age and sex of the speaker etc. CSR systems that use probabilistic techniques have been more successful in accurately recognizing speech than techniques that seek an exact correspondence.

One such probabilistic technique that is commonly used for speech recognition is hidden Markov modeling. A CSR system may use a hidden Markov model HMM for each word in the vocabulary. The HMM for a word includes probabilistic information from which can be derived the probability that any sequence of codewords corresponds to that word. Thus to recognize an utterance a CSR system converts the utterance to a sequence of codewords and then uses the HMM for each word to determine the probability that the word corresponds to the utterance. The CSR system recognizes the utterance as the word with the highest probability.

An HMM is represented by a state diagram. State diagrams are traditionally used to determine a state that a system will be in after receiving a sequence of inputs. A state diagram comprises states and transitions between source and destination states. Each transition has associated with it an input which indicates that when the system receives that input and it is in the source state the system will transition to the destination state. Such a state diagram could for example be used by a system that recognizes each sequence of codewords that compose the words in a vocabulary. As the system processes each codeword the system determines the next state based on the current state and the codeword being processed. In this example the state diagram would have a certain final state that corresponds to each word. However if multiple pronunciations of a word are represented then each word may have multiple final states. If after processing the codewords the system is in a final state that corresponds to a word then that sequence of codewords would be recognized as the word of the final state.

An HMM however has a probability associated with each transition from one state to another for each codeword. For example if an HMM is in state 2 then the probability may be 0.1 that a certain codeword would cause a transition from the current state to a next state and the probability may be 0.2 that the same codeword would cause a transition from the current state to a different next state. Similarly the probability may be 0.01 that a different codeword would cause a transition from the current state to a next state. Since an HMM has probabilities associated with its state diagram the determination of the final state for a given sequence of codewords can only be expressed in terms of probabilities. Thus to determine the probability of each possible final state for a sequence of codewords each possible sequence of states for the state diagram of the HMM needs to be identified and the associated probabilities need to be calculated. Each such sequence of states is referred to as a state path.

To determine the probability that a sequence of codewords represents a phoneme the CSR system may generate a probability lattice. The probability lattice for the HMM of a phoneme represents a calculation of the probabilities for each possible state path for the sequence of codewords. The probability lattice contains a node for each possible state that the HMM can be in for each codeword in the sequence. Each node contains the accumulated probability that the codewords processed so far will result in the HMM being in the state associated with that node. The sum of the probabilities in the nodes for a particular codeword indicates the likelihood that the codewords processed so far represent a prefix portion of the phoneme.

The accuracy of a CSR system depends in part on the accuracy of the output and transition probabilities of the HMM for each phoneme. Typical CSR systems train the CSR system so that the output and transition probabilities accurately reflect speech of the average speaker. During training the CSR system gathers codeword sequences from various speakers for a large variety of words. The words are selected so that each phoneme is spoken a large number of times. From these codeword sequences the CSR system calculates output and transition probabilities for each HMM. Various iterative approaches for calculating these probabilities are well known.

A problem with such training techniques however is that such average HMMs may not accurately model the speech of people whose speech pattern is different than the average. In general every person will have certain speech patterns that differ from the average. Consequently CSR systems allow a speaker to train the HMMs to adapt to the speaker s speech patterns. In such training CSR systems refine the HMM parameters such as the output and transition probabilities and the quantization vectors represented by the codewords by using training utterances spoken by the actual user of the system. The adapted parameters are derived by using both the user supplied data as well as the information and parameters generated from the large amount of speaker independent data. Thus the probabilities reflect speaker dependent characteristics.

A CSR system is typically trained by presenting a large variety of pre selected words to a speaker. These words are selected to ensure that a representative sample of speech corresponding to each phoneme can be collected. With this representative sample the CSR system can ensure that any HMM that does not accurately reflect the speaker s pronunciation of that phoneme can be adequately adapted. Since the CSR system functions in terms of probabilities the more training that is provided the more accurate subsequent speech recognition will be. However as more and more training is done the degree to which recognition accuracy will increase for a given amount of additional training begins to decline. Further requiring user s to provide substantial investments in training time may diminish the user s experience.

Accordingly there is a balance between the degree to which the user is called upon to train the system and the degree to which the user can effectively use the system. Given the complexities of human language it is very conceivable that even after extensive training the system will occasionally generate errors. Another reason that causes a spoken utterance to not be matched with a corresponding model of a word is when the word is new. A possible solution includes increasing the vocabulary size which may lower recognition accuracy. Another solution is through user training in which the user adds new words. Current systems allow user s to manually add new words with his or her pronunciation to a suitable lexicon whether it be a system lexicon a vendor or application lexicon or a user specific lexicon by using a user interface that allows a user to add or delete a word like an ADD DELETE Words Dialog box. However this can become troublesome in cases where users may need to add a significant number of words. It is also known to adapt the language model LM using documents and e mails authored by the user. This approach is limited in that pronunciations are not added into the lexicon and the quality of the language model adaptation depends largely on the filtering of the source documents.

Thus a need exists for a system that can easily learn new words and pronunciations thereof from users without requiring significant user intervention. Achieving this object would allow enhanced automatic speech recognition system learning without diminishing the user experience by requiring undue training effort.

An automatic speech recognition system recognizes user changes to dictated text and infers whether such changes result from the user changing his her mind or whether such changes are a result of correcting a recognition error. If a correction from a recognition error is detected the system uses the type of user correction to modify itself to reduce the chance that such recognition error will occur again. Accordingly the system and methods provide for significant speech recognition learning with little or no additional user interaction.

The invention is operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with the invention include but are not limited to personal computers server computers hand held or laptop devices multiprocessor systems microprocessor based systems set top boxes programmable consumer electronics network PCs minicomputers mainframe computers telephony systems distributed computing environments that include any of the above systems or devices and the like.

The invention may be described in the general context of computer executable instructions such as program modules being executed by a computer. Generally program modules include routines programs objects components data structures etc. that perform particular tasks or implement particular abstract data types. The invention may also be practiced in distributed computing environments where tasks are performed by remote processing devices that are linked through a communications network. In a distributed computing environment program modules may be located in both local and remote computer storage media including memory storage devices.

With reference to an exemplary system for implementing the invention includes a general purpose computing device in the form of a computer . Components of computer may include but are not limited to a central processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit .

The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. By way of example and not limitation such architectures include Industry Standard Architecture ISA bus Micro Channel Architecture MCA bus Enhanced ISA EISA bus Video Electronics Standards Association VESA local bus and Peripheral Component Interconnect PCI bus also known as Mezzanine bus.

Computer typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer and includes both volatile and nonvolatile media removable and non removable media. By way of example and not limitation computer readable media may comprise computer storage media and communication media. Computer storage media includes both volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by computer . Communication media typically embodies computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media includes wired media such as a wired network or direct wired connection and wireless media such as acoustic RF infrared and other wireless media. Combinations of any of the above should also be included within the scope of computer readable media.

The system memory includes computer storage media in the form of volatile and or nonvolatile memory such as read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within computer such as during start up is typically stored in ROM . RAM typically contains data and or program modules that are immediately accessible to and or presently being operated on by processing unit . By way of example and not limitation illustrates operating system application programs other program modules and program data .

The computer may also include other removable non removable volatile nonvolatile computer storage media. By way of example only illustrates a hard disk drive that reads from or writes to non removable nonvolatile magnetic media a magnetic disk drive that reads from or writes to a removable nonvolatile magnetic disk and an optical disk drive that reads from or writes to a removable nonvolatile optical disk such as a CD ROM or other optical media. Other removable non removable volatile nonvolatile computer storage media that can be used in the exemplary operating environment include but are not limited to magnetic tape cassettes flash memory cards digital versatile disks digital video tape solid state RAM solid state ROM and the like. The hard disk drive is typically connected to the system bus through a non removable memory interface such as interface and magnetic disk drive and optical disk drive are typically connected to the system bus by a removable memory interface such as interface .

Drives and their associated computer storage media discussed above and illustrated in provide storage of computer readable instructions data structures program modules and other data for the computer . In for example hard disk drive is illustrated as storing operating system application programs other program modules and program data . Note that these components can either be the same as or different from operating system application programs other program modules and program data . Operating system application programs other program modules and program data are given different numbers here to illustrate that at a minimum they are different copies.

A user may enter commands and information into the computer through input devices such as a keyboard a microphone and a pointing device such as a mouse trackball or touch pad. Other input devices not shown may include a joystick game pad satellite dish scanner or the like. These and other input devices are often connected to the processing unit through a user input interface that is coupled to the system bus but may be connected by other interface and bus structures such as a parallel port game port or a universal serial bus USB . A monitor or other type of display device is also connected to the system bus via an interface such as a video interface . In addition to the monitor computers may also include other peripheral output devices such as speakers and printer which may be connected through an output peripheral interface .

The computer may operate in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be a personal computer a hand held device a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computer . The logical connections depicted in include a local area network LAN and a wide area network WAN but may also include other networks. Such networking environments are commonplace in offices enterprise wide computer networks intranets and the Internet.

When used in a LAN networking environment the computer is connected to the LAN through a network interface or adapter . When used in a WAN networking environment the computer typically includes a modem or other means for establishing communications over the WAN such as the Internet. The modem which may be internal or external may be connected to the system bus via the user input interface or other appropriate mechanism. In a networked environment program modules depicted relative to the computer or portions thereof may be stored in the remote memory storage device. By way of example and not limitation illustrates remote application programs as residing on remote computer . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

Memory is implemented as non volatile electronic memory such as random access memory RAM with a battery back up module not shown such that information stored in memory is not lost when the general power to mobile device is shut down. A portion of memory is preferably allocated as addressable memory for program execution while another portion of memory is preferably used for storage such as to simulate storage on a disk drive.

Memory includes an operating system application programs as well as an object store . During operation operating system is preferably executed by processor from memory . Operating system in one preferred embodiment is a WINDOWS CE brand operating system commercially available from Microsoft Corporation. Operating system is preferably designed for mobile devices and implements database features that can be utilized by applications through a set of exposed application programming interfaces and methods. The objects in object store are maintained by applications and operating system at least partially in response to calls to the exposed application programming interfaces and methods.

Communication interface represents numerous devices and technologies that allow mobile device to send and receive information. The devices include wired and wireless modems satellite receivers and broadcast tuners to name a few. Mobile device can also be directly connected to a computer to exchange data therewith. In such cases communication interface can be an infrared transceiver or a serial or parallel communication connection all of which are capable of transmitting streaming information.

Input output components include a variety of input devices such as a touch sensitive screen buttons rollers and a microphone as well as a variety of output devices including an audio generator a vibrating device and a display. The devices listed above are by way of example and need not all be present on mobile device . In addition other input output devices may be attached to or found with mobile device within the scope of the present invention.

Aspects of the present invention generally leverage natural user interaction to automatically learn new words pronunciations and word pairs. Generally this is accomplished by inferring whether the user is modifying text because he or she has changed his or her mind or whether the user is making a correction due to an inability of the system to recognize the user s speech.

In order to identify the segment s where the correction occurs Dynamic Time Warping DTW is illustratively used. Then the speech recognition engine score of the dictated text and the corrected text can be compared. This allows the system to determine if the user is correcting to a similar sounding word or perhaps editing to a new word based upon a change of mind. Additional confidence scores or metrics can be used to improve the inference between corrections vs. editing as desired. If the result is that the system determines that the user is simply changing his or her mind control returns to block via line .

At block the system consults the lexicon to determine if the corrected word is in the lexicon. If the corrected word is not in the user lexicon control passes to block where the word is added to the lexicon and selectively add the new pronunciation and the language model is adapted accordingly. The process of determining whether to add a new pronunciation is also described in greater detail with respect to . After block control returns to block .

If however the corrected word is in the user lexicon control passes to block where the system determines if the pronunciation is new. A new pronunciation can be caused by the pronunciation of a new word or a user specific pronunciation of existing words. The process of determining if the pronunciation is new will be described in greater detail with respect to . If the pronunciation is a new pronunciation control passes to block where the new pronunciation may be selectively learned. After block control returns to block .

If the pronunciation is not new control passes from block to block . This is a situation in which the corrected word is in the user lexicon and the pronunciation of the corrected word is also known. In this case the word pair and or phrase is added into the lexicon or the language model score associated with the corrected text is updated to boost the chance that the words are connected. In most cases this is a temporary change lasting for example one to two days. Thus if wave two is misrecognized as wave too and gets corrected by the user the system automatically adds wave two into the user lexicon temporarily. Temporarily is somewhat dynamic based on the most recent time the word pair is observed and the relative frequency that the pair has been observed in the past. Besides adding word pairs and or phrases the probability of the newly observed known pronunciation might also be increased if the system supports it. Although all of the steps illustrated in can be implemented in a single system for best results embodiments of the present invention can be practiced without such steps necessarily coexisting in a single system. After block control returns to block .

At block the phone confusion matrix and Dynamic Time Warping are used to calculate the distance between the newly identified pronunciation and existing pronunciations. Alternative distance calculation methods may also be used. For example the distance can be calculated based on the acoustic model score on the new pronunciation and the existing pronunciations. The distance is preferably compared to a pre selected or dynamic threshold to determine whether the pronunciation should be learned. Thus only pronunciations where the distance exceeds some threshold will be learned.

At block the system determines whether the new pronunciation should be added. This decision is preferably based upon the calculated distance from block the closest existing pronunciation the Acoustic Model AM confidence and the frequency with which the new pronunciation has appeared in the user s dictation. Deciding whether to add the pronunciation selectively based upon these factors will help ensure that errors from misalignment and or incorrect inferences of whether the user is doing a correction will not cause learning that decreases system effectiveness. Examples of pronunciation confidence calculations include the following pron 1 1 and pron 1 log len1 len2 where d is the distance between the recognized pronunciation and the best match in the lexicon f is the frequency that the same recognized pronunciation is pronounced and p d AM is the probability that a pronunciation with such a distance d and AM score is the correct pronunciation. Len1 and len2 are the lengths of the phonemes in the new pronunciation and the closest pronunciation respectively. P d AM is learned with training.

At block the system selectively adds the new pronunciation. Preferably the pronunciation is added if the confidence score calculated in block is high enough and the new pronunciation has occurred a selected number N times in the user s dictation.

By applying a threshold to the calculated confidence score the system in accordance with embodiments of the present invention will only learn the minimum amount in order to ensure that the system is not unduly affected by learning from user editing resulting from the user simply changing his or her mind.

Although the present invention has been described with reference to preferred embodiments workers skilled in the art will recognize that changes may be made in form and detail without departing from the spirit and scope of the invention.

