---

title: Performing memory accesses using memory context information
abstract: In one embodiment, a processor includes an address generation unit having a memory context logic to determine whether a memory context identifier associated with an address of a memory access request corresponds to an agent memory context identifier for the processor, and to handle the memory address request based on the determination.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08521944&OS=08521944&RS=08521944
owner: Intel Corporation
number: 08521944
owner_city: Santa Clara
owner_country: US
publication_date: 20100831
---
Modern processors are incorporating ever greater amounts of circuitry within a single processor. For example multi core and many core processors are being introduced that include a number of different processing cores or engines in addition to other logic internal memory such as one or more levels of cache memory storage and so forth. In addition such processors are often programmed to perform multiple threads of execution concurrently. Also individual processors can be connected together in multichip clusters.

As a result maintaining full compliance with memory ordering rules of an instruction set architecture ISA while still providing efficiency in memory accesses is becoming very difficult. The term memory ordering refers to the order in which a processor issues reads loads and writes stores to system memory. Different processor architectures support different memory ordering models depending on the architecture. In so called program or strong ordering reads and writes are issued in program order. To allow performance optimization of instruction execution some architectures provide for a memory ordering model that allows for performance enhancements such as allowing reads to proceed ahead of buffered writes.

Since data may be used by different agents in the same or different processors copies of data can be stored in various locations of a system e.g. one or more cache memories associated with different processors. However a coherent view of the data is to be maintained. While efficient cache line sharing between all agents in a system is a design goal some of the data may only be used locally or shared only by a few threads. Some regions of memory can be defined as cacheable meaning that when a processor seeks to write data to a memory the data can initially be stored in a cache memory associated with the processor without immediately writing the data to the system memory. Thus data can be stored in a cache and also a store can occur to such a cache. In contrast an uncacheable memory region immediately causes the requested write operation to be written to the memory and data of an uncacheable request is not stored in a cache. In current systems any memory access to cacheable memory may have to look up a given memory location in all caches in the system which increases latency and traffic. To reduce this overhead mechanisms such as snoop filters are implemented in hardware but these resources can consume a large chip area particularly as the number of collaborative agents increase.

In various embodiments information regarding a potential data location of data stored in a memory system can be leveraged to provide for optimized memory flows and reduced data structures as well as to reduce the amount of communication within a system to obtain access to such data. While the scope of the present invention is not limited in this regard both operating system and application software may take advantage of this information in allocating memory to a given thread and in handling access requests within the thread. In this way memory ordering rules of an instruction set architecture ISA can be met with reduced overhead both as to communications and to physical structures such as snoop filters directories and so forth. That is information regarding potential data location can be used by hardware to optimize memory flows and reduce the need for dedicated physical structures and as a result reduce the memory ordering rules overhead.

To this end embodiments may provide for memory context information. As used herein a memory context is an identifier or other information associated with one or more components of a system that can be used to identify a potential data location for data of a given application thread or other user level context in which data can be stored and modified. To maintain such memory contexts various components of a system may include hardware to maintain and store a memory context associated with the component. For example various structures such as processors caches memory bus agents such as peripheral devices and so forth each may include a storage such as a register to provide for storage of this context information.

While these memory contexts can be set in different manners in various embodiments in one particular embodiment such memory contexts may be set by an operating system at system startup. For example in a given implementation a processor identification instruction e.g. a so called CPUID instruction of an instruction set architecture ISA may cause firmware and or an operating system OS to obtain information regarding available system agents and to populate a table with such information. Accordingly this table may be a memory context table that is accessible to the OS. In various embodiments the table may be only accessible to the OS and thus can be stored in a kernel level space e.g. of system memory such as a dynamic random access memory DRAM although the location for storage of this table may be in other places of a system such as in a flash memory that provides for storage of other OS tables. Further understand that in different implementations the actual identifications and number of memory contexts available can vary greatly.

Referring now to shown is a block diagram of a portion of a system in accordance with one embodiment of the present invention. As shown in system may be a multiprocessor system. Specifically shows a first processor and a second processor . While the scope of the present invention is not limited in this regard such processors may be physical or logical processors and may be separate cores of a multi core processor or single core processors of a system. While not shown for ease of illustration understand that processors may further include additional structures such as a low level cache memory uncore and other logic.

As seen each processor couples to a corresponding cache memory and cache memory . As one example these cache memories may be so called L2 caches that may be present on the same semiconductor die as the corresponding processor although the scope of the present invention is not limited in this regard. In turn these caches which may be private caches are coupled to a shared cache which in one embodiment may be a last level cache LLC . In some such implementations shared cache may be of the same semiconductor die as the processors and second level caches such that all of these components form a single multi core processor.

Still referring to in turn shared cache may be coupled to an interconnect which in various embodiments may be a shared bus such as a front side bus or so forth. Also coupled to interconnect may be one or more bus agents and . In various embodiments such bus agents may be peripheral devices such as input output I O devices fixed or programmable functional units and which may be of a heterogeneous arrangement and ISA to processor and so forth. Understand that while shown these limited components in the embodiment of a system may include many more components such as a system memory mass storage devices additional bus agents peripheral devices and so forth.

Still with reference to note that the various components each may have a memory context associated therewith. Thus each component may be configured with a memory context identifier e.g. by way of a fuse setting or other permanent configuration setting or by storage of a memory context identifier as part of a configuration of the component. For example the identifier for the memory context can be stored in a memory context identifier register configuration register or other storage of the component. Memory context identifiers may be implemented in a hierarchy in which components closest to a processor core are allocated with a memory context identifier having a higher priority than those that are located further away from the processor core. Furthermore as the memory context hierarchy increases to the more remote memories such contexts encompass the lower memory contexts as well.

As discussed above responsive to an identification instruction for example firmware may build this memory context table. While different variations are possible in one embodiment the table for the system components shown in may be generated as follows in Table 1. As seen in Table 1 the higher priority memory contexts may be associated with processors which are identified in Table 1 as logical processors although the scope of the present invention is not limited in this regard. As memory contexts advanced to the lower priority memory contexts note that components of the higher priority memory context may also be included.

In various embodiments system software may thus generate this table whether by the OS or by another system firmware or software agent such as basic input output system BIOS running at power on self test POST . Referring now to shown is a flow diagram of a method for performing generation of a memory context table in accordance with an embodiment of the present invention. Method may begin by receiving an identification instruction block . For example this CPUID instruction may be received e.g. by BIOS on POST or by the OS in its initialization process. Assume for purposes of discussion that it is the BIOS that performs the method. Thus as seen in at block BIOS may send a request for memory context identifiers to various system devices. For example messages may be sent to system components such as those shown in .

Responses to these messages may be received. Then at block a memory context table may be generated that has an entry for each of multiple memory contexts. The number of memory contexts available in a given system may be set by the system designer. Then BIOS may populate the entries of this table with the corresponding system device block . An example of this population is shown in Table 1 above. Specifically in this example as the memory context identifiers increase in value more system components become part of the memory context. That is at the lowest value e.g. memory context 0 only a single component is associated with the identifier while as the value of the memory context identifier increases both additional components as well as components of at least some of the preceding memory contexts are part of the given memory context. Referring again to method may conclude at block where the memory context table is stored in an OS accessible storage. This storage as discussed above may be system memory flash memory or so forth. While shown with this particular implementation in the embodiment of the scope of the present invention is not limited in this regard.

Referring now to shown is a block diagram of a page table entry in accordance with one embodiment of the present invention. As shown in page table entry may be stored in a page table and used to provide address and attribute information for a memory page. Such page table entries may be accessed by the memory address generation hardware and at least portions of the page table entry can be stored in a cache memory such as a translation lookaside buffer TLB to enable faster access to recently and or frequently used page table entries.

As seen in page table entry may include an address field that stores an address e.g. of a memory page which may correspond to a 4 kilobyte KB range of memory locations of a system memory. In addition a memory context identifier field may store a memory context identifier for the associated memory page to indicate in which memory context data from the page is likely to be stored. When used in this way with a valid memory context identifier present in memory context identifier field an enable indicator of a status field may be set to indicate that a valid memory context identifier field is present. When reset indicator may thus cause memory context operations not to be performed. In various embodiments other additional information such as other enable bits e.g. write protect bits and so forth may be stored in status field . In addition a presence indicator may indicate whether the corresponding page table entry maps to a given page in memory. While shown with this particular implementation in the embodiment of understand the scope of the present invention is not limited in this regard.

Referring now to shown is a block diagram of a method for setting a page table entry in accordance with one embodiment of the present invention. As shown in method may be performed by an OS when a page is allocated in memory e.g. responsive to a memory allocation request from a given application. Thus as seen in method begins by receiving a memory allocation request from a thread block . Next it may be determined the expected locality of usage for the allocated memory block . While the scope of the present invention is not limited in this regard different manners of determining the expected usage can occur in various embodiments. For example a user level process can provide locality information to the OS to guide its decision. Or the OS can determine locality based on its knowledge of the application system or even based on heuristics such as selection of a local memory first . As examples for a single threaded application the OS may determine the expected locality to be the local memory closest to the processor that is to perform the thread. As for a multi threaded application the expected locality may be all memories associated with one or more processors on which the threads are to run. In some embodiments the OS may provide an application programming interface API so that user level applications can request local memory storage for non shared data.

Based on the determined expected locality a memory context may be associated with the expected locality block . Then a page table entry for the allocated memory page or pages may be set with the memory context field having a memory context identifier of the associated memory context block .

For example assume a single threaded application is to run on a first processor that has a memory context identifier value of 1. One or more page table entries to be associated with memory allocated for this thread may thus store a memory context identifier value of 1. Then as shown at block the memory context enable indicator for the page table entry may also be set. Thus for memory allocation operations as to a uni threaded application memory may be allocated to always use local memory which may be the closest memory context ID of the hardware context to which the application is assigned . As to a multi threaded application a memory context that includes all hardware contexts where its threads are running may be selected. Note while shown with this particular implementation in the embodiment of with an example in which only a single page table entry is allocated per request understand the scope of the present invention is not limited in this regard and in various examples many page table entries can be set responsive to a single memory allocation request e.g. depending on the amount of memory requested by the application and the OS policy.

After the page table entry is set with the memory context information such information may be used during program execution to enable more efficient accesses to memory and reduced memory traffic such as snoop requests and so forth. Referring now to shown is a block diagram of a method for handling a memory access request in accordance with an embodiment of the present invention. As shown in method may begin by generating a memory request with a memory address block . Next it may be determined whether the memory access request is a cacheable type diamond . As one example this may be determined by attributes in the page table entry along with some other hardware control register state that can also be set by the OS . If not control passes to block where the memory access request may be handled according to conventional ISA ordering rules.

Otherwise control passes to diamond where it may be determined whether a memory context enable indicator is set for the memory page. This determination may be based on an enable indicator in a page table entry for the memory page. Note that in various implementations for ease of access by the AGU the memory context information from the page table entry may be stored in a TLB or other buffer of the processor along with the memory context field of the page table entry and translation information. If the enable indicator is not set this indicates that no memory context information is available and accordingly control passes again to block . Otherwise assuming a valid memory context enable indicator control passes to diamond .

Still referring to it next may be determined whether the memory context field of the page matches an agent memory context identifier diamond . That is a comparison may be made between the memory context field value stored in the memory context field of the page table entry and a memory context identifier associated with the requesting agent which in various embodiments may be a processor bus agent peripheral device or other such agent . If the context identifiers match control passes to block where the memory access request may be handled locally as cacheable memory with a local hint. That is hardware of the processor may be configured to optimize the memory access request on the basis of this local hint. Of course various other optimizations are possible. Examples may include quickly allocating a memory cache line to be written without having to snoop any other agent in the system directly accessing the memory controllers eliminating a memory request snoop phase if the line is not present or avoiding a request for ownership phase whenever a memory location is to be modified. In this way no additional memory traffic is needed and the memory may be accessed locally to the requestor e.g. in a cache associated with a processor. In addition the need for snooping traffic or access to directory information snoop filters and so forth can be avoided or at least reduced.

Otherwise if at diamond it is determined that the memory context identifiers do not match control passes instead to diamond . At diamond it may be determined whether the OS wants hardware to handle the case of this mismatch. If so control passes to block where the memory access request may be handled as uncacheable. Then depending on implementation the request may be forwarded to the memory context of the requested memory page as this is the likely location of the requested memory. Note that by first seeking to request the data at the memory context stored in the page table a reduced amount of memory traffic may occur.

If instead at diamond hardware is not to be used a page fault may be produced to seek the requested memory information. Thus control passes to the OS which may resolve the issue by moving the page back to a non local state. That is the OS may reset the enable indicator for the corresponding page. Various operations may be performed in bringing a page back to non local state. For example the OS may determine whether any copy of a cache line of the page is modified in the local storage hierarchy of the context ID of the page. This may cause flushing of caches or performing writeback invalidations of cache lines of the page. A similar mechanism may be in place for page deallocation as well whenever a process deallocates memory or finishes and the OS wants to re assign one of its local pages all the cache lines of the old context ID hierarchy may be flushed. If an agent of the system lacks memory context ID capability the AGU of such agent may generate a page fault if it is accessing a page with the local bit set. Then the OS can resolve as above to clear the enable indicator for the page this agent is trying to access. While shown with this particular implementation in the embodiment of understand the scope of the present invention is not limited in this regard.

Referring now to shown is a block diagram of logic in accordance with one embodiment of the present invention. More specifically logic of may be a memory context logic or module that may be incorporated within an address generation unit of a core although this logic may be located in other places in different embodiments. As seen logic includes a comparator that may receive a local memory context identifier and a memory context identifier . As discussed above the local memory context identifier may come from a requesting agent e.g. a processor bus agent or so forth while memory context identifier may be received from information present in a given page table entry. Based on the comparison and further based on an enable indicator which may also be received from the corresponding page table entry a locality engine may determine a memory type for the associated request namely whether a given access request is for a local access and thus a local hint is generated or is not likely to be for a local access and thus an external memory context hint is generated . In one embodiment logic may operate in accordance with method of .

In general the determination of locality for a given memory access request may proceed in accordance with a typical address generation logic and further using logic . Referring now to Table 2 shown is pseudo code for performing a memory context analysis in accordance with one embodiment of the present invention. Generally this pseudo code shows that if a memory access request is for writeback e.g. cacheable and the memory context identifier of the page table matches an agent memory context identifier and an enable indicator for the page table is valid the memory access request may be handled locally optimizing the request and reducing communications and power consumption.

Embodiments can be implemented in many different systems. For example embodiments can be realized in a processor such as a multicore processor. Referring now to shown is a block diagram of a processor core in accordance with one embodiment of the present invention. As shown in processor core may be a multi stage pipelined out of order processor. Processor core is shown with a relatively simplified view in to illustrate various features used in connection with a memory context system in accordance with an embodiment of the present invention.

As shown in core includes front end units which may be used to fetch instructions to be executed and prepare them for use later in the processor. For example front end units may include a fetch unit an instruction cache and an instruction decoder . In some implementations front end units may further include a trace cache along with microcode storage as well as a micro operation storage. Fetch unit may fetch macro instructions e.g. from memory or instruction cache and feed them to instruction decoder to decode them into primitives i.e. micro operations for execution by the processor.

Coupled between front end units and execution units is an out of order OOO engine that may be used to receive the micro instructions and prepare them for execution. More specifically OOO engine may include various buffers to re order micro instruction flow and allocate various resources needed for execution as well as to provide renaming of logical registers onto storage locations within various register files such as register file and extended register file . Register file may include separate register files for integer and floating point operations. Extended register file may provide storage for vector sized units e.g. 256 or 512 bits per register.

Various resources may be present in execution units including for example various integer floating point and single instruction multiple data SIMD logic units among other specialized hardware. For example such execution units may include one or more arithmetic logic units ALUs . In addition an AGU may be present to generate addresses from which to obtain data and or instructions. For example AGU may be configured to receive a register address from a register in one of the register files and . AGU may translate that address into a physical address. Such translation may be effected using information stored in a page table entry that includes translation information in addition to memory context identification information in accordance with an embodiment of the present invention. To enable ease of access to such translation data virtual address to physical address translations may be stored in a TLB within a cache discussed further below.

As seen AGU may further include a memory context logic to perform a determination of whether a memory context for an address matches a corresponding memory context identifier for the core. In addition such logic may enable memory accesses to be performed in an optimized manner while maintaining a given memory order model.

When operations are performed on data within the execution units results may be provided to retirement logic namely a reorder buffer ROB . More specifically ROB may include various arrays and logic to receive information associated with instructions that are executed. This information is then examined by ROB to determine whether the instructions can be validly retired and result data committed to the architectural state of the processor or whether one or more exceptions occurred that prevent a proper retirement of the instructions. Of course ROB may handle other operations associated with retirement.

As shown in ROB is coupled to cache which in one embodiment may be a low level cache e.g. an L1 cache and which may also include TLB although the scope of the present invention is not limited in this regard. Also execution units can be directly coupled to cache . In various embodiments TLB may store in addition to virtual address to physical address translations a memory context identifier for the corresponding page of memory to enable faster access to such information by AGU . When AGU sends a request for information to cache and information is not present in the cache a cache miss occurs and the information may be obtained from other portions of a memory hierarchy. From cache data communication may occur with higher level caches system memory and so forth. Note that while the implementation of the processor of is with regard to an out of order machine such as of a so called x86 ISA architecture the scope of the present invention is not limited in this regard. That is other embodiments may be implemented in an in order processor a reduced instruction set computing RISC processor such as an ARM based processor or a processor of another type of ISA that can emulate instructions and operations of a different ISA via an emulation engine and associated logic circuitry.

Accordingly in various embodiments memory accesses can be optimized without any extra hardware overhead. Further by storing a memory context identifier in page table structures software can provide hints to the hardware and faster local storage can be realized that completely follows memory ordering rules of a given ISA. As such embodiments may substantially reduce memory access latency in a variety of systems and reduce hardware resources that are presently used to handle the coherency issues discussed above.

Embodiments may be implemented in many different system types. Referring now to shown is a block diagram of a system in accordance with an embodiment of the present invention. As shown in multiprocessor system is a point to point interconnect system and includes a first processor and a second processor coupled via a point to point interconnect . As shown in each of processors and may be multicore processors including first and second processor cores i.e. processor cores and and processor cores and although potentially many more cores may be present in the processors. As described above the processor cores may include an AGU or other logic to perform memory context operations in accordance with an embodiment of the present invention. Still further both the cores and other components of the system may include storage for a corresponding agent memory context identifier associated with the component.

Still referring to first processor further includes a memory controller hub MCH and point to point P P interfaces and . Similarly second processor includes a MCH and P P interfaces and . As shown in MCH s and couple the processors to respective memories namely a memory and a memory which may be portions of main memory e.g. a dynamic random access memory DRAM locally attached to the respective processors. First processor and second processor may be coupled to a chipset via P P interconnects and respectively. As shown in chipset includes P P interfaces and .

Furthermore chipset includes an interface to couple chipset with a high performance graphics engine by a P P interconnect . In turn chipset may be coupled to a first bus via an interface . As shown in various input output I O devices may be coupled to first bus along with a bus bridge which couples first bus to a second bus . Various devices may be coupled to second bus including for example a keyboard mouse communication devices and a data storage unit such as a disk drive or other mass storage device which may include code in one embodiment. Further an audio I O may be coupled to second bus .

Embodiments may be implemented in code and may be stored on a storage medium having stored thereon instructions which can be used to program a system to perform the instructions. The storage medium may include but is not limited to any type of disk including floppy disks optical disks optical disks solid state drives SSDs compact disk read only memories CD ROMs compact disk rewritables CD RWs and magneto optical disks semiconductor devices such as read only memories ROMs random access memories RAMs such as dynamic random access memories DRAMs static random access memories SRAMs erasable programmable read only memories EPROMs flash memories electrically erasable programmable read only memories EEPROMs magnetic or optical cards or any other type of media suitable for storing electronic instructions.

While the present invention has been described with respect to a limited number of embodiments those skilled in the art will appreciate numerous modifications and variations therefrom. It is intended that the appended claims cover all such modifications and variations as fall within the true spirit and scope of this present invention.

