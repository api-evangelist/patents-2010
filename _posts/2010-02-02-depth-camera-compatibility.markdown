---

title: Depth camera compatibility
abstract: Compatibility between a depth image consumer and a plurality of different depth image producers is provided by receiving a native depth image having unsupported depth camera parameters that are not compatible with a depth image consumer, and converting the native depth image to a virtual depth image having supported virtual depth camera parameters that are compatible with the depth image consumer. This virtual depth image is then output to the depth image consumer.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08687044&OS=08687044&RS=08687044
owner: Microsoft Corporation
number: 08687044
owner_city: Redmond
owner_country: US
publication_date: 20100202
---
Cameras can be used to capture still images of a scene. Several still images taken in rapid succession can be used to generate a movie including a plurality of frames each frame corresponding to a different still image. While such images are very useful in a variety of different applications such images are not well suited for some purposes. In particular conventional still images and movies do not provide adequate information to accurately assess the relative depths of the various surfaces captured in the scene. Different types of depth cameras have been developed to fill this need. However the various different types of depth cameras may produce depth images that differ from one another in one or more respects. As such applications that consume depth images may only be compatible with a particular type of depth camera that produces depth images with expected characteristics.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter. Furthermore the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure.

According to one aspect of this disclosure compatibility between a depth image consumer and a plurality of different depth image producers may be provided by receiving a native depth image having unsupported depth camera parameters that are not compatible with a depth image consumer and converting the native depth image to a virtual depth image having supported virtual depth camera parameters that are compatible with the depth image consumer. This virtual depth image may then be output to the depth image consumer.

The present disclosure is directed to depth camera compatibility. Compatibility technology as described herein allows different models of depth cameras to be used by the same computing system. An application developer may easily create applications that are compatible with a variety of different depth cameras including depth cameras that do not exist at the time the application developer creates an application. According to one aspect of the disclosure an application developer may create an application that is configured to work with a generic virtual depth camera and the compatibility technology can transform native input from a variety of different depth cameras into a form that is compatible with the generic virtual depth camera. In this way an application can be compatible with a variety of different depth cameras including depth cameras that utilize completely different depth finding technologies e.g. structured light time of flight stereo vision etc. .

Using the technology described herein an application developer need not be concerned with the tedious and difficult job of supporting a variety of different depth cameras but instead may develop for a single generic virtual depth camera. At the same time the compatibility technology facilitates the development and implementation of new depth cameras that may offer improved performance lower manufacturing and or operating costs improved energy efficiency and or other useful characteristics. As such as new camera technologies are developed such technologies may be implemented in new cameras that are backward compatible with previously developed applications.

A computing system such as gaming console may be used to recognize analyze and or track one or more targets such as user . Target movements may be interpreted as operating system and or application controls. Virtually any controllable aspect of an operating system and or application may be controlled by movements of a target such as user .

Additionally or alternatively depth information acquired via depth camera may be used for purposes other than tracking a target. As an example depth camera may be used to visually survey a room and or one or more objects. Depth information acquired from the depth camera may be used to create a three dimensional computer readable model of the room and or object s being surveyed.

As another example a depth camera may be used as a proximity sensor on a moving computing system such as a vehicle or a robot.

A depth camera such as depth camera may be used to provide these and a variety of other useful functions. In particular applications can be developed and deployed for providing any number of different functions that take advantage of depth information from a depth camera. However most applications include complicated algorithms and processing strategies for deducing real world characteristics of a viewed scene from the depth information. For example skeletal tracking algorithms and processing strategies may be employed to model a target such as user with a machine representation of a skeleton that tracks the target. As another example floor finding algorithms may be employed to find a floor within a scene.

Applications that do not utilize the herein described camera compatibility technology may be designed to receive depth information in a particular format and or with particular parameters. Such an application may not be able to use depth information that does not match the desired format and or parameters. In some cases depth information that does not match the desired format and or parameters may crash such an application.

Each different type of depth camera may produce depth information having one or more formatting differences and or different parameters. As such up until now applications have been designed with a particular depth camera in mind. However when an application is designed to be used with only a particular depth camera that application may not be able to support new cameras which may output depth information in an unfamiliar format and or with unexpected parameters.

Depth camera interchangeability system provides a great deal of flexibility with respect to which models of depth cameras can be used with depth image consumers. Cameras having different parameters cameras based on different depth finding technologies and cameras producing different types of depth images may be interchangeably used with depth camera interchangeability system .

As an example depth image producer may include a depth camera that uses structured light technology to assess depth images and depth image producer may include a depth camera that uses time of flight technology to assess depth images. Both such cameras are compatible with depth image consumer because of depth camera interchangeability system . Depth image consumer need not have been specifically designed for compatibility with either camera. Depth image consumer may be developed before either camera is released. In other words depth camera interchangeability system facilitates compatibility with depth image consumers and a variety of different depth cameras including depth cameras developed after the depth image consumer is developed.

As shown in depth camera interchangeability system includes an initialization module a capture module a virtualization module and an output module . In at least some embodiments the initialization module the capture module the virtualization module the output module and or other components of depth camera interchangeability system may be implemented as part of an application programming interface API . An API may be implemented by operating systems applications libraries and or other objects to establish the communication conventions to be used between a depth image producer and a depth image consumer. An API may include specifications for routines data structures object classes and protocols used to communicate between the depth image producer and the depth image consumer. Furthermore the API may include or interface with functional blocks configured to process various aspects of a depth image as described below with reference to virtualization module and emulation module for example.

Initialization module may be configured to acknowledge supported virtual depth camera parameters for a depth image consumer such as depth image consumer . To enable such an acknowledgement a depth image consumer such as a skeletal tracking game application may be developed for compatibility with a virtual depth camera in addition to or instead of a particular depth camera e.g. a structured light depth camera of a particular make and model . In such instances the depth image consumer may be configured to provide an indication of the virtual depth camera parameters expected by that depth image consumer e.g. x y z resolution view frustum depth image type etc. . Furthermore the initialization module may be configured to receive an indication of the supported virtual depth camera parameters from the depth image consumer. As discussed above the initialization module may optionally be implemented as part of an API that the depth image consumer and depth image producer can use to communicate with one another.

The indication of the supported virtual depth camera parameters may specify an expected view frustum of a virtual depth camera and or an expected resolution of a virtual depth camera. Such an expected view frustum and or expected resolution may be different than the native view frustum and native resolution of a particular depth camera. The indication of the supported virtual depth camera parameters may specify a model of an actual depth camera on which a virtual depth camera is based. The following description summarizes a nonlimiting sampling of different parameters that may be acknowledged by the initialization module .

Depth cameras generally measure at each pixel the radial distance from a nearest surface to the depth camera sensor. This distance may be converted into world coordinates by projecting the distance along rays from the depth camera into a three dimensional space modeling the real world. This process produces a range of three dimensional samples for which tight linear boundaries may not exist. Instead the points are bounded by the view frustum of the depth camera the space caught between two three dimensional arcs of two concentric balls assuming no lens distortions . shows an example view frustum for an example depth camera .

The position of the concentric balls is determined by the depth of field measurable by the depth camera. The smaller of the concentric balls has a radius that is equivalent to the minimum measurable distance for that depth camera and the larger of the concentric balls has a radius that is equivalent to the maximum measurable distance for that depth camera. The size of the three dimensional arcs in space is determined by the field of view i.e. view angles for that depth camera. The position and direction of the view frustum of the depth camera is determined by the physical position and physical orientation of the depth camera. How many different pixels are included in the view frustum is determined by the resolution of the depth camera. The sensitivity resolution of the depth depends on the number of bits per pixel that are received as output from the depth camera. However the actual sensitivity may be lower.

The parameters e.g. depth of field field of view resolution position direction lens distortion etc. may be different for different cameras. The difference in parameters between cameras can pose difficulties because depth image consumers may be highly sensitive to such differences. As such instead of being developed to receive depth images from a particular depth camera having certain depth camera parameters a depth image consumer may be developed for compatibility with a virtual depth camera having virtual depth camera parameters. As described below the depth camera interchangeability system may effectively translate depth information received from an actual camera in accordance with the virtual parameters of the virtual depth camera so that the depth information from the actual camera may be used by the depth image consumer.

In some embodiments the initialization module may be configured to generate aiming instructions to facilitate aiming a depth camera for compatibility with the supported virtual depth camera parameters of the depth image consumer. As an example a depth image consumer may expect a depth camera to be aimed down toward a floor so that the feet of a user can be scanned. As such aiming instructions can be generated to communicate this expectation. In embodiments in which the depth camera includes positioning motors or other means for automatically repositioning itself e.g. the depth camera of depth image producer the initialization module may be configured to send the aiming instructions to the depth camera so that the depth camera may reposition itself in accordance with the aiming instructions. In some embodiments the aiming instructions may be conveyed to a user via visual and or audio instructions so that a user may manually reposition the depth camera.

In some embodiments a coordinate system may be abstracted so that a depth image consumer will receive depth images having an expected coordinate system. In some instances this may involve rotating the coordinate system to be aligned with the floor for example by using hardware that measures the line of sight or by measuring the angle of the floor relative to the camera.

Capture module may be configured to receive a native depth image from a depth image producer. As a nonlimiting example the capture module may receive a native depth image from a depth image producer including a time of flight camera. As discussed above the capture module may optionally be implemented as part of an API that the depth image consumer and depth image producer can use to communicate with one another.

A depth image producer may include only a depth camera or a depth image producer may include a depth camera as well as off board processing engines. As shown in a depth image producer may include an on camera processor configured to translate raw depth camera data not shown into a native depth image . In such cases the capture module may be configured to receive the native depth image via the on camera processor . As shown in a depth image producer may include an off camera processor that translates raw depth camera data into a native depth image . The off camera processor may be part of a computing system on which a depth image consumer is running for example. In such cases the capture module may be configured to receive the native depth image via the off camera processor . As shown in a depth image producer may include an on camera processor and an off camera processor that cooperate to translate raw depth camera data not shown into a native depth image . In such cases the capture module may be configured to receive the native depth image via the on camera processor and the off camera processor . In particular the on camera processor may pass the off camera processor intermediate data that has been partially processed from the raw depth camera data. As in the example of the off camera processor may be part of a computing system on which a depth image consumer is running for example.

Returning to virtualization module may be configured to convert the native depth image to a virtual depth image having supported virtual depth camera parameters compatible with the depth image consumer. In particular the virtualization module may be configured to convert the native depth image to a virtual depth image in accordance with the indication of the virtual parameters received via the initialization module from the depth image consumer. The virtualization module may optionally be implemented as part of an API or as an application or service that is compatible with the API that the depth image consumer and depth image producer use to communicate.

As one example conversion the virtualization module may be configured to convert the native depth image to the virtual depth image by clipping a view frustum of the native depth image. An example of this is somewhat schematically shown in . A virtual view frustum is illustrated in solid lines. Virtual view frustum can be specified by virtual depth camera parameters. Superimposed in the same view is a native view frustum of an actual depth camera . The native view frustum is illustrated with dashed lines. As can be seen by comparison the native view frustum has a wider field of view and a deeper depth of field than virtual view frustum . As such the native view frustum may not be compatible for a depth image consumer expecting virtual view frustum .

The virtualization module may clip the native view frustum to that of the virtual view frustum. In other words depth information read by the depth camera that is outside the field of view and or depth of field of the virtual view frustum may be removed from the depth image. In some embodiments the removed depth information may simply be ignored while depth information in the virtual view frustum is left unchanged. In some embodiments the removed depth information may be used to selectively modify depth information in the virtual view frustum.

As another example conversion the virtualization module may be configured to convert the native depth image to the virtual depth image by changing a resolution of the native depth image e.g. decreasing a resolution of the native depth image . Virtually any resampling algorithm may be used to change the resolution. As a nonlimiting example a nearest neighbor algorithm may be used in which a sample grid having a desired resolution is conceptually aligned with the depth image from the actual depth camera. Each sample pixel of the sample grid may be assigned the depth value of the pixel from the actual depth camera that is nearest to the sample pixel. As other nonlimiting examples resampling algorithms may take an average or distance weighted average of nearest pixels.

The above provided examples of clipping a view frustum and changing a resolution are nonlimiting. It is to be understood that a depth image consumer may be designed to expect a virtual camera with any number of different virtual parameters and an actual depth image may be converted in accordance with such parameters in order to achieve compatibility with the virtual depth camera of the depth image consumer.

Returning to in some embodiments virtualization module may include an emulation module configured to convert the native depth image to the virtual depth image by processing the native depth image into an emulation depth image having a supported type that is supported by the depth image consumer. In other words a depth image consumer may be designed to expect depth images that are created using a specific model of depth camera and or a specific depth acquisition technology e.g. structured light or time of flight . While depth images from different types of depth cameras may ultimately produce depth images in which each pixel is assigned a depth value differences between the different types of depth cameras may result in various differences between the depth values that are assigned to each pixel. A depth image consumer may be designed to process a particular type of depth image from a particular depth camera. As such emulation module may be configured to change a depth image from an unsupported depth camera to seem as if it originated from a supported depth camera.

For instance the emulation module may convert a native depth image from a time of flight depth camera to an emulation depth image emulating a depth image produced by a structured light depth camera. This example is not limiting. The emulation module may be configured to convert a native depth image from virtually any camera technology to an emulation depth image emulating a depth image produced by virtually any other type of camera technology.

While emulation module may be part of a virtualization module in some embodiments it is to be understood that the emulation techniques described herein may be performed independently of any other virtualization techniques such as view frustum clipping and or resolution changing. In either case the emulation module may optionally be implemented as part of an API or as an application or service that is compatible with the API that the depth image consumer and depth image producer use to communicate.

When included an emulation module may include one or more of the following a noise suppression module an edge enhancement module an invalidation module a depth quantization module a small object correction module and a shadow simulation module .

Noise suppression module may be configured to preserve depth edges between adjacent pixel regions with different depth values in the native depth image. The functionality of noise suppression module is described in more detail below with reference to of .

Edge enhancement module may be configured to enhance depth edges between adjacent pixel regions with different depth values in the native depth image. The functionality of edge enhancement module is described in more detail below with reference to of .

Invalidation module may be configured to invalidate pixels having a combined illumination and obliqueness outside a predetermined range. The functionality of invalidation module is described in more detail below with reference to of .

Depth quantization module may be configured to quantize depth values. The functionality of depth quantization module is described in more detail below with reference to of .

Small object correction module may be configured to assign deeper depth values to pixels belonging to objects below a threshold size. The functionality of small object correction module is described in more detail below with reference to of .

Shadow simulation module may be configured to assign shadow pixel values to pixels that are virtually occluded from a virtual vantage point of a virtual illuminator virtually spaced away from a time of flight depth camera. The functionality of shadow simulation module is described in more detail below with reference to of .

The virtualization module converts the native depth image from an actual depth camera to a virtual depth image having supported virtual depth camera parameters compatible with the depth image consumer. An output module is configured to output this virtual depth image to the depth image consumer. The depth image consumer may then receive the virtual depth image for analysis and or further processing. The output module may optionally be implemented as part of an API that the depth image consumer and depth image producer can use to communicate with one another. While the initialization module the capture module the virtualization module and the output module are described above as being discrete modules it is to be understood that two or more of the modules may be operatively combined into a common API.

At method includes receiving an indication of the supported virtual depth camera parameters. As described above virtual depth camera parameters may specify a view frustum of a virtual depth camera a resolution of a virtual depth camera a model of an actual depth camera on which a virtual depth camera is based and or other characteristics of the virtual depth camera.

At method includes receiving a native depth image having unsupported depth camera parameters that are not compatible with a depth image consumer. At method includes converting the native depth image to a virtual depth image having supported virtual depth camera parameters that are compatible with the depth image consumer. As described above converting the native depth image may include one or more of clipping a view frustum of the native depth image decreasing a resolution of the native depth image and or processing the native depth image into an emulation depth image having a supported type that is supported by the depth image consumer.

At method includes outputting the virtual depth image to the depth image consumer. Because the virtual depth image is specifically tailored to the supported virtual depth camera parameters it can be used by the depth image consumer. However because method can be applied to a variety of different native depth images from different depth cameras the depth image consumer is not restricted to receiving depth images from a single particular type of depth camera.

At method includes receiving a native depth image having an unsupported type that is not supported by the depth image consumer. For example the native depth image may be from a source other than the model depth camera. As such the native depth image may be incompatible with the depth image consumer and thus have an unsupported type. Continuing with the example introduced above the depth image consumer may support depth images from a structured light depth camera. However the native depth image may be received from another source such as a time of flight depth camera and thus the native depth image has an unsupported type for the depth image consumer.

At method includes processing the native depth image into an emulation depth image having a supported type that is supported by the depth image consumer. In other words the emulation depth image can be modified to emulate a depth image that is compatible with the depth image consumer such as a depth image produced by the model depth camera. Continuing with the example introduced above a native depth image received from a time of flight depth camera for example may be processed into an emulation depth image which emulates a depth image produced by a structured light depth camera.

As shown in processing the native depth image into an emulation depth image may utilize a variety of techniques as described in more detail below. It can be appreciated that such techniques are nonlimiting. Further additional techniques not shown in may alternatively or additionally be applied.

At method may include applying an edge preserving filter to the native depth image. For the case of processing a native depth image received from a time of flight depth camera such a native depth image may have random noise that is a standard byproduct of time of flight depth cameras. However structured light depth cameras inherently have a smoother signal and such a signal may even be further filtered in software. Thus to emulate this type of smoother signal a native depth image received from a time of flight depth camera may be processed to suppress noise from the time of flight depth map without compromising significant depth features. To do so an edge preserving filter may be used to suppress the noise from the native depth image. Any suitable approach may be used such as by utilizing a nonlinear partial differential equation based off of those described in the works of Perona Malik Scale Space and Edge Detection Using Anisotropic Diffusion IEEE Transactions on Pattern Analysis and Machine Intelligence v. 12 n. 7 p. 629 639 1990 and Weickert et al. J. Weickert B. M. ter Haar Romeny M. A. Viergever Efficient and reliable schemes for nonlinear diffusion filtering IEEE Trans. Image Proc. v. 7 n. 3 pp. 398 410 1998 . The edge threshold parameter may be set to the upper bound of the depth accuracy of the camera e.g. K 10 cm . By applying the edge preserving filter to the native depth image the level of noise in the native depth image can drop significantly while discontinuities between objects in the native depth image are well preserved.

Continuing with at method may include building a confidence map by passing a median filter on an illumination image e.g. as measured from the IR light used to illuminate the scene in time of flight analysis . For the case of processing a native depth image received from a time of flight depth camera to emulate that of a structured light depth camera such a confidence map may be utilized to emulate a pixel invalidation phenomenon that occurs in structured light depth cameras. In the structured light technology depth is computed by finding pattern matches if a match is found then the depth can be computed relatively accurately however if a match is not found then the depth cannot be computed and the measurement at that pixel is invalid. In time of flight technology depth can typically be measured everywhere but at different accuracies e.g. depending on the level of illumination . Thus the level of illumination in a time of flight depth image can readily predict where depth measurements are inaccurate e.g. the signal is too noisy in dark regions and thus emulate a structured light depth image. To do so a confidence map may be built using an illumination image as an input. The confidence map may be built by first passing a median filter on the illumination image to remove outliers and suppress noise. Then for each pixel a soft threshold function can be used such as

In addition to building the confidence map it may be desirable to identify other regions within the image. Thus at method may include building an oblique surface map from the native depth image. For the case of processing a native depth image received from a time of flight depth camera to emulate that of a structured light depth camera the pixel invalidation phenomenon occurring in structured light depth cameras may be further emulated by identifying regions likely to correspond to pattern matching difficulties. The patterns projected by a structured light illumination device may be smeared on oblique objects i.e. surfaces of sharp angles with respect to the illumination rays and thus pattern matches often fail there and yield invalid measurements. Accordingly a native depth image received from a time of flight depth camera may be processed by building an oblique surface map to identify oblique surfaces. This may include computing the world surface angle for each pixel for example using camera parameters such as the field of view and resolution and then smoothing this by Gaussian filtering. Further a soft thresholding function may be used such as Fas defined above with k 45.

At method may include unifying the confidence map and the oblique surface map into an invalidation testing map. For the case of the confidence map and the oblique surface map both having values between zero and one the two maps may be unified for example by multiplying the two maps and thresholding with a threshold value of 0.5. A median filter can be used to regularize the result. Such a process can be used to invalidate pixels that are outside a predetermined range.

Continuing with at method may include enhancing edges between adjacent pixel regions with different depth values in the native depth image. For the case of processing a native depth image received from a time of flight depth camera such a time of flight depth camera tends to blur the edges since the depth value as calculated in this technology is an average of the depth in the pixel field of view. However a structured light depth camera s depth on edges is typically not measured and the data is synthesized creating sharp transitions between objects. Thus to emulate a depth image from a structured light depth camera the native depth image from the time of flight depth camera may be processed to enhance edges between adjacent pixel regions with different depth values to make the edges sharper. Any suitable approach may be used for such edge enhancement and one such suitable approach is described in detail as follows.

A forward difference D and a backward difference D in the x direction can be computed for a pixel. Then a mask can be determined as follows Mask x 1 if min D D 4 cm 0 otherwise which ramps to avoid step edges. A similar calculation can then be done in the y direction to compute Mask y. Then for each pixel where either Mask x or Mask y 1 the maximum in a 3 3 neighborhood is taken.

Continuing with at method may include quantizing depth values. For the case of processing a native depth image received from a time of flight depth camera depth values may be quantized to emulate a depth image received from a structured light depth camera. Structured light technology is based on triangulation to compute the depth. The depth is a function of the pattern displacement which is quantized as the native sensor resolution is finite. In time of flight technology the depth measurements are not related to the native resolution. Thus it might be desired to incorporate the quantization effect into the transformed depth map. This may be done by any suitable approach. One such suitable approach includes taking the parameters of the structured light depth camera e.g. field of view native sensor resolution focal length distance between the sensor and illumination centers and the depth map to construct a nonlinear quantization formula similar to the one occurring in triangulation based cameras. As an example the translation T in pixels as a function of the depth D may be defined as follows INT focal length camera illum TAN angle pixel size 0.5 where focal length is the focal length of the structured light depth camera pixel size is the sensor pixel size camera illum is the distance between the camera sensor and illumination centers and angle is the angle of the object with respect to the line perpendicular from the camera sensor center. Then the quantization Q as a function of the translation T D may be described as follows camera illum pixel size focal length TAN angle camera illum 1 pixel size focal length TAN angle . As such the depth map has quantized depth in a similar manner to that created by triangulation computations in the structured light technology.

Continuing with at method may include assigning deeper depth values to pixels belonging to objects below a threshold size. For the case of processing a native depth image received from a time of flight depth camera deeper depth values may be assigned to pixels of small objects to emulate a depth image received from a structured light depth camera. A structured light depth camera is based on patterns which cannot be projected well on objects that are too small. Thus such small objects are often assigned the background depth values. Deeper depth values may be assigned to pixels of small objects in any suitable manner. One such approach includes performing a morphological closing. As such the structuring element depends on the resolution. As an example a ball element with an approximate 3 3 size may be used. Thus the smallest object size that can be observed in the depth image can be corrected and the effects are similar to the post processing done in structured light depth cameras.

Continuing with at method may include assigning shadow pixel values to pixels that are virtually occluded from a virtual vantage point of a virtual illuminator virtually spaced away from the time of flight depth camera. For the case of processing a native depth image received from a time of flight depth camera shadow pixel values may be assigned to emulate a depth image received from a structured light depth camera. As described above a structured light depth camera works on the principal of triangulation. Due to this fact the light source is distanced from the sensor and a shadow effect is created on the sensor of the camera. Thus shadowed pixels are pixels that are visible to the sensor but not directly visible from the position of the light source as illustrated in . Here a first object occludes a second object from receiving direct light from light emitter . Thus in addition to receiving an image of the first object and an image of the second object sensor also received shadowed pixels of second object .

Thus it may be desired to emulate this shadowing artifact in the native depth image received from the time of flight depth camera. It can be appreciated that a shadow effect already occurs in a time of flight depth camera due to a small distance between the sensor and surrounding emitters however this may be normalized in the camera and therefore it may not be visible in the produced depth video.

An example solution for synthesizing the shadows uses a lightweight algorithm. The algorithm creates a virtual camera in the same place as where the light emitter would be in the modeled depth camera. The algorithm then transforms the depth samples from the original sensor to this virtual sensor. This transformation can be done using the following equations 

Samples that are occluded in this virtual sensor are shadowed. Each row of emitter image can be scanned and pixels may be shadowed if they do not have the maximum Xvalue among the already scanned values. illustrates the idea where the shadowed pixels are considered as shadowed due to the drop in the values of X.

In order to overcome some small fluctuations due to inaccuracy of the depth measurements a morphological open filtering can be applied to the map of shadowed pixels. This step removes small shadows and makes the edges appear more square like thus emulating edges from structured light depth cameras.

Continuing with at method includes outputting the emulation depth image having the supported type. Such an emulation depth image may be used by a depth image consumer that cannot process native depth images from the depth camera. Using the above example an application designed to process structured light depth images from a structured light depth camera may receive and process emulation depth images based off of native depth images measured using a time of flight depth camera.

As described below with reference to a variety of different computing systems may be used without departing from the spirit of this disclosure. The operating environment described with reference to is provided as an example but is not meant to be limiting in any way. To the contrary the illustrated operating environment is intended to demonstrate a general concept which may be applied to a variety of different operating environments without departing from the scope of this disclosure. Similarly the schematic depictions of the depth camera interchangeability systems illustrated in provide a simplified framework for describing depth image virtualization and depth image emulation but are not intended to limit the application to only those configurations shown in the drawings. To the contrary the methods and processes described herein may be tied to a variety of different types of computing systems.

Computing system may include a logic subsystem a data holding subsystem operatively connected to the logic subsystem a display subsystem and or a depth image producer . The computing system may optionally include components not shown in FIG. and or some components shown in may be peripheral components that are not integrated into the computing system.

Logic subsystem may include one or more physical devices configured to execute one or more instructions. For example the logic subsystem may be configured to execute one or more instructions that are part of one or more programs routines objects components data structures application programming interfaces or other logical constructs. Such instructions may be implemented to perform a task implement a data type transform the state of one or more devices communicate information to and or from different computing objects or otherwise arrive at a desired result. The logic subsystem may include one or more processors that are configured to execute software instructions. Additionally or alternatively the logic subsystem may include one or more hardware or firmware logic machines configured to execute hardware or firmware instructions. The logic subsystem may optionally include individual components that are distributed throughout two or more devices which may be remotely located in some embodiments.

Data holding subsystem may include one or more physical non transitory devices configured to hold data and or instructions executable by the logic subsystem to implement the herein described methods and processes. When such methods and processes are implemented the state of data holding subsystem may be transformed e.g. to hold different data . Data holding subsystem may include removable media and or built in devices. Data holding subsystem may include optical memory devices semiconductor memory devices e.g. RAM EEPROM flash etc. and or magnetic memory devices among others. Data holding subsystem may include devices with one or more of the following characteristics volatile nonvolatile dynamic static read write read only random access sequential access location addressable file addressable and content addressable. In some embodiments logic subsystem and data holding subsystem may be integrated into one or more common devices such as an application specific integrated circuit or a system on a chip.

The terms module and engine may be used to describe an aspect of computing system that is implemented to perform one or more particular functions. In some cases such a module or engine may be instantiated via logic subsystem executing instructions held by data holding subsystem . It is to be understood that different modules and or engines may be instantiated from the same application code block object routine and or function. Likewise the same module and or engine may be instantiated by different applications code blocks objects routines and or functions in some cases. As an example one or more of the modules described with reference to may be implemented as an API.

Display subsystem may be used to present a visual representation of data held by data holding subsystem . As the herein described methods and processes change the data held by the data holding subsystem and thus transform the state of the data holding subsystem the state of display subsystem may likewise be transformed to visually represent changes in the underlying data. Display subsystem may include one or more display devices utilizing virtually any type of technology. Such display devices may be combined with logic subsystem and or data holding subsystem in a shared enclosure or such display devices may be peripheral display devices as shown in .

Computing system further includes a depth image producer configured to obtain depth images of one or more targets and or scenes. Depth image producer may be configured to capture video with depth information via any suitable technique e.g. time of flight structured light stereo image etc. . As such depth image producer may include a depth camera a video camera stereo cameras and or other suitable capture devices. As described with reference to above a depth image producer may include one or more on camera processors and or off camera processors to translate raw depth camera data into depth images. In other words a depth camera may optionally include one or more onboard processing units configured to perform one or more depth analysis functions. A depth camera may include firmware to facilitate updating such onboard processing logic.

For example in time of flight analysis the depth image producer may include a time of flight camera configured to emit infrared light to the scene and may then use sensors to detect the backscattered light from the surfaces of the scene. In some cases pulsed infrared light may be used wherein the time between an outgoing light pulse and a corresponding incoming light pulse may be measured and used to determine a physical distance from the capture device to a particular location on the scene. In some cases the phase of the outgoing light wave may be compared to the phase of the incoming light wave to determine a phase shift and the phase shift may be used to determine a physical distance from the capture device to a particular location in the scene.

In another example time of flight analysis may be used to indirectly determine a physical distance from the capture device to a particular location in the scene by analyzing the intensity of the reflected beam of light over time via a technique such as shuttered light pulse imaging.

In another example structured light analysis may be utilized by depth image producer to capture depth information. In such an analysis patterned light i.e. light displayed as a known pattern such as a grid pattern or a stripe pattern may be projected onto the scene. On the surfaces of the scene the pattern may become deformed and this deformation of the pattern may be studied to determine a physical distance from the capture device to a particular location in the scene.

In another example the capture device may include two or more physically separated cameras that view a scene from different angles to obtain visual stereo data. In such cases the visual stereo data may be resolved to generate a depth image. In other embodiments depth image producer may utilize other technologies to measure and or calculate depth values.

In some embodiments two or more different cameras may be incorporated as part of a depth image producer. For example a depth camera and a video camera e.g. RGB video camera may be incorporated into a depth image producer. When a video camera is used it may be used to provide target tracking data confirmation data for error correction of scene analysis image capture face recognition high precision tracking of fingers or other small features light sensing and or other functions.

Furthermore while the description above has focused on the use emulation of a single depth camera it is to be understood that the above described compatibility technologies may be used to use emulate two or more depth cameras at the same time. For example two cameras may be used to view adjacent scenes and the API may effectively combine information from both cameras to emulate a single camera with a wider field of view. As another example a single wide angle camera may be used and the API may produce two separate depth images as if generated by two cameras with narrow fields of view looking in different directions.

In some embodiments two or more depth cameras may be used to look at the same scene from different vantage points. In such cases the API may effectively combine information from both cameras to provide more better 3D data in a way that is transparent to the depth image consumer.

It is to be understood that the configurations and or approaches described herein are exemplary in nature and that these specific embodiments or examples are not to be considered in a limiting sense because numerous variations are possible. The specific routines or methods described herein may represent one or more of any number of processing strategies. As such various acts illustrated may be performed in the sequence illustrated in other sequences in parallel or in some cases omitted. Likewise the order of the above described processes may be changed.

The subject matter of the present disclosure includes all novel and nonobvious combinations and subcombinations of the various processes systems and configurations and other features functions acts and or properties disclosed herein as well as any and all equivalents thereof.

