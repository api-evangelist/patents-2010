---

title: Assisted reader
abstract: An electronic reading device for reading ebooks and other digital media items combines a touch surface electronic reading device with accessibility technology to provide a visually impaired user more control over his or her reading experience. In some implementations, the reading device can be configured to operate in at least two modes: a continuous reading mode and an enhanced reading mode.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08452600&OS=08452600&RS=08452600
owner: Apple Inc.
number: 08452600
owner_city: Cupertino
owner_country: US
publication_date: 20100818
---
This disclosure relates generally to electronic book readers and accessibility applications for visually impaired users.

A conventional electronic book reading device ebook reader enables users to read electronic books displayed on a display of the ebook reader. Visually impaired users however often require additional functionality from the ebook reader in order to interact with the ebook reader and the content displayed on its display. Some modern ebook readers provide a continuous reading mode where the text of the ebook is read aloud to a user e.g. using synthesized speech. The continuous reading mode however may not provide a satisfying reading experience for a user particularly a visually impaired user. Some users will desire more control over the ebook reading experience.

An electronic reading device for reading ebooks and other digital media items e.g. .pdf files combines a touch surface electronic reading device with accessibility technology to provide a user in particular a visually impaired user more control over his or her reading experience. In some implementations the electronic reading device can be configured to operate in at least two assisted reading modes a continuous assisted reading mode and an enhanced assisted reading mode.

In some implementations a method performed by one or more processors of an assisted reading device includes providing a user interface on a display of the assisted reading device the user interface displaying text and configured to receive touch input for selecting a continuous assisted reading mode or an enhanced assisted reading mode. The method further includes receiving first touch input selecting a line of text to be read aloud determining that the enhanced assisted reading mode is selected based on the first touch input and invoking the enhanced assisted reading mode. The method further includes outputting audio for each word in the selected line.

In some implementations a method performed by one or more processors of the assisted reading device includes receiving first user input to a device the first user input selecting a first presentation granularity for content presented by the device and storing data indicating that the first presentation granularity was selected. The method further includes receiving second user input to the device the second user input requesting presentation of the content and presenting the content according to the first presentation granularity.

In some implementations a method performed by one or more processors of the assisted reading device includes displaying content on a display of a device wherein the content is displayed as lines of content each having a location on the display. The method further includes receiving user input at a first location on the device and in response to the user input identifying one of the lines of content having a location corresponding to the first location. The method further includes presenting audio corresponding to the identified line of content and not presenting audio corresponding to any of the other lines of content.

These features provide a visually impaired user with additional accessibility options for improving his or her reading experience. These features allow a user to control the pace and granularity level of the reading using touch inputs. Users can easily and naturally change between an enhanced and a continuous reading mode.

Other implementations of the assisted reader can include systems devices and computer readable storage mediums. The details of one or more implementations of the assisted reader are set forth in the accompanying drawings and the description below. Other features aspects and advantages will become apparent from the description the drawings and the claims.

In some implementations assisted reading device can be a portable computer electronic tablet electronic book reader or any other device that can provide assisted reading of electronic documents. In some implementations assisted reading device can include a touch sensitive display or surface e.g. surface that is responsive to touch input or gestures by one or more fingers or another source of input e.g. a stylus.

In the example shown in Chapter 12 of an ebook is displayed on touch sensitive surface of assisted reading device . The user interface of assisted reading device includes one or more controls for customizing the user s interactions with the displayed content. For example one or more controls can be used to magnify portions of text or to adjust the size or font of the text. As another example control can be used to move through pages of the ebook. For example a user can touch control and make a sliding gesture to the to left or right to move through pages of the ebook.

In some implementations assisted reading device can be configured to operate in at least two assisted reading modes a continuous reading mode and an enhanced reading mode. The continuous reading mode reads content continuously e.g. using speech synthesization or other conventional techniques until the end of the content is reached or the user stops or pauses the reading. The enhanced reading mode provides the user with a finer granularity control over his or her experience in comparison to the continuous reading mode.

The user can enter the continuous reading mode by providing a first touch input e.g. a two finger swipe down gesture on touch sensitive surface of assisted reading device . Once the device is in the continuous reading mode the content can be automatically presented to the user. The user can start and stop presentation of the content using other touch inputs e.g. a double tap touch input to start the presentation and a finger down on touch surface to stop or pause the presentation . During presentation of the content audio corresponding to the text of the content is presented. For example a synthesized speech generator in assisted reading device can continuously read a digital media item aloud line by line until the end of the digital media item is reached or until the user stops or pauses the reading with a touch input.

When the speech synthesizer reaches the end of the current page the current page is automatically turned to the next page and the content of the next page is read aloud automatically until the end of the page is reached. Assisted reading device turns the page by updating the content displayed on display to be the content of the next page and presenting the content on that page to the user. Assisted reading device can also provide an audio cue to indicate that a page boundary has been crossed because of a page turn e.g. a chime or that a chapter boundary has been crossed e.g. a voice snippet saying next chapter . In some implementations the audio cue is presented differently from the spoken text. For example the audio cue can be in a different voice a different pitch or at a different volume than the spoken text. This can help a user distinguish between content being spoken and other information being provided to the user.

In some implementations the language of the speech used by device during continuous reading mode is automatically selected based on the content of the digital media item. For example the digital media item can have associated formatting information that specifies the language of the content. Device can then select an appropriate synthesizer and voice for the language of the content. For example if the digital media item is an ebook written in Spanish the device will generate speech in the Spanish language e.g. using a Spanish synthesizer and Spanish voice that speaks the words with the appropriate accent. In some implementations the formatting information can also specific a particular regional format e.g. Spanish from Spain or Spanish from Mexico and the appropriate synthesizer and voice for that region can be used.

In the enhanced reading mode the user is provided with a finer level of control over his or her reading experience than the user has in the continuous reading mode. For example in the enhanced reading mode a page of the digital media item can be read line by line by the user manually touching each line. The next line is not read aloud until the user touches the next line. This allows the user to manually select the line to be read aloud and thus control the pace of his or her reading. For example the user can touch line and the words in line will be synthesized into speech and output by device . If the digital media item contains an image with a caption the caption can be read aloud when the user touches the image.

The user can turn to the previous or next page by making a left right up or down touch gesture e.g. a three finger swipe gesture . The direction of the gesture can depend on whether pages scroll from top to bottom or left to right from the perspective of a user facing the display of device . An audio cue can be provided to indicate a page turn or a chapter boundary as described in more detail above. When the user makes a gesture associated with the enhanced reading mode the device interprets the input as a request that the device be placed in the enhanced reading mode and that the requested feature be invoked.

In the enhanced reading mode a user can also step through the content at a user specified granularity as described in more detail below with reference to .

In the example shown the user has selected word mode. In word mode the user can provide a touch input to step through the content displayed on display word by word. With each touch input the appropriate item of content word is read aloud. The user can step forwards and backwards through the content.

When the user hears a desired word read aloud the user can provide a first touch input e.g. a single tap to get a menu with options. In the example shown in the word is accost and a menu is displayed with the options to get a definition of the selected word e.g. from a dictionary to invoke a search of the text of the document using the selected word as a query or invoke a search of documents accessible over a network e.g. the web using the selected word as a query. While menu is graphically shown on display in assisted reading device can alternatively or additionally present the menu to the user for example by presenting synthesized speech corresponding to the options of the menu.

In some implementations process can begin by receiving touch input . Based on the touch input received an assisted reading mode is determined . In some implementations the user can enter the continuous reading mode with a two finger swipe down gesture on a touch sensitive surface e.g. surface of the reading device e.g. device and can enter the enhanced reading mode by making a gesture associated with one of the features of the enhanced reading mode.

If the reading mode is determined to be the continuous assisted reading mode the device can be configured to operate in the continuous assisted reading mode . In some implementations once in the continuous assisted reading mode the user can start the reading aloud of content for example using a double tap touch input as described above with reference to . In other implementations the reading aloud begins automatically once the device is in the continuous assisted reading mode.

Each word of each line of the text of the currently displayed page of the digital media item is synthesized into speech and outputted until the end of the current page is reached. Alternatively other forms of audio other than synthesized speech can also be used.

At the end of the current page the current page is automatically turned to the next page e.g. updated to be the next page and text on the next page is read aloud automatically until the end of the page is reached. An audio cue can be provided to indicate a page turn e.g. a chime or a chapter boundary e.g. a voice snippet saying next chapter or identifying the chapter number e.g. chapter 12 . The continuous reading of text continues until the end of the digital media item is reached or until the user provides a third touch input to stop or pause the reading aloud of the content . In some implementations the user gestures by placing a finger down on a touch surface of the device to stop or pause the reading aloud of the content. The user can resume the reading by for example providing a double tap touch input.

If the reading mode is determined to be the enhanced assisted reading mode the device can be configured to operate in an enhanced reading mode . In the enhanced reading mode the user is provided with a finer level of control over his or her reading experience. When input from a user manually touching the desired line is received a line of text in a page of the digital media item can be read to the user. The device maps the location of the touch input to a location associated with one of the lines of text displayed on the display. The touched line and only the touched line is synthesized into speech and output through a loudspeaker or headphones. The user can then touch another line to have that line spoken aloud. Thus the enhanced assisted reading mode allows the user to manually select the line to be read aloud thereby controlling the pace of his or her reading.

The device can determine what text should be read aloud when a line is touched as follows. First the device maps the location touched by the user to data describing what is currently displayed on the screen in order to determine that content rather than some other user interface element was touched by the user. Then the device identifies the item of content touched by the user and determines the beginning and end of the line of content. For example the device can access metadata for the content that specifies where each line break falls.

In enhanced assisted reading mode the user can turn to the previous or next page by making a left right up or down touch gesture e.g. a three finger swipe gesture depending on whether pages scroll from top to bottom or left to right from the perspective of a user facing the display of device . If the digital media item contains an image with a caption the caption can be read aloud when the user touches the image. An audio cue can be provided to indicate a page turn e.g. a chime or a chapter boundary e.g. a voice snippet saying next chapter .

In enhanced assisted reading mode a user can also specify the granularity with which he or she wants content to be presented.

The process begins by receiving first user input to a device . The first user input selects a first presentation granularity for content presented by the device. For example the user can use a rotational touch gesture as if turning a virtual knob or dial. With each turn the device can provide feedback e.g. audio indicating which granularity the user has selected. For example when the user makes a first rotational movement the device can output audio speech saying character indicating that the granularity is a character granularity. When the user makes a subsequent second rotational movement the device can output audio speech saying word indicating that the granularity is word granularity. When the user makes a subsequent third rotational movement the device can output audio speech saying phrase indicating that the granularity is phrase granularity. If the user makes no additional rotational inputs for at least a threshold period of time the last granularity selected by the user is selected as the first presentation granularity. For example if the user stopped making rotational movements after selecting phrase granularity phrase granularity would be selected as the first presentation granularity. The user can select from various presentation granularities including for example character word phrase sentence and paragraph.

Data indicating that the first presentation granularity was selected is stored . Second user input to the device is received . The second user input requests presentation of content by the device. For example the user can use touch input to move forward and backwards through the content presented on the device at a desired granularity. For example the user can use a single finger swipe down motion to move to the next item of content and a single finger swipe up motion to move to the previous item of content. The content on the device is presented according to the first presentation granularity . For example if the input indicated that the next item of content according to the first presentation granularity should be presented the next item at the first presentation granularity e.g. the next character word phrase sentence etc. is presented. If the input indicated that the previous item of content according to the first presentation granularity should be presented the previous item is presented. The content is presented for example through synthesized speech.

In some implementations before stepping forwards and backwards through the content the user selects a line of interest. For example the user can touch the display of the device to indicate a line of interest and then use additional touch inputs to step through the line of interest. In other implementations the user steps forwards and backwards through the content relative to a cursor that is moved with each input. For example when a page is first displayed on the device the cursor can be set at the top of the page. If the user provides input indicating that the next item of content should be presented the first item of content on the page is presented. The cursor is updated to the last presented piece of content. This updating continues as the user moves forwards and backwards through the content.

If the cursor is at the beginning of the page and the user provides input indicating that the previous item of content should be presented or if the cursor is at the end of the page and the user provides input indicating that the next item of content should be presented the device provides feedback indicating that the cursor is already at the beginning or end of the page. For example in some implementations the device outputs a border sound. This alerts the user that he or she needs to turn the page before navigating to the desired item of content.

In some implementations when the user hears an item of interest the user can provide additional input requesting a menu for the item of interest. When the device receives that input the device can present the menu. An example menu is described above with reference to .

Operating system provides an interface to the hardware layer e.g. a capacitive touch display or device . Operating system can include one or more software drivers that communicate with the hardware. For example the drivers can receive and process touch input signals generated by a touch sensitive display or device in the hardware layer. The operating system can process raw input data received from the driver s . This processed data can then be made available to touch services layer through one or more application programming interfaces APIs . These APIs can be a set of APIs that are included with operating systems such as for example Linux or UNIX APIs as well as APIs specific for sending and receiving data relevant to touch input.

Touch services module can receive touch inputs from operating system layer and convert one or more of these touch inputs into touch input events according to an internal touch event model. Touch services module can use different touch models for different applications. For example a reading application such as an ebook reader will be interested in events that correspond to input as described in reference to and the touch model can be adjusted or selected accordingly to reflect the expected inputs.

The touch input events can be in a format e.g. attributes that are easier to use in an application than raw touch input signals generated by the touch sensitive device. For example a touch input event can include a set of coordinates for each location at which a touch is currently occurring on a drafting user interface. Each touch input event can include information on one or more touches occurring simultaneously.

In some implementations gesture touch input events can also be detected by combining two or more touch input events. The gesture touch input events can contain scale and or rotation information. The rotation information can include a rotation value that is a relative delta in degrees. The scale information can also include a scaling value that is a relative delta in pixels on the display device. Other gesture events are possible.

All or some of these touch input events can be made available to developers through a touch input event API. The touch input API can be made available to developers as a Software Development Kit SDK or as part of an application e.g. as part of a browser tool kit .

Assisted reading application can be an electronic book reading application executing on a mobile device e.g. an electronic tablet . Assisted reading application can include various components for receiving and managing input generating user interfaces and performing audio output for example speech synthesis. Speech synthesis can be implemented using any known speech synthesis technology including but not limited to concatenative synthesis formant synthesis diphone synthesis domain specific synthesis unit selection synthesis articulatory synthesis and Hidden Markov Model HHM based synthesis. These components can be communicatively coupled to one or more of each other. These components can be separate or distinct two or more of the components may be combined in a single process or routine. The functional description provided herein including separation of responsibility for distinct functions is by way of example. Other groupings or other divisions of functional responsibilities can be made as necessary or in accordance with design preferences.

Sensors devices and subsystems can be coupled to peripherals interface to facilitate multiple functionalities. For example motion sensor light sensor and proximity sensor can be coupled to peripherals interface to facilitate various orientation lighting and proximity functions. For example in some implementations light sensor can be utilized to facilitate adjusting the brightness of touch screen . In some implementations motion sensor can be utilized to detect movement of the device. Accordingly display objects and or media can be presented according to a detected orientation e.g. portrait or landscape.

Other sensors can also be connected to peripherals interface such as a temperature sensor a biometric sensor a gyroscope or other sensing device to facilitate related functionalities.

For example device can receive positioning information from positioning system . Positioning system in various implementations can be a component internal to device or can be an external component coupled to device e.g. using a wired connection or a wireless connection . In some implementations positioning system can include a GPS receiver and a positioning engine operable to derive positioning information from received GPS satellite signals. In other implementations positioning system can include a compass e.g. a magnetic compass and an accelerometer as well as a positioning engine operable to derive positioning information based on dead reckoning techniques. In still further implementations positioning system can use wireless signals e.g. cellular signals IEEE 802.11 signals to determine location information associated with the device. Other positioning systems are possible.

Broadcast reception functions can be facilitated through one or more radio frequency RF receiver s . An RF receiver can receive for example AM FM broadcasts or satellite broadcasts e.g. XM or Sirius radio broadcast . An RF receiver can also be a TV tuner. In some implementations RF receiver is built into wireless communication subsystems . In other implementations RF receiver is an independent subsystem coupled to device e.g. using a wired connection or a wireless connection . RF receiver can receive simulcasts. In some implementations RF receiver can include a Radio Data System RDS processor which can process broadcast content and simulcast data e.g. RDS data . In some implementations RF receiver can be digitally tuned to receive broadcasts at various frequencies. In addition RF receiver can include a scanning function which tunes up or down and pauses at a next frequency where broadcast content is available.

Camera subsystem and optical sensor e.g. a charged coupled device CCD or a complementary metal oxide semiconductor CMOS optical sensor can be utilized to facilitate camera functions such as recording photographs and video clips.

Communication functions can be facilitated through one or more communication subsystems . Communication subsystem s can include one or more wireless communication subsystems and one or more wired communication subsystems. Wireless communication subsystems can include radio frequency receivers and transmitters and or optical e.g. infrared receivers and transmitters. Wired communication system can include a port device e.g. a Universal Serial Bus USB port or some other wired port connection that can be used to establish a wired connection to other computing devices such as other communication devices network access devices a personal computer a printer a display screen or other processing devices capable of receiving and or transmitting data. The specific design and implementation of communication subsystem can depend on the communication network s or medium s over which device is intended to operate. For example device may include wireless communication subsystems designed to operate over a global system for mobile communications GSM network a GPRS network an enhanced data GSM environment EDGE network 802.x communication networks e.g. WiFi WiMax or 3G networks code division multiple access CDMA networks and a Bluetooth network. Communication subsystems may include hosting protocols such that device may be configured as a base station for other wireless devices. As another example the communication subsystems can allow the device to synchronize with a host device using one or more protocols such as for example the TCP IP protocol HTTP protocol UDP protocol and any other known protocol.

Audio subsystem can be coupled to speaker and one or more microphones . One or more microphones can be used for example to facilitate voice enabled functions such as voice recognition voice replication digital recording and telephony functions.

I O subsystem can include touch screen controller and or other input controller s . Touch screen controller can be coupled to touch screen . Touch screen and touch screen controller can for example detect contact and movement or break thereof using any of a number of touch sensitivity technologies including but not limited to capacitive resistive infrared and surface acoustic wave technologies as well as other proximity sensor arrays or other elements for determining one or more points of contact with touch screen or proximity to touch screen .

Other input controller s can be coupled to other input control devices such as one or more buttons rocker switches thumb wheel infrared port USB port and or a pointer device such as a stylus. The one or more buttons not shown can include an up down button for volume control of speaker and or microphone .

In one implementation a pressing of the button for a first duration may disengage a lock of touch screen and a pressing of the button for a second duration that is longer than the first duration may turn power to device on or off. The user may be able to customize a functionality of one or more of the buttons. Touch screen can for example also be used to implement virtual or soft buttons and or a keyboard.

In some implementations device can present recorded audio and or video files such as MP3 AAC and MPEG files. In some implementations device can include the functionality of an MP3 player.

Memory interface can be coupled to memory . Memory can include high speed random access memory and or non volatile memory such as one or more magnetic disk storage devices one or more optical storage devices and or flash memory e.g. NAND NOR . Memory can store operating system such as Darwin RTXC LINUX UNIX OS X WINDOWS or an embedded operating system such as VxWorks. Operating system may include instructions for handling basic system services and for performing hardware dependent tasks. In some implementations operating system can be a kernel e.g. UNIX kernel .

Memory may also store communication instructions to facilitate communicating with one or more additional devices one or more computers and or one or more servers. Communication instructions can also be used to select an operational mode or communication medium for use by the device based on a geographic location obtained by the GPS Navigation instructions of the device. Memory may include graphical user interface instructions to facilitate graphic user interface processing sensor processing instructions to facilitate sensor related processing and functions e.g. the touch services layer described above with reference to phone instructions to facilitate phone related processes and functions electronic messaging instructions to facilitate electronic messaging related processes and functions web browsing instructions to facilitate web browsing related processes and functions media processing instructions to facilitate media processing related processes and functions GPS Navigation instructions to facilitate GPS and navigation related processes and instructions e.g. mapping a target location and camera instructions to facilitate camera related processes and functions. Reading application instructions facilitate the features and processes as described in reference to . Memory may also store other software instructions not shown such as web video instructions to facilitate web video related processes and functions and or web shopping instructions to facilitate web shopping related processes and functions. In some implementations media processing instructions are divided into audio processing instructions and video processing instructions to facilitate audio processing related processes and functions and video processing related processes and functions respectively.

Each of the above identified instructions and applications can correspond to a set of instructions for performing one or more functions described above. These instructions need not be implemented as separate software programs procedures or modules. Memory can include additional instructions or fewer instructions. Furthermore various functions of device may be implemented in hardware and or in software including in one or more signal processing and or application specific integrated circuits.

Devices and can also establish communications by other means. For example wireless device can communicate with other wireless devices e.g. other devices or cell phones etc. over wireless network . Likewise devices and can establish peer to peer communications e.g. a personal area network by use of one or more communication subsystems such as a Bluetooth communication device. Other communication protocols and topologies can also be implemented.

Devices or can for example communicate with one or more services over one or more wired and or wireless networks . These services can include for example mobile services and assisted reading services . Mobile services provide various services for mobile devices such as storage syncing an electronic store for downloading electronic media for user with the reading application e.g. ebooks or any other desired service. Assisted reading service provides a web application for providing an assisted reading application as described in reference to .

Device or can also access other data and content over one or more wired and or wireless networks . For example content publishers such as news sites RSS feeds web sites blogs social networking sites developer networks etc. can be accessed by device or . Such access can be provided by invocation of a web browsing function or application e.g. a browser in response to a user touching for example a Web object.

The features described can be implemented in digital electronic circuitry or in computer hardware firmware software or in combinations of them. The features can be implemented in a computer program product tangibly embodied in an information carrier e.g. in a machine readable storage device for execution by a programmable processor and method steps can be performed by a programmable processor executing a program of instructions to perform functions of the described implementations by operating on input data and generating output. Alternatively or in addition the program instructions can be encoded on a propagated signal that is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a programmable processor.

The described features can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from and to transmit data and instructions to a data storage system at least one input device and at least one output device. A computer program is a set of instructions that can be used directly or indirectly in a computer to perform a certain activity or bring about a certain result. A computer program can be written in any form of programming language e.g. Objective C Java including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment.

Suitable processors for the execution of a program of instructions include by way of example both general and special purpose microprocessors and the sole processor or one of multiple processors or cores of any kind of computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Generally a computer will also include or be operatively coupled to communicate with one or more mass storage devices for storing data files such devices include magnetic disks such as internal hard disks and removable disks magneto optical disks and optical disks. Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non volatile memory including by way of example semiconductor memory devices such as EPROM EEPROM and flash memory devices magnetic disks such as internal hard disks and removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in ASICs application specific integrated circuits .

To provide for interaction with a user the features can be implemented on a computer having a display device such as a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer.

The features can be implemented in a computer system that includes a back end component such as a data server or that includes a middleware component such as an application server or an Internet server or that includes a front end component such as a client computer having a graphical user interface or an Internet browser or any combination of them. The components of the system can be connected by any form or medium of digital data communication such as a communication network. Examples of communication networks include e.g. a LAN a WAN and the computers and networks forming the Internet.

The computer system can include clients and servers. A client and server are generally remote from each other and typically interact through a network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

One or more features or steps of the disclosed embodiments can be implemented using an Application Programming Interface API . An API can define on or more parameters that are passed between a calling application and other software code e.g. an operating system library routine function that provides a service that provides data or that performs an operation or a computation.

The API can be implemented as one or more calls in program code that send or receive one or more parameters through a parameter list or other structure based on a call convention defined in an API specification document. A parameter can be a constant a key a data structure an object an object class a variable a data type a pointer an array a list or another call. API calls and parameters can be implemented in any programming language. The programming language can define the vocabulary and calling convention that a programmer will employ to access functions supporting the API.

In some implementations an API call can report to an application the capabilities of a device running the application such as input capability output capability processing capability power capability communications capability etc.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be made. For example while audio output such as speech synthesization is described above other modes of providing information to users for example outputting information to Braille devices can alternatively or additionally be used. As another example elements of one or more implementations may be combined deleted modified or supplemented to form further implementations. As yet another example the logic flows depicted in the figures do not require the particular order shown or sequential order to achieve desirable results. In addition other steps may be provided or steps may be eliminated from the described flows and other components may be added to or removed from the described systems. Accordingly other implementations are within the scope of the following claims.

