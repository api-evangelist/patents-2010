---

title: System, methods and apparatus for program optimization for multi-threaded processor architectures
abstract: Methods, apparatus and computer software product for source code optimization are provided. In an exemplary embodiment, a first custom computing apparatus is used to optimize the execution of source code on a second computing apparatus. In this embodiment, the first custom computing apparatus contains a memory, a storage medium and at least one processor with at least one multi-stage execution unit. The second computing apparatus contains at least two multi-stage execution units that allow for parallel execution of tasks. The first custom computing apparatus optimizes the code for parallelism, locality of operations and contiguity of memory accesses on the second computing apparatus. This Abstract is provided for the sole purpose of complying with the Abstract requirement rules. This Abstract is submitted with the explicit understanding that it will not be used to interpret or to limit the scope or the meaning of the claims.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08930926&OS=08930926&RS=08930926
owner: Reservoir Labs, Inc.
number: 08930926
owner_city: New York
owner_country: US
publication_date: 20100416
---
This application is a continuation in part of U.S. application Ser. No. 12 365 780 entitled METHODS AND APPARATUS FOR LOCAL MEMORY COMPACTION filed Feb. 4 2009 which claims priority to U.S. Provisional Application Ser. No. 61 065 294 filed Feb. 8 2008. Additionally this application is a Continuation in part of U.S. application Ser. No. 12 561 152 entitled METHODS AND APPARATUS FOR JOINT PARALLELISM AND LOCALITY OPTIMIZATION IN SOURCE CODE COMPILATION filed Sep. 16 2009 which claims priority to U.S. Provisional Application Ser. No. 61 097 799 filed Sep. 17 2008. Further this application is related to and claims the benefit of priority to U.S. Provisional Application Ser. No. 61 170 261 entitled AUTOMATIC CUDA MAPPING IN THE R STREAM COMPILER filed Apr. 17 2009. Priority is claimed to each of the above applications which are incorporated by reference herein in their entirety.

This invention was made with Government support under contract no. DE FG02 08ER85149 awarded by the Department of Energy contract no. F30602 03 C 0033 and W31P4Q 07 C 0147 awarded by the .Defense Advanced Research Projects Agency contract no. W9113M 07 C 0072 and W9113M 08 C 0146 awarded by the Missile Defense Agency and contract no. FA8650 07 M 8129 awarded by the Office of the Secretary of Defense. The Government has certain rights in the invention.

The present invention generally concerns computer programming. More particularly the invention concerns a system methods and apparatus for source code compilation.

The progression of the computer industry in recent years has illustrated the need for more complex processor architectures capable of processing large volumes of data and executing increasingly complex software. A number of systems resort to multiple processing cores on a single processor. Other systems include multiple processors in a single computing device. Additionally many of these systems utilize multiple threads of execution per processing core. One limitation that these architectures experience is that the current commercially available compilers cannot efficiently take advantage of the increase of computational resources.

In the software design and implementation process compilers are responsible for translating the abstract operational semantics of the source program into a form that makes efficient use of a highly complex heterogeneous machine. Multiple architectural phenomena occur and interact simultaneously this requires the optimizer to combine multiple program transformations. For instance there is often a tradeoff between exploiting parallelism and exploiting locality to reduce the ever widening disparity between memory bandwidth and the frequency of processors the memory wall. Indeed the speed and bandwidth of the memory subsystems have always been a bottleneck which worsens when going to multi core. This memory wall is further exacerbated by non contiguous memory accesses.

On many architectures the order in which memory locations are read and written has a profound effect on how they are issued in hardware. Bad memory access patterns may result in multiple factors of loss of memory bandwidth. Since optimization problems are associated with huge and unstructured search spaces the combinational task of optimizing a program balancing these hardware requirements is poorly achieved by current compilers resulting in weak scalability and disappointing sustained performance.

Even when programming models are explicitly parallel threads data parallelism vectors they usually rely on advanced compiler technology to relieve the programmer from scheduling and mapping the application to computational cores understanding the memory model and communication details. Even provided with enough static information or annotations OpenMP directives pointer aliasing separate compilation assumptions compilers have a hard time exploring the huge and unstructured search space associated with these mapping and optimization challenges. Indeed the task of the compiler can hardly been called optimization anymore in the traditional meaning of reducing the performance penalty entailed by the level of abstraction of a higher level language. Together with the run time system whether implemented in software or hardware the compiler is responsible for most of the combinatorial code generation decisions to map the simplified and ideal operational semantics of the source program to the highly complex and heterogeneous machine.

Current trends in computer architecture amplify the utilization of multiple processor cores on a chip. Modern multiple core computer architectures that include general purpose multi core architectures and specialized parallel architectures such as the Cell Broadband Engine and Graphics Processing Units GPUs have very high computation power per chip. Current and future architectures are increasingly evolving towards heterogeneous mixes of general purpose and specialized parallel architectures. One architectural concept of particular interest is the massively multi threaded execution model. In this model a large number of virtual threads of execution are mapped to a multiplicity of physical execution units. These virtual threads can be quickly switched in and out of the execution unit by the hardware runtime. In particular when a long latency memory access is requested another thread is scheduled to hide the latency of the memory access. Such an execution model comes with the need for the application to exhibit enough parallelism. Increased parallelism may be obtained by explicitly writing programs with more parallelism or by using auto parallelizing compilers.

While programming such systems by hand has been demonstrated for a range of applications this is a difficult and costly endeavor likely one to be revisited to allow the application to port to rapidly arriving new generations and configurations of heterogeneous architectures and programming abstractions that change the optimization tradeoffs. Recent programming models and abstractions include but are not limited to Partitioned Global Address Space PGAS Compute Unified Device Architecture CUDA and Open Computing Language OpenCL . The application developer is also confronted to a programmability wall in addition to the memory wall and is responsible for writing a correct parallel application using one of these recent programming abstractions. Obtaining reasonable performance is an additional difficult task best left to a compiler.

The polyhedral model is a powerful framework to unify coarse grained and fine grained parallelism extraction with locality and communication contiguity optimizations. To date this promise has not yet been completely fulfilled as no existing affine scheduling fusion and communication contiguity technique can perform all these optimizations in a unified i.e. non phase ordered and unbiased manner. Typically parallelism optimization algorithms optimize for degrees of parallelism but cannot be used to optimize both locality and contiguity of communications. In like manner algorithms used for locality optimization cannot be used both for extracting parallelism and optimizing the contiguity of communications. Additional difficulties arise when optimizing source code for the particular architecture of a target computing apparatus.

Therefore there exists a need for improved source code optimization methods and apparatus that can optimize parallelism locality and contiguity of memory accesses at multiple level of the heterogeneous hardware hierarchy.

The present invention provides a system apparatus and methods for overcoming some of the difficulties presented above. Various embodiments of the present invention provide a method apparatus and computer software product for optimization of a computer program on a first computing apparatus for execution on a second computing apparatus.

In an exemplary provided method computer program source code is received into a memory on a first computing apparatus. In this embodiment the first computing apparatus processor contains at least one multi stage execution unit. The source code contains at least one arbitrary loop nest. The provided method produces program code that is optimized for execution on a second computing apparatus. In this method the second computing apparatus contains at least two multi stage execution units. With these units there is an opportunity for parallel operations. In its optimization of the code the first computing apparatus takes into account the opportunity for parallel operations and locality and analyses the tradeoff of execution costs between parallel execution and serial execution on the second computing apparatus. In this embodiment the first computing apparatus minimizes the total costs and produces code that is optimized for execution on the second computing apparatus.

In another embodiment a custom computing apparatus is provided. In this embodiment the custom computing apparatus contains a storage medium such as a hard disk or solid state drive a memory such as a Random Access Memory RAM and at least one processor. In this embodiment the at least one processor contains at least one multi stage execution unit. In this embodiment the storage medium is customized to contain a set of processor executable instructions that when executed by the at least one processor configure the custom computing apparatus to optimize source code for execution on a second computing apparatus. The second computing apparatus in this embodiment is configured with at least two multi stage execution units. This configuration allows the execution of some tasks in parallel across the at least two execution units and others in serial on a single execution unit. In the optimization process the at least one processor takes into account the tradeoff between the cost of parallel operations on the second computing apparatus and the cost of serial operations on a single multi stage execution unit in the second computing apparatus.

In a still further embodiment of the present invention a computer software product is provided. The computer software product contains a computer readable medium such as a CDROM or DVD medium. The computer readable medium contains a set of processor executable instructions that when executed by a multi stage processor within a first computing apparatus configure the first computing apparatus to optimize computer program source code for execution on a second computing apparatus. Like in the above described embodiments the second computing apparatus contains at least two execution units. With at least two execution units there is an opportunity for parallel operations. The configuration of the first computing apparatus includes a configuration to receive computer source code in a memory on the first computing apparatus and to optimize the costs of parallel execution and serial execution of tasks within the program when executed on the second computing apparatus. The configuration minimizes these execution costs and produces program code that is optimized for execution on the second computing apparatus.

It will be recognized that some or all of the figures are schematic representations for purposes of illustration and do not necessarily depict the actual relative sizes or locations of the elements shown. The Figures are provided for the purpose of illustrating one or more embodiments with the explicit understanding that they will not be used to limit the scope or the meaning of the claims.

In the following paragraphs the present invention will be described in detail by way of example with reference to the attached drawings. While this invention is capable of embodiment in many different forms there is shown in the drawings and will herein be described in detail specific embodiments with the understanding that the present disclosure is to be considered as an example of the principles of the invention and not intended to limit the invention to the specific embodiments shown and described. That is throughout this description the embodiments and examples shown should be considered as exemplars rather than as limitations on the present invention. Descriptions of well known components methods and or processing techniques are omitted so as to not unnecessarily obscure the invention. As used herein the present invention refers to any one of the embodiments of the invention described herein and any equivalents. Furthermore reference to various feature s of the present invention throughout this document does not mean that all claimed embodiments or methods must include the referenced feature s .

The trend of increasing the frequency at which processors perform computations seems to have come to an end. Power consumption and control complexity have reached such high levels that manufacturers are backing out of this design path. Current machines have evolved to multiprocessor architectures on a chip with increasingly many cores per chip and multiple threads per core. This trend is expected to dramatically increase reaching thousands of cores per chip in the next few years. Thus modern computers increasingly need to exploit parallelism at different levels to provide sustained performance. On the other hand parallel programming techniques have not evolved at the same speed and the gap between theoretical machine speed and actual utilization continues to increase.

Compilers are responsible for translating the abstract operational semantics of the source program i.e. a text description of what the program s execution is supposed to perform into an executable form that makes efficient use of a highly complex heterogeneous machine. Multiple architectural phenomena occur and interact simultaneously within the targeted computer during the execution of the program this requires the optimizing compiler to combine multiple program transformations in order to define a program execution that takes advantage of those architectural phenomena. For instance when targeting computers that have multiple processing elements multi core computers there is often a trade off between exploiting more processing elements simultaneously parallelism and exploiting data access locality to reduce memory traffic. Indeed the speed and bandwidth of the memory subsystems are almost always a bottleneck. The problem is typically worse for multi core computers. Since in traditional compilers optimization problems are associated with huge and unstructured search spaces this combinational task is poorly achieved in general resulting in poor scalability and disappointing sustained performance of the supposedly optimized program.

Generating efficient code for deep parallelism and deep memory hierarchies with complex and dynamic hardware components is a difficult task the compiler and run time system has to take the burden of tasks that only expert programmers would be able to carry. In order to exploit parallelism the first necessary step is to compute a representation which models the producer consumer relationships of a program as closely as possible. The power of an automatic optimizer or parallelizer greatly depends on its capacity to decide whether two portions of the program execution may be interchanged or run in parallel. Such knowledge is related to the task of dependence analysis which aims at precisely disambiguating memory references. The issue is to statically form a compact description of the dynamic properties of a program. Forming a precise description is generally undecidable and approximations have to be made.

Once dependence analysis has been computed a compiler performs program transformations to the code with respect to different sometimes conflicting performance criteria. Any program transformation must ultimately respect the dependence relations in order to guarantee the correct execution of the program. A class of transformations targeting the loop nests of a program such as DO loops in the FORTRAN language and for and while loops in languages derived from the C language are known to account for the most compute intensive parts of many programs. The polyhedral model is a representation of a program s structure particularly suited for expressing complex sequences of loop nests complex sequences of loop nest transformations and other relevant information such as for instance dependences communications and array layouts.

A polyhedron is defined as a set of points verifying a set of affine inequalities and equalities on a number of variables. There exist alternate but equivalent definitions for polyhedrons such as the one based on a combination of vertices rays and lines proposed by Minkowski. There are also alternate representations often based on the alternate definitions. While the present disclosure teaches using one of those definitions and representations to illustrate the various embodiments various embodiments are in no way restricted to a particular definition or representation.

A polyhedral domain is defined as a finite union of polyhedrons. One of the main interests in using polyhedral domains is that they provide a precise representation of sets and relations among sets on which many optimization problems can be phrased and solved using a rich set of algorithms which are mostly available in the literature. Some embodiments of the sets in question represent loop iterations mono and multi dimensional data sets sets of processing elements data transfers synchronizations and dependences. Thus essential characteristics of the execution of a program can be summarized into compact mathematical objects polyhedrons which can be manipulated and transcribed into an executable program that has desired execution properties.

By considering a subset of the variables of a polyhedron as symbolic constants also called parameters it is possible to perform program optimizations and parallelization as a function of the symbolic constants. Hence programs involving loops that depend on a constant value that is not known at the time when compilation is performed but only when the program is executed can be modeled using polyhedrons that are defined as a function of those constant values. A polyhedron involving parameters is called a parametric polyhedron. Similarly a parametric polyhedral domain is defined by a finite union of parametric polyhedrons. For instance the set of values that the counters of a loop nest reach during the execution of the loop nest is represented by the loop nest s iteration domain . The iteration domain of the following loop nest using the C language s syntax where F is a C function call can be written as the parametric domain

While most of the transformations applied to the polyhedral representation of a program are defined for any element of the polyhedral domain to transform a class of more complex and precise transformations is obtained by partitioning the vector space in which the polyhedral domain is defined into sub polyhedrons and by defining a different transformation for each polyhedron of the partition. The resulting transformation is called a piecewise transformation. For example consider the transformation that takes two numbers i and j and computes three numbers x y and z as x 2i 1 y i j 2 z 3j 4 when i is greater than j and x i y i j 3 z 2j when i is less than or equal to j. It is a piecewise affine function since it has different definitions for each set of values i j and i j which define a partition of the i j vector space.

The context of various embodiments the use of polyhedral representations to perform complex optimizations on programs either independently or within a system of optimizing components. An exemplary embodiment of such a system is illustrated in where it is described as being part of a compiler. Flow of the exemplary embodiment starts in block where the compiler is processing a program. Flow continues in block where the compiler analyzes the program to decide if there are portions of the program that should be optimized and mapped using a polyhedral system. If it is the case the flow goes to block where the compiler provides the system of optimizing components with a polyhedral representation of a portion of the program to be optimized. If not the compiler continues to process the program without using the system of optimizing components and completes. The components of the system are in charge of a part of the global optimization of the input program. In the flow of the embodiment illustrated in the polyhedral representation of the input code is analyzed in block to produce dependence information. Flow continues in block where such information is used in a local memory compaction component or module that modifies array layouts in a way that removes some dependencies schedules loop iterations in a way that exposes loops that scan independent iterations and schedules the execution of operations using a same data to be executed within a close time interval. Flow continues in block where the modified polyhedral representation of the program is processed by another optimizing component which partitions the represented loop operations into entities called tasks which have good data locality properties they access a data set that involves an optimized use of the memory subsystem of the target computer and assigns a processing element of the target machine to each task. In this exemplary embodiment the flow continues to decision block which decides which block is next in the flow as a function of the target machine. If the target machine requires the execution of explicit communication commands to transfer data to and from its processing elements flow goes to block where the representation of the program thus modified is then processed by a series of optimizing modules which define a new layout for data that is transferred to a processing element s local memory. Otherwise the flow goes to block . From block flow continues to block where a representation of the explicit communications is produced based on polyhedrons and then to block where the execution of the communications are scheduled for parallel execution with the tasks using multi buffering. Whether the target machine requires explicit communications or not the flow continues to block where an optimizing component processes the polyhedral representation of the program obtained from the previous components by inserting a polyhedral representation of synchronization operations which ensure that the execution of the modified program produces the same results or similar results as the original input program. The flow of the exemplary embodiment then goes to block where an optimizing component partitions the tasks into subtasks whose execution reduces traffic between the processing elements memories and their registers. Then in block a polyhedral representation of commands that trigger the execution of a series of tasks on the different processing elements of the target machine and that wait for the completion of those is generated by the next optimizing component. Finally in block the polyhedral representation of the optimized program is transformed by polyhedral code generation component into a representation Abstract Syntax Tree high level language code or a compiler s internal representation that can be either processed by a compiler or processed further by the user. In the exemplary embodiment the flow continues back to block where it may cycle again through the whole flow if there is more code to be optimized using the system of optimizing components.

In contrast to compilers based on polyhedral domains traditional loop oriented optimizing compilers typically perform syntactic transformations. As a result many interesting optimizations are often not available such as fusion of loops with different bounds or imperfectly nested loop tiling.

In some embodiments the optimizing components or modules comprise processor executable code that when executed by a processor convert source code into other forms of source code or in some instances machine code. In other embodiments various modules may be implemented in hardware such as monolithic circuits Application Specific Integrated Circuits ASIC or Field Programmable Gate Arrays FPGA . These modules may comprise software hardware firmware or a combination of these implementations. It is important to note that various embodiments are illustrated in specific programming languages these illustrations are mere examples and the scope is not therefore limited to any particular programming language.

Embodiments of a provided optimization module described above as local memory compaction are illustrated in . illustrates the flow of a provided method for local memory compaction. Flow begins in block where source code is received into memory. In this embodiment the source code represents loops with arbitrary parametric affine iteration domain and contains at least one array reference. An array reference is an operation that represents an access typically a read or a write to an array. Such a reference represents either explicitly or implicitly for instance by using programming language conventions a function to retrieve the memory address of an element of the array. In loop programs that function is typically a direct or indirect function of the loop indices and of some loop constant values. For instance in C arrays are typically referenced through mono and multi dimensional affine functions of some input values. In the C language the declaration of an array includes parameters called array size which implicitly define the address of an array element as a function of the input values to references to this array declaring char A 100 200 allocates an array of 20000 elements 100 200 named A and defines that for any two integer values x and y the memory address of the element of A referenced through A x y is b 200x y where b is a value called the base address of array A. b is constant for each array and is determined at some point in the compilation process. Flow continues to block where inefficiencies in memory usage in the at least one array are identified. In one embodiment the inefficiencies are related to access and memory footprint of the array. Flow then continues to block where at least one local array is allocated and in block a portion of the array with inefficient memory usage is mapped into the local array. The mapping portion of the module outputs code that is more efficient than the original code in terms of the memory size requirements of the local array versus the original array. In some embodiments the accessed data is arbitrarily complex. In further embodiments the mapping produces a piecewise affine index function for the local arrays. Other embodiments include the rendering of a visualization of the optimized code on a monitor.

Arrays are typically allocated sets of contiguous memory blocks. Some loop operations may access only portions of the allocated memory. When reorganizing the data layout for a specific processor there is an opportunity to take advantage of the inefficiencies in memory access requirements versus the actual utilization of the array. For example given the following code fragment 900 000 contiguous memory blocks are allocated but only 100 are accessed in this operation. Furthermore access to the array is not contiguous but contains gaps and thus will have less than optimal locality. Thus keeping the original data layout and array size in a remote processor is extremely inefficient. Moreover if there are less than 900 000 blocks available in the local memory the local memory cannot hold the entirety of the array and the program cannot be executed properly. In the provided code fragments we are using . . . to elude other operations which do not have any specific illustrative purpose.

One embodiment of a provided method illustrated in would map this code fragment into a local array with 100 elements. An exemplary mapping would produce the following pseudo code fragment in which the storage requirement of a local array is reduced from 300 300 elements to the optimal 100 elements.

One feature of this embodiment is that it provides a method of compacting local memory in a computing apparatus. This method provides a more efficient memory structure in terms of both access to the elements and the amount of memory occupied by the data that is actually accessed. The memory requirements are reduced from the initial allocation to an allocation that is large enough to contain the data that is actually used in the operations. In contrast to other methods the provided method handles loops whose iteration domains are non rectangular and loops that have a parametric iteration domain. In this document we refer to polyhedral iteration domains that are either non rectangular or parametric or both as arbitrary parametric iteration domains . In addition the provided methods handle non convex accessed data sets. The provided embodiments are very useful in image and video processing. Imaging applications typically utilize significant multi dimensional arrays where data representations of physical objects and systems are stored. Many image processing steps such as discrete wavelet transforms for example only utilize discrete portions of the stored data. In these situations various embodiments provide significant optimizations to local data storage.

Another embodiment of a provided method is illustrated in . In this embodiment flow begins in block where source code is received in memory. Similar to the above embodiment the source code contains loops with arbitrary parametric iteration domain and contains at least one array reference. Flow continues to block where inefficiencies in memory usage in the at least one array are identified. Flow then continues to block where at least one local array is allocated and in block a portion of the array with inefficient memory usage is mapped into the local array. In this embodiment mapping block includes partitioning references to form compatible references in block determining a relation within compatible references in block grouping compatible references based on the relation in block performing algebraic simplification in block and performing geometric arrangement through re indexing the elements of the local array in block . In some embodiments the set of references partitioned are references that access a portion of the array. The following pseudo code example illustrates this embodiment.

In this case all three references to array A are disjoint in that they access disjoint portions of the array. In this case they are transformed into three local arrays A  A  and A  in the following manner.

Performing transformations of the way data are allocated in memory i.e. transforming the data layouts has a combinational aspect since the data sets accessed through each array reference may overlap with one or more data sets accessed by other array references. Since each one of those overlaps entail constraints in the way that data layouts can be transformed analyzing all the combinations of overlaps for all the references is a source of high computational complexity. Hence references are grouped into sets in such a way that data accessed through one set of references does not overlap data accessed through another set of references. In this embodiment references of the same set are called compatible references . Since there is no overlap among sets of compatible references the following parts of the memory layout transformation which consider the overlaps can be applied independently to each set of compatible references. In particular they will decide if the overlapping data sets accessed by a set of compatible references should be partitioned further and how.

In some embodiments compatible references are identified by overlapping memory footprints during the execution of a particular subset of loop iterations. In an exemplary embodiment the provided method identifies array references having overlapping memory footprints duplicates a portion of the identified references and associates each of the duplicates with disjoint subsets of the memory footprint. An example pseudo code illustrates this embodiment.

The two references A i j and A j i overlap when i j. However if the references are allocated together it is impossible to reduce the local memory usage using only affine transformations. This is because the data footprint of the two references is a 2 dimensional set a cross while the data footprints of the individual references are both 1 dimensional. In order to compute better allocations in situations like this one embodiment first estimates how much overlapping is in the references. If the references are read only and if the overlapping data set is a small percentage of the overall data set the embodiment splits the references into two distinct references to one dimensional data sets. In the above example the embodiment will generate the following local memory allocation. Note that the center element of the data foot print A i i has been replicated and put into the locations A  i and A  i .

The geometric re arrangements provided by a further exemplary embodiment are defined by a piecewise affine transformation. In other words the transformation applied to the references is defined as a set of functions each element of the set being valid within a polyhedral domain of the loop values the parameters and the coordinates of the data accessed through the set of compatible references. In an exemplary embodiment when some of the data accessed by a set of compatible references are written by some of the references the written data subset and a subset of the data set that is only read define a partition for the piecewise affine transformation. Consider the program represented by the following pseudo code 

In this example the data set accessed by the both references to array A form a two dimensional set while the data sets accessed through each reference are one dimensional. The data accessed through both references overlap in A i i . In the exemplary embodiment a piecewise transformation of A is applied which separates A into two subsets one for each one dimensional data set and marks one of them as receiving the updates let us call it the writing reference to the duplicated data. In the example the duplicated data is A i i and the iteration domain is partitioned into three polyhedral domains 0 j

In other exemplary embodiments the geometric rearrangement is a piecewise affine transformation that defines a partition of the iteration domain and of the data sets in such a way that the number of references to a local array varies from one element of the partition to another. In the following example in which the possible values of variable i are 0 i 99900 the data sets accessed through reference A j and A i j overlap when i is less than 100. Otherwise they do not overlap.

Since those data sets overlap for some values of i both references are put in the same group of compatible references. If the accessed data sets are allocated as a single local array the amount of memory necessary to contain the array is 10000 memory cells. On the other hand if they are allocated as two separate arrays some of the data would have to be duplicated and the iteration domain the j loop here would have to be partitioned as in the previous exemplary embodiment. The amount of overlap when i is less than 100 may not be small enough and it may not be profitable to perform the duplication. The geometric rearrangement provided by the embodiment is a piecewise affine transformation that defines a partition of the set of parameters in the current example i A  j A j for 0 i

One advantage of the geometric rearrangement that is performed by this exemplary embodiment is that the j loops are not partitioned. Partitioning the loops into smaller loops is often a factor of performance degradation which is avoided in this exemplary embodiment. The partition of i is obtained by computing the domain in which both data sets intersect by projecting the intersection onto the vector space of the parameters in the current example the parameter is i and the projected domain is i

The operation flow of a further provided embodiment of a local memory compaction module is illustrated in . In this embodiment flow begins at block where source code is received in memory. Similar to the above embodiment the source code represents loops with arbitrary parametric affine iteration domains and contain at least one array reference. Flow continues to block where inefficiencies in memory usage in the at least one array are identified. In this embodiment the identification of inefficiencies includes block where strides in the polyhedral domain that defines the accessed dataset are identified and block where a lattice of integer points within the domain is extracted from the domain. These integer points indicate that only a regular subset of the accessed data region is accessed. In this manner more efficient allocation of local arrays is accomplished because portions of the array that are not accessed are identified and excluded from the mapping from the array to the local array.

An additional provided embodiment is illustrated in . In this embodiment like earlier embodiments flow begins at block where source code is received in memory. Similar to the above embodiment the source code represents loops with arbitrary parametric affine iteration domain and contains at least one array reference. Flow continues to block where inefficiencies in memory usage in the at least one array are identified. Flow then continues to block where at least one local array is allocated and in block a portion of the array with inefficient memory usage is mapped into the local array. In this embodiment like in the embodiment illustrated by mapping block includes partitioning references to form compatible references in block determining a relation within compatible references in block grouping compatible references based on the relation in block performing algebraic simplification in block and performing geometric arrangement in block . The algebraic simplification block includes block where a representative array reference is extracted from a set of references accessing a portion of the array. In some embodiments the representative array reference represents a set of references which access polyhedral datasets whose accessed points all lie on a lattice of integer points that is not the standard lattice on which any integer point lies. These embodiments take advantage of the fact that array references represent affine functions which can be represented as matrices called access matrices . In the exemplary embodiment the flow within block goes from block to block where a Hermite factorization is performed for the access matrix representing the representative array reference. The Hermite factorization produces a piecewise affine index function.

One purpose of Hermite factorization is to reduce the dimension of the reference to the actual geometric dimension of the data footprint. In addition if the access pattern contains strides i.e. regular intervals between accessed data using the non unimodular matrix that results from the Hermite factorization in the transformation removes these strides in the resulting local references. For example given an affine access function f x y on loop indices x and parameters y we first decompose it into the sum of g x h y where g x is a linear function on x and h y is an affine function on y. This decomposition is an algebraic simplification that makes it possible to perform further computations on the part of f x y that involves variables only. Function g x can be decomposed into g x HU where H H 0 is the Hermite Normal Form of g x and U is unimodular matrix. Let

Hermite factorizations have many uses is lattice computations. The Hermite factorization of a matrix G written G HU writes matrix G as the product of two matrices H and U. H called the Hermite normal form is a canonical representation of the lattice also represented by G. U is a unimodular matrix which entails that U when used as a transformation always transforms any point that has integer coordinates into another point that has integer coordinates. Also any point that has integer coordinates can be obtained by transforming a point with integer coordinates using a unimodular transformation. This is important since most programming language conventions enforce that data elements and particularly array elements must have integer coordinates.

The flow of a still further provided embodiment is illustrated in . In this embodiment like previous embodiments flow begins at block where source code is received in memory. Similar to the above embodiment the source code represents loops with arbitrary parametric affine iteration domain and contain at least one array reference. Flow continues to block where inefficiencies in memory usage in the at least one array are identified. Flow then continues to block where at least one local array is allocated and in block a portion of the array with inefficient memory usage is mapped into the local array. In this embodiment mapping block includes partitioning references to form compatible references in block determining a relation within compatible references in block grouping compatible references based on the relation in block performing algebraic simplification in block and performing geometric arrangement in block . Geometric rearrangement contains blocks where linear constraints are formed block where sets of linear programming problems are formed from the linear constraints and solved and block where a data reindexing is computed. In some embodiments the flow goes back to block . In such embodiments geometric rearrangements are applied iteratively until no reindexing function is found that reduces memory requirements.

Most modern programming languages abide by the convention that multi dimensional arrays are allocated in memory as if they were canonical rectangular parallelotopes. In a space of d dimensions a parallelotope is a finite polyhedron defined by 2d faces and whose faces are pair wise parallel. A canonical rectangular parallelotope is a parallelotope for which the normal vectors to its faces are either a canonical vector or the negation of a canonical vector. Examples of rectangular parallelotopes are a cube in a 3 dimensional space and a rectangle in a 2 dimensional space . In an exemplary embodiment the transformation is a unimodular reindexing of the accessed data that minimizes the size of the smallest canonical rectangular parallelotope that encloses the accessed dataset. The smaller the enclosing rectangular parallelotope the smaller the amount of memory that has to be allocated for the dataset.

In some embodiments this is accomplished by formulating a first set of linear constraints through the use of Farkas Lemma. This first set of linear programming constraints is decomposed dimension by dimension to form a set of integer linear programming problems. This set of problems is then solved to provide the data reindexing function which can then be applied to the at least one local array. Unimodular reindexings transform integer points into integer points. Hence the convention that data elements have integer coordinates is preserved by such a reindexing. In the case of affine transformations the linear part of the transformation can be represented by a unimodular matrix.

Farkas lemma is a basic linear algebra theorem which is often used to obtain from a set of affine constraints i.e. inequalities and equalities on variables with unknown coefficients constraints that apply to the unknown coefficient themselves. In this embodiment it is used to obtain a set of constraints involving the coefficients of the unimodular data reindexing function which is represented as a matrix and the width of the enclosing rectangular parallelotope along each dimension. From those obtained constraints the method embodiment finds values of the coefficients of the unimodular data reindexing function for which the width is minimal using integer linear programming. For example the data set accessed through reference B i j j in the following pseudo code can be reindexed so as to occupy only 100 memory cells 

The coordinates x x of the elements of array B accessed by that loop node are defined by the constraints D n x

An integer linear programming problem defines a linear function of a set of variables called the objective function and whose minimal or alternatively maximal value over a polyhedral domain called the feasible set is looked for. Solvers for such problems typically return a polyhedral domain within the feasible set for which the value of the objective function is minimal. In the running example the embodiment finds 

In one of the exemplary embodiments the unimodular nature of the reindexing matrix U is obtained by forcing U to be triangular and forcing the absolute value of the diagonal elements to be one. In another embodiment the unimodular nature of the reindexing matrix is obtained by composition of an upper triangular unimodular and a lower triangular unimodular matrix. The advantage of that other embodiment is that the class of unimodular reindexing functions produced is not limited to the reindexing functions represented by a triangular matrix. Finding those two matrices is equivalent to reindexing data twice first by finding an upper triangular reindexing matrix as described above and applying the reindexing and then by finding a lower triangular reindexing matrix for the reindexed set and by applying that second reindexing. Yet another embodiment produces in the same way a unimodular reindexing by composition of an upper triangular unimodular matrix a permutation matrix and a lower triangular unimodular matrix. The advantage of the embodiment is that the class of reindexing function that can be produced is the whole class of integer unimodular matrices.

Turning to which illustrates another embodiment of a provided method like the previous embodiments flow begins in block where source code is received in memory. Similar to the above embodiment the source code represents loops with arbitrary parametric affine iteration domain and contains at least one array reference. Flow continues to block where inefficiencies in memory usage in the at least one array are identified. Flow then continues to block where at least one local array is allocated and in block a portion of the array with inefficient memory usage is mapped into the local array. In this illustration block contains block where a parallelotope of minimal volume is derived this parallelotope enclosing the domain of the data set accessed by the local arrays. Block additionally contains block where a finite prism of triangular base is derived.

As used herein a finite prism is a polyhedron defined by a set of translations of a base polyhedron which lies in a subspace of the considered space by a finite convex set of linear combinations of vectors of the complementary subspace. Since they are finite it is possible to characterize the maximum extent of a finite prism along the directions of the complementary subspace. In this document those extents are called height of the prism there is one height along every direction of the complementary subspace . A triangular prism is a prism whose base polyhedron is a triangle. In two dimensions it is just a triangle. In one embodiment this finite prism has a minimum volume that encloses the data footprint domain. In block the prism is compared to the parallelotope. In block the prism is partitioned into two prisms. One of the two is then transformed using a central symmetry such that the union of the transformed prism and the non transformed prism has a smaller memory footprint than the enclosing parallelotope. One advantage of that embodiment is that it provides data layouts that have smaller memory requirements for a class of accessed datasets for which methods based on parallelotopes are not optimal.

For instance the dataset accessed by the program represented by the following pseudo code through reference B is triangular 

The embodiment finds three constraints that enclose the accessed data set in a similar way as in the embodiment depicted in using the Farkas lemma. The minimal volume for a parallelotope that encloses the dataset would be about twice the volume of the triangle. Hence using such a parallelotope to determine the memory allocation of the dataset is bound to be sub optimal. Instead the current embodiment depicted in defines a tighter enclosing polyhedron using three inequalities it is then a prism of triangular base . Using the enclosing prism the data set is partitioned in two subsets say A and B and subset A is re indexed in such a way that both the array elements in B and the re indexed elements are enclosed in a smaller parallelotope than the original parallelotope. The volume of the new parallelotope is about the volume of the prism of triangular base. Since there is a parallelotope of smaller volume enclosing the reindexed data set its memory requirements are smaller. The result is a piecewise affine array reindexing which typically partitions the loop iterations into the iterations that access A and the ones that access B.

In the current embodiment the three inequalities a aI a 0 b bI b 0 c cI c 0 that define the triangular prism P where I is the vector of data coordinates are used to devise the partitioning. Let xa point in the intersection of b and c and let w axI a. The prism is partitioned into A and B as follows 

The tightest enclosing triangle defined by 0 x 0 x x x 9 by comparison includes 55 elements which is about half the number of elements required for the enclosing parallelotope. Since the number of array elements in the enclosing triangle is less than the number of array elements in the enclosing parallelotope the embodiment considers the tightest enclosing triangle and partitions the enclosed data into data subsets A 0 x 5 x x x 9 and B 0 x 0 x 4 x x 9. Point

Many computers that contain processors that have an explicitly managed local memory also have the ability to transfer data at the same time as they are performing other computations. Such transfers are called asynchronous . The main reason for using that feature is that the typical time necessary for such transfers is often comparable to the time taken to perform computations between two consecutive transfers of input data. Since doing both transfer and computation at the same time takes less time than doing one after another the effect of overlapping them is to improve the overall program execution time. The use of several memory zones specialized to either execution reception or sending of data makes the overlap possible. Such a use is called multi buffering . The specialization of the buffers is also modified at certain times. Such a modification is called a rotation of the buffers since a buffer is cyclically assigned the same specialization.

One embodiment computes a local memory mapping adds a polyhedral representation of the communications and schedules communications and computations in a multi buffering scheme for the program represented by the following pseudo code. In this pseudo code every iteration of the k loop works on a distinct instance of local memory 

Illustrated in are computing apparatus and computer software products consistent with provided embodiments. Computing apparatus includes processor memory storage medium and in some embodiments input port and network interface . In many provided embodiments storage medium contains a set of processor executable instructions that when executed by processor configure computing apparatus to implement the modules and methods described herein. In one embodiment storage medium containing the set of processor executable instructions resides in another computing apparatus across network . In an embodiment of a computer software product computer software product is a computer readable storage medium containing processor executable instructions sufficient that when executed by processor configure computing apparatus to implement the above described modules and methods. Further computer software product in some embodiments consists of a physical medium configured to interface with input port to allow its contents to be copied to storage medium . In other embodiments computer software product is an internal storage medium such as . An additional embodiment of computing apparatus includes a plurality of processors a plurality of memories a storage medium and in some embodiments input port and network connection . In some embodiments one or more processors is a host while others are modeled in the form of a grid.

Other embodiments of the present invention provide a custom computing apparatus illustrated in that is configured to optimize computer source code for operation on a second computing apparatus. As illustrated first custom computing apparatus is configured to communicate with second computing apparatus across network . A further illustration of computing apparatus is provided in . In this illustration custom computing apparatus contains at least one processor a communication port communicating with the at least one processor . Custom computing apparatus additionally includes memory which in some embodiments includes dependence analysis module . Custom computing apparatus in some embodiments additionally includes drive configured to accept external storage medium . In some embodiments external storage medium is a CD in others a DVD. In these embodiments drive is configured to accept the appropriate external storage medium . While CD and DVD are specifically enumerated in these embodiments there are many external storage media that can be used to practice various aspects of the invention therefore some embodiments are not limited to the particular drive configuration or external media . Custom computing apparatus additionally includes storage medium . Storage medium in some embodiments is a hard disk drive and in others is a solid state drive. In some embodiments storage medium contains a set of processor executable instructions that when executed by the at least one processor configure custom computing apparatus to optimize computer code for execution on computing apparatus . While custom computing apparatus and computing apparatus are illustrated in communicating over network various embodiments of the invention do not require this inter computer communication.

Various embodiments of the present invention are directed to processors containing multi stage execution units and in some embodiments multiple execution units. By way of example and not limitation to the particular multi stage execution unit illustrates exemplary multi stage execution units . In one embodiment a 6 stage execution unit is utilized. In this embodiment the stages may include instruction fetch instruction decode operand address generation operand fetch instruction execute and result store. In another depicted multi stage architecture the stages include instruction fetch instruction fetch register decode execute memory access and register write back. During routine operation of a multi stage execution unit instructions are processed sequentially moving from stage to stage. In scheduling operations on multi stage execution unit processors there are inherent difficulties that arise. For example one instruction in one stage of the pipeline may attempt to read from a memory location while another instruction is writing to that location. This is problem is confounded in the instance of multiple processing cores. Additionally in multiple processor and or multiple core architectures the locality of data to the execution unit attempting access can create significant delays in processing.

A further illustration of a multiple execution unit system is depicted in . In this illustration a first execution unit Execution Unit is attempting to write to a specific memory location while a second execution unit Execution unit is attempting to read from that same location. When both read and write occur at the same time this causes a condition known in the art as a conflicting access which can significantly impact the speed and the correctness of execution. While it may appear that parallel execution of instructions across multiple execution units and or processors would produce an optimal result this is not always the case. Further as previously discussed optimization of source code for parallelism may result in code that is poor in terms of locality or communications. In the prior approaches to code optimization the converse is additionally true. Optimization of code for locality can result in poor parallelism and under utilization of computing resources. It is therefore an object of embodiments of the present invention to provide a customized computing apparatus methods and computer software product that simultaneously optimizes a computer program for execution on a particular computing device with multiple execution units. It is another object of the invention to provide embodiments of methods which can explore the complete solution space for legal schedules for potential solutions. It is a further object of the invention to provide methods containing new formulations that encode the tradeoffs between locality and parallelism directly in the constraints and the objective functions of an optimization problem. It is a further object of the invention to automatically generate conditional synchronizations between execution units at different levels in the hierarchy of multiple execution units.

A consequence of loop fusion is that memory locations a i and b i referenced by the former loops are now accessed in an interleaved fashion. In the former code memory locations were accessed in the order a a . . . a then b b . . . b . In the code comprising the fused loops the memory locations are now accessed in the order a b a b . . . a b . Loop fusion can lead to better locality when multiple loops access the same memory locations. It is common general knowledge in the field of compilers that better locality reduces the time a processing element must wait for the data resident in memory to be brought into a local memory such as a cache or a register. In the remainder of this document we shall say that loops are fused or equivalently that they are executed together when such a loop fusion transformation is applied to the received program to produce the optimized program.

Loop fusion can change the order in which memory locations of a program are accessed and require special care to preserve original program semantics 

The problem of parallelism extraction is related to the problem of loop fusion in the aspect of preserving original program semantics. A loop in a program can be executed in parallel if there are no dependences between its iterations. For example the first program loop below can be executed in parallel while the second loop must be executed in sequential order 

It is common knowledge in the field of high level compiler transformations that the problems of fusion and parallelism heavily influence each other. In some cases fusing 2 loops can force them to be executed sequentially.

Loop permutability is another important property of program optimizations. A set of nested loop is said permutable if their order in the loop nest can be interchanged without altering the semantics of the program. It is common knowledge in the field of high level compiler optimization that loop permutability also means the loops in the permutable set of loops dismiss the same set of dependences. It is also common knowledge that such dependences are forward only when the loops are permutable. This means the multi dimensional vector of the dependence distances has only non negative components. Consider the following set of loops 

Loop permutability is important because it allows loop tiling alternatively named loop blocking . Loop tiling is a transformation that changes the order of the iterations in the program and ensures all the iterations of a tile are executed before any iteration of the next tile. When tiling by sizes i 2 j 4 is applied to the previous code the result is 

Loop tiling is traditionally performed with respect to tiling hyperplanes. In this example the tiling hyperplanes used are the trivial i and j hyperplanes. In the general case any linearly independent combination of hyperplanes may be used for tiling provided it does not violate program semantics. For example i j and i 2 j could as well be used and the resulting program would be much more complex.

In reference to some embodiments the following terminology is used the doall and reduction indicate potential parallelism rather than actual usage of parallelism. In this case doall indicates that a loop may be executed in a data parallel manner while reduction indicates that a loop is a reduction i.e. its order of execution may be permuted. We additionally decorate loop by the comment perm if they can be legally permuted in the optimized program.

Another important loop transformation is loop skewing. It is common knowledge that loop permutability combined with loop skewing results in the production of parallelism. In the following permutable loops the inner loop can be executed in parallel after loop skewing 

Transformations such as loop permutations and loop skewing result in modified memory access order that can make accesses contiguous. For instance interchanging loop i and k assuming row major storage mode 

The memory accesses are now a a . . . a a . . . a . . . a . . . a and the latencies of accesses are generally reduced.

The problem of jointly optimizing parallelism and locality and contiguity of loop references by means of loop fusion parallelism loop permutability loop tiling and loop skewing is a non trivial tradeoff. It is one of the further objects of this invention to jointly optimize this tradeoff.

In certain situations this combined tradeoff has no good solutions because trying to enforce all contiguity constraints is unfeasible. Some provided embodiments therefore relax the constraints on contiguity and exploit the hierarchical structure of the memories and execution units by creating explicit copies of the data to a memory in which latency penalties arising from non contiguity are less expensive or inexistent. As simple illustration consider the case of a matrix multiplication code 

All accesses to a b and c cannot be made contiguous because the global problem is unfeasible. If k is chosen as the innermost loop accesses to a are contiguous but not accesses to b and c. If j is chosen as the innermost loop accesses to b and c are contiguous but not accesses to a.

It is one of the further objects of this invention to create explicit memory locations and data transfers into a secondary memory. One possible outcome after such a copy has been inserted for array a and loops j k have been permuted is the following 

In other embodiments the explicit copy from a into a l may also change the data layout in a l. It is a further object of this invention to modify the layout such that accesses to a l are contiguous too during the computation. A permutability of the jj and kk dimensions in the data layout makes the accesses to a l contiguous in the computation part of the program 

In other embodiments the execution model for the target hierarchy of execution units and memories requires explicit copying of the data in a secondary memory. It is a further object of the invention to comply with this requirement by creating explicit copies of the data to the secondary memory.

When considering high level loop transformations it is common practice to represent dependences in the form of affine relations. The first step is to assign to each statement in the program an iteration space and an iteration vector. Consider the program composed of the 2 loops below 

The iteration domain of the statement S is D i j in Z 1 i n 1 j n. The second step is to identify when two operations may be executed in parallel or when a producer consumer relationship prevents parallelism. This is done by identifying the set of dependences in the program. In this example the set of dependences is R i j i j i i j j 1 i j in D i j in D 

It is common practice to represent the dependence relations using a directed dependence graph whose nodes represent the statements in the program and whose edges represent the dependence relations. In the previous example the dependence graph has 1 node and 2 edges. It is common practice to decompose the dependence graph in strongly connected components. Usually strongly connected components represent loops whose semantics require them to be fused in the optimized code. There are many possible cases however and one of the objects of this invention is also to perform the selective tradeoff of which loops to fuse at which depth. It is common knowledge that a strongly connected component of a graph is a maximal set of nodes that can be reached from any node of the set when following the directed edges in the graph.

One embodiment incorporates fusion objectives into affine scheduling constraints. Affine fusion as used herein means not just merging two adjacent loop bodies together into the same loop nests but also include loop shifting loop scaling loop reversal loop interchange and loop skewing transformations. This corresponds to general affine scheduling functions that orchestrate the order of operations in the optimized program. In the convention this means that we would like to have the ability to modify the linear part of the schedule instead of just and . Previous fusion works are mostly concerned with adjusting the component fusion only and sometimes both the and components fusion with loop shifting . One embodiment of the invention produces a scheduling function used to assign a partial execution order between the iterations of the operations of the optimized program and to produce the resulting optimized code respecting this partial order.

As a simple motivational example demonstrating the power of affine fusion consider the example above. Dependencies between the loop nests prevents the loops from being fused directly unless loop shifting is used to peel extra iterations oft the first and second loops. The resulting transformation is shown below.

On the other hand affine fusion gives a superior transformation as shown above. In this transformation the fusion preventing dependencies between the loop nests are broken with a loop reversal rather than loop shifting and as a result no prologue and epilogue code is required. Furthermore the two resulting loop nests are permutable. Thus we can further apply tiling and extract one degree of parallelism out of the resulting loop nests.

Many prior art algorithms cannot find this transformation if their restrictions are applied. Some of the restrictions prune out the solution space based on loop reversals and thus these algorithms can only find the loop shifting based solutions. Another important criteria is that fusion should not be too greedy i.e. aggressive fusion that destroys parallelism should be avoided. On the other hand fusion that can substantially improve locality may sometimes be preferred over an extra degree of parallelism if we already have obtained sufficient degrees of parallelism to fill the hardware resources. For instance consider the combined matrix multiply example. This transformation is aggressive and it gives up an additional level of synchronization free parallelism that may be important on some highly parallel architectures. It is a further object of this invention to model the tradeoff between benefits of locality and parallelism for different hardware configurations.

The code below shows the result of only applying non parallelism destroying fusion. The two inner i loops are fissioned in this transformation allowing a second level of synchronization free parallelism.

The tension between fusion and scheduling implies that fusion and scheduling should be solved in a unified manner. For any loop p we compute a cost which measures the slowdown in execution if the loop is executed sequentially rather than in parallel. Similarly for each pair of loop nests p q we estimate upq the cost in performance if the two loops p and q remains unfused. The cost can be interpreted to be the difference between sequential and parallel execution times and the cost upq can be interpreted as the savings due to cache or communication based locality. In one embodiment the cost is related to a difference in execution speed between sequential operations of the at least one loop on a single execution unit in the second computing apparatus and parallel operations of the at least one loop on more than one of the at least two execution units in the second computing apparatus. In another embodiment the cost upq is related to a difference in execution speed between operations where the pair of loops are executed together on the second computing apparatus and where the pair of loops are not executed together on the second computing apparatus.

In an illustrative example let the Boolean variable denote whether the loop p is executed in sequence and let the variable fpq denote whether the two loops p and q remain unfused i.e. 0 means that p is executed in parallel and fpq 0 means that edge loops p and q have been fused. Then by minimizing the weighted sum

In some embodiment the value of the cost wis determined by a static evaluation of a model of the execution cost of the instructions in the loop. In another embodiment the value of the cost wis determined through the cost of a dynamic execution on the second computing apparatus of at least a set of instructions representative of the code in the loop. In a further embodiment the value of the cost wis determined by an iterative process consisting of at least one static evaluation of a model of the execution cost and at least one dynamic execution on the second computing apparatus of at least a set of instructions representative of the code in the loop.

In some embodiment the value of the cost uis determined by a static evaluation of a model of the execution cost of the instructions in the loop pair. In another embodiment the value of the cost uis determined through the cost of a dynamic execution on the second computing apparatus of at least a set of instructions representative of the code in the loop pair. In a further embodiment the value of the cost uis determined by an iterative process consisting of at least one static evaluation of a model of the execution cost and at least one dynamic execution on the second computing apparatus of at least a set of instructions representative of the code in the loop pair.

The optimization can be formulated as follows. In one embodiment we divide up the generalized dependence graph GDG G V E into strongly connected components SCCs and consider each SCC to be a separate fusible loop candidate. Let G V E denote the SCC induced subgraph where V denotes the SCCs and E the edges between SCCs. Given a node v V let sec v denote the component in which v belongs to in the SCC decomposition. Given p q E E let the Boolean variables fdenote whether two SCCs has been fused i.e. f 0 denotes that the loops corresponding to p and q have been fused. f0 1 p q E 5 f 0loops p and q are fused 6 

There are multiple possible strategies to encode the restrictions implied by E . In one embodiment we directly encode the transitivity relation E as constraints i.e. i given edges p q and q r and p q if loops p q or q is not fused then p r cannot be fused and ii if p q and q r are fused then p q must be fused f f f p q q r p r E 7 f f f p q q r p r E 8 

One potential deficiency of this strategy is that up to O V constraints are required. In the second embodiment we adopt involves the encoding of the schedule coordinates directly in the constraints. In this encoding implies that loops p and q have been fused 0 V 1 p V 9 10 11 

Given the constraints on fin place we can now provide a suitable modification to the schedule constraints. The constraints are divided into two types the first involves edges within the same SCC and the second involves edges crossing different SCCs 0 12 

Here the term Nf y is defined in such a way that Nf y 0 when f 0 and is equal to a sufficiently large negative function when f. Thus j y i y 0 only needs to hold only if the edge e has been fused or is a loop carried edge. The final set of constraints is to enforce the restriction that y y if p q has been fused. The constraints encoding this are as follows 0 15 0 16 0 17 

Some embodiments additionally specify that a schedule dimension at a given depth must be linearly independent from all schedule dimensions already computed. Such an embodiment computes the linear algebraic kernel of the schedule dimensions found so far. In such an embodiment for a given statement S h denotes the linear part of S the set of schedule dimensions already found and J denotes a subspace linearly independent of h. A further embodiment derives a set of linear independence constraints that represent Jh 0 and does not restrict the search to Jh 0. Such linear independence constraints may be used to ensure successive schedule dimensions are linearly independent. In particular such an embodiment that does not restrict the search to Jh 0 exhibits an optimization process that can reach any legal multidimensional affine scheduling of the received program including combinations of loop reversal.

In some embodiments the set of conditions preserving semantics is the union of all the constraints of the form j y i y 0. In another embodiment the optimizing search space that encompasses all opportunities in parallelism and locality is the conjunction of all the constraints 5 17 .

In further embodiments the set of affine constraints 12 and 13 is linearized using the affine form of Farkas lemma and is based on at least one strongly connected component of the generalized dependence graph.

In other embodiments the constraints of the form 12 are used to enforce dimensions of schedules of loops belonging to the same strongly connected component are permutable.

In further embodiments the constraints of the form 13 are used to ensure that dimensions of schedules of loops that are not executed together in the optimized program do not influence each other. In such embodiments the constraints of the form 13 use a large enough constant to ensure that dimensions of schedules of loops that are not executed together in the optimized program do not influence each other.

The embodiments described so far depend on a term or multiple terms y which bounds the maximal dependence distance. Another embodiment may opt for the following simpler formulation. First we assign each SCC p in the GDG a Boolean variable where 0 means a dependence distance of zero i.e. parallel and 1 means some non zero dependence distance 0 1 p V 18 Define the functions p y and pq y as 1 19 1 20 Then the affine fusion constraints can be rephrased as follows 0 21 

Affine fusion formulation is a depth by depth optimization embodiment. A further embodiment described in B C A and B shows a method to derive scheduling functions for a given hardware parallelism and memory hierarchy. A further embodiment described in shows a method to derive scheduling functions for multiple levels of hardware parallelism and memory hierarchies more specifically by formulating and optimizing at least one global weighted parametric function for each level of the parallelism and memory hierarchy of the second computing apparatus. In a further embodiment it is a further object of the invention to build a single multi dimensional affine fusion formulation as an alternative or as a supplement to the depth by depth affine fusion formulation. The single multi dimensional fusion formulation relies on a single multi dimensional convex affine space. More specifically an embodiment of such a single multi dimensional convex affine space assigns variables and relations for loops loops pairs and dependence edges e at each scheduling dimension k.

In some embodiment the strong satisfaction variable E k e assigned to each schedule dimension k and each edge e of the at least one strongly connected component is which is equal to 1 when the schedule difference at dimension k strictly satisfies edge e i.e. when i y j y 1 e E 0 otherwise. In other embodiments the loop permutability Boolean variable p k e assigned to each schedule dimension and each edge e of the at least one strongly connected component is p.

In a further embodiment the statement permutability Boolean variable p k a assigned to each schedule dimension and each statement a of the at least one strongly connected component is p. In another embodiment constraints of the form 27 28 and 29 are added to ensure dimensions of schedules of statements linked by a dependence edge in the generalized dependence graph do not influence each other at depth k if the dependence has been strongly satisfied up to depth k 1. In a further embodiment constraints of the form 30 and 31 are added to link the strong satisfiability variables to the corresponding loop permutability Boolean variables. In another embodiment constraints of the form 34 to 43 are added to ensure statement permutability Boolean variables are equal for all the statements in the same loop nest in the optimized program. In a further embodiment the conjunction of the previous constraints forms a single multi dimensional convex affine search space of all legal multi dimensional schedules that can be traversed exhaustively or using a speeding heuristic to search for schedules to optimize any global cost function.

One example of an embodiment tailored for successive parallelism and locality optimizations is provided for an architecture with coarse grained parallel processors each of them featuring fine grained parallel execution units such as SIMD vectors. One such architecture is the Intel Pentium E 5300. The following example illustrates how an embodiment of the invention computes schedules used to devise multi level tiling hyperplanes and how a further embodiment of the invention may compute different schedules for different levels of the parallelism and memory hierarchy of the second computing apparatus. Consider the following code representing a 3 dimensional Jacobi iteration stencil. In a first loop the array elements A i j k are computed by a weighted sum of the 7 elements B i j k B i 1 j k B i 1 j k B i j 1 k B i j 1 k B i j k 1 and B i j k 1 . In a symmetrical second loop the array elements B i j k are computed by a weighted sum of 7 elements of A. The computation is iterated Titer times.

When computing a schedule for the first level of parallelism the multiple cores embodiments may produce the following optimized code in which permutable loops are marked as such.

The innermost doall dimensions may further be exploited to produce vector like instructions while the outermost permutable loops may be skewed to produce multiple dimensions of coarse grained parallelism.

In a further embodiment the schedules that produce the innermost doall dimensions may be further used to produce another level of multi level tiling hyperplanes. The resulting code may have the following structure 

In the following example dependencies between the loop nests prevent the loops from being fused directly unless loop shifting is used to peel extra iterations off the first and second loops. The resulting transformation is illustrated in the code below.

On the other hand affine fusion i.e. fusion combined with other affine transformations gives a superior transformation as shown below. In this transformation the fusion preventing dependencies between the loop nests are broken with a loop reversal rather than loop shifting and as a result no prologue or epilogue code is required. Furthermore the two resulting loop nests are permutable. In some embodiments tiling and extraction of one degree of parallelism out of the resulting loop nests is performed.

In some embodiments loop fusion is limited to not be too greedy i.e. aggressive fusion that destroys parallelism should be avoided. On the other hand fusion that can substantially improve locality may sometimes be preferred over an extra degree of parallelism if we already have obtained sufficient degrees of parallelism to exploit the hardware resources. For example given the following code 

If fusion is applied too aggressively it gives up an additional level of synchronization free parallelism.

The below code illustrates the result of only applying fusion that does not destroy parallelism. The two inner j loops are fissioned in this transformation exposing a second level of synchronization free parallelism.

The above illustrates that this tension between fusion and scheduling implies that fusion and scheduling should be solved in a unified manner. Turning now to where the flow of provided method of source code optimization is illustrated. Flow begins in block where source code is received in memory on a custom first computing apparatus . Flow continues to block where a selective tradeoff of parallelism and locality is created for execution of the code on the second computing apparatus . Flow then continues to block where a scheduling function is produced which optimizes the selective tradeoff. Flow then continues to block where the scheduling function is used to assign a partial order to the statements of the source code and an optimized program is produced for execution on the second computing apparatus . In one embodiment the received program code contains at least one arbitrary loop nest. As previously discussed the custom first computing apparatus contains memory a storage medium and at least one processor with a multi stage execution unit.

A provided method for source code optimization is illustrated in . In this embodiment flow begins in block where source code is received in memory on a custom first computing apparatus . Flow continues to block where the code is optimized in terms of both locality and parallelism for execution on a second computing apparatus . In this embodiment the optimization block additionally includes additional functional blocks. Within block flow begins with block where an unassigned loop is identified. Flow then continues on two paths. In a first path flow continues to block where a first cost function is assigned in block . This first cost function is related to a difference in execution speed between parallel and sequential operations of the statements within the loop on second computing apparatus . Flow then continues to block where a decision variable is assigned to the loop under consideration this decision variable indicating whether the loop is to be executed in parallel in the optimized program. In some embodiments the cost is determined through static evaluation of a model of the execution cost of the instructions in the loop under consideration. In other embodiments the cost is determined through a dynamic execution on the second computing apparatus of at least a set of instructions representative of the code in the loop under consideration. In a further embodiment the cost is determined by an iterative refining process consisting of at least one static evaluation of a model of the execution cost and at least one dynamic execution on the second computing apparatus of at least a set of instructions representative of the code in the loop under consideration. Flow then continues to decision block where it is determined if there are additional unassigned loops.

As used herein executed together means fused in the sense of the code examples 0032 0037 . Specifically executed together means that loops that are consecutive in the original program become interleaved in the optimized program. In particular loops that are not executed together in the sense of loop fusion can be executed together on the same processor in the more general sense. In the second optimization path illustrated in flow continues from block to block where an unassigned loop pair is identified. Flow then continues to block where a second cost function is assigned for locality optimization. This second cost function is related to a difference in execution speed between operations where the loops in the pair of loops are executed together on the second computing apparatus and where the loops in the pair of loops are not executed together on the second computing apparatus. Flow then continues to block where a decision variable is assigned for locality. This second decision variable specifying if the loops in the loop pair under consideration are to be executed together in the optimized program. In one embodiment the second cost is determined through static evaluation of a model of the execution cost of the instructions in the at least one loop pair.

In another embodiment the second cost is determined through of a dynamic execution on the second computing apparatus of at least a set of instructions representative of the code in the at least one loop pair. In a further embodiment the cost is determined through an iterative refining process consisting of at least one static evaluation of a model of the execution cost and at least one dynamic execution on the second computing apparatus of at least a set of instructions representative of the code in the at least one loop pair. Flow then continues to decision block where it is determined if additional unassigned loop pairs exist. If additional unassigned loop pairs exist flow continues back to block and the process iterates until no additional unassigned loop pairs are found. When decision block determines no additional loop pairs are present flow continues to decision block . If in decision block it is determined that additional unassigned loops exist flow continues back to block and the process iterates until no additional unassigned loops may be identified. Flow then continues to block where a selective tradeoff is created for locality and parallelism during the execution on second computing apparatus . Flow then continues to block where a scheduling function is produced that optimizes the selective tradeoff. Flow then continues to block where optimized code is produced.

The flow of a further provided embodiment of a method for source code optimization is illustrated in . In this embodiment flow begins in block where source code is received in memory on a custom first computing apparatus . Flow continues to block where the code is optimized in terms of both locality and parallelism for execution on a second computing apparatus . Flow then continues to block where a scheduling function is produced that optimizes the tradeoff. In this embodiment the scheduling function block additionally includes additional functional blocks. Within block flow continues to block where the conditions for semantic correctness of the program are determined. Flow then continues to block where a search space is derived that meet the conditions for semantic correctness. In one embodiment the search space characterizes all parallelism and locality opportunities that meet the conditions of semantic correctness. Flow then continues to block where the selective trade off is optimized. Flow then continues to block where the scheduling function is derived from the optimized tradeoff. Flow then continues to block where optimized code is produced.

The flow of a further provided method is illustrated in . This embodiment illustrates alternate embodiments of the flow within blocks and in previous embodiments. As illustrated flow begins in block where the conditions for semantic correctness of the program are determined. Flow then continues to block where a search space is derived that meet the conditions for semantic correctness. In one embodiment the search space characterizes all parallelism and locality opportunities that meet the conditions of semantic correctness. Like previous embodiments flow then continues to block where the selective trade off is optimized. In these embodiments block includes additional functionality. Block as illustrated contains three independent optimization paths that may be present in any given embodiment. In the first embodiment flow begins at block where an element is selected from the search space. Flow then continues to block where a potential scheduling function is derived for the element. Flow then continues to block where the performance of the potential scheduling function is evaluated. Flow then continues to decision block where it is determined if additional elements exist in the search space. If additional elements exist flow continues back to block . When no additional elements exist in the search space flow then continues to block where the element with the best evaluated performance is selected.

In the second illustrated embodiment flow continues from block to block where an element is selected from the search space. Flow continues to block where a potential scheduling function is derived for the element. Flow then continues to block where the performance of the potential scheduling function is evaluated. Flow then continues to block where the search space is refined using the performance of evaluated schedules. Flow then continues to decision block where it is determined if additional elements exist in the search space. If additional elements are present flow continues back to block and the process iterated until no other elements exist in the search space. When no additional elements exist in the search space flow then continues to block where the element with the best evaluated performance is selected.

In the third illustrated embodiment flow continues from block to block where the tradeoff is directly optimized in the search space with a mathematical problem solver. Flow then continues to block where an element is selected that is a result of the direct optimization. Flow then continues to block there the performance of the selected element is evaluated. Flow then continues to block where the element with the best evaluated performance is selected. As illustrated some embodiments may utilize more than one of these paths in arriving at an optimal solution. From selection block flow then continues to block where the scheduling function is derived from the optimized tradeoff. Flow then continues to block where optimized code is produced.

The flow of a further provided embodiment of a method for optimization of source code on a first custom computing apparatus for execution on a second computing apparatus is illustrated in . In this embodiment flow begins in block where source code is received in memory on a custom first computing apparatus . Flow continues to block where the source code is optimized in terms of both locality and parallelism for execution on a second computing apparatus . In this embodiment block contains additional functional blocks. Flow continues from block to block where the conditions for semantic correctness are determined from the received code. Flow then continues to block where these conditions are represented as a generalized dependence graph. Flow then continues to two paths.

On a first path flow continues to block where a search space is derived that meet the conditions for semantic correctness. In this embodiment the search space characterizes all parallelism and locality opportunities that meet the conditions of semantic correctness. Flow then continues to block where a weighted parametric tradeoff is derived and optimized on the elements of the search space. On the second path flow begins with block where an unassigned loop is identified. Flow then continues on two additional paths. In a first path flow continues to block where a first cost function is assigned in block . This first cost function is related to a difference in execution speed between parallel and sequential operations of the statements within the unidentified loop on second computing apparatus . Flow then continues to block where a decision variable is assigned to the loop under consideration this decision variable indicating whether the loop is to be executed in parallel in the optimized program. In some embodiments the cost is determined through static evaluation of a model of the execution cost of the instructions in the loop under consideration. In other embodiments the cost is determined through a dynamic execution on the second computing apparatus of at least a set of instructions representative of the code in the loop under consideration. In a further embodiment the cost is determined by an iterative refining process consisting of at least one static evaluation of a model of the execution cost and at least one dynamic execution on the second computing apparatus of at least a set of instructions representative of the code in the loop under consideration. Flow then continues to decision block where it is determined if there are additional unassigned loops.

Returning to block where an unassigned loop is identified. On the second path flow continues to block where an unassigned loop pair is identified. Flow then continues to block where a second cost function is assigned for locality optimization. This second cost function is related to a difference in execution speed between operations where the loops of the pair of loops are executed together on the second computing apparatus and where the loops of the pair of loops are not executed together on the second computing apparatus. Flow then continues to block where a decision variable is assigned for locality. This second decision variable specifying if the loops of the loop pair under consideration is to be executed together in the optimized program. In one embodiment the second cost is determined through static evaluation of a model of the execution cost of the instructions in the at least one loop pair. In another embodiment the second cost is determined through of a dynamic execution on the second computing apparatus of at least a set of instructions representative of the code in the at least one loop pair. In a further embodiment the cost is determined through an iterative refining process consisting of at least one static evaluation of a model of the execution cost and at least one dynamic execution on the second computing apparatus of at least a set of instructions representative of the code in the at least one loop pair. Flow then continues to decision block where it is determined if additional unassigned loop pairs exist. If additional unassigned loop pairs exist flow continues back to block and the process iterates until no additional unassigned loop pairs are found. When decision block determines no additional loop pairs are present flow continues to decision block . If in decision block it is determined that additional unassigned loops exist flow continues back to block and the process iterates until no additional unassigned loops may be identified. Flow then continues to block where a selective trade off is created for locality and parallelism during the execution on second computing apparatus .

In this embodiment flow then continues to block where as discussed a weighted parametric tradeoff is derived and optimized on the elements of the search space. Flow then continues to block where a multi dimensional piecewise affine scheduling function is derived that optimizes the code for execution on second computing apparatus . Flow then continues to block where the optimized program is produced.

The operational flow of a further provided method for source code optimization is illustrated in . In this embodiment flow begins in block where source code is received in memory on a custom first computing apparatus . Flow continues to block where the a level of parallelism and memory hierarchy in the second computing apparatus are selected. Flow then continues to block where a selective tradeoff for parallelism and locality for execution of that level of hierarchy is created. Flow then continues to block where a piecewise affine multi dimensional scheduling function is derived that optimizes the specific tradeoff. Flow then continues to block where tiling hyper planes are produced based on the scheduling function. Flow then continues to decision block where it is determined if additional levels of parallelism and memory hierarchy exist on second computing apparatus . If additional levels exist flow continues back to block and the process iterates until it is determined that no additional levels exist. Flow then continues to block where the scheduling functions and tiling hyper planes are used to assign a partial order to the statements of the source code and an optimized program is produced. In some embodiments a global weighted parametric function is used to optimize each level of parallelism and hierarchy on second computing apparatus .

The operational flow of a further provided method for source code optimization is illustrated in . In this embodiment flow begins in block where source code is received in memory on a custom first computing apparatus . Flow continues to block where the conditions for semantic correctness are determined for the program. Flow then continues to block where these conditions are represented as a generalized dependence graph. Alternatively as indicated in block schedule dimensions may have been found through the methods disclosed in other embodiments. Flow continues to block where the generalized dependence graph is decomposed into at least one strongly connected component. Flow then continues to block where a strongly connected component is selected. Flow then continues to a number of independent paths.

In the first path flow continues to block where a set of affine constraints are derived using the affine form of Farkas lemma. On the second path flow continues to block where linear independence constraints are derived and used to ensure the successive scheduling dimensions are linearly independent. In some embodiment these linear independence constraints are derived using orthogonally independent subspaces. In another embodiment these constraints are formed using a Hermite Normal form decomposition. In the third path flow continues to block where a set of schedule difference constraints are derived and used to enforce dimensions of schedules of loops belonging to the same strongly connected component are permutable. In the last path a set of loop independence constraints are derived and used to ensure that dimensions of schedules of loops that are not executed together do not influence each other. In one embodiment this set of constraints includes a large enough constraint to cancel an effect of constraints on statements that are not executed together in the optimized program.

Flow then continues to block where these derived constraints are added to the search space. Flow then continues to decision block where it is determined if there are additional strongly connected components. If there are additional strongly connected components flow continues back to block and the process iterates until there are no further strongly connected components. Flow then continues to block where a search space is derived that characterizes all parallelism and locality opportunities that meet the conditions of semantic correctness. Flow then proceeds to block where a weighted parametric tradeoff is optimized on the elements of the search space. Flow continues to block where a multi dimensional piecewise affine scheduling function is derived from the optimization and to block where this function is used to create an optimized program for execution on second computing apparatus . In one embodiment the optimization can reach any legal dimensional affine scheduling of the received program. In another embodiment the legal multi dimensional affine scheduling of the received program includes loop reversals.

The operational flow of a further provided method for source code optimization is illustrated in . As with other embodiments this embodiment may be used in conjunction with other provided methods. In this embodiment flow begins in block where source code is received in memory on a custom first computing apparatus . Flow continues to block which contains additional functionality. Flow continues to block where the conditions for semantic correctness are determined for the program. Flow then continues to block where these conditions are represented as a generalized dependence graph. Flow then continues to decision block where it is determined if there are additional dimensions to schedule. If there are no additional dimensions flow continues to block where a scheduling function is derived and to block where an optimized program is produced for second computing apparatus .

If at decision block determines that there are additional scheduling dimensions flow continues to block where the generalized dependence graph is decomposed into at least one strongly connected component. Flow continues to block where a strongly connected component is selected. Flow then continues to block where affine constraints are derived using the affine form of Farkas lemma linear independence constraints permutability constraints and independence constraints are derived as previously discussed. Flow then continues to block where these constraints are added to the search space. Flow then continues to decision block where it is determined if additional strongly connected components exits. If others exist flow continues back to and the process iterates until there are no remaining strongly connected components.

When decision block indicates that there are no remaining strongly connected components flow continues to block where a weighted parametric tradeoff function is optimized on the search space. Flow then continues to decision block where it is determined if new independent permutable schedule dimensions exist. If they exist flow continues to block where an existing scheduling dimension is selected. Flow continues to block where additional constraints are added to the search space for independence and linear independence. From block flow continues to block where a weighted parametric tradeoff function is optimized on the search space. Flow then continues back to decision block and this part of the process iterates until no new independent permutable schedule dimensions are found. Flow then continues to block where satisfied edges are removed from the dependence graph and to block where the remaining edges and nodes are partitioned into smaller dependence graphs. Flow then continues back to block and the process is iterated on these smaller dependence graphs until decision block determines there are no additional dimensions to schedule.

The flow of a further provided embodiment of a method for optimization of source code on a first custom computing apparatus for execution on a second computing apparatus is illustrated in . In this embodiment flow begins in block where source code is received in memory on a custom first computing apparatus . On a first path flow continues to block where a selective tradeoff of parallelism and locality for execution of the program on second computing apparatus is created. Flow continues to block where the conditions for semantic correctness are determined. Flow continues to block where a single multi dimensional convex space of all legal schedules is derived. Additional information on block is provided in . Like some previous embodiments flow then continues on alternate three paths. On the first path flow continues to block where a element from the search space is selected. Flow then continues to block where a scheduling function is derived for the selected element. Flow then continues to block where the scheduling function is evaluated for its performance on the optimized program. Flow continues to decision block . If it is determined that there are additional elements in the search space flow continues back to block where another element is selected. The process iterates until no additional elements remain in the search space.

On the second path flow continues to block where an element of the search space is selected. Flow then continues to block where a scheduling function is derived for the selected element. Flow then continues to block where the performance of the scheduling function is evaluated. Flow then continues to block where the search space is refined using the performance of evaluated schedules. Flow then continues to decision block . If there are additional elements remaining in the search space flow continues back to block and another element is selected from the search space. The process iterates until there are no remaining elements in the search space.

On the third path flow continues to block where the selective tradeoff is directly optimized using a mathematical solver. Flow then continues to block where an element is selected from the search space that is a solution to the optimization. Flow then continues to block where the performance of the selected element is evaluated. Flow then continues to block which selects the element with the best evaluated performance for all of its inputs. Flow then continues to block which produces a scheduling function from the selective tradeoff and the selected element. Flow then continues to block where the scheduling function is used to assign a partial order to the statements of the source code and an optimized program is produced.

An exemplary embodiment of block is illustrated in . In this embodiment flow from block continues to block where the conditions for semantic correctness are represented as a generalized dependence graph. Flow continues on two parallel paths. On the first path an edge E is selected from the dependence graph in block . Flow then continues to block where a strong satisfaction variable is assigned to edge E at dimension K. Block receives the current dimension K from block . Flow then continues to block where multi dimensional constraints are derived to ensure independence of the nodes linked by edge E if scheduling is satisfied before dimension K. Flow then continues to decision block . If there are additional edges in the dependence graph flow continues back to block where another edge is selected and the process iterates until no additional edges exist.

On the second path flow continues from block to block where a node N is selected. Flow continues to block where a statement permutability variable is assigned to node N at dimension K. Block receives dimension K from block . Flow continues to decision block . If there are remaining nodes in the dependence graph flow continues back to block where another node N is selected. The process iterates until no additional nodes exist in the graph. Block receives input from blocks and and assigns constraints to link edge permutability variable and statement permutability variable at dimension K. Flow then continues to block where constraints to equate statement permutability variables for source and sink of edge E at dimension K are assigned. Flow then continues to decision block . If additional scheduling dimensions exist flow continues back to block the next scheduling dimension is selected and the entire process repeated for all dimensions. When all dimensions have been scheduled flow continues to block where a single multi dimensional convex affine space is constructed from all of the legal schedules.

The flow of another provided method for program code optimization is illustrated in . In this method flow begins in block where program source code is received in memory on a custom first computing apparatus . Flow continues to block where a level of parallelism and memory hierarchy is selected from the second computing apparatus . Flow then continues to block which is illustrated in and discussed in detail above. Flow then continues to decision block . If the performance of the scheduling function is not satisfactory for the current level flow continues to block where a partial evaluation of the code produced for the current level of parallelism and memory hierarchy is performed and used to iteratively refine the schedule. Flow continues back to block a iterates until the performance of the schedule is satisfactory for the level. Flow then continues to block where tiling hyper planes are produced based on the scheduling function. Flow then continues to decision block . If there are additional levels of parallelism and memory hierarchy flow continues back to block and the process iterates. Once no additional levels exist flow continues to block where the scheduling functions and tiling hyper planes are used to assign a partial order to the statements of the source code and an optimized program is produced.

In further embodiments a selective tradeoff of the cost of parallelism locality and contiguity of memory references is provided. In one embodiment a memory reference to an array A in the program is denoted by . When a statement accessing the reference has d enclosing loops and the array referenced by has r dimensions is an r d matrix with r rows and d columns. In the most general formulation we express contiguity along any dimension of access to array A. Let 0

A further embodiment of a provided method is illustrated in . In this embodiment flow begins at block where a program source code is received in a memory on a first computing apparatus. Flow then continues to block where a selective tradeoff between parallelism locality and contiguity of memory references is created for execution on a second computing apparatus. In block a scheduling function is products that optimizes the tradeoff. Flow continues to block where the scheduling function is used to assign a partial order to the statements of the source code and an optimized program is produced. In some embodiments where the second computing apparatus contains multiple execution units flow may continue to block where conditional synchronizations between a plurality of execution units are inserted into the code.

In further embodiments we also require that a loop accessing a contiguous reference must also be parallel. In which case we write the additional constraints pfor all loops p and all references r accessed by loop p.

A further embodiment applies multiple optimizations using successive parallelism locality and contiguity optimizations. In one embodiment each tradeoff in the plurality of tradeoffs is optimized using different costs corresponding to the costs of parallelism locality and contiguity for the particular level of the hardware hierarchy. Example is provided for architectures with a hierarchy of coarse grained parallel execution units each of them featuring fine grained parallel execution units. One such architecture is the Intel Pentium E 5300 another example of such architecture is the NVIDIA Geforce GTX 280 Graphics Processing Unit GPU and a further example of such architecture is any custom configuration of traditional x 86 processors with GPUs attached to them. The following example illustrates how an embodiment of the invention computes schedules used to devise multi level tiling hyperplanes and how a further embodiment of the invention computes different schedules for different levels of the parallelism and memory hierarchy of the second computing apparatus.

The flow of additional source code optimization embodiments are illustrated in . In this illustration flow begins at block where source code is received in memory on a first computing apparatus. Flow then continues to block where a selective tradeoff between parallelism locality and contiguity of memory references is created for execution on a second computing apparatus. In this illustration block is expanded to illustrate various embodiments. In some embodiments flow continues to block where an unassigned loop is identified in the code. Flow then continues along three paths. In the first path flow continues to block where a first cost function is assigned. In this illustration the first cost function is related to the cost of parallel execution on the second computing apparatus. Flow then continues to block where a decision variable for parallelism is assigned then to conditional block .

In the second path a memory reference is identified in block and flow continues to block where a third cost is assigned. In some embodiments the third cost is related to the cost of contiguous storage and access of the memory reference. Flow then continues to block where a decision variable is assigned to this cost then on to decision block where it is determined if additional memory references are present within the loop under consideration. If additional memory references exist flow continues back to block and the process iterates until decision block determines that no additional memory references are present. Flow then continues to decision block .

In the third path flow begins at block where an unassigned loop pair is identified. Flow then continues to block where a second cost function is assigned. In some embodiments this second cost function is related to locality. Flow then continues to block where a locality decision variable is assigned to the loop pair and on to decision block where a determination is made on the existence of additional unassigned loop pairs. If additional unassigned loop pairs are present flow continues back to block and the process iterates until no additional loop pairs are found. Flow then continues to block where it is determined if additional unassigned loops are present in the code. If so flow continues back to block and the process iterates until no unassigned loops are present. Flow then continues to block where the selective tradeoffs are formulated for the execution of the code on the second computing apparatus. Flow then continues to block where a scheduling function is produced which optimizes the selective tradeoff and to block where an optimized program is produced.

Various provided embodiments of method are illustrated in . In these embodiments flow begins in block where program source code is received in a memory on a first computing apparatus. In block the hierarchical architecture of a second computing apparatus is provided to the first computing apparatus or in some embodiments derived by the first computing apparatus. Flow continues to block where the next level of computing hierarchy of the second computing apparatus is identified. Flow then continues to block where scheduling coefficients and constraints are fixed for the prior level of hierarchy under consideration. Flow then continues to decision block where it is determined if the architecture level under consideration contains execution units. If no execution units are present at the level under consideration flow continues to block where it is determined if the architecture at that level can benefit from contiguity. If so flow continues to block where scheduling is performed using fine grained tradeoff and memory contiguity. If not flow continues to block where scheduling is performed using a coarse grained tradeoff. Flow then continues to block where tiling is performed. At decision block it is determined if the architecture contains faster memory than the one currently targeted at the level under consideration. If so memory promotion and explicit copy operations are performed in block . If not flow continued to block where conditional synchronizations are generated. Flow then continues back to block and iterates through the remaining levels of hierarchy.

Returning to decision block if it is determined that the level under consideration contains execution units flow continues to block where it is determined whether the current level under consideration benefits from contiguity. If so flow continues to block where scheduling is performed using fine grained tradeoff and memory contiguity. If not flow continues to block where scheduling is performed using fine grained tradeoff only. Flow then continues to block where unimodular re indexing for memory contiguity is performed. Flow then continues to block where placement of code onto the virtual execution threads is performed. Flow then continues to decision block where it is determined if the level under consideration contains private memory. If so some embodiments privatize the tasks and associated data in block and flow continues to block . If the level under consideration does not contain private memory flow continues directly to block where conditional synchronizations are inserted. Flow then continues to block where unroll and jam operations are performed then to decision block where it is determined if the architecture on the second computing apparatus contains additional levels. If there are additional levels of hierarchy flow continues back to block and iterates until all levels have been considered.

For example consider the following code representing a 3 dimensional Jacobi iteration stencil. In a first loop the array elements A i j k are computed by a weighted sum of the 7 elements B i j k B i 1 j k B i 1 j k B i j 1 k B i j 1 k B i j k 1 and B i j k 1 . In a symmetrical second loop the array elements B i j k are computed by a weighted sum of 7 elements of A. The computation is iterated Titer times.

When computing a schedule for the first level of parallelism the multiple cores embodiments may produce the following optimized code in which permutable loops are marked as such.

In this form the loops have been fused at the innermost level on loop l and the coarse grained parallelism and locality are optimized by a first scheduling function that orchestrates the operations in the program as coarse grained tasks of parallel execution. More specifically the schedule represented by the following components correspond to one possible outcome of provided embodiments that generate the code above after the first optimization for statement S i j 2 i k 2 i l 0 0 0 0 0 and 0 0 0 0 for statements S i j 2 i k 2i l 0 0 0 0 1 and 0 1 1 1 .

Loop tiling by tiling factors 16 8 8 1 may be applied to further improve locality and the program would have the following form where the inner loops m n o are permutable.

Without further optimization the loops are fused on all loops i j k l m n and o. In this form the program does not take advantage of fine grained parallelism and contiguity on each fine grained parallel execution unit along the loops m n and o. More specifically the following components correspond to one possible outcome of provided embodiments that generate the code above after tiling optimization for statement S i j k l m 2 m n 2 m o 0 0 0 0 0 0 0 0 and 0 0 0 0 0 0 0 for statements S i j k l m 2 m n 2 m o 0 0 0 0 0 0 0 1 and 0 0 0 0 1 1 1 . It is a further object to perform a second optimization of another selective tradeoff to express maximal innermost parallelism and contiguity at the expense of fusion. The selective tradeoff gives a much more important cost to parallelism than locality and in some aspects may find a different schedule for the intra tile loops that result in a program that may display the following pattern 

The innermost doall dimensions traverse memory with contiguous accesses. It is a further object to allow this successive optimizations. We do as follows using the provided generalized tradeoff of parallelism locality and contiguity formulation. In a first step one embodiment we may give a very low priority to contiguity constraints. In a further embodiment we may achieve this goal by setting all the costs of contiguity references to zero. In a second step of some embodiments tiling may be performed to create coarse grained tasks of execution. Performing tiling to create tasks of execution is a well known process to an engineer skilled in the art. In a third step of this embodiment we may give a very low priority to fusion constraints. In another embodiment we may achieve this goal by setting all the costs of fusion to zero. It is a further object to explicitly force the component values of the statements S and S to retain the values optimized during the first optimization on a certain number of dimensions. This corresponds to fixing schedule coefficients to enforce invariance of parallelism locality and contiguity of memory references across the plurality of selective tradeoffs. For statement S we may enforce i j k l 0 0 0 0 and 0 0 0 0 for statements S i j k l 0 0 0 0 and 0 0 0 0 where mean no additional constraint is enforced. After a second application of provided embodiments the following schedules may be generated for statement S i j k l m n o 0 0 0 0 0 0 0 0 and 0 0 0 0 0 0 0 for statements S i j k l m n o 0 0 0 0 1 0 0 0 and 0 0 0 0 0 0 0 .

In a further embodiment the schedules that produce the innermost doall dimensions may be further used to produce another level of multi level tiling hyperplanes. The engineer skilled in the field will appreciate multi level tiling will be useful to perform transformations such as register tiling. The resulting code may have the following structure 

It is a further object to represent the target architecture using a representative model and to exploit this model in the different steps of the optimization. Different embodiments may use different machine models for the same architecture and explore different tradeoffs based on features of the machine model. Machine models can be composed hierarchically and the smallest machine model granularity for which embodiments optimize a program is denoted as a morph. In some embodiments the machine model for a single GPU has one single morph and only represents the GPU level of the machine model. In one embodiment the GPU specific attributes for a GeForce 9800GX2 are 

Another provided embodiment of an optimization method is illustrated in . In this illustration flow begins at block where source code is received in a memory on a first computing apparatus. Flow then continue to block where a first selective tradeoff of parallelism locality and contiguity of memory references for execution of the code on a second computing apparatus is created. Flow then continues to block where a first scheduling function is produced that optimizes the first selective tradeoff. Flow then continues to block where the first scheduling function is used to assign a partial order to the statements of the source code and produce a coarse grained parallel optimized program. Flow then continues to block where tiling is performed. Flow then continues to block where scheduling coefficients and constraints are fixed for the previous levels in the hierarchy.

Flow then continues to block where a second selective tradeoff of parallelism locality and contiguity of memory references for execution of the code on the second computing apparatus is created. In block a second scheduling function is produced that optimizes the second tradeoff and flow continues to block where the second scheduling function is used to assign a partial order of statements in the code and produce a fine grained parallel optimized program. Flow then continues to block where the costs of memory access in the program are analyzed. In block explicit copies of memory accessed are inserted between primary and secondary memory memories for non contiguous access.

Flow then continues to block where a third selective tradeoff of parallelism locality and contiguity of memory references for execution of the code on the second computing apparatus is created. In block a third scheduling function is used to assign a partial order to the statements of the source code and to produce a fine grained parallel optimized subprogram and copies. Flow then continues to block where a unimodular re indexing data re layout transformation is applied for contiguity of references inside the second memory. Flow then continues to block costs are evaluated and a number of virtual execution threads along parallel dimensions is selected. Flow then continues to block where the code is placed along the virtual execution threads. In block the referenced memory is promoted to private memory conditional synchronizations between the virtual execution threads are inserted in block an unroll and jam transformation is then performed in .

It is a further object to automatically optimize programs for execution on massively multi threaded hardware. Massively multi threaded hardware is defined as hardware that exhibits more virtual threads of execution than physical threads of execution. The hardware or sometimes the runtime layer manages the physical scheduling of virtual threads to the actual physical processing units during execution. For the purpose of illustration we will use the CUDA programming model for execution on GPGPUs. Modern General Purpose Graphics Processing Units GPGPUs are massively parallel multiprocessors capable of high floating point operations performance and large memory bandwidth. While GPGPU programming is eased by the introduction of higher level data parallel languages such as CUDA maximizing the performance of an application still requires the precise balancing of many different types of constraints in the GPU architecture including the utilization of SIMD and coarse grained data parallelism and the management of the memory hierarchy. The typical architecture of modern GPUs consists of array SIMD multiprocessors that share a global address space subdivided into multiple types of memories. The execution model of such architecture is the following each GPU multiprocessor executes a set of parallel threads typically 32 as a single warp in a SIMD manner. Multithreading between large a number of cores is used to hide the latency of memory transfers and thread synchronization. Given such architectural constraints a CUDA kernel generated by embodiments and executed on the GPU may exhibit the following standard pseudo code form a set of perfectly nested parallel doall loops divided in a set of outer synchronization free threads blocks blocks for short then further subdivided into a set of inner parallel threads. The tripcounts for the block B B B and thread dimensions T T T are limited by architectural resources. Threads belonging to the same block can synchronize with each other and access their own subset of shared memory. Each thread also contains its own private local memory and registers which can be used to hold thread private data.

It is a further objective to take programs written in a high level language such as C and automatically transform them into programs in CUDA by performing loop and data transformations alleviating all the resource management and assignment tasks that are necessary to program in CUDA. It is another objective to perform system level mapping in which the program s data is in the main memory of a system and has to be explicitly transferred to and from the GPU and the CPU. In further embodiments the general structure of the code produced by various embodiments may be related to the tripcounts for the GPU cards C C limited by architectural resources look like 

To clarify the presentation of various embodiments we shall illustrate them using a simple 4096 4096 single precision matrix multiply kernel as our standard running example.

In one embodiment the standard target is one core of the GeForce 9800GX21. While we disclose embodiments on a very simple kernel mapping it onto a massively multi threaded architecture is already a non trivial task and the mapping process will demonstrate many of the benefits of various embodiments.

One embodiment solves two main problems in CUDA mapping i assigning the available set of parallelism to blocks and threads and ii managing the memory hierarchies. The problems are currently handled by various embodiments using the following sub phases 

It is an objective to perform a selective tradeoff of fusion parallelism and contiguity costs to expose all the available parallelism in the program including both coarse grained and fine grained parallelism. In one embodiment the strategy consists of transforming coarse grained parallelism into the thread block parallelism in a CUDA kernel and fine grained parallelism into SIMD parallelism within a thread block. In the running example the available amount of parallelism is easy to expose and the resulting loop nests are as follows 

In further embodiments tiling is applied as the next step. Tiling is of common knowledge for the engineer knowledgeable in the art. The tiling algorithm chooses a tile size that satisfies the following criteria 

The first constraint ensures that the all the memory storage within one tile after tiling can be fit within the local memory of the GPU. In other embodiments a tile size of 32 32 32 is chosen and the resulting loop nests loop is 

It is a further object to perform a shared memory promotion step and to promote the memory used within a tile to the secondary shared memory. Copies between the device memory and shared memory are inserted during this transformation. It is another object to perform Local Memory Compaction of the shared memory as disclosed in a previously filed patent application. It is yet another object of to insert explicit copies between memory locations in the primary and memory locations in the secondary memory. In the following pseudo code the  device memory corresponds to the original primary memory and the  shared memory to the memory locations in the secondary memory. In some embodiments the result is 

It is a further object to transform code of the previous form into CUDA kernels. This step in general involves a sequence of orthogonal loop and data transformations including loop fusion fission interchange stripmining and data permutation. The first step of this process is block and thread placement i.e. determining the set of loop dimensions to be used for block and thread dimensions. This first step is related to the contiguity properties of the optimized program. Modern GPUs implements memory coalescing whereby aligned stride array accesses assigned to adjacent threads are merged into a single memory transaction. By taking advantage of this hardware feature programs can drastically improve their memory transfer rate. However memory coalescing interacts with data layout and thread placement in non trivial way and so the two optimizations must be determined together.

In some embodiment unified parallelism locality and contiguity optimization finds the following loop structure with contiguity along dimension i 

The triples for the statement S can be computed by merging the triples for its references. In this case we have A 0 i 2PQR A 1 j PQR A 0 k PQR . By exhaustive inspection a further embodiment finds it is most beneficial to choose loop i as the outermost thread dimension. The resulting code is 

In further embodiments additional costs for the synchronizations between threads are introduced to account for nonparallel loop dimensions. This is because using an inner doall loop as a thread dimension can increase the amount of synchronization that we require in the final CUDA kernel. For example consider the following loop nest with two parallel and one interleaved sequential loop dimensions 

It is an object of provided embodiments to deduct a thread synchronization penalty from the weight of a coalescing tuple. Thread synchronization code is required if a loop dimension is nested under a sequential loop within a tile. In some embodiments the total penalty of the synchronization is proportional to the trip count of the sequential loops which is an estimate of the minimal amount of thread synchronization calls that the program has to execute per block.

It is another object of provided embodiments to reflect the cost of the synchronizations in the selective tradeoff for locality parallelism and contiguity. In some embodiments this is achieved by multiplying the benefit of outermost doall loops by constants proportional to the inverse of the depth of the loop.

It is an object of provided embodiments to generalize the previous analysis to imperfectly nested loops with multiple statements. Consider two coalescing tuples A d j w and A d j w for statements S and S respectively S could be equal to S. We say that the two tuples are compatible if 

Given the definition of compatibility it is another object of provided embodiments to optimize memory coalescing as follows given a set of coalescing tuples T find a compatible subset of T Topt such that the weight of Topt is maximized.

The previous method is used in some embodiments to determine the threads and block dimensions. In further embodiments we find one dimension at a time starting from the first thread dimension. During the selection of the first thread dimension memory coalescing optimization is considered. When choosing other thread and block dimensions where memory coalescing is no longer a concern some embodiments may use the following heuristics instead 

In further embodiments can only choose loop i as the block dimension for S and S i.e. it is illegal to choose j as the block dimension for S and k for S. This is because the merging transformation cannot be applied to parallel block loops only to parallel thread loops i.e. blocks must be synchronization free. The result of block and thread placement on the running example is shown in the following loop nests. The first thread dimension has a trip count of 32. Since we are only allowed a maximum of 512 threads on the 9800GX2 the second thread dimension is limited to 16. The second selected thread dimension has a trip count of 32. To maintain the limit of 512 threads we stripmine the loop nest by and use the stripmined loop as the second thread dimension. After this we have exhausted the number of threads. We then proceed to select the block dimensions which are loops i and j. Both block dimensions have trip counts of 128.

The above set loop nests is still not in the standard CUDA kernel form. Further embodiments may use the following merging heuristics to transform arbitrary loop nests into the standard form 

In the present running example the loop dimensions i and j are used as the block dimensions. Since there are no loop dimensions above i in this example the entire loop nests may be executed in the CUDA kernel and the host side code contains only a kernel launch. The reduction loop dimension k can be sunken into the CUDA kernel doing so requires the introduction of  syncthreads  calls to sequentialize the execution within this loop.

It is a further objective of some embodiments to further optimize the memory usage of the above program by recognizing that each thread writes to its own disjoint set of locations in C I. In some embodiments the following transformation on references is possible C I 16 j th.y th.x C I j and C I 16 j th.y th.x C I i . The resulting loop nests after privatization is as follows. In this example each thread keeps around 2 running sums for inside the local array C I.

In one embodiment the following pseudo code describes how certain embodiments may implement privatization 

It is another object to optimize a third weighted parametric function of parallelism locality and contiguity of the operations in the program after explicit memory copy operations have been inserted by some embodiments between memory locations in a primary memory and memory locations in a secondary memory. Generally various embodiments may fix schedule coefficients to enforce invariance of parallelism locality and contiguity of memory references across the plurality of selective tradeoffs as described above. Embodiments then may compute dependences between communication and computation statements using techniques well known by engineers knowledgeable in the field. A selective tradeoff is further derived and optimized which comprises communication and computation statements. Costs are determined as described above. As an illustration embodiments may produce the following pseudo program before the third weighted parametric function is optimized. In this pseudo program pr stands for private memory C I

Optimizing the third weighted function may completely modify the schedule of the computations and the interleaving of communications. In the following example communications between the primary memory for B and the non primary secondary or third memory are interleaved with computations 

It is a further object of provided embodiments to explore different costs that produce different weighted tradeoffs and result in different optimized programs.

It is a further object of various embodiments to automatically insert synchronizations between a plurality of execution units. It is another object to minimize a weighted function of the cost of the synchronization depending for instance on the number of synchronizations their nesting depth and the number of threads synchronized. In one embodiment synchronizations are inserted as follows where lex lex and 

In a further provided embodiment a complementary unroll and jam transformation is applied. Unroll and jam is common knowledge for an engineer knowledgeable in the art. In one embodiment it is implemented as a simple stripmining transformation of a doall loop followed by an interchange to sink the loop at the innermost level. A late unrolling duplicates the code and results in the following code 

In some embodiments unroll and jam on the outermost block dimensions has the effect to further improve the reuse within the innermost kernels by keeping more than running sums in the local array C I. However since unrolling the outer dimensions increases the sizes of A I and or B I there is a need to limit the unrolling factor to avoid running out of secondary memory limits. In further embodiments we try successive unroll and jam factors of 2 4 8 and further powers of 2 until we run out of secondary memory. We keep the highest such admissible factor.

The CUDA kernel output in some embodiments is very similar to the pseudo code above. Two additional and trivial adjustments are made to the output code 

Thus it is seen that methods and an apparatus for optimizing source code on a custom first computing apparatus for execution on a second computing apparatus are provided. One skilled in the art will appreciate that the present invention can be practiced by other than the above described embodiments which are presented in this description for purposes of illustration and not of limitation. The specification and drawings are not intended to limit the exclusionary scope of this patent document. It is noted that various equivalents for the particular embodiments discussed in this description may practice the invention as well. That is while the present invention has been described in conjunction with specific embodiments it is evident that many alternatives modifications permutations and variations will become apparent to those of ordinary skill in the art in light of the foregoing description. Accordingly it is intended that the present invention embrace all such alternatives modifications and variations as fall within the scope of the appended claims. The fact that a product process or method exhibits differences from one or more of the above described exemplary embodiments does not mean that the product or process is outside the scope literal scope and or other legally recognized scope of the following claims.

