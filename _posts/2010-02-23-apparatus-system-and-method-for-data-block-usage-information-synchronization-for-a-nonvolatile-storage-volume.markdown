---

title: Apparatus, system, and method for data block usage information synchronization for a non-volatile storage volume
abstract: An apparatus, system, and method are disclosed for data block usage information synchronization for a non-volatile storage volume. The method includes referencing first data block usage information for data blocks of a non-volatile storage volume managed by a storage manager. The first data block usage information is maintained by the storage manager. The method also includes synchronizing second data block usage information managed by a storage controller with the first data block usage information maintained by the storage manager. The storage manager maintains the first data block usage information separate from second data block usage information managed by the storage controller.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08935302&OS=08935302&RS=08935302
owner: Intelligent Intellectual Property Holdings 2 LLC
number: 08935302
owner_city: Wilmington
owner_country: US
publication_date: 20100223
---
This application is a continuation in part of and claims priority to U.S. patent application Ser. No. 11 952 109 entitled APPARATUS SYSTEM AND METHOD FOR MANAGING DATA FROM A REQUESTING DEVICE WITH AN EMPTY DATA TOKEN DIRECTIVE filed on Dec. 6 2007 for David Flynn et al. and U.S. patent application Ser. No. 11 952 113 entitled APPARATUS SYSTEM AND METHOD FOR MANAGING DATA IN A STORAGE DEVICE WITH AN EMPTY DATA TOKEN DIRECTIVE filed on Dec. 6 2007 for David Flynn et al. which are incorporated herein by reference and which claim the benefit of U.S. Provisional Patent Application No. 60 873 111 entitled Elemental Blade System and filed on Dec. 6 2006 for David Flynn et al. and U.S. Provisional Patent Application No. 60 974 470 entitled Apparatus System and Method for Object Oriented Solid State Storage and filed on Sep. 22 2007 for David Flynn et al. which are incorporated herein by reference.

This invention relates to block storage on a non volatile storage volume and more particularly relates to data block usage information synchronization for a non volatile storage volume.

Many conventional storage devices treat storage block addresses received from a storage client as logical block addresses having a one to one direct mapping to a corresponding physical addresses on a storage media where data is actually stored. For storage devices that maintain a mapping from a logical block address to an arbitrary physical address conventional storage clients operating systems file systems volume mangers and the like have begun to communicate when data on physical media corresponding to a logical block address no longer needs to be retained. This unused data block usage information enables deallocation of the corresponding physical blocks and or stops preserving the data in the corresponding physical blocks. As a result data on the storage device corresponding to logical blocks that are not in use by a storage client is no longer unnecessarily preserved by the storage device. Without this capability unused data blocks must be preserved by the storage device as used data blocks which slows performance and requires additional unnecessary overhead to maintain.

However certain storage clients are not designed to communicate unused data block usage information. Additionally certain storage clients that have the ability to communicate unused data block usage information do so ineffectively or lack the ability to communicate unused data block usage information for certain storage configurations. In addition in certain storage configurations even though the unused block usage information is communicated the information is not passed on to the storage device.

The present invention has been developed in response to the present state of the art and in particular in response to the problems and needs in the art that have not yet been fully solved by currently available storage systems. Accordingly the present invention has been developed to provide an apparatus system and method for data block usage information synchronization for a non volatile storage volume that overcome many or all of the above discussed shortcomings in the art.

The method for data block usage information synchronization for a non volatile storage volume includes referencing first data block usage information for data blocks of a non volatile storage volume managed by a storage manager. The first data block usage information is maintained by the storage manager. The method also includes synchronizing second data block usage information managed by a storage controller with the first data block usage information maintained by the storage manager. The storage manager maintains the first data block usage information separate from second data block usage information managed by the storage controller.

In one embodiment the method includes determining one or more unused blocks from the first data block usage information and sending a message directly to the storage controller directly managing the non volatile storage volume. The message indicates to the storage controller unused blocks identified by the storage manager. The storage controller deallocates the unused blocks identified by the storage manager in response to the message. In one embodiment synchronizing second data block usage information further includes deallocating blocks identified by the storage controller as in use or used corresponding to one or more unused blocks identified by the storage manager based on the first data block usage information.

In certain embodiments synchronizing second data block usage information further includes determining that the storage controller identifies one or more unused blocks indicated by the first data block usage information as used blocks and deallocating the used blocks identified by the storage controller corresponding to the one or more unused blocks. In one embodiment the method includes updating the first data block usage information based on storage operations that modify the first data block usage information. The storage operations are executed by the storage controller subsequent to referencing the first data block usage information and executed by the storage controller prior to synchronizing the second data block usage information.

In one embodiment referencing the first data block usage information further includes referencing the first data block usage information by way of a storage Application Programming Interface API of the storage manager. In one embodiment the non volatile storage volume includes a live volume actively servicing storage requests.

In one embodiment the storage controller includes a redundant array of independent drives RAID controller storing data in a RAID configuration on two or more storage devices. In this embodiment synchronizing second data block usage information synchronizes second data block usage information managed for the two or more storage devices with the first data block usage information. The two or more storage devices are managed by the RAID controller. In one embodiment the method further includes determining a RAID configuration of the RAID controller and synchronizing second data block usage information managed for the two or more storage devices with the first data block usage information based on the determined RAID configuration.

In one embodiment the RAID controller manages one or more sub controllers each sub controller storing data on one or more of the two or more storage devices. In certain embodiments the RAID configuration includes a RAID 0 configuration that stores data as a stripe across the two or more storage devices. In this embodiment synchronizing second data block usage information includes identifying a first portion of the first data block usage information corresponding to data blocks stored on a first storage device. The method also includes identifying a second portion of the first data block usage information corresponding to data blocks stored on a second storage device. The method also includes synchronizing second data block usage information managed for the first storage device with the first portion of the first data block usage information. The method also includes synchronizing second data block usage information managed for the second storage device with the second portion of the first data block usage information.

In one embodiment the RAID configuration includes a RAID 1 configuration that mirrors data stored on a first storage device to a second storage device. In this embodiment synchronizing second data block usage information includes synchronizing second data block usage information managed for the first storage device with the first data block usage information. The method also includes synchronizing the second data block usage information managed for the second storage device with the first data block usage information.

In one embodiment the RAID configuration includes a RAID 5 configuration that stores data as a stripe across three or more storage devices. The stripe includes two or more data strides and a distributed parity data stride. Each data stride is stored on a storage device and each data stride includes one or more data blocks. In this embodiment synchronizing second data block usage information includes determining that each data stride in the stripe comprises no used blocks based on the first data block usage information. The method also includes synchronizing second data block usage information managed by the RAID controller for the stripe by designating data blocks of the second data block usage information corresponding to the stripe as unused.

In one embodiment the RAID configuration includes a RAID 10 configuration that mirrors a stride of data between two or more storage devices using a RAID 1 configuration and that stores stripes of data across two or more storage device sets using a RAID 0 configuration. In this embodiment synchronizing second data block usage information includes identifying a first portion of the first data block usage information corresponding to data blocks stored in a first stride managed by the RAID controller. The method also includes identifying a second portion of the first data block usage information corresponding to data blocks stored in a second stride managed by the RAID controller. The method also includes synchronizing second data block usage information managed for the first stride with the first portion of the first data block usage information. The method also includes synchronizing second data block usage information managed for the second stride with the second portion of the first data block usage information.

A computer program product is also provided for data management on non volatile storage media managed by a storage manager. The computer program product includes referencing a block map defining data block usage information for data blocks of non volatile storage media managed by a storage manager. The block map is maintained by the storage manager. The computer program product also includes sending a message directly to a storage controller managing the non volatile storage media. The message identifies to the storage controller one or more unused blocks identified by the block map. In one embodiment the computer program product includes determining one or more unused blocks from the block map.

In one embodiment the computer program product includes deallocating used blocks identified by the storage controller corresponding to the one or more unused blocks identified by the storage manager in response to the message. In certain embodiments the computer program product includes determining that the storage controller identifies the one or more unused blocks as used blocks and the computer program product deallocates the used blocks identified by the storage controller corresponding to the one or more unused blocks in response to the message.

In one embodiment determining one or more unused blocks from the block map includes monitoring storage operations on data blocks represented by the block map. The storage operations are executed by the storage controller subsequent to referencing the block map and executed by the storage controller prior to deallocating the one or more unused blocks in response to the message. In this embodiment determining one or more unused blocks from the block map includes recording data block usage information for the storage operations that change unused blocks of the block map to used blocks. In a further embodiment recording data block usage information for the storage operations further includes recording the data block usage information in an in flight block map. In this embodiment the computer program product includes combining the block map and the in flight block map to identify the one or more unused blocks of the data blocks.

In one embodiment the computer program product includes obtaining a lock on a logical to physical map managed by the storage controller prior to determining one or more unused blocks from the block map and releasing the lock on the logical to physical map subsequent to the storage controller deallocating the unused blocks. The storage controller stores data on the non volatile storage media using an append only writing process and recovers storage space on the non volatile storage media using a storage space recovery process that re uses non volatile storage media storing blocks that have become unused blocks.

In one embodiment the message complies with an interface operable to communicate storage information between the storage manager and the storage controller. The message includes a notification passing the unused blocks identified by the storage manager to the storage controller. The notification includes no requirement for action by the storage controller in accordance with the interface. In certain embodiments the message complies with an interface operable to communicate storage information between the storage manager and the storage controller and includes a directive passing the unused blocks identified by the storage manager to the storage controller. The directive requires the storage controller to erase the non volatile storage media comprising the unused blocks in accordance with the interface.

In one embodiment the message complies with an interface operable to communicate storage information between the storage manager and the storage controller and the message includes a purge instruction passing the unused blocks identified by the storage manager to the storage controller. The purge instruction requires the storage controller to erase the non volatile storage media comprising the unused blocks and to overwrite the unused blocks in accordance with the interface.

In one embodiment the storage controller includes a redundant array of independent drives RAID controller storing data in a RAID configuration on two or more storage devices managed by the RAID controller. In certain embodiments sending a message to the storage controller includes sending one or more messages communicating the unused blocks identified by the storage manager to one or more sub controllers.

In one embodiment the RAID configuration includes a RAID 0 configuration that stores data as a stripe across the two or more storage devices. In this embodiment the computer program product further includes identifying a first portion of the block map corresponding to data blocks stored on a first storage device managed by the RAID controller. The computer program product also includes identifying a second portion of the block map corresponding to data blocks stored on a second storage device managed by the RAID controller. Sending a message to the storage controller includes sending a first message to the RAID controller. The first message identifies one or more unused blocks on the first storage device identified by the first portion of the block map. Sending a message also includes sending a second message to the RAID controller. The second message identifies one or more unused blocks on the second storage device identified by the second portion of the block map.

In one embodiment the RAID configuration includes a RAID 1 configuration that mirrors data stored on a first storage device to a second storage device. Sending a message to the storage controller includes sending a first message to the RAID controller managing the first storage device. The first message identifies one or more unused blocks on the first storage device identified by the block map. Sending a message to the storage controller also includes sending a second message to the RAID controller managing the second storage device. The second message identifies one or more unused blocks on the second storage device identified by the block map.

In one embodiment the RAID configuration includes a RAID 5 configuration that stores data as a stripe across three or more storage devices. The stripe includes two or more data strides and a distributed parity data stride. Each data stride is stored on a storage device and each data stride includes one or more data blocks. In this embodiment the computer program product further includes determining that each data stride in the stripe comprises no used blocks based on the block map. Sending a message to the storage controller includes sending a message to the RAID controller. The message designates data blocks corresponding to the stripe as unused.

In one embodiment the RAID configuration includes a RAID 10 configuration that that mirrors a stride of data between two or more storage devices using a RAID 1 configuration and that stores stripes of data across two or more storage device sets using a RAID 0 configuration. In this embodiment the computer program product further includes identifying a first portion of the block map corresponding to data blocks stored in a first stride managed by the RAID controller. The computer program product may also include identifying a second portion of the block map corresponding to data blocks stored in a second stride managed by the RAID controller. Sending a message to the storage controller includes sending a first message to the RAID controller managing the first stride. The message identifies one or more unused blocks in the first stride identified by the first portion of the block map. Sending a message to the storage controller also includes sending a second message to the RAID controller managing the second stride. The second message identifies one or more unused blocks in the second stride identified by the second portion of the block map.

A system and method are also presented for data block usage information synchronization for a non volatile storage volume managed by a storage manager that includes the necessary components and steps to execute the functions described above in relation to the computer program product. In addition the system includes a block usage synchronizer that in one embodiment is initiated in response to one or more predetermined events. In certain embodiments the block usage synchronizer is initiated at a predetermined time interval.

Furthermore the method includes calling a function of a storage Application Programming Interface API to reference a block map defining data block usage information for a set of data blocks of a non volatile storage volume such as a flash storage volume. The block map is maintained by a storage manager. The non volatile storage volume is exclusively managed by a storage controller configured to use a logical to physical address translation layer that translates logical block addresses received from a storage client to physical block addresses on the non volatile storage volume. In one embodiment the non volatile storage volume includes one or more non volatile storage media. In certain embodiments the storage API includes a defragmentation API for block oriented storage devices.

Reference throughout this specification to features advantages or similar language does not imply that all of the features and advantages that may be realized with the present invention should be or are in any single embodiment of the invention. Rather language referring to the features and advantages is understood to mean that a specific feature advantage or characteristic described in connection with an embodiment is included in at least one embodiment of the present invention. Thus discussion of the features and advantages and similar language throughout this specification may but do not necessarily refer to the same embodiment.

Furthermore the described features advantages and characteristics of the invention may be combined in any suitable manner in one or more embodiments. One skilled in the relevant art will recognize that the invention may be practiced without one or more of the specific features or advantages of a particular embodiment. In other instances additional features and advantages may be recognized in certain embodiments that may not be present in all embodiments of the invention.

These features and advantages of the present invention will become more fully apparent from the following description and appended claims or may be learned by the practice of the invention as set forth hereinafter.

Many of the functional units described in this specification have been labeled as modules in order to more particularly emphasize their implementation independence. For example a module may be implemented as a hardware circuit comprising custom VLSI circuits or gate arrays off the shelf semiconductors such as logic chips transistors or other discrete components. A module may also be implemented in programmable hardware devices such as field programmable gate arrays programmable array logic programmable logic devices or the like.

Modules may also be implemented in software for execution by various types of processors. An identified module of executable code may for instance comprise one or more physical or logical blocks of computer instructions which may for instance be organized as an object procedure or function. Nevertheless the executables of an identified module need not be physically located together but may comprise disparate instructions stored in different locations which when joined logically together comprise the module and achieve the stated purpose for the module.

Indeed a module of executable code may be a single instruction or many instructions and may even be distributed over several different code segments among different programs and across several memory devices. Similarly operational data may be identified and illustrated herein within modules and may be embodied in any suitable form and organized within any suitable type of data structure. The operational data may be collected as a single data set or may be distributed over different locations including over different storage devices and may exist at least partially merely as electronic signals on a system or network. Where a module or portions of a module are implemented in software the software portions are stored on one or more computer readable media.

Reference throughout this specification to one embodiment an embodiment or similar language means that a particular feature structure or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Thus appearances of the phrases in one embodiment in an embodiment and similar language throughout this specification may but do not necessarily all refer to the same embodiment.

Reference to a computer readable medium may take any form capable of storing machine readable instructions on a digital processing apparatus memory device. A computer readable medium may be embodied by a compact disk digital video disk a magnetic tape a Bernoulli drive a magnetic disk a punch card flash memory NAND or NOR other types of solid state memory integrated circuits or other digital processing apparatus memory device.

Furthermore the described features structures or characteristics of the invention may be combined in any suitable manner in one or more embodiments. In the following description numerous specific details are provided such as examples of programming software modules user selections network transactions database queries database structures hardware modules hardware circuits hardware chips etc. to provide a thorough understanding of embodiments of the invention. One skilled in the relevant art will recognize however that the invention may be practiced without one or more of the specific details or with other methods components materials and so forth. In other instances well known structures materials or operations are not shown or described in detail to avoid obscuring aspects of the invention.

The schematic flow chart diagrams included herein are generally set forth as logical flow chart diagrams. As such the depicted order and labeled steps are indicative of one embodiment of the presented method. Other steps and methods may be conceived that are equivalent in function logic or effect to one or more steps or portions thereof of the illustrated method. Additionally the format and symbols employed are provided to explain the logical steps of the method and are understood not to limit the scope of the method. Although various arrow types and line types may be employed in the flow chart diagrams they are understood not to limit the scope of the corresponding method. Indeed some arrows or other connectors may be used to indicate only the logical flow of the method. For instance an arrow may indicate a waiting or monitoring period of unspecified duration between enumerated steps of the depicted method. Additionally the order in which a particular method occurs may or may not strictly adhere to the order of the corresponding steps shown.

The system includes at least one solid state storage device . In another embodiment the system includes two or more solid state storage devices . Each solid state storage device may include non volatile solid state storage media such as flash memory nano random access memory nano RAM or NRAM magneto resistive RAM MRAM dynamic RAM DRAM phase change RAM PRAM etc. The solid state storage device is described in more detail with respect to . The solid state storage device is depicted in a computer connected to a client through a computer network . In one embodiment the solid state storage device is internal to the computer and is connected using a system bus such as a peripheral component interconnect express PCI e bus a Serial Advanced Technology Attachment serial ATA bus or the like. In another embodiment the solid state storage device is external to the computer and is connected a universal serial bus USB connection an Institute of Electrical and Electronics Engineers IEEE 1394 bus FireWire or the like. In other embodiments the solid state storage device is connected to the computer using a peripheral component interconnect PCI express bus using external electrical or optical bus extension or bus networking solution such as Infiniband or PCI Express Advanced Switching PCIe AS or the like.

In various embodiments the solid state storage device may be in the form of a dual inline memory module DIMM a daughter card or a micro module. In another embodiment the solid state storage device is an element within a rack mounted blade. In another embodiment the solid state storage device is contained within a package that is integrated directly onto a higher level assembly e.g. mother board lap top graphics processor . In another embodiment individual components comprising the solid state storage device are integrated directly onto a higher level assembly without intermediate packaging.

The solid state storage device includes one or more solid state storage controllers each may include a write data pipeline and a read data pipeline and each includes a solid state storage media which are described in more detail below with respect to .

The system includes one or more computers connected to the solid state storage device . A computer may be a host a server a storage controller of a storage area network SAN a workstation a personal computer a laptop computer a handheld computer a supercomputer a computer cluster a network switch router or appliance a database or storage appliance a data acquisition or data capture system a diagnostic system a test system a robot a portable electronic device a wireless device or the like. In another embodiment a computer may be a client and the solid state storage device operates autonomously to service data requests sent from the computer . In this embodiment the computer and solid state storage device may be connected using a computer network system bus or other communication means suitable for connection between a computer and an autonomous solid state storage device .

In one embodiment the system includes one or more clients connected to one or more computer through one or more computer networks . A client may be a host a server a storage controller of a SAN a workstation a personal computer a laptop computer a handheld computer a supercomputer a computer cluster a network switch router or appliance a database or storage appliance a data acquisition or data capture system a diagnostic system a test system a robot a portable electronic device a wireless device or the like. The computer network may include the Internet a wide area network WAN a metropolitan area network MAN a local area network LAN a token ring a wireless network a fiber channel network a SAN network attached storage NAS ESCON or the like or any combination of networks. The computer network may also include a network from the IEEE 802 family of network technologies such Ethernet token ring WiFi WiMax and the like.

The computer network may include servers switches routers cabling radios and other equipment used to facilitate networking computers and clients . In one embodiment the system includes multiple computers that communicate as peers over a computer network . In another embodiment the system includes multiple solid state storage devices that communicate as peers over a computer network . One of skill in the art will recognize other computer networks comprising one or more computer networks and related equipment with single or redundant connection between one or more clients or other computer with one or more solid state storage devices or one or more solid state storage devices connected to one or more computers . In one embodiment the system includes two or more solid state storage devices connected through the computer network to a client without a computer .

In one embodiment at least one solid state controller is field programmable gate array FPGA and controller functions are programmed into the FPGA. In a particular embodiment the FPGA is a Xilinx FPGA. In another embodiment the solid state storage controller comprises components specifically designed as a solid state storage controller such as an application specific integrated circuit ASIC or custom logic solution. Each solid state storage controller typically includes a write data pipeline and a read data pipeline which are describe further in relation to . In another embodiment at least one solid state storage controller is made up of a combination FPGA ASIC and custom logic components.

The solid state storage media is an array of non volatile solid state storage elements arranged in banks and accessed in parallel through a bi directional storage input output I O bus . The storage I O bus in one embodiment is capable of unidirectional communication at any one time. For example when data is being written to the solid state storage media data cannot be read from the solid state storage media . In another embodiment data can flow both directions simultaneously. However bi directional as used herein with respect to a data bus refers to a data pathway that can have data flowing in only one direction at a time but when data flowing one direction on the bi directional data bus is stopped data can flow in the opposite direction on the bi directional data bus.

A solid state storage element e.g. SSS . is typically configured as a chip a package of one or more dies or a die on a circuit board. As depicted a solid state storage element e.g. operates independently or semi independently of other solid state storage elements e.g. even if these several elements are packaged together in a chip package a stack of chip packages or some other package element. As depicted a column of solid state storage elements is designated as a bank . As depicted there may be n banks and m solid state storage elements per bank in an array of n m solid state storage elements in a solid state storage media . In one embodiment a solid state storage media includes twenty solid state storage elements per bank e.g. in bank in bank in bank where m 22 with eight banks e.g. where n 8 and a solid state storage media includes two solid state storage elements e.g. where m 2 per bank with one bank . There is no requirement that two solid state storage media have the same number of solid state storage elements and or same number of banks . In one embodiment each solid state storage element is comprised of a single level cell SLC devices. In another embodiment each solid state storage element is comprised of multi level cell MLC devices.

In one embodiment solid state storage elements for multiple banks that share a common storage I O bus row e.g. are packaged together. In one embodiment a solid state storage element may have one or more dies per chip with one or more chips stacked vertically and each die may be accessed independently. In another embodiment a solid state storage element e.g. SSS . may have one or more virtual dies per die and one or more dies per chip and one or more chips stacked vertically and each virtual die may be accessed independently. In another embodiment a solid state storage element SSS . may have one or more virtual dies per die and one or more dies per chip with some or all of the one or more dies stacked vertically and each virtual die may be accessed independently.

In one embodiment two dies are stacked vertically with four stacks per group to form eight storage elements e.g. SSS . SSS . each in a separate bank . In another embodiment 20 storage elements e.g. SSS . SSS . form a virtual bank so that each of the eight virtual banks has 20 storage elements e.g. SSS. SSS . . Data is sent to the solid state storage media over the storage I O bus to all storage elements of a particular group of storage elements SSS . SSS . . The storage control bus is used to select a particular bank e.g. Bank so that the data received over the storage I O bus connected to all banks is written just to the selected bank

In certain embodiments the storage control bus and storage I O bus are used together by the solid state controller to communicate addressing information storage element command information and data to be stored. Those of skill in the art recognize that this address data and command information may be communicated using one or the other of these buses or using separate buses for each type of control information. In one embodiment addressing information storage element command information and storage data travel on the storage I O bus and the storage control bus carries signals for activating a bank as well as identifying whether the data on the storage I O bus lines constitute addressing information storage element command information or storage data.

For example a control signal on the storage control bus such as command enable may indicate that the data on the storage I O bus lines is a storage element command such as program erase reset read and the like. A control signal on the storage control bus such as address enable may indicate that the data on the storage I O bus lines is addressing information such as erase block identifier page identifier and optionally offset within the page within a particular storage element. Finally an absence of a control signal on the storage control bus for both command enable and address enable may indicate that the data on the storage I O bus lines is storage data that is to be stored on the storage element at a previously addressed erase block physical page and optionally offset within the page of a particular storage element.

In one embodiment the storage I O bus is comprised of one or more independent I O buses IIOBa m comprising wherein the solid state storage elements within each row share one of the independent I O buses across each solid state storage element in parallel so that all banks are accessed simultaneously. For example one IIOB of the storage I O bus may access a first solid state storage element of each bank simultaneously. A second IIOB of the storage I O bus may access a second solid state storage element of each bank simultaneously. Each row of solid state storage elements is accessed simultaneously. In one embodiment where solid state storage elements are multi level physically stacked all physical levels of the solid state storage elements are accessed simultaneously. As used herein simultaneously also includes near simultaneous access where devices are accessed at slightly different intervals to avoid switching noise. Simultaneously is used in this context to be distinguished from a sequential or serial access wherein commands and or data are sent individually one after the other.

Typically banks are independently selected using the storage control bus . In one embodiment a bank is selected using a chip enable or chip select. Where both chip select and chip enable are available the storage control bus may select one level of a multi level solid state storage element using either of the chip select signal and the chip enable signal. In other embodiments other commands are used by the storage control bus to individually select one level of a multi level solid state storage element . Solid state storage elements may also be selected through a combination of control and of address information transmitted on storage I O bus and the storage control bus .

In one embodiment each solid state storage element is partitioned into erase blocks and each erase block is partitioned into pages. A typical page is 2000 bytes 2 kB . In one example a solid state storage element e.g. SSS. includes two registers and can program two pages so that a two register solid state storage element has a page size of 4 kB. A single bank of 20 solid state storage elements would then have an 80 kB capacity of pages accessed with the same address going out of the storage I O bus .

This group of pages in a bank of solid state storage elements of 80 kB may be called a logical or virtual page. Similarly an erase block of each storage element of a bank may be grouped to form a logical erase block. In one embodiment erasing a logical erase block causes a physical erase block of each storage element of a bank to be erased. In one embodiment an erase block of pages within a solid state storage element is erased when an erase command is received within a solid state storage element . In another embodiment a single physical erase block on each storage element e.g. SSS M.N collectively forms a logical erase block for the solid state storage media . In such an embodiment erasing a logical erase block comprises erasing an erase block at the same address within each storage element e.g. SSS M.N in the solid state storage array . Whereas the size and number of erase blocks pages planes or other logical and physical divisions within a solid state storage element may change over time with advancements in technology it is to be expected that many embodiments consistent with new configurations are possible and are consistent with the general description herein.

In one embodiment data is written in packets to the storage elements. The solid state controller uses the storage I O bus and storage control bus to address a particular bank storage element physical erase block physical page and optionally offset within a physical page for writing the data packet. In one embodiment the solid state controller sends the address information for the data packet by way of the storage I O bus and signals that the data on the storage I O bus is address data by way of particular signals set on the storage control bus . The solid state controller follows the transmission of the address information with transmission of the data packet of data that is to be stored. The physical address contains enough information for the solid state storage element to direct the data packet to the designated location within the page.

In one embodiment the storage I O bus connects to each storage element in a row of storage elements e.g. SSS . SSS .N . In such an embodiment the solid state controller activates a desired bank using the storage control bus such that data on storage I O bus reaches the proper page of a single storage element e.g. SSS . .

In addition in certain embodiments the solid state controller simultaneously activates the same bank using the storage control bus such that different data a different data packet on storage I O bus reaches the proper page of a single storage element on another row e.g. SSS . . In this manner multiple physical pages of multiple storage elements may be written to simultaneously within a single bank to store a logical page.

Similarly a read command may require a command on the storage control bus to select a single bank and the appropriate page within that bank . In one embodiment a read command reads an entire physical page from each storage element and because there are multiple solid state storage elements in parallel in a bank an entire logical page is read with a read command. However the read command may be broken into subcommands as will be explained below with respect to bank interleave. A logical page may also be accessed in a write operation.

In one embodiment a solid state controller may send an erase block erase command over all the lines of the storage I O bus to erase a physical erase block having a particular erase block address. In addition the solid state controller may simultaneously activate a single bank using the storage control bus such that each physical erase block in the single activated bank is erased as part of a logical erase block.

In another embodiment the solid state controller may send an erase block erase command over all the lines of the storage I O bus to erase a physical erase block having a particular erase block address on each storage element SSS . SSS M.N . These particular physical erase blocks together may form a logical erase block. Once the address of the physical erase blocks is provided to the storage elements the solid state controller may initiate the erase command on a bank by bank by bank basis either in order or based on some other sequence . Other commands may also be sent to a particular location using a combination of the storage I O bus and the storage control bus . One of skill in the art will recognize other ways to select a particular storage location using the bi directional storage I O bus and the storage control bus .

In one embodiment the storage controller sequentially writes data on the solid state storage media in a log structured format and within one or more physical structures of the storage elements the data is sequentially stored on the solid state storage media . Sequentially writing data involves the storage controller streaming data packets into storage write buffers for storage elements such as a chip a package of one or more dies or a die on a circuit board. When the storage write buffers are full the data packets are programmed to a designated virtual or logical page LP . Data packets then refill the storage write buffers and when full the data packets are written to the next LP. The next virtual page may be in the same bank or another bank e.g. . This process continues LP after LP typically until a virtual or logical erase block LEB is filled. LPs and LEBs are described in more detail below.

In another embodiment the streaming may continue across LEB boundaries with the process continuing LEB after LEB. Typically the storage controller sequentially stores data packets in an LEB by order of processing. In one embodiment where a write data pipeline is used the storage controller stores packets in the order that they come out of the write data pipeline . This order may be a result of data segments arriving from a requesting device mixed with packets of valid data that are being read from another storage location as valid data is being recovered from another LEB during a recovery operation.

The sequentially stored data in one embodiment can serve as a log to reconstruct data indexes and other metadata using information from data packet headers. For example in one embodiment the storage controller may reconstruct a storage index by reading headers to determine the data structure to which each packet belongs and sequence information to determine where in the data structure the data or metadata belongs. The storage controller in one embodiment uses physical address information for each packet and timestamp or sequence information to create a mapping between the physical locations of the packets and the data structure identifier and data segment sequence. Timestamp or sequence information is used by the storage controller to replay the sequence of changes made to the index and thereby reestablish the most recent state.

In one embodiment erase blocks are time stamped or given a sequence number as packets are written and the timestamp or sequence information of an erase block is used along with information gathered from container headers and packet headers to reconstruct the storage index. In another embodiment timestamp or sequence information is written to an erase block when the erase block is recovered.

In a read modify write operation data packets associated with the logical structure are located and read in a read operation. Data segments of the modified structure that have been modified are not written to the location from which they are read. Instead the modified data segments are again converted to data packets and then written to the next available location in the virtual page currently being written. Index entries for the respective data packets are modified to point to the packets that contain the modified data segments. The entry or entries in the index for data packets associated with the same logical structure that have not been modified will include pointers to original location of the unmodified data packets. Thus if the original logical structure is maintained for example to maintain a previous version of the logical structure the original logical structure will have pointers in the index to all data packets as originally written. The new logical structure will have pointers in the index to some of the original data packets and pointers to the modified data packets in the virtual page that is currently being written.

In a copy operation the index includes an entry for the original logical structure mapped to a number of packets stored on the solid state storage media . When a copy is made a new logical structure is created and a new entry is created in the index mapping the new logical structure to the original packets. The new logical structure is also written to the solid state storage media with its location mapped to the new entry in the index. The new logical structure packets may be used to identify the packets within the original logical structure that are referenced in case changes have been made in the original logical structure that have not been propagated to the copy and the index is lost or corrupted. In another embodiment the index includes a logical entry for a logical block.

Beneficially sequentially writing packets facilitates a more even use of the solid state storage media and allows the solid storage device controller to monitor storage hot spots and level usage of the various virtual pages in the solid state storage media . Sequentially writing packets also facilitates a powerful efficient garbage collection system which is described in detail below. One of skill in the art will recognize other benefits of sequential storage of data packets.

The system may comprise a log structured storage system or log structured array similar to a log structured file system and the order that data is stored may be used to recreate an index. Typically an index that includes a logical to physical mapping is stored in volatile memory. The index is referred to as a logical to physical map herein. If the index is corrupted or lost the index may be reconstructed by addressing the solid state storage media in the order that the data was written. Within a logical erase block LEB data is typically stored sequentially by filling a first logical page then a second logical page etc. until the LEB is filled. The solid state storage controller then chooses another LEB and the process repeats. By maintaining an order that the LEBs were written to and by knowing that each LEB is written sequentially the index can be rebuilt by traversing the solid state storage media in order from beginning to end. In other embodiments if part of the index is stored in non volatile memory such as on the solid state storage media the solid state storage controller may only need to replay a portion of the solid state storage media to rebuild a portion of the index that was not stored in non volatile memory. One of skill in the art will recognize other benefits of sequential storage of data packets.

In various embodiments the solid state storage device controller also includes a data bus a local bus a buffer controller buffers N a master controller a direct memory access DMA controller a memory controller a dynamic memory array a static random memory array a management controller a management bus a bridge to a system bus and miscellaneous logic which are described below. In other embodiments the system bus is coupled to one or more network interface cards NICs some of which may include remote DMA RDMA controllers one or more central processing unit CPU one or more external memory controllers and associated external memory arrays one or more storage controllers peer controllers and application specific processors which are described below. The components connected to the system bus may be located in the computer or may be other devices.

In one embodiment the solid state storage controller s communicate data to the solid state storage media over a storage I O bus . In a certain embodiment where the solid state storage is arranged in banks and each bank includes multiple storage elements accessible in parallel the storage I O bus comprises an array of busses one for each row of storage elements spanning the banks . As used herein the term storage I O bus may refer to one storage I O bus or an array of data independent busses . In one embodiment each storage I O bus accessing a row of storage elements e.g. may include a logical to physical mapping for storage divisions e.g. erase blocks accessed in a row of storage elements . This mapping allows a logical address mapped to a physical address of a storage division to be remapped to a different storage division if the first storage division fails partially fails is inaccessible or has some other problem. Remapping is explained further in relation to the remapping module of .

Data may also be communicated to the solid state storage controller s from a requesting device through the system bus bridge local bus buffer s and finally over a data bus . The data bus typically is connected to one or more buffers controlled with a buffer controller . The buffer controller typically controls transfer of data from the local bus to the buffers and through the data bus to the pipeline input buffer and output buffer . The buffer controller typically controls how data arriving from a requesting device can be temporarily stored in a buffer and then transferred onto a data bus or vice versa to account for different clock domains to prevent data collisions etc. The buffer controller typically works in conjunction with the master controller to coordinate data flow. As data arrives the data will arrive on the system bus be transferred to the local bus through a bridge .

Typically the data is transferred from the local bus to one or more data buffers as directed by the master controller and the buffer controller . The data then flows out of the buffer s to the data bus through a solid state controller and on to the solid state storage media such as NAND flash or other storage media. In one embodiment data and associated out of band metadata metadata arriving with the data is communicated using one or more data channels comprising one or more solid state storage controllers and associated solid state storage media while at least one channel solid state storage controller solid state storage media is dedicated to in band metadata such as index information and other metadata generated internally to the solid state storage device .

The local bus is typically a bidirectional bus or set of busses that allows for communication of data and commands between devices internal to the solid state storage device controller and between devices internal to the solid state storage device and devices connected to the system bus . The bridge facilitates communication between the local bus and system bus . One of skill in the art will recognize other embodiments such as ring structures or switched star configurations and functions of buses and bridges .

The system bus is typically a bus of a computer or other device in which the solid state storage device is installed or connected. In one embodiment the system bus may be a PCI e bus a Serial Advanced Technology Attachment serial ATA bus parallel ATA or the like. In another embodiment the system bus is an external bus such as small computer system interface SCSI FireWire Fiber Channel USB PCIe AS or the like. The solid state storage device may be packaged to fit internally to a device or as an externally connected device.

The solid state storage device controller includes a master controller that controls higher level functions within the solid state storage device . The master controller in various embodiments controls data flow by interpreting requests directs creation of indexes to map identifiers associated with data to physical locations of associated data coordinating DMA requests etc. Many of the functions described herein are controlled wholly or in part by the master controller .

In one embodiment the master controller uses embedded controller s . In another embodiment the master controller uses local memory such as a dynamic memory array dynamic random access memory DRAM a static memory array static random access memory SRAM etc. In one embodiment the local memory is controlled using the master controller . In another embodiment the master controller accesses the local memory via a memory controller . In another embodiment the master controller runs a Linux server and may support various common server interfaces such as the World Wide Web hyper text markup language HTML etc. In another embodiment the master controller uses a nano processor. The master controller may be constructed using programmable or standard logic or any combination of controller types listed above. The master controller may be embodied as hardware as software or as a combination of hardware and software. One skilled in the art will recognize many embodiments for the master controller .

In one embodiment where the storage controller solid state storage device controller manages multiple data storage devices solid state storage media the master controller divides the work load among internal controllers such as the solid state storage controllers . For example the master controller may divide a data structure to be written to the data storage devices e.g. solid state storage media so that a portion of the data structure is stored on each of the attached data storage devices. This feature is a performance enhancement allowing quicker storage and access to a data structure. In one embodiment the master controller is implemented using an FPGA. In another embodiment the firmware within the master controller may be updated through the management bus the system bus over a network connected to a NIC or other device connected to the system bus .

In one embodiment the master controller emulates block storage such that a computer or other device connected to the storage device solid state storage device views the storage device solid state storage device as a block storage device and sends data to specific physical addresses in the storage device solid state storage device . The master controller then divides up the blocks and stores the data blocks. The master controller then maps the blocks and physical address sent with the block to the actual locations determined by the master controller . The mapping is stored in the index. Typically for block emulation a block device application program interface API is provided in a driver in the computer client or other device wishing to use the storage device solid state storage device as a block storage device.

In another embodiment the master controller coordinates with NIC controllers and embedded RDMA controllers to deliver just in time RDMA transfers of data and command sets. NIC controller may be hidden behind a non transparent port to enable the use of custom drivers. Also a driver on a client may have access to the computer network through an I O memory driver using a standard stack API and operating in conjunction with NICs .

In one embodiment the master controller is also a redundant array of independent drive RAID controller. Where the data storage device solid state storage device is networked with one or more other data storage devices solid state storage devices the master controller may be a RAID controller for single tier RAID multi tier RAID progressive RAID etc. The master controller may also allow some objects and other data structures to be stored in a RAID array and other data structures to be stored without RAID. In another embodiment the master controller may be a distributed RAID controller element. In another embodiment the master controller may comprise many RAID distributed RAID and other functions as described elsewhere.

In one embodiment the master controller coordinates with single or redundant network managers e.g. switches to establish routing to balance bandwidth utilization failover etc. In another embodiment the master controller coordinates with integrated application specific logic via local bus and associated driver software. In another embodiment the master controller coordinates with attached application specific processors or logic via the external system bus and associated driver software. In another embodiment the master controller coordinates with remote application specific logic via the computer network and associated driver software. In another embodiment the master controller coordinates with the local bus or external bus attached hard disk drive HDD storage controller.

In one embodiment the master controller communicates with one or more storage controllers where the storage device solid state storage device may appear as a storage device connected through a SCSI bus Internet SCSI iSCSI fiber channel etc. Meanwhile the storage device solid state storage device may autonomously manage objects or other data structures and may appear as an object file system or distributed object file system. The master controller may also be accessed by peer controllers and or application specific processors .

In another embodiment the master controller coordinates with an autonomous integrated management controller to periodically validate FPGA code and or controller software validate FPGA code while running reset and or validate controller software during power on reset support external reset requests support reset requests due to watchdog timeouts and support voltage current power temperature and other environmental measurements and setting of threshold interrupts. In another embodiment the master controller manages garbage collection to free erase blocks for reuse. In another embodiment the master controller manages wear leveling. In another embodiment the master controller allows the data storage device solid state storage device to be partitioned into multiple virtual devices and allows partition based media encryption. In yet another embodiment the master controller supports a solid state storage controller with advanced multi bit ECC correction. One of skill in the art will recognize other features and functions of a master controller in a storage controller or more specifically in a solid state storage device .

In one embodiment the solid state storage device controller includes a memory controller which controls a dynamic random memory array and or a static random memory array . As stated above the memory controller may be independent or integrated with the master controller . The memory controller typically controls volatile memory of some type such as DRAM dynamic random memory array and SRAM static random memory array . In other examples the memory controller also controls other memory types such as electrically erasable programmable read only memory EEPROM etc. In other embodiments the memory controller controls two or more memory types and the memory controller may include more than one controller. Typically the memory controller controls as much SRAM as is feasible and by DRAM to supplement the SRAM .

In one embodiment the logical to physical index is stored in memory and then periodically off loaded to a channel of the solid state storage media or other non volatile memory. One of skill in the art will recognize other uses and configurations of the memory controller dynamic memory array and static memory array .

In one embodiment the solid state storage device controller includes a DMA controller that controls DMA operations between the storage device solid state storage device and one or more external memory controllers and associated external memory arrays and CPUs . Note that the external memory controllers and external memory arrays are called external because they are external to the storage device solid state storage device . In addition the DMA controller may also control RDMA operations with requesting devices through a NIC and associated RDMA controller .

In one embodiment the solid state storage device controller includes a management controller connected to a management bus . Typically the management controller manages environmental metrics and status of the storage device solid state storage device . The management controller may monitor device temperature fan speed power supply settings etc. over the management bus . The management controller may support the reading and programming of erasable programmable read only memory EEPROM for storage of FPGA code and controller software. Typically the management bus is connected to the various components within the storage device solid state storage device . The management controller may communicate alerts interrupts etc. over the local bus or may include a separate connection to a system bus or other bus. In one embodiment the management bus is an Inter Integrated Circuit IC bus. One of skill in the art will recognize other related functions and uses of a management controller connected to components of the storage device solid state storage device by a management bus .

In one embodiment the solid state storage device controller includes miscellaneous logic that may be customized for a specific application. Typically where the solid state device controller or master controller is are configured using a FPGA or other configurable controller custom logic may be included based on a particular application customer requirement storage requirement etc.

The write data pipeline includes a packetizer that receives a data or metadata segment to be written to the solid state storage either directly or indirectly through another write data pipeline stage and creates one or more packets sized for the solid state storage media . The data or metadata segment is typically part of a data structure such as an object but may also include an entire data structure. In another embodiment the data segment is part of a block of data but may also include an entire block of data. Typically a set of data such as a data structure is received from a computer client or other computer or device and is transmitted to the solid state storage device in data segments streamed to the solid state storage device or computer . A data segment may also be known by another name such as data parcel but as referenced herein includes all or a portion of a data structure or data block.

Each data structure is stored as one or more packets. Each data structure may have one or more container packets. Each packet contains a header. The header may include a header type field. Type fields may include data attribute metadata data segment delimiters multi packet data structures data linkages and the like. The header may also include information regarding the size of the packet such as the number of bytes of data included in the packet. The length of the packet may be established by the packet type. The header may include information that establishes the relationship of the packet to a data structure. An example might be the use of an offset in a data packet header to identify the location of the data segment within the data structure. One of skill in the art will recognize other information that may be included in a header added to data by a packetizer and other information that may be added to a data packet.

Each packet includes a header and possibly data from the data or metadata segment. The header of each packet includes pertinent information to relate the packet to the data structure to which the packet belongs. For example the header may include an object identifier or other data structure identifier and offset that indicates the data segment object data structure or data block from which the data packet was formed. The header may also include a logical address used by the storage bus controller to store the packet. The header may also include information regarding the size of the packet such as the number of bytes included in the packet. The header may also include a sequence number that identifies where the data segment belongs with respect to other packets within the data structure when reconstructing the data segment or data structure. The header may include a header type field. Type fields may include data data structure attributes metadata data segment delimiters multi packet data structure types data structure linkages and the like. One of skill in the art will recognize other information that may be included in a header added to data or metadata by a packetizer and other information that may be added to a packet.

The write data pipeline includes an ECC generator that that generates one or more error correcting codes ECC for the one or more packets received from the packetizer . The ECC generator typically uses an error correcting algorithm to generate ECC check bits which are stored with the one or more data packets. The ECC codes generated by the ECC generator together with the one or more data packets associated with the ECC codes comprise an ECC chunk. The ECC data stored with the one or more data packets is used to detect and to correct errors introduced into the data through transmission and storage. In one embodiment packets are streamed into the ECC generator as un encoded blocks of length N. A syndrome of length S is calculated appended and output as an encoded block of length N S. The value of N and S are dependent upon the characteristics of the algorithm which is selected to achieve specific performance efficiency and robustness metrics. In one embodiment there is no fixed relationship between the ECC blocks and the packets the packet may comprise more than one ECC block the ECC block may comprise more than one packet and a first packet may end anywhere within the ECC block and a second packet may begin after the end of the first packet within the same ECC block. In one embodiment ECC algorithms are not dynamically modified. In one embodiment the ECC data stored with the data packets is robust enough to correct errors in more than two bits.

Beneficially using a robust ECC algorithm allowing more than single bit correction or even double bit correction allows the life of the solid state storage media to be extended. For example if flash memory is used as the storage medium in the solid state storage media the flash memory may be written approximately 100 000 times without error per erase cycle. This usage limit may be extended using a robust ECC algorithm. Having the ECC generator and corresponding ECC correction module onboard the solid state storage device the solid state storage device can internally correct errors and has a longer useful life than if a less robust ECC algorithm is used such as single bit correction. However in other embodiments the ECC generator may use a less robust algorithm and may correct single bit or double bit errors. In another embodiment the solid state storage device may comprise less reliable storage such as multi level cell MLC flash in order to increase capacity which storage may not be sufficiently reliable without more robust ECC algorithms.

In one embodiment the write pipeline includes an input buffer that receives a data segment to be written to the solid state storage media and stores the incoming data segments until the next stage of the write data pipeline such as the packetizer or other stage for a more complex write data pipeline is ready to process the next data segment. The input buffer typically allows for discrepancies between the rate data segments are received and processed by the write data pipeline using an appropriately sized data buffer. The input buffer also allows the data bus to transfer data to the write data pipeline at rates greater than can be sustained by the write data pipeline in order to improve efficiency of operation of the data bus . Typically when the write data pipeline does not include an input buffer a buffering function is performed elsewhere such as in the solid state storage device but outside the write data pipeline in the computer such as within a network interface card NIC or at another device for example when using remote direct memory access RDMA .

In another embodiment the write data pipeline also includes a write synchronization buffer that buffers packets received from the ECC generator prior to writing the packets to the solid state storage media . The write synch buffer is located at a boundary between a local clock domain and a solid state storage clock domain and provides buffering to account for the clock domain differences. In other embodiments synchronous solid state storage media may be used and synchronization buffers may be eliminated.

In one embodiment the write data pipeline also includes a media encryption module that receives the one or more packets from the packetizer either directly or indirectly and encrypts the one or more packets using an encryption key unique to the solid state storage device prior to sending the packets to the ECC generator . Typically the entire packet is encrypted including the headers. In another embodiment headers are not encrypted. In this document encryption key is understood to mean a secret encryption key that is managed externally from a solid state storage controller .

The media encryption module and corresponding media decryption module provide a level of security for data stored in the solid state storage media . For example where data is encrypted with the media encryption module if the solid state storage media is connected to a different solid state storage controller solid state storage device or server the contents of the solid state storage media typically could not be read without use of the same encryption key used during the write of the data to the solid state storage media without significant effort.

In a typical embodiment the solid state storage device does not store the encryption key in non volatile storage and allows no external access to the encryption key. The encryption key is provided to the solid state storage controller during initialization. The solid state storage device may use and store a non secret cryptographic nonce that is used in conjunction with an encryption key. A different nonce may be stored with every packet. Data segments may be split between multiple packets with unique nonces for the purpose of improving protection by the encryption algorithm.

The encryption key may be received from a client a server key manager or other device that manages the encryption key to be used by the solid state storage controller . In another embodiment the solid state storage media may have two or more partitions and the solid state storage controller behaves as though it was two or more solid state storage controllers each operating on a single partition within the solid state storage media . In this embodiment a unique media encryption key may be used with each partition.

In another embodiment the write data pipeline also includes an encryption module that encrypts a data or metadata segment received from the input buffer either directly or indirectly prior sending the data segment to the packetizer the data segment encrypted using an encryption key received in conjunction with the data segment. The encryption keys used by the encryption module to encrypt data may not be common to all data stored within the solid state storage device but may vary on an per data structure basis and received in conjunction with receiving data segments as described below. For example an encryption key for a data segment to be encrypted by the encryption module may be received with the data segment or may be received as part of a command to write a data structure to which the data segment belongs. The solid state storage device may use and store a non secret cryptographic nonce in each data structure packet that is used in conjunction with the encryption key. A different nonce may be stored with every packet. Data segments may be split between multiple packets with unique nonces for the purpose of improving protection by the encryption algorithm.

The encryption key may be received from a client a computer key manager or other device that holds the encryption key to be used to encrypt the data segment. In one embodiment encryption keys are transferred to the solid state storage controller from one of a solid state storage device computer client or other external agent which has the ability to execute industry standard methods to securely transfer and protect private and public keys.

In one embodiment the encryption module encrypts a first packet with a first encryption key received in conjunction with the packet and encrypts a second packet with a second encryption key received in conjunction with the second packet. In another embodiment the encryption module encrypts a first packet with a first encryption key received in conjunction with the packet and passes a second data packet on to the next stage without encryption. Beneficially the encryption module included in the write data pipeline of the solid state storage device allows data structure by data structure or segment by segment data encryption without a single file system or other external system to keep track of the different encryption keys used to store corresponding data structures or data segments. Each requesting device or related key manager independently manages encryption keys used to encrypt only the data structures or data segments sent by the requesting device .

In one embodiment the encryption module may encrypt the one or more packets using an encryption key unique to the solid state storage device . The encryption module may perform this media encryption independently or in addition to the encryption described above. Typically the entire packet is encrypted including the headers. In another embodiment headers are not encrypted. The media encryption by the encryption module provides a level of security for data stored in the solid state storage media . For example where data is encrypted with media encryption unique to the specific solid state storage device if the solid state storage media is connected to a different solid state storage controller solid state storage device or computer the contents of the solid state storage media typically could not be read without use of the same encryption key used during the write of the data to the solid state storage media without significant effort.

In another embodiment the write data pipeline includes a compression module that compresses the data for metadata segment prior to sending the data segment to the packetizer . The compression module typically compresses a data or metadata segment using a compression routine known to those of skill in the art to reduce the storage size of the segment. For example if a data segment includes a string of 512 zeros the compression module may replace the 512 zeros with code or token indicating the 512 zeros where the code is much more compact than the space taken by the 512 zeros.

In one embodiment the compression module compresses a first segment with a first compression routine and passes along a second segment without compression. In another embodiment the compression module compresses a first segment with a first compression routine and compresses the second segment with a second compression routine. Having this flexibility within the solid state storage device is beneficial so that clients or other devices writing data to the solid state storage device may each specify a compression routine or so that one can specify a compression routine while another specifies no compression. Selection of compression routines may also be selected according to default settings on a per data structure type or data structure class basis. For example a first data structure of a specific data structure may be able to override default compression routine settings and a second data structure of the same data structure class and data structure type may use the default compression routine and a third data structure of the same data structure class and data structure type may use no compression.

In one embodiment the write data pipeline includes a garbage collector bypass that receives data segments from the read data pipeline as part of a data bypass in a garbage collection system. A garbage collection system typically marks packets that are no longer valid typically because the packet is marked for deletion or has been modified and the modified data is stored in a different location. At some point the garbage collection system determines that a particular section of storage may be recovered. This determination may be due to a lack of available storage capacity the percentage of data marked as invalid reaching a threshold a consolidation of valid data an error detection rate for that section of storage reaching a threshold or improving performance based on data distribution etc. Numerous factors may be considered by a garbage collection algorithm to determine when a section of storage is to be recovered.

Once a section of storage has been marked for recovery valid packets in the section typically must be relocated. The garbage collector bypass allows packets to be read into the read data pipeline and then transferred directly to the write data pipeline without being routed out of the solid state storage controller . In one embodiment the garbage collector bypass is part of an autonomous garbage collector system that operates within the solid state storage device . This allows the solid state storage device to manage data so that data is systematically spread throughout the solid state storage media to improve performance data reliability and to avoid overuse and underuse of any one location or area of the solid state storage media and to lengthen the useful life of the solid state storage media .

The garbage collector bypass coordinates insertion of segments into the write data pipeline with other segments being written by clients or other devices. In the depicted embodiment the garbage collector bypass is before the packetizer in the write data pipeline and after the depacketizer in the read data pipeline but may also be located elsewhere in the read and write data pipelines . The garbage collector bypass may be used during a flush of the write pipeline to fill the remainder of the virtual page in order to improve the efficiency of storage within the solid state storage media and thereby reduce the frequency of garbage collection.

In one embodiment the write data pipeline includes a write buffer that buffers data for efficient write operations. Typically the write buffer includes enough capacity for packets to fill at least one virtual page in the solid state storage media . This allows a write operation to send an entire page of data to the solid state storage media without interruption. By sizing the write buffer of the write data pipeline and buffers within the read data pipeline to be the same capacity or larger than a storage write buffer within the solid state storage media writing and reading data is more efficient since a single write command may be crafted to send a full virtual page of data to the solid state storage media instead of multiple commands.

While the write buffer is being filled the solid state storage media may be used for other read operations. This is advantageous because other solid state devices with a smaller write buffer or no write buffer may tie up the solid state storage when data is written to a storage write buffer and data flowing into the storage write buffer stalls. Read operations will be blocked until the entire storage write buffer is filled and programmed. Another approach for systems without a write buffer or a small write buffer is to flush the storage write buffer that is not full in order to enable reads. Again this is inefficient because multiple write program cycles are required to fill a page.

For depicted embodiment with a write buffer sized larger than a virtual page a single write command which includes numerous subcommands can then be followed by a single program command to transfer the page of data from the storage write buffer in each solid state storage element to the designated page within each solid state storage element . This technique has the benefits of eliminating partial page programming which is known to reduce data reliability and durability and freeing up the destination bank for reads and other commands while the buffer fills.

In one embodiment the write buffer is a ping pong buffer where one side of the buffer is filled and then designated for transfer at an appropriate time while the other side of the ping pong buffer is being filled. In another embodiment the write buffer includes a first in first out FIFO register with a capacity of more than a virtual page of data segments. One of skill in the art will recognize other write buffer configurations that allow a virtual page of data to be stored prior to writing the data to the solid state storage media .

In another embodiment the write buffer is sized smaller than a virtual page so that less than a page of information could be written to a storage write buffer in the solid state storage media . In the embodiment to prevent a stall in the write data pipeline from holding up read operations data is queued using the garbage collection system that needs to be moved from one location to another as part of the garbage collection process. In case of a data stall in the write data pipeline the data can be fed through the garbage collector bypass to the write buffer and then on to the storage write buffer in the solid state storage media to fill the pages of a virtual page prior to programming the data. In this way a data stall in the write data pipeline would not stall reading from the solid state storage device .

In another embodiment the write data pipeline includes a write program module with one or more user definable functions within the write data pipeline . The write program module allows a user to customize the write data pipeline . A user may customize the write data pipeline based on a particular data requirement or application. Where the solid state storage controller is an FPGA the user may program the write data pipeline with custom commands and functions relatively easily. A user may also use the write program module to include custom functions with an ASIC however customizing an ASIC may be more difficult than with an FPGA. The write program module may include buffers and bypass mechanisms to allow a first data segment to execute in the write program module while a second data segment may continue through the write data pipeline . In another embodiment the write program module may include a processor core that can be programmed through software.

Note that the write program module is shown between the input buffer and the compression module however the write program module could be anywhere in the write data pipeline and may be distributed among the various stages . In addition there may be multiple write program modules distributed among the various states that are programmed and operate independently. In addition the order of the stages may be altered. One of skill in the art will recognize workable alterations to the order of the stages based on particular user requirements.

The read data pipeline includes an ECC correction module that determines if a data error exists in ECC blocks a requested packet received from the solid state storage media by using ECC stored with each ECC block of the requested packet. The ECC correction module then corrects any errors in the requested packet if any error exists and the errors are correctable using the ECC. For example if the ECC can detect an error in six bits but can only correct three bit errors the ECC correction module corrects ECC blocks of the requested packet with up to three bits in error. The ECC correction module corrects the bits in error by changing the bits in error to the correct one or zero state so that the requested data packet is identical to when it was written to the solid state storage media and the ECC was generated for the packet.

If the ECC correction module determines that the requested packets contains more bits in error than the ECC can correct the ECC correction module cannot correct the errors in the corrupted ECC blocks of the requested packet and sends an interrupt. In one embodiment the ECC correction module sends an interrupt with a message indicating that the requested packet is in error. The message may include information that the ECC correction module cannot correct the errors or the inability of the ECC correction module to correct the errors may be implied. In another embodiment the ECC correction module sends the corrupted ECC blocks of the requested packet with the interrupt and or the message.

In one embodiment a corrupted ECC block or portion of a corrupted ECC block of the requested packet that cannot be corrected by the ECC correction module is read by the master controller corrected and returned to the ECC correction module for further processing by the read data pipeline . In one embodiment a corrupted ECC block or portion of a corrupted ECC block of the requested packet is sent to the device requesting the data. The requesting device may correct the ECC block or replace the data using another copy such as a backup or mirror copy and then may use the replacement data of the requested data packet or return it to the read data pipeline . The requesting device may use header information in the requested packet in error to identify data required to replace the corrupted requested packet or to replace the data structure to which the packet belongs. In another embodiment the solid state storage controller stores data using some type of RAID and is able to recover the corrupted data. In another embodiment the ECC correction module sends an interrupt and or message and the receiving device fails the read operation associated with the requested data packet. One of skill in the art will recognize other options and actions to be taken as a result of the ECC correction module determining that one or more ECC blocks of the requested packet are corrupted and that the ECC correction module cannot correct the errors.

The read data pipeline includes a depacketizer that receives ECC blocks of the requested packet from the ECC correction module directly or indirectly and checks and removes one or more packet headers. The depacketizer may validate the packet headers by checking packet identifiers data length data location etc. within the headers. In one embodiment the header includes a hash code that can be used to validate that the packet delivered to the read data pipeline is the requested packet. The depacketizer also removes the headers from the requested packet added by the packetizer . The depacketizer may directed to not operate on certain packets but pass these forward without modification. An example might be a container label that is requested during the course of a rebuild process where the header information is required for index reconstruction. Further examples include the transfer of packets of various types destined for use within the solid state storage device . In another embodiment the depacketizer operation may be packet type dependent.

The read data pipeline includes an alignment module that receives data from the depacketizer and removes unwanted data. In one embodiment a read command sent to the solid state storage media retrieves a packet of data. A device requesting the data may not require all data within the retrieved packet and the alignment module removes the unwanted data. If all data within a retrieved page is requested data the alignment module does not remove any data.

The alignment module re formats the data as data segments of a data structure in a form compatible with a device requesting the data segment prior to forwarding the data segment to the next stage. Typically as data is processed by the read data pipeline the size of data segments or packets changes at various stages. The alignment module uses received data to format the data into data segments suitable to be sent to the requesting device and joined to form a response. For example data from a portion of a first data packet may be combined with data from a portion of a second data packet. If a data segment is larger than a data requested by the requesting device the alignment module may discard the unwanted data.

In one embodiment the read data pipeline includes a read synchronization buffer that buffers one or more requested packets read from the solid state storage media prior to processing by the read data pipeline . The read synchronization buffer is at the boundary between the solid state storage clock domain and the local bus clock domain and provides buffering to account for the clock domain differences.

In another embodiment the read data pipeline includes an output buffer that receives requested packets from the alignment module and stores the packets prior to transmission to the requesting device . The output buffer accounts for differences between when data segments are received from stages of the read data pipeline and when the data segments are transmitted to other parts of the solid state storage controller or to the requesting device . The output buffer also allows the data bus to receive data from the read data pipeline at rates greater than can be sustained by the read data pipeline in order to improve efficiency of operation of the data bus .

In one embodiment the read data pipeline includes a media decryption module that receives one or more encrypted requested packets from the ECC correction module and decrypts the one or more requested packets using the encryption key unique to the solid state storage device prior to sending the one or more requested packets to the depacketizer . Typically the encryption key used to decrypt data by the media decryption module is identical to the encryption key used by the media encryption module . In another embodiment the solid state storage media may have two or more partitions and the solid state storage controller behaves as though it was two or more solid state storage controllers each operating on a single partition within the solid state storage media . In this embodiment a unique media encryption key may be used with each partition.

In another embodiment the read data pipeline includes a decryption module that decrypts a data segment formatted by the depacketizer prior to sending the data segment to the output buffer . The data segment may be decrypted using an encryption key received in conjunction with the read request that initiates retrieval of the requested packet received by the read synchronization buffer . The decryption module may decrypt a first packet with an encryption key received in conjunction with the read request for the first packet and then may decrypt a second packet with a different encryption key or may pass the second packet on to the next stage of the read data pipeline without decryption. When the packet was stored with a non secret cryptographic nonce the nonce is used in conjunction with an encryption key to decrypt the data packet. The encryption key may be received from a client a computer key manager or other device that manages the encryption key to be used by the solid state storage controller .

In another embodiment the read data pipeline includes a decompression module that decompresses a data segment formatted by the depacketizer . In one embodiment the decompression module uses compression information stored in one or both of the packet header and the container label to select a complementary routine to that used to compress the data by the compression module . In another embodiment the decompression routine used by the decompression module is dictated by the device requesting the data segment being decompressed. In another embodiment the decompression module selects a decompression routine according to default settings on a per data structure type or data structure class basis. A first packet of a first object may be able to override a default decompression routine and a second packet of a second data structure of the same data structure class and data structure type may use the default decompression routine and a third packet of a third data structure of the same data structure class and data structure type may use no decompression.

In another embodiment the read data pipeline includes a read program module that includes one or more user definable functions within the read data pipeline . The read program module has similar characteristics to the write program module and allows a user to provide custom functions to the read data pipeline . The read program module may be located as shown in may be located in another position within the read data pipeline or may include multiple parts in multiple locations within the read data pipeline . Additionally there may be multiple read program modules within multiple locations within the read data pipeline that operate independently. One of skill in the art will recognize other forms of a read program module within a read data pipeline . As with the write data pipeline the stages of the read data pipeline may be rearranged and one of skill in the art will recognize other orders of stages within the read data pipeline .

The solid state storage controller includes control and status registers and corresponding control queues . The control and status registers and control queues facilitate control and sequencing commands and subcommands associated with data processed in the write and read data pipelines . For example a data segment in the packetizer may have one or more corresponding control commands or instructions in a control queue associated with the ECC generator . As the data segment is packetized some of the instructions or commands may be executed within the packetizer . Other commands or instructions may be passed to the next control queue through the control and status registers as the newly formed data packet created from the data segment is passed to the next stage.

Commands or instructions may be simultaneously loaded into the control queues for a packet being forwarded to the write data pipeline with each pipeline stage pulling the appropriate command or instruction as the respective packet is executed by that stage. Similarly commands or instructions may be simultaneously loaded into the control queues for a packet being requested from the read data pipeline with each pipeline stage pulling the appropriate command or instruction as the respective packet is executed by that stage. One of skill in the art will recognize other features and functions of control and status registers and control queues .

The solid state storage controller and or solid state storage device may also include a bank interleave controller a synchronization buffer a storage bus controller and a multiplexer MUX which are described in relation to .

The bank interleave controller directs one or more commands to two or more queues in the bank interleave controller and coordinates among the banks of the solid state storage media execution of the commands stored in the queues such that a command of a first type executes on one bank while a command of a second type executes on a second bank . The one or more commands are separated by command type into the queues. Each bank of the solid state storage media has a corresponding set of queues within the bank interleave controller and each set of queues includes a queue for each command type.

The bank interleave controller coordinates among the banks of the solid state storage media execution of the commands stored in the queues. For example a command of a first type executes on one bank while a command of a second type executes on a second bank . Typically the command types and queue types include read and write commands and queues but may also include other commands and queues that are storage media specific. For example in the embodiment depicted in erase and management queues are included and would be appropriate for flash memory NRAM MRAM DRAM PRAM etc.

For other types of solid state storage media other types of commands and corresponding queues may be included without straying from the scope of the invention. The flexible nature of an FPGA solid state storage controller allows flexibility in storage media. If flash memory were changed to another solid state storage type the bank interleave controller storage bus controller and MUX could be altered to accommodate the media type without significantly affecting the data pipelines and other solid state storage controller functions.

In the embodiment depicted in the bank interleave controller includes for each bank a read queue for reading data from the solid state storage media a write queue for write commands to the solid state storage media an erase queue for erasing an erase block in the solid state storage an a management queue for management commands. The bank interleave controller also includes corresponding read write erase and management agents . In another embodiment the control and status registers and control queues or similar components queue commands for data sent to the banks of the solid state storage media without a bank interleave controller .

The agents in one embodiment direct commands of the appropriate type destined for a particular bank to the correct queue for the bank . For example the read agent may receive a read command for bank and directs the read command to the bank read queue . The write agent may receive a write command to write data to a location in bank of the solid state storage media and will then send the write command to the bank write queue . Similarly the erase agent may receive an erase command to erase an erase block in bank and will then pass the erase command to the bank erase queue . The management agent typically receives management commands status requests and the like such as a reset command or a request to read a configuration register of a bank such as bank . The management agent sends the management command to the bank management queue

The agents typically also monitor status of the queues and send status interrupt or other messages when the queues are full nearly full non functional etc. In one embodiment the agents receive commands and generate corresponding sub commands. In one embodiment the agents receive commands through the control status registers and generate corresponding sub commands which are forwarded to the queues . One of skill in the art will recognize other functions of the agents .

The queues typically receive commands and store the commands until required to be sent to the solid state storage banks . In a typical embodiment the queues are first in first out FIFO registers or a similar component that operates as a FIFO. In another embodiment the queues store commands in an order that matches data order of importance or other criteria.

The bank controllers typically receive commands from the queues and generate appropriate subcommands. For example the bank write queue may receive a command to write a page of data packets to bank . The bank controller may receive the write command at an appropriate time and may generate one or more write subcommands for each data packet stored in the write buffer to be written to the page in bank . For example bank controller may generate commands to validate the status of bank and the solid state storage array select the appropriate location for writing one or more data packets clear the input buffers within the solid state storage memory array transfer the one or more data packets to the input buffers program the input buffers into the selected location verify that the data was correctly programmed and if program failures occur do one or more of interrupting the master controller retrying the write to the same physical location and retrying the write to a different physical location. Additionally in conjunction with example write command the storage bus controller will cause the one or more commands to multiplied to each of the each of the storage I O buses with the logical address of the command mapped to a first physical addresses for storage I O bus and mapped to a second physical address for storage I O bus and so forth as further described below.

Typically bus arbiter selects from among the bank controllers and pulls subcommands from output queues within the bank controllers and forwards these to the Storage Bus Controller in a sequence that optimizes the performance of the banks . In another embodiment the bus arbiter may respond to a high level interrupt and modify the normal selection criteria. In another embodiment the master controller can control the bus arbiter through the control and status registers . One of skill in the art will recognize other means by which the bus arbiter may control and interleave the sequence of commands from the bank controllers to the solid state storage media .

The bus arbiter typically coordinates selection of appropriate commands and corresponding data when required for the command type from the bank controllers and sends the commands and data to the storage bus controller . The bus arbiter typically also sends commands to the storage control bus to select the appropriate bank . For the case of flash memory or other solid state storage media with an asynchronous bi directional serial storage I O bus only one command control information or set of data can be transmitted at a time. For example when write commands or data are being transmitted to the solid state storage media on the storage I O bus read commands data being read erase commands management commands or other status commands cannot be transmitted on the storage I O bus . For example when data is being read from the storage I O bus data cannot be written to the solid state storage media .

For example during a write operation on bank the bus arbiter selects the bank controller which may have a write command or a series of write sub commands on the top of its queue which cause the storage bus controller to execute the following sequence. The bus arbiter forwards the write command to the storage bus controller which sets up a write command by selecting bank through the storage control bus sending a command to clear the input buffers of the solid state storage elements associated with the bank and sending a command to validate the status of the solid state storage elements associated with the bank . The storage bus controller then transmits a write subcommand on the storage I O bus which contains the physical addresses including the address of the logical erase block for each individual physical erase solid stage storage element as mapped from the logical erase block address. The storage bus controller then muxes the write buffer through the write sync buffer to the storage I O bus through the MUX and streams write data to the appropriate page. When the page is full then storage bus controller causes the solid state storage elements associated with the bank to program the input buffer to the memory cells within the solid state storage elements . Finally the storage bus controller validates the status to ensure that page was correctly programmed.

A read operation is similar to the write example above. During a read operation typically the bus arbiter or other component of the bank interleave controller receives data and corresponding status information and sends the data to the read data pipeline while sending the status information on to the control and status registers . Typically a read data command forwarded from bus arbiter to the storage bus controller will cause the MUX to gate the read data on storage I O bus to the read data pipeline and send status information to the appropriate control and status registers through the status MUX .

The bus arbiter coordinates the various command types and data access modes so that only an appropriate command type or corresponding data is on the bus at any given time. If the bus arbiter has selected a write command and write subcommands and corresponding data are being written to the solid state storage media the bus arbiter will not allow other command types on the storage I O bus . Beneficially the bus arbiter uses timing information such as predicted command execution times along with status information received concerning bank status to coordinate execution of the various commands on the bus with the goal of minimizing or eliminating idle time of the busses.

The master controller through the bus arbiter typically uses expected completion times of the commands stored in the queues along with status information so that when the subcommands associated with a command are executing on one bank other subcommands of other commands are executing on other banks . When one command is fully executed on a bank the bus arbiter directs another command to the bank . The bus arbiter may also coordinate commands stored in the queues with other commands that are not stored in the queues .

For example an erase command may be sent out to erase a group of erase blocks within the solid state storage media . An erase command may take 10 to 1000 times more time to execute than a write or a read command or 10 to 100 times more time to execute than a program command. For N banks the bank interleave controller may split the erase command into N commands each to erase a virtual erase block of a bank . While bank is executing an erase command the bus arbiter may select other commands for execution on the other banks . The bus arbiter may also work with other components such as the storage bus controller the master controller etc. to coordinate command execution among the buses. Coordinating execution of commands using the bus arbiter bank controllers queues and agents of the bank interleave controller can dramatically increase performance over other solid state storage systems without a bank interleave function.

In one embodiment the solid state controller includes one bank interleave controller that serves all of the storage elements of the solid state storage media . In another embodiment the solid state controller includes a bank interleave controller for each column of storage elements . For example one bank interleave controller serves one column of storage elements SSS . SSS M. . . . a second bank interleave controller serves a second column of storage elements SSS . SSS M. . . . etc.

The solid state storage controller includes a synchronization buffer that buffers commands and status messages sent and received from the solid state storage media . The synchronization buffer is located at the boundary between the solid state storage clock domain and the local bus clock domain and provides buffering to account for the clock domain differences. The synchronization buffer write synchronization buffer and read synchronization buffer may be independent or may act together to buffer data commands status messages etc. In one embodiment the synchronization buffer is located where there are the fewest number of signals crossing the clock domains. One skilled in the art will recognize that synchronization between clock domains may be arbitrarily moved to other locations within the solid state storage device in order to optimize some aspect of design implementation.

The solid state storage controller includes a storage bus controller that interprets and translates commands for data sent to and read from the solid state storage media and status messages received from the solid state storage media based on the type of solid state storage media . For example the storage bus controller may have different timing requirements for different types of storage storage with different performance characteristics storage from different manufacturers etc. The storage bus controller also sends control commands to the storage control bus .

In one embodiment the solid state storage controller includes a MUX that comprises an array of multiplexers where each multiplexer is dedicated to a row in the solid state storage array . For example multiplexer is associated with solid state storage elements . MUX routes the data from the write data pipeline and commands from the storage bus controller to the solid state storage media via the storage I O bus and routes data and status messages from the solid state storage media via the storage I O bus to the read data pipeline and the control and status registers through the storage bus controller synchronization buffer and bank interleave controller .

In one embodiment the solid state storage controller includes a MUX for each row of solid state storage elements e.g. SSS . SSS . SSS .N . A MUX combines data from the write data pipeline and commands sent to the solid state storage media via the storage I O bus and separates data to be processed by the read data pipeline from commands. Packets stored in the write buffer are directed on busses out of the write buffer through a write synchronization buffer for each row of solid state storage elements SSS x. to SSS x.N to the MUX for each row of solid state storage elements SSS x. to SSS x.N . The commands and read data are received by the MUXes from the storage I O bus . The MUXes also direct status messages to the storage bus controller .

The storage bus controller includes a mapping module . The mapping module maps a logical address of an erase block to one or more physical addresses of an erase block. For example a solid state storage media with an array of twenty storage elements e.g. SSS . to SSS M. per block may have a logical address for a particular erase block mapped to twenty physical addresses of the erase block one physical address per storage element. Because the storage elements are accessed in parallel erase blocks at the same position in each storage element in a row of storage elements will share a physical address. To select one erase block e.g. in storage element SSS . instead of all erase blocks in the row e.g. in storage elements SSS . . . . . .N one bank in this case bank is selected.

This logical to physical mapping for erase blocks is beneficial because if one erase block becomes damaged or inaccessible the mapping can be changed to map to another erase block. This mitigates the loss of losing an entire virtual erase block when one element s erase block is faulty. The remapping module changes a mapping of a logical address of an erase block to one or more physical addresses of a virtual erase block spread over the array of storage elements . For example virtual erase block may be mapped to erase block of storage element SSS . to erase block of storage element SSS . . . . and to storage element M. virtual erase block may be mapped to erase block of storage element SSS . to erase block of storage element SSS . . . . and to storage element M. etc. Alternatively virtual erase block may be mapped to one erase block from each storage element in an array such that virtual erase block includes erase block of storage element SSS . to erase block of storage element SSS . to storage element M. and erase block of storage element SSS . to erase block of storage element SSS . . . . and to storage element M. for each storage element in the array up to erase block of storage element M.N

If erase block of a storage element SSS. is damaged experiencing errors due to wear etc. or cannot be used for some reason the remapping module could change the logical to physical mapping for the logical address that pointed to erase block of virtual erase block . If a spare erase block call it erase block of storage element SSS . is available and currently not mapped the remapping module could change the mapping of virtual erase block to point to erase block of storage element SSS . while continuing to point to erase block of storage element SSS . erase block of storage element SSS . not shown . . . and to storage element M. . The mapping module or remapping module could map erase blocks in a prescribed order virtual erase block to erase block of the storage elements virtual erase block to erase block of the storage elements etc. or may map erase blocks of the storage elements in another order based on some other criteria.

In one embodiment the erase blocks could be grouped by access time. Grouping by access time meaning time to execute a command such as programming writing data into pages of specific erase blocks can level command completion so that a command executed across the erase blocks of a virtual erase block is not limited by the slowest erase block. In other embodiments the erase blocks may be grouped by wear level health etc. One of skill in the art will recognize other factors to consider when mapping or remapping erase blocks.

In one embodiment the storage bus controller includes a status capture module that receives status messages from the solid state storage media and sends the status messages to the status MUX . In another embodiment when the solid state storage media is flash memory the storage bus controller includes a NAND bus controller . The NAND bus controller directs commands from the read and write data pipelines to the correct location in the solid state storage media coordinates timing of command execution based on characteristics of the flash memory etc. If the solid state storage media is another solid state storage type the NAND bus controller would be replaced by a bus controller specific to the storage type. One of skill in the art will recognize other functions of a NAND bus controller .

The storage controller directly manages a solid state storage array . In one embodiment the storage controller directly manages a solid state storage array by managing and performing operations on the solid state storage media in the solid state storage array without any intervening independent hardware and or software layers or interfaces. In one embodiment the storage controller directly manages the solid state storage media by directly performing storage operations on the solid state storage media . A storage controller that directly manages solid state storage media may include various hardware and software controllers drivers and software such as the depicted hardware controllers .

The non volatile storage media may be embodied as solid state storage media a single solid state storage die a solid state storage drive a hard disk drive a set of hard disk drives and the like. The non volatile storage media may be one or more non volatile storage volumes embodied as one or more block oriented volumes comprising non volatile storage media that stores a plurality of data blocks. In one embodiment the one or more non volatile storage volumes are flash storage volumes each including one or more flash storage media. The non volatile storage media may also be embodied in one or more virtual or logical volumes formed by a physical volume partition or a plurality of physical volumes partitions. The non volatile storage media may also be embodied in one or more hybrid or hybrid virtual volumes. The non volatile storage media may reside in a single solid state storage device or a plurality of solid state storage devices. The non volatile storage media may also reside on other block oriented devices and systems such as a Storage Area Network SAN .

In one embodiment the depicted hardware controllers may be substantially similar to and include similar functionality as the solid state controllers and accompanying controllers and modules depicted in and or the bank interleave controller and storage bus controller depicted in . Furthermore the ECC correction module may be substantially similar and include similar functionality to the ECC correction module and or the ECC generator depicted in . In addition the read data pipeline and the write data pipeline may be substantially similar to the read data pipeline and the write data pipeline depicted in and . The solid state storage array may include an array of solid state storage banks similar to the solid state storage media and corresponding solid state storage banks depicted in .

In one embodiment the user application is a software application operating on or in conjunction with the storage client . The storage client manages files and data and utilizes the functions and features of the storage controller and associated solid state storage array . Representative examples of storage clients include but are not limited to a server a file system an operating system a database management system DBMS a volume manager and the like. The storage client is in communication with the storage controller . In one embodiment the storage client communicates through an Input Output I O interface represented by a block I O emulation layer .

Certain conventional block storage devices divide the storage media into volumes or partitions. Each volume or partition may include a plurality of sectors. One or more sectors are organized into a logical block. In certain storage systems such as those interfacing with the Windows operating systems the logical blocks are referred to as clusters. In other storage systems such as those interfacing with UNIX Linux or similar operating systems the logical blocks are referred to simply as blocks. A logical block or cluster represents a smallest physical amount of storage space on the storage media that is managed by the storage manager. A block storage device may associate n logical blocks available for user data storage across the storage media with a logical block address numbered from 0 to n. In certain block storage devices the logical block addresses may range from 0 to n per volume or partition. In conventional block storage devices a logical block address maps directly to a particular logical block. In conventional block storage devices each logical block maps to a particular set of physical sectors on the storage media.

However storage device does not directly or necessarily associate logical block addresses with particular physical blocks. These storage devices may emulate a conventional block storage interface to maintain compatibility with block storage clients .

When the storage client communicates through the block I O emulation layer the storage device appears to the storage client as a conventional block storage device. In one embodiment the storage controller provides a block I O emulation layer which serves as a block device interface or API. In this embodiment the storage client communicates with the storage device through this block device interface. In one embodiment the block I O emulation layer receives commands and logical block addresses from the storage client in accordance with this block device interface. As a result the block I O emulation layer provides the storage device compatibility with block storage clients .

In one embodiment a storage client communicates with the storage controller through a direct interface layer . In this embodiment the storage device directly exchanges information specific to non volatile storage devices. A storage device using direct interface may store data on the solid state storage media as blocks sectors pages logical blocks logical pages erase blocks logical erase blocks ECC chunks logical ECC chunks or in any other format or structure advantageous to the technical characteristics of the solid state storage media . The storage controller receives a logical address and a command from the storage client and performs the corresponding operation in relation to the non volatile solid state storage media . The storage controller may support a block I O emulation layer a direct interface or both a block I O emulation layer and a direct interface .

As described above certain storage devices while appearing to a storage client to be a block storage device do not directly associate particular logical block addresses with particular physical blocks also referred to in the art as sectors. Such storage devices may use a logical to physical translation layer . The logical to physical translation layer provides a level of abstraction between the logical block addresses used by the storage client and the physical block addresses at which the storage controller stores the data. The logical to physical translation layer maps logical block addresses to physical block addresses of data stored on solid state storage media . This mapping allows data to be referenced in a logical address space using logical identifiers such as a logical block address. A logical identifier does not indicate the physical location of data on the solid state storage media but is an abstract reference to the data.

The storage controller manages the physical block addresses in the physical address space. In one example contiguous logical block addresses may in fact be stored in non contiguous physical block addresses as the logical to physical translation layer determines the location on the solid state storage media to perform data operations.

Furthermore in one embodiment the logical address space is substantially larger than the physical address space. This thinly provisioned embodiment allows the number of logical identifiers for data references to greatly exceed the number of possible physical addresses.

In one embodiment the logical to physical translation layer includes a map or index that maps logical block addresses to physical block addresses. The map may be in the form of a b tree a content addressable memory CAM a binary tree and or a hash table and the like. In certain embodiments the logical to physical translation layer is a tree with nodes that represent logical block addresses and comprise corresponding physical block addresses.

As stated above in conventional block storage devices a logical block address maps directly to a particular physical block. When a storage client communicating with the conventional block storage device deletes data for a particular logical block address the storage client may note that the particular logical block address is deleted and can re use the physical block associated with that deleted logical block address without the need to perform any other action.

Conversely when a storage client communicating with a storage controller with a logical to physical translation layer a storage controller that does not map a logical block address directly to a particular physical block deletes a logical block address the corresponding physical block address remains allocated because the storage client does not communicate the change in used blocks to the storage controller . The storage client may not be configured to communicate changes in used blocks also referred to herein as data block usage information . Because the storage client uses the block I O emulation layer the storage client may erroneously believe that the storage controller is a conventional storage controller that would not utilize the data block usage information. Or in certain embodiments other software layers between the storage client and the storage controller may fail to pass on data block usage information.

Consequently the storage controller preserves the relationship between the logical block address and a physical address and the data on the storage device corresponding to the physical block. As the number of allocated blocks increases the performance of the storage controller may suffer depending on the configuration of the storage controller .

Specifically in certain embodiments the storage controller is configured to store data sequentially using an append only writing process and use a storage space recovery process that re uses non volatile storage media storing deallocated unused logical blocks. Specifically as described above the storage controller may sequentially write data on the solid state storage media in a log structured format and within one or more physical structures of the storage elements the data is sequentially stored on the solid state storage media .

As a result of storing data sequentially and using an append only writing process the storage controller achieves a high write throughput and a high number of I O operations per second IOPS . The storage controller includes a storage space recovery or garbage collection process that re uses data storage cells to provide sufficient storage capacity. The storage space recovery process reuses storage cells for logical blocks marked as deallocated invalid unused or otherwise designated as available for storage space recovery in the logical physical translation layer .

As described above the storage space recovery process determines that a particular section of storage may be recovered. Once a section of storage has been marked for recovery the storage controller may relocate valid blocks in the section. The storage space recovery process when relocating valid blocks copies the packets and writes them to another location so that the particular section of storage may be reused as available storage space typically after an erase operation on the particular section. The storage controller may then use the available storage space to continue sequentially writing data in an append only fashion. Consequently the storage controller expends resources and overhead in preserving data in valid blocks. Therefore physical blocks corresponding to deleted logical blocks may be unnecessarily preserved by the storage controller which expends unnecessary resources in relocating the physical blocks during storage space recovery.

Some storage devices are configured to receive messages or commands notifying the storage device of these unused logical blocks so that the storage device may deallocate the corresponding physical blocks. As used herein to deallocate a physical block includes marking the physical block as invalid unused or otherwise designating the physical block as available for storage space recovery its contents on storage media no longer needing to be preserved by the storage controller . Data block usage information in reference to the storage controller may also refer to information maintained by the storage controller regarding which physical blocks are allocated and or deallocated unallocated and changes in the allocation of physical blocks and or logical to physical block mapping information. Data block usage information in reference to the storage controller may also refer to information maintained by the storage controller regarding which blocks are in use and which blocks are not in use by a storage client. Use of a block may include storing of data in the block on behalf of the client reserving the block for use by a client and the like.

While physical blocks may be deallocated in certain embodiments the storage controller may not immediately erase the data on the storage media. An erase operation may be performed later in time. In certain embodiments the data in a deallocated physical block may be marked as unavailable by the storage controller such that subsequent requests for data in the physical block return a null result or an empty set of data.

One example of a command or message for such deallocation is the Trim function of the Data Set Management command under the T13 technical committee command set specification. A storage device upon receiving a Trim command may deallocate physical blocks for logical blocks whose data is no longer needed by the storage client . A storage controller that deallocates physical blocks may achieve better performance and increased storage space especially storage controllers that write data using certain processes and or use a similar data storage recovery process as that described above.

Consequently the performance of the storage controller is enhanced as physical blocks are deallocated when they are no longer needed such as through the Trim command or other similar deallocation commands issued to the storage controller . However certain storage clients such as operating systems or other software layers between the storage controller and the user application are not designed to issue or forward on these commands. For example a storage client may issue a deallocation command that never reaches the storage controller due to the failure of a software layer to forward the command. Additionally many storage clients that have the ability to issue deallocation commands do so insufficiently or lack the ability to issue commands for certain storage configurations. For example in event driven configurations that issue deallocation commands in response to changes to block usage when a deallocation command is dropped or lost such as when a storage device is improperly shut down the opportunity for the blocks corresponding to the dropped command to be trimmed has already passed until new changes are made which would allow them to be reevaluated as a trim candidate. Furthermore many storage clients cannot issue deallocation commands for a live storage volume that is actively servicing storage requests due to active storage operations continually modifying the physical blocks and or a block mapping index such as the logical physical translation layer .

A storage controller whose performance is enhanced with deallocation commands that never receives deallocation commands may suffer decreased performance as the actions of the storage client unsynchronize its unused logical blocks with the physical blocks of the storage controller . Therefore as depicted in embodiments of the present invention provide an alternate path for communicating data block usage information from the storage client to the storage controller . Those of skill in the art recognize that variations on the embodiments presented herein as examples also come within the scope and intent of the present invention as set forth in the claims. The present invention communicates the data block usage information such that the storage controller can use the data block usage information to operate more efficiently. In one embodiment the storage controller uses the data block usage information to synchronize the mapping of logical block addresses in the logical to physical layer to the mapping maintained by the storage client . In another embodiment the storage controller combines the data block usage information with other metadata in order to more efficiently manage the solid state storage array .

The system in certain embodiments also includes software operating in kernel mode the software including a storage client with a storage manager and a storage controller that includes a block usage synchronizer with an in flight block map and a combined block map . The storage controller also includes a control interface and a hardware interface manager . As is known in the art code in kernel mode has full access to system resources and runs the kernel and certain device drivers. Kernel mode memory is typically protected from applications running in user mode.

Furthermore the system also includes a hardware interface to solid state storage bank controllers operating an interface to solid state storage banks in a solid state storage array . The solid state storage array supports read write or program and erase operations and may include an array of solid state storage banks similar to the solid state storage array depicted in and the solid state storage media and corresponding solid state storage banks depicted in .

The solid state storage bank controllers may comprise solid state storage controller firmware and may be similar to and embodied by the solid state controllers depicted in and and similar controllers and hardware depicted in and . The hardware interface manager and the hardware interface cooperate to provide DMA data transfers command queuing command completion queuing interrupts ECC correction append only write functionality and other functionality similar to that provided by the storage controller of .

The storage controller may also be similar to the storage controller depicted in . Specifically the storage controller in certain embodiments registers with the host as a conventional block device driver with the associated device by providing block device emulation implements a log structured storage system maintains the logical to physical map implements storage space recovery and other functionality similar to that provided by the storage controller of . The storage controller may also include all or a portion of the hardware interface manager the hardware interface and the solid state storage bank controllers .

The storage manager manages the allocation of storage space for data structures that are stored or will be stored in the future on one or more storage devices including a storage device such as storage device . The storage manager determines which logical blocks are in use which logical blocks are unused which logical blocks are reserved and which logical blocks have changed state between used unused and reserved to a different state.

Typically the storage manager associates logical block addresses with files directories and or other storage data structures such as but not limited to objects or other data structures that are stored or will be stored in the future on the non volatile storage media such as the solid state storage media discussed above. The storage manager may include interface with or be included as part of a file system DBMS volume manager or portions of a storage client or operating system that manage files objects and other data structures that require storage capacity to be allocated for non volatile storage of the data structure. The storage manager may maintain one or more logical address to logical address mappings and or one or more logical address to physical address mappings for the storage data structures. In the depicted embodiment the storage manager resides in kernel mode and interfaces with applications operating in user mode. However in certain embodiments the storage manager or portions thereof may reside in user mode.

The storage manager maintains stores records provides and or manages data block usage information for logical blocks that are managed by the storage manager . Data block usage information includes information regarding which one or more logical blocks are allocated used and or which logical blocks are unallocated unused.

As used herein a logical block is allocated when it is considered a valid block when the logical block stores content corresponding to existing data of a file or other data structure when the logical block is unavailable for storing other content when the logical block reserves storage capacity on behalf of one or more storage clients and the like. Likewise a logical block is unallocated when it is considered an invalid block when the logical block does not store content corresponding to existing data of a file or other data structure when the logical block is available for storing other content when the logical block does not reserve storage capacity on behalf of one or more storage clients and the like.

Data block usage information in one embodiment includes free blocks and used blocks. Free blocks are blocks that are unallocated blocks. Unallocated blocks includes blocks that that were previously allocated and have now been freed as well as blocks that have not yet been allocated. The data block usage information may also include the identity of blocks currently allocated. Those of skill in the art recognize that given the number of blocks for a volume and the block sequencing free blocks can readily be derived from used blocks and vice versa.

Data block usage information may be in the form of metadata. In certain embodiments the data block usage information maintained by the storage manager is accessible retrievable and or referenceable by utilities or applications separate from the storage manager .

These utilities which interface with the storage controller by way of the control interface provide certain management maintenance optimization configuration and tuning functionality for storage devices coupled to or in communication with a host system. The utilities may include defragmentation utilities volume reconfiguration utilities disk performance utilities and the like.

The utilities may interface with the storage manager to obtain data about a file system disk or volume. As stated above the utilities may read access obtain or otherwise reference the data block usage information maintained by the storage manager . Specifically in one embodiment the utilities reference the data block usage information by way of a storage Application Programming Interface API of the storage manager through for example a function call.

In one embodiment the storage API is a pre existing API provided by the storage manager that describes data block usage information for a completely different purpose and in particular data block usage information for block devices. In one embodiment the API is a public API for block storage maintenance utilities. In a further embodiment the storage API is configured for storage media such as a hard disk drive which is a different storage media technology than solid state storage media . In one embodiment the API is a defragmentation API that block storage maintenance utilities use to defragment hard disk drive volumes.

In one embodiment the utilities include a block usage utility . The block usage utility interacts with the storage manager and communicates data block usage information from the storage manager to the storage controller such that the storage controller can use the data block usage information to operate more efficiently. The block usage utility in the depicted embodiment operates in user mode as a utility. In other embodiments all or a portion of the block usage utility operates in kernel mode.

The block usage utility facilitates access to the data block usage information that is managed by the storage manager . The block usage utility may directly interface with the storage manager to reference retrieve copy access and or obtain a pointer to the data block usage information. Alternatively the block usage utility cooperates with the storage client to obtain or reference the data block usage information.

The block usage utility provides the data block usage information to the storage controller . The storage controller utilizes the data block usage information to operate more efficiently. For example in one embodiment the storage controller may use the data block usage information to synchronize a mapping of logical block addresses in the logical to physical layer to a mapping maintained by the storage manager and or storage client . Of course the storage controller may use the data block usage information in other ways as well to improve operation of the storage device .

As described above the data block usage information may include allocated blocks or unallocated blocks. The block usage utility may determine the identity of allocated blocks by using the identity of unallocated blocks and the block usage utility may determine the identity of unallocated blocks by using the identity of allocated blocks. For example using the volume size and the allocated block information the block usage utility may determine which blocks are unallocated blocks.

In one embodiment the block usage utility references the data block usage information directly such as in a shared memory structure. In a further embodiment the block usage utility references the data block usage information in user mode through an API of the storage client and or the storage manager . The block usage utility may operate as an application or service in user mode or the equivalent functionality may be embedded directly into other modules such as the storage controller . In other words the storage controller may reference block usage information via a block usage utility or directly from the storage client or the storage manager . This may be done in one embodiment by mapping the block usage information user level API into kernel space and calling the user level API directly from the storage controller or by some other similar mechanism.

Those of skill in the art recognize that the data block usage information may be represented and communicated in many forms. For example in response to the function call the storage client and or the storage manager may return a data structure or identifier for a data structure that provides the data block usage information. The data structure storing the data block usage information may include a list file object table bit map and the like. One skilled in the art realizes that the data block usage information is not restricted to any particular data structure but may be embodied as one or more data structures known in the art. Furthermore the data block usage information may represent used unused block information for a single logical block a set of logical blocks the logical blocks for a particular volume logical blocks for a set of volumes and the like.

In one embodiment a data structure returned by the API function call is a block map . The API function call serves as an interface between the storage manager and the block usage utility . The block map is a bit map with each bit representing an allocable unit and the binary value for the bit representing whether the allocable unit is an allocated block or an unallocated block. An allocable unit may include a block one or more blocks a cluster or the like. The block map may represent every allocable unit of a volume a subset of allocable units of a volume or allocable units corresponding to a particular set of units for a volume such as a set of logical block addresses. For example the block usage utility may execute a function call to the storage API requesting a block map for ten logical blocks associated with a set of ten logical block addressees. The API may return a 10 1 block bit map indicating an active bit for the logical blocks that are allocated.

In one embodiment the block usage utility requests references or executes a function call for a block map for all logical blocks managed by the storage manager . In certain embodiments the block usage utility requests references or executes a function call for a block map for a set or group of logical blocks. For example the storage manager may provide through the storage API a block map defining data block usage information for a storage volume a group of storage volumes or for a set of logical blocks in a storage volume. In one embodiment the storage API receives a contiguous range of logical blocks and returns a block map indicating block usage for that range of logical blocks.

In certain embodiments the block usage utility calls a block usage function of the storage API designed and intended for use in defragmenting a block oriented storage device. Instead the block usage utility uses the same storage API function calls for communicating deallocation messages and or storage block allocation synchronization within the storage controller .

In one embodiment the storage API is a defragmentation API for block oriented storage devices. For example certain utilities may reference data block usage information from the defragmentation API in order to execute block defragmentation operations. Advantageously this data block usage information is used by the present invention to facilitate block usage synchronization. Re purposing this defragmentation API for communicating data block usage information to the storage controller enables the present invention to operate in existing storage architectures that provide a defragmentation API but do not support communication of data block usage information for improving operation of storage devices such as storage device that can use the data block usage information for more efficient operation.

The block usage utility operating in user mode may reference the block map for a set of logical blocks of a volume and communicate the data block usage information from the block map to the storage controller . The block usage utility may identify the set of logical blocks by providing the storage API a set of clusters or blocks for a partition volume. Alternatively the block usage utility may use other methods to identify the set of logical blocks and or logical block addresses.

In one embodiment with the block usage information of the block map the block usage utility sends a Trim command or other deallocation command for the unused blocks. The block usage utility may send the Trim command in response to the storage controller supporting the Trim command. The block usage utility may iterate through the logical block addresses of a volume selecting a set of logical block addresses to evaluate and sending messages to the storage controller identifying unused blocks from each set of logical blocks.

Advantageously the block usage utility may issue deallocation commands using data block usage information obtained directly from the storage manager . The storage controller does not need to rely on deallocation commands or notifications issued by other storage software layers. Similarly the block usage utility may also complement the deallocation methodologies of storage clients .

In another embodiment the block usage utility initiates a block usage synchronizer which is described in greater detail below to synchronize the data block usage information of the storage controller with the data bock usage information of the storage manager . In one embodiment the block usage utility initiates the block usage synchronizer by way of issuing the Trim command or by simply making a function call.

The block usage utility may initiate the block usage synchronizer in response to one or more predetermined events or at a predetermined time interval. In certain embodiments the block usage utility operates in such a manner that minimizes the workload on the storage controller and or computer resources. In addition the block usage utility may operate such that the synchronization operations of the block usage synchronizer impose a minimal workload on the storage controller and or computer resources. In certain embodiments the block usage utility minimizes the workload by passing a reference to the block map to the block usage synchronizer operating in kernel mode rather than passing a copy of the block map .

The block usage synchronizer facilitates synchronization of the storage manager s data block usage information and the storage controller s data block usage information which includes in one embodiment the physical block allocation mappings managed by the storage controller . Therefore in one embodiment the block usage synchronizer facilitates synchronization between the physical block allocation mappings managed by the storage controller and the logical block allocation mappings managed by the storage manager . In one embodiment the block usage synchronizer uses the data block usage information to synchronize the mapping of logical block addresses in the logical to physical layer See to the mapping maintained by the storage client .

In the depicted embodiment the block usage synchronizer executes in kernel mode. In other embodiments a portion of the block usage synchronizer may execute in user mode. In the depicted embodiment the block usage synchronizer executes inside of the storage controller . However in alternate embodiments the block usage synchronizer may execute outside the storage controller .

The block usage synchronizer accesses the data block usage information directly by way of the storage API or through the block usage utility . In one embodiment the block usage synchronizer accesses or receives data block usage information from the block usage utility operating in user mode. In another embodiment the block usage synchronizer calls a function of references and or accesses the data block usage information directly from within kernel mode. For example the block usage synchronizer may call the storage API directly from kernel mode to reference the block map .

Advantageously the block usage synchronizer may deallocate unused physical blocks and synchronize data block usage information when the storage controller communicates with storage clients that do not issue deallocation commands and without reliance on deallocation commands that may not reach the storage controller . Similarly the block usage synchronizer may also complement the deallocation methods of storage clients operating systems or file servers to provide more efficient block usage synchronization or to ensure full coverage of the block space.

In certain embodiments the storage controller manages a live volume actively servicing storage requests. To keep data block usage information current with storage operations on the live volume the block usage synchronizer may combine the data block usage information with other metadata reflecting added potential changes to data block usage information. In one embodiment the block usage synchronizer monitors certain storage operations after the block map is referenced. The block usage synchronizer may manage provide and or implement information about the block usage of in flight storage operations not included in the data block usage information. As used herein in flight storage operations are storage operations whose data block usage information is not included in the data block usage information due to the timing of the storage operations. In flight operations may include storage operations that modify a logical block and are executed by the storage controller subsequent to subsequent in time to or after the moment in time when the block usage synchronizer and or the block usage utility references the data block usage information. Similarly these in flight storage operations may be executed by the storage controller prior to the moment in time when the block usage synchronizer or storage controller deallocates the unused blocks based on the data block usage information or when the block usage utility communicates the data block usage information to the storage controller .

As with the data block usage information the in flight information may be represented stored and or communicated in many forms such as a data structure or identifier for a data structure. The data structure storing the in flight information may include a list file object table or the like. One skilled in the art realizes that the in flight information is not restricted to any particular data structure but may be embodied as one or more data structures known in the art. Furthermore the in flight information may represent storage operations the blocks modified by storage operations or the like. Examples of the storage operations may include writing data or reserving storage space in previously unused certain logical blocks. In one embodiment the in flight information indicates changes to the current set of logical blocks analyzed by the block usage synchronizer and corresponding to the logical blocks represented by the data block usage information.

In one embodiment the block usage synchronizer maintains the in flight information as a block map. The block usage synchronizer may use this in flight block map to update the data block usage information referenced through the storage manager . The block usage synchronizer may modify the data block usage information in the in flight block map for certain storage operations that change unused blocks represented in the block map to used blocks.

For example if a storage operation in a FIFO command queue is not yet executed when the block usage synchronizer references the block map the block map may become inaccurate because the storage operation may execute before a Trim command issued by the block usage synchronizer . To account for in flight block usage changes the block usage synchronizer cooperates with the storage controller to maintain the block usage information in the in flight block map . The in flight block map may be used to update the data block usage information of the block map .

In one embodiment the block usage synchronizer combines the block map and the in flight block map to produce a combined block map used to identify unused blocks. In certain embodiments the combined block map is a separate data structure such as a separate bit map of the same size as the block map and in flight block map . Alternatively the block usage synchronizer merges the in flight block map into the block map by way of an operation such as an OR binary operation. In such an embodiment the block map becomes the combined block map instead of using a separate data structure.

By monitoring in flight data operations with the in flight block map and building the combined block map the block usage synchronizer has the most current block usage information for performing the block usage synchronization. In addition the block usage information accurately represents unused blocks as identified by the storage manager . In one embodiment the block usage utility may detect or determine storage operations and thereby maintain manage and or store the in flight block map and or the combined block map .

In one embodiment the sub controllers include functionality and features similar to the storage controller described above. However the sub controllers may be configured to operate with the RAID storage controller . Furthermore in one embodiment each sub controller is configured to manage and operate a single solid state storage device . Alternatively or in addition a sub controller may manage and operate a plurality of solid state storage devices . For example a single sub controller may operate a two or more solid state storage devices in a RAID configuration such as a RAID 0 1 5 and or cooperate with the RAID storage controller to implement a composite RAID configuration such as RAID 10 or 01.

Although a RAID storage controller managing a plurality of sub controllers is depicted in one of ordinary skill in the art realizes that a single RAID storage controller may also manage a plurality of storage devices in a RAID configuration without using sub controllers . Furthermore the depicted RAID configuration may comprise a RAID 0 RAID 1 RAID 10 1 0 or RAID 5 configuration. In addition the RAID storage controller and sub controllers may be implemented in hardware software or a combination of hardware and software.

In certain RAID configurations i.e. RAID 1 storage devices may store identical data blocks as other storage devices in the RAID array such as a mirror storage device . For example storage device may mirror storage device and storage device may mirror storage device . In other RAID configurations each storage device in the RAID array may store different data blocks than other storage devices in the RAID array such as in a RAID 0 3 4 or 5 configuration in which data is striped across storage devices . Consequently certain portions of the data block usage information may pertain to certain storage devices . For example data may be striped across storage device and with a stride stored on each storage device .

Therefore when data is stored in a RAID configuration the block usage utility ensures that data block usage information is communicated to each storage sub controller of the RAID in accordance with the RAID configuration . For example for a RAID 1 configuration the block usage utility communicates the data block usage information to each storage sub controller participating in the mirroring configuration. Similarly for a RAID 0 configuration the block usage utility communicates the applicable portion of data block usage information to each applicable storage sub controller participating in the stripe configuration. Furthermore in other embodiments the block usage synchronizer see may ensure that data block usage information is synchronized for each storage device of the RAID array including mirror storage devices and that the data block usage information for each storage device is synchronized with its corresponding portion of the data block usage information from the storage manager when data is striped.

In one embodiment the RAID storage controller is configured to pass along unused block information to the appropriate sub controllers and or storage devices . In this embodiment the block usage utility communicates data block usage information to the RAID storage controller . The RAID storage controller may then communicate the data block usage information or unused block information to each sub controller and or storage device . The RAID storage controller may also determine the portion of the data block usage information to send to each sub controller storage device . Similarly in one embodiment the block usage synchronizer may synchronize the data block usage information on the RAID storage controller which then updates the unused blocks for each storage device or notifies the sub controller for each storage device of the unused blocks. The unused block information for each storage device in the RAID configuration may be maintained by the sub controllers or the RAID storage controller or by both in cooperation with each other.

In one embodiment the block usage utility communicates data block usage information unused block information to the RAID storage controller and also directs the RAID storage controller regarding one or more portions of the data block usage information unused block information to send to each sub controller storage device .

In another embodiment the block usage utility directly communicates the data block usage information unused block information to the sub controller managing each storage device . The sub controller receives the data block usage information unused block information for the blocks stored on the storage device under its control. Likewise the block usage synchronizer may also directly synchronize data block usage information for each storage device of the RAID by communicating directly with the sub controller for each storage device .

In certain embodiments the block usage utility directly communicates the data block usage information unused block information to one sub controller which then acts as a master sub controller and communicates the data block usage information unused block information to the other sub controllers . Similarly the block usage synchronizer may also synchronize data block usage information with a master sub controller that directs the other sub controllers accordingly.

The block usage utility block usage synchronizer may determine a RAID configuration also referred to as a device layout of the RAID storage controller and communicate data block usage information unused block information or synchronize data block usage information based on the determined RAID configuration. In another embodiment the RAID configuration is predetermined.

In one embodiment the RAID configuration comprises a RAID 0 configuration that stores data as a stripe across two or more storage devices . In this RAID configuration as is known in the art each storage device stores a portion of the data for the stripe. Similarly data block usage information pertaining to data that spans multiple storage devices in the stripe is divided among the storage devices of the stripe. In one embodiment the block usage utility block usage synchronizer identifies portions of the data block usage information corresponding to data blocks stored on each storage device and then sends a message or synchronizes with the appropriate storage controller block usage information based on the blocks stored on each storage device .

In one embodiment the RAID configuration comprises a RAID 1 configuration that mirrors data stored on a first storage device to a second storage device or that mirrors data stored on a first plurality of storage devices to a second plurality of storage devices . The block usage utility block usage synchronizer may communicate similar unused block information to or make similar synchronization changes of block usage information for the first storage device and the second mirror storage device or for the first plurality of storage devices and the second mirror plurality of storage devices 

In one embodiment the RAID configuration comprises a RAID 5 configuration that stores data as a stripe across three or more storage devices . The stripe comprises two or more data strides and a distributed parity data stride and each data stride is stored on a storage device . For example a first storage device and a second storage device may each store a data stride and a third storage device may store a parity data stride. The sub controller for each storage device storing a particular stride may maintain the data block usage information for that particular stride.

The parity calculation of the parity data stride is dependent on the data in each stride forming the stripe. In one embodiment as blocks of a stripe change state from used to unused the parity stride may be recalculated and rewritten. In another embodiment the block usage utility block usage synchronizer determines that each data stride in the stripe has no used blocks. If all of the data blocks of the data strides in the stripe are unused the block usage utility may then communicate that the stripe is unused and thus overhead in managing the parity data stride is avoided.

Similarly the block usage synchronizer may synchronize the data block usage information for the storage devices storing data strides of the stripe without affecting the parity calculation of the parity data stride because the whole stripe is unused. In certain embodiments after determining that the stripe has no used blocks the block usage utility block usage synchronizer designates data block usage information corresponding to the stripe as unused. The data block usage information corresponding to the stripe may be maintained by the RAID controller and or the sub controllers .

The RAID 10 configuration may mirror a stride of data between two or more storage devices and storage devices using a RAID 1 configuration and store stripes of data across two or more storage device sets using a RAID 0 configuration. For example storage device may include a first data stride mirrored onto storage device and storage device may include a second data stride mirrored onto storage device . In one embodiment the block usage utility block usage synchronizer identifies portions of the data block usage information corresponding to data blocks stored in each data stride sends a message to the corresponding RAID 0 controller RAID 1 controller and or sub controller or synchronizes data block usage information based on the blocks stored on each data stride and then sends a similar message or performs similar synchronization operations for the mirrored data strides.

The reference module facilitates access to data block usage information maintained by the storage manager . Specifically the reference module may reference retrieve copy access and or create a pointer to data block usage information which may include unused or unallocated data block information maintained by the storage manager . The reference module may reference this information for data blocks associated with logical block addresses of a non volatile storage volume managed by a storage manager or other non volatile storage media including solid state storage media .

In one embodiment the reference module references data block usage information for a set of logical block addresses for the non volatile storage volume. In certain embodiments the reference module references data block usage information for a subset of logical blocks from a total number of logical blocks maintained by the storage manager . For example the reference module may reference a set of logical blocks a group of logical blocks a range of logical blocks logical blocks associated with a volume and the like.

The non volatile storage volume may be a block oriented volume comprising non volatile storage media that stores a plurality of data blocks. In one embodiment the non volatile storage volume is a flash storage volume including one or more flash memory storage media. In one embodiment the non volatile storage volume is a storage device such as a hard disk drive or a solid state storage drive. In one embodiment the non volatile storage volume is a live online mounted volume actively servicing storage requests.

As described above in one embodiment the storage manager maintains the data block usage information for the storage client . In other embodiments the storage manager stores records provides and or manages the data block usage information for logical blocks stored by one or more storage clients and or storage managers .

As stated above the data block usage information may include the identity of used blocks or allocated blocks unused blocks or free blocks freed blocks or unallocated blocks that the storage manager has not allocated. In one embodiment the reference module references data block usage information comprising freed blocks unallocated by the storage manager within a certain period of time or subsequent to a certain event. For example the reference module may reference data block usage information for freed blocks unallocated since the last time the reference module referenced unallocated data block usage information.

In one embodiment referencing data block usage information requires a plurality of steps. The reference module may first reference data block usage information providing the identity of allocated blocks. The reference module may then determine the identity of unused or unallocated data blocks. The reference module may then determine if the unused blocks are recently freed or have never been allocated.

In one embodiment the reference module references data block usage information by way of a storage Application Programming Interface API of the storage manager . Alternatively the reference module references data block usage information by way of a storage Application Programming Interface API of the storage client . In one embodiment the storage API is a pre existing API included with the storage manager . In certain embodiments the storage API is not intended for deallocation commands or block synchronization. In one embodiment the storage API is a defragmentation API for block oriented storage devices .

In one embodiment the reference module operates in kernel mode and the reference module references data block usage information in kernel mode through the API. In another embodiment a portion of the reference module operates in user mode such as in the block usage synchronization utility and references the API in user mode. In this embodiment the portion of the reference module in user mode provides copies or otherwise makes available a pointer to the data block usage information or a copy of the data block usage information to the portion of the reference module in kernel mode.

As stated above the reference module may reference the data block usage information as a block map including a bit map that uses bits to represent allocated blocks or unallocated blocks although other data structures besides a block map may be used. The reference module may request a block map for a specific set of logical blocks.

The synchronization module synchronizes data block usage information managed by the storage controller with the data block usage information maintained by the storage manager . Data block usage information managed by the storage controller may include information in the logical to physical translation layer regarding logical block address to physical block address mapping. As stated above in one embodiment the storage manager and the storage controller communicate through a block device interface and the storage controller uses a logical to physical translation layer that maps logical block addresses to physical block addresses of data stored on solid state storage media . As a result in this embodiment the storage manager maintains the data block usage information separate from data block usage information managed by the storage controller and the data block usage information of the storage manager and the data block usage information of the storage controller can become unsynchronized particularly when the storage manager and or block device interface does not support deallocation message passing.

The synchronization module in one embodiment uses the data block usage information which represents unallocated or unused logical data blocks and deallocates the corresponding physical blocks on the non volatile solid state storage media managed by the storage controller . The synchronization module may also deallocate the corresponding physical blocks or cause the corresponding physical blocks to be deallocated. The synchronization module may directly deallocate the physical blocks. In another embodiment the synchronization module issues a command or sends a message for the storage controller to deallocate the physical blocks. In a further embodiment the storage controller returns a confirmation when the physical blocks have been successfully deallocated. Those of skill in the art recognize various ways that the synchronization module can deallocate the physical blocks in relation to the logical block identifiers or addresses including updating of flags or other metadata relating to the data block usage status.

In one embodiment the synchronization module synchronizes the logical to physical translation layer maintained by the storage controller . Specifically in one embodiment the synchronization module deallocates unused blocks by removing entries for the unused blocks in a logical to physical map or index or by removing nodes for the unused blocks in a logical to physical tree data structure. In another embodiment the synchronization module causes the storage controller to deallocate the unused blocks by removing marking or updating entries for the unused blocks in the logical to physical map or index.

Referring also to in one embodiment the storage controller includes a RAID storage controller storing data in a RAID configuration . The synchronization module may synchronize the data block usage information managed for the storage devices in the RAID array with the data block usage information from the storage manager . In one embodiment the synchronization module determines a RAID configuration of either the RAID storage controller or RAID storage controller and sub controllers . The synchronization module may then synchronize the data block usage information based on the determined RAID configuration . The RAID configuration may include information on the types of volumes in the RAID array the RAID configuration such as RAID 0 RAID 1 the number of storage devices and the like.

As described above the synchronization module may synchronize data block usage information by communicating signaling or sending a message to the RAID storage controller . For example the synchronization module may communicate with the RAID storage controller to synchronize one or more storage devices in the RAID array. The RAID storage controller may identify and or deallocate unused blocks in each appropriate storage device in the RAID array. In another embodiment the synchronization module may also communicate signal or send a message to the RAID storage controller and indicate the appropriate storage device and portion of the data block usage information for each storage device in the RAID array.

In another embodiment the synchronization module communicates signals or sends a message directly to a storage controller managing a storage device in the RAID array to synchronize the data block usage information.

In one embodiment the RAID configuration comprises a RAID 0 configuration that stores data as a stripe across two or more storage devices . The synchronization module may synchronize the data blocks of a storage device in the RAID array with data block usage information for the data blocks of the storage device . Specifically the synchronization module may identify a first portion of the data block usage information from the storage manager that corresponds to data blocks stored on a first storage device of the RAID array. The synchronization module may identify a second portion of the data block usage information corresponding to data blocks stored on a second storage device

The synchronization module may synchronize data block usage information managed for the first storage device with the first portion of the data block usage information from the storage manager . The synchronization module may also synchronize data block usage information managed for the second storage device with the second portion of the data block usage information from the storage manager . As a result the synchronization module synchronizes data block usage information for each storage device with the portion of the data block usage information particular to the blocks stored by each storage device .

In one embodiment the RAID configuration comprises a RAID 1 configuration. In this embodiment the synchronization module also synchronizes one or more mirror storage devices . Specifically the synchronization module may synchronize data block usage information managed for a first storage device with the data block usage information from the storage manager . The synchronization module may also synchronize data block usage information managed for a second storage device storing mirrored data of the first storage device with the data block usage information of the storage manager . However in one embodiment a storage device and a mirror storage device may share common data block usage information such as a common logical to physical mapping tree . Consequently when the synchronization module synchronizes such storage devices the mirror storage device is automatically synchronized when the synchronization module synchronizes the mirrored storage device . Therefore in this embodiment the synchronization module would not actively synchronize the mirror storage device

In one embodiment the RAID configuration comprises a RAID 5 configuration that stores data as a stripe across three or more storage devices and includes a distributed parity stride along with two or more data strides. The synchronization module may ensure that parity calculations for the parity data stride remain accurate. Specifically the synchronization module in one embodiment determines based on the data block usage information from the storage manager that each data stride in the stripe has no used blocks. The synchronization module may synchronize data block usage information managed for these data strides of the stripe with a corresponding portion of the data block usage information of the storage manager by designating or identifying the blocks in the stripe as unused. Therefore if the entire stripe not including the parity stride is made up of unused blocks the synchronization module may identify the entire stripe as unused without destroying the parity calculation for the parity data stride.

Referring now to in one embodiment the RAID configuration comprises a RAID 10 configuration that mirrors a stride of data between two or more storage devices using a RAID 1 configuration and that stores stripes of data across two or more storage device sets using a RAID 0 configuration. The synchronization module may synchronize data block usage information for each storage device and also synchronize data block usage information for each of the mirror storage devices

Specifically the synchronization module in one embodiment identifies a first portion of the data block usage information from the storage manager corresponding to data blocks stored in a first stride managed by the RAID storage controller . For example a sub controller may maintain data block usage information for the first stride on a first storage device . The synchronization module may identify a second portion of the data block usage information from the storage manager corresponding to data blocks stored on a second stride managed by the RAID storage controller . For example a sub controller may maintain data block usage information for the second stride on a second storage device . The synchronization module may synchronize data block usage information managed for the first stride with the first portion of the data block usage information from the storage manager . The synchronization module may synchronize data block usage information managed for the second stride with the second portion of the data block usage information from the storage manager .

In one embodiment the synchronization module synchronizes the data block usage information for the storage devices mirroring the first and second storage devices . In another embodiment as stated above a storage device and a mirror storage device may share common data block usage information. Consequently when the synchronization module synchronizes a storage device the mirrored storage device is synchronized also.

The block determination module determines one or more unused blocks from the data block usage information. In one embodiment the block determination module determines unused blocks by referencing bits in the bit map . The block map may be a bit map with each bit representing an allocable block and the binary value for the bit representing whether the allocable block is an allocated block or an unallocated block. If the block map shows allocated blocks the determination module may determine unallocated blocks from the allocated block information.

The deallocation module directly or indirectly deallocates physical blocks in the storage controller that correspond to unused logical blocks identified from the data block usage information from the storage manager . By deallocating the corresponding physical blocks the deallocation module synchronize data block usage information managed by the storage controller with the data block usage information maintained by the storage manager .

In one embodiment the deallocation module sends a message directly to the storage controller directly managing the non volatile storage volume. The message indicates to the storage controller unused blocks identified by the storage manager . The storage controller deallocates the unused blocks identified by the storage manager in response to the message.

For example the deallocation module may send a message indicating unused logical blocks to the storage controller . The storage controller may then deallocate the physical blocks that are mapped to the logical blocks within the mapping used by the logical to physical translation layer in response to the message. Those of skill in the art recognize a variety of different techniques for deallocating the logical blocks in a logical to physical index See . In one embodiment the storage controller deallocates a logical block by removing an entry for the logical block from the logical to physical index map or similar data structure. In certain embodiments the storage controller sends a reply or confirmation to the deallocation module indicating that the blocks have been successfully deallocated.

In one embodiment the deallocation module deallocates unused blocks in data block usage information maintained by the storage controller corresponding to the unused blocks in the data block usage information from the storage manager . If the storage controller already shows the block as unallocated or unused marking the block as unallocated or unused again causes no ill effects. In certain embodiments simply updating the data block usage information maintained by the storage controller may be more efficient than checking first to determine if the data block usage information differs.

In another embodiment the deallocation module first determines whether the storage controller indicates particular blocks as used blocks in contrast to the storage manager showing the blocks as unused. Specifically in certain embodiments the deallocation module deallocates blocks that the storage controller had maintained as used blocks. In these embodiments the deallocation module determines that the storage controller identifies unused blocks indicated by the data block usage information as used blocks and deallocates the used blocks identified by the storage controller corresponding to the one or more unused blocks.

As stated above in certain embodiments the logical to physical translation layer depicted in is a tree with nodes that represent logical block addresses and comprise corresponding physical block addresses. In one embodiment the deallocation module deallocates unused blocks by removing entries for the unused blocks in the logical to physical map. In another embodiment the deallocation module causes the storage controller to deallocate the unused blocks by removing entries for the unused blocks in the logical to physical map.

The update module updates data block usage information to account for operations of a live storage volume partition actively serving storage requests. In one embodiment the update module monitors in flight storage operations that modify the data block usage information. As described above these in flight storage operations may be executed by the storage controller subsequent to referencing the data block usage information. These in flight storage operations may be executed by the storage controller prior to synchronizing the data block usage information.

In one embodiment these in flight storage operations include the storage operations executed by the storage controller subsequent to referencing the block map and executed by the storage controller prior to deallocating the unused blocks as indicated by the storage manager . These in flight storage operations may not be included in the data block usage information having been launched or queued for execution before the data block usage information was accessed. However these in flight storage operations may still modify blocks that change the data block usage information because they are executed before the data block usage information is synchronized the unused blocks as indicated by the storage manager are deallocated or marked as unused . Therefore the update module accounts for these in flight storage operations.

In one embodiment the update module monitors storage operations on data blocks represented in the block map . Specifically in one embodiment the update module monitors the in flight storage operations for the particular set of data blocks for the block map referenced by the reference module .

The update module records data block usage information for the storage operations that change unused blocks of the block map to used blocks. In one embodiment the update module records the data block usage information of these storage operations in an in flight block map described above.

The user mode reference module facilitates access to the data block usage information when the storage API is accessible in user mode. The user mode reference module resides in the block usage utility and references the storage API in user mode. For example in certain embodiments the user mode reference module calls a storage API function. In one embodiment the user mode reference module provides copies or otherwise makes available the data block usage information or a pointer to the data block usage information to the user mode reference module in kernel mode. In another embodiment the user mode reference module provides copies or otherwise makes available the data block usage information or a pointer to the data block usage information to the synchronization module .

The initiation module initiates the processes of the block usage synchronizer . Referring to in certain embodiments the initiation module initiates the block usage synchronizer in response to one or more predetermined events. For example the initiation module may initiate the block usage synchronizer in response to a performance threshold an amount of storage space dropping below a threshold level a certain number of file deletions following a startup sequence a dual boot transition phase and the like.

In certain embodiments the initiation module may initiate the block usage synchronizer at a predetermined time interval. For example the initiation module may initiate the block usage synchronizer at a predetermined time every day or every hour after a certain amount of up time by the computer system and the like. The initiation module may also determine a set of logical blocks for the block usage synchronizer and send an indication of these logical blocks to the reference module . The initiation module may select sets of logical block addresses for analysis during a scan of one or more volumes.

The reference module facilitates access to data block usage information managed by the storage manager . Specifically the reference module may reference retrieve copy access and or create a pointer to data block usage information of the storage manager . The reference module may be similar to the reference module depicted in . In one embodiment the reference module operates in user mode and references data block usage information in user mode. In another embodiment the reference module operates in kernel mode and references data block usage information from kernel mode.

In one embodiment the reference module references data block usage information for a set of logical block addresses for the non volatile storage volume. In certain embodiments the reference module references data block usage information for a subset of logical blocks from a total number of logical blocks for a volume maintained by the storage manager . For example the reference module may reference data block usage information for a set of logical blocks a group of logical blocks a range of logical blocks and the like.

In one embodiment the reference module references the data block usage information by way of a storage API of the storage manager . In one embodiment the reference module references a block map such as block map defining data block usage information for the logical data blocks selected by the reference module . The reference module may request a block map for a specific set of logical blocks. One or more events may trigger or activate the reference module . In addition or alternatively the reference module may operate according to a predetermined schedule.

The data block usage information may include the identity of free blocks freed blocks or blocks that the storage manager has not allocated. In one embodiment the reference module references freed blocks deallocated by the storage manager within a certain period of time or subsequent to a certain event.

In one embodiment referencing data block usage information includes a plurality of steps. The reference module may first reference data block usage information showing the identity of allocated logical blocks. The reference module may next determine the identity of unused or unallocated data blocks. The reference module may then determine if the unused blocks are recently freed blocks or logical blocks that have never been allocated.

In one embodiment the reference module determines one or more unused blocks from the block map . The unused blocks may be logical blocks. In one embodiment the reference module determines unused blocks by reading bits in the bit map . Each bit may depending on the embodiment represent a used block one that corresponds to valid data or an unused block.

In one embodiment the reference module does not determine unused blocks from the block map . In this embodiment the reference module receives a list of unused blocks from the storage manager which the reference module passes to the message module directly without the need to determine unused blocks.

The message module communicates the data block usage information to the storage controller . In one embodiment the message module sends a message directly to the storage controller managing the non volatile storage media. The message may include unused block information identifying to the storage controller the unused logical blocks that the storage manager identifies. The message module may receive a list of unused blocks directly from the reference module. In one embodiment the message module sends a message for each logical block identified that is no longer in use as defined by the storage manager . In another embodiment the message module sends a message for a set of logical blocks.

In certain embodiments the message complies with an interface operable to communicate storage information between the storage manager and the storage controller . In one embodiment the message is a Trim message or command. In one embodiment the message comprises a notification passing the block usage information to the storage controller . In one embodiment the message comprises a notification passing unused block information to the storage controller . The unused block information may include the unused blocks identified by the storage manager. In certain embodiments the notification includes no requirement for action by the storage controller in accordance with the interface. As a result the storage controller may or may not deallocate the physical blocks identified from the unused block information. In accordance with the interface the storage controller determines if deallocating the physical blocks is advantageous.

In one embodiment according to an interface the message includes a directive passing block usage information and or unused block information to the storage controller . The block usage information and or unused block information may include the unused blocks identified by the storage manager. In this embodiment the directive requires the storage controller to erase the non volatile storage media comprising the unused blocks in accordance with the interface. As a result the message module may ensure that the storage controller erases non volatile storage media corresponding to the unused blocks. In one embodiment the storage controller passes a response message or confirmation that indicates the storage controller has complied with the directive and erased the non volatile storage media.

In one embodiment the storage controller may delay or defer performing the erase operation of the non volatile storage media comprising the unused blocks until later in time or until the storage media for the unused blocks is needed. Instead the storage controller may update the logical to physical map to mark the appropriate logical blocks as unused blocks. In certain embodiments marking the logical blocks as unused is sufficient to erase the logical blocks without erasing the media because the storage controller is configured to respond to read requests for those logical blocks with an indication that no data exists for example by returning all zeros or null values instead of the data stored on the non volatile storage media. In certain embodiments the marking that the logical blocks as unused may be lost due to not recording the marking in non volatile memory prior to a power loss. Consequently when the storage controller reconstructs an index used by the logical to physical translation layer by scanning the solid state storage media in the order that the data was written the storage controller may still identify the logical blocks as used. However the storage manager indicates the logical blocks as unused so no read requests will be made for these logical blocks.

In one embodiment according to an interface the message includes a purge instruction passing the block usage information and or unused block information to the storage controller . The block usage information and or unused block information may include the unused blocks identified by the storage manager. In this embodiment the purge instruction requires the storage controller to perform an erase operation on the non volatile storage media comprising the unused blocks and to overwrite the unused blocks one or more times using a predefined pattern in accordance with the interface. In one embodiment the storage controller uses one or more iterations of writing one or more different data patterns in order to completely alter the binary values in the unused blocks to ensure that the original data is unrecoverable.

As a result the message module ensures that the storage controller overwrites data corresponding to the unused blocks. Advantageously the storage controller may overwrite sensitive data to prevent the chance of unauthorized access. In one embodiment the purge instruction requires the storage controller to identify and overwrite previous versions of data stored in earlier locations in a log based storage format as described above to ensure a complete overwrite and secure erasure of the data. In one embodiment the storage controller passes a response message or confirmation that indicates the storage controller has complied with the purge instruction and overwritten the non volatile storage media.

Referring also to in one embodiment the storage controller includes a RAID storage controller storing data in a RAID configuration . The message module may send one or more messages communicating the unused blocks identified by the storage manager to a RAID storage controller or to one or more sub controllers . In one embodiment the message module determines a RAID configuration of either the RAID storage controller or the RAID storage controller and sub controllers . The message module may then send messages to communicate the unused blocks based on the determined RAID configuration .

As described above the message module may send a message to the RAID storage controller if the RAID storage controller is configured to update unused block information for the appropriate storage device in the RAID array. In another embodiment the message module may send a message directly to a sub controller managing a storage device in the RAID array.

In one embodiment the RAID configuration comprises a RAID 0 configuration that stores data as a stripe across two or more storage devices . The message module may send a message to a sub controller of a storage device in the RAID array with unused block information specific to that storage device . Specifically the message module may identify a first portion of the block map that corresponds to data blocks stored on a first storage device managed by a RAID controller or sub controller . The message module may identify a second portion of the block map corresponding to data blocks stored on a second storage device

The message module may send a first message to the RAID controller or sub controller identifying one or more unused blocks on the first storage device identified by the first portion of the block map . The message module may send a second message to the RAID controller or sub controller identifying one or more unused blocks on the second storage device identified by the second portion of the block map . As a result the message module may customize the messages sent to the RAID controller or sub controller for each storage device .

In one embodiment the RAID configuration comprises a RAID 1 configuration with one or more mirrored storage devices . In this embodiment the message module may also send a message to the RAID controller or sub controller managing one or more mirror devices . Specifically the message module may send a first message to the RAID controller or sub controller managing a first storage device the message identifying one or more unused blocks on the first storage device identified by the block map . The message module may send a second message to the RAID controller or sub controller managing the second storage device storing mirrored data of the first storage device identifying one or more unused blocks on the second storage device identified by the block map .

However as described above in one embodiment a storage device and a mirrored storage device may share common data block usage information. Consequently the message for one storage device may have equal applicability to the mirror storage device without the need for additional messages.

In one embodiment the RAID configuration comprises a RAID 5 configuration that stores data as a stripe across three or more storage devices and includes a distributed parity stride along with two or more data strides. To maintain parity integrity the message module in one embodiment determines based on the block map that each data stride in the stripe has no used blocks. The message module may send a message to the RAID storage controller designating data blocks corresponding to the stripe as unused.

Referring now to in one embodiment the RAID configuration comprises a RAID 10 configuration that mirrors a stride of data between two or more storage devices using a RAID 1 configuration and that stores stripes of data across two or more storage device sets using a RAID 0 configuration. The message module may send messages particular to the stride on each storage device and also send messages communicating unused blocks for the mirror storage devices

Specifically the message module in one embodiment identifies a first portion of the block map corresponding to data blocks stored in a first stride managed by the RAID storage controller . For example a sub controller may maintain data block usage information for data blocks of the first stride on a first storage device . The message module may identify a second portion of the block map corresponding to data blocks stored in a second stride managed by the RAID storage controller . For example a sub controller may maintain data block usage information for data blocks of the second stride on a second storage device . The message module may send a first message to the sub controller managing the first stride identifying one or more unused blocks in the first stride identified by the first portion of the block map . The message module may also send a second message to the sub controller managing the second stride identifying one or more unused blocks in the second stride identified by the second portion of the block map .

In one embodiment the message module also sends messages for the storage devices mirroring the first and second storage devices . In another embodiment as stated above a storage device and a mirrored storage device may share common data block usage information.

The determination module determines one or more unused blocks from the block map . The unused blocks may be logical blocks. In one embodiment the determination module determines unused blocks by reading bits in the bit map . Each bit may depending on the embodiment represent a used block one that corresponds to valid data or an unused block.

The monitor module monitors storage operations on data blocks represented by the block map to account for operation of a live volume actively servicing storage requests. Specifically in one embodiment the monitor module monitors the in flight storage operations for the particular set of data blocks for the block map referenced by the reference module . These in flight storage operations include the storage operations executed by the storage controller subsequent to referencing the block map and executed by the storage controller prior to deallocating the unused blocks.

The record module records data block usage information for the in flight storage operations that change unused blocks of the block map to used blocks. In one embodiment the record module records the data block usage information of these storage operations in an in flight block map as described above. The record module may record the logical block addresses of logical blocks affected by the in flight storage operations monitored by the monitor module . In one embodiment the in flight block map is a bit map having the same size and structure as the block map . Accordingly the record module may record used blocks by setting a corresponding bit in the in flight block map .

The map combination module updates the block map See to reflect changes from in flight storage operations. In one embodiment the map combination module combines the block map and the in flight block map to identify the unused blocks of the data blocks. In one embodiment the map combination module combines the block map and the in flight block map into a combined block map that identifies the unused blocks of the data blocks being monitored. In one embodiment the block map is OR ed with the in flight block map to combine the maps and determine updated data block usage information.

The deallocation module deallocates unused physical blocks to synchronize the data block usage information managed by the storage controller with the data block usage information maintained by the storage manager . In certain embodiments the deallocation module deallocates blocks that the storage controller maintains as used blocks or blocks that hold data that the storage controller is preserving. Specifically in one embodiment the deallocation module deallocates used blocks identified by the storage controller corresponding to unused blocks identified by the storage manager based on data block usage information. In another embodiment the deallocation module determines that the storage controller identifies used blocks indicated by the data block usage information as unused blocks and deallocates the used blocks identified by the storage controller corresponding to the one or more unused blocks.

In one embodiment the deallocation module deallocates blocks by removing entries for the unused blocks in the logical to physical map. In another embodiment the deallocation module signals the storage controller to perform the deallocation. In one embodiment the deallocation module updates unused block information and or data block usage information recorded on the non volatile storage media in place of or in addition to updates to the unused block information and or data block usage information in the logical to physical map. In this embodiment the deallocation module may indicate in log based storage that the unused blocks are deallocated and available for storage space recovery. The deallocation module may update a storage space recovery data structure stored in volatile memory or in non volatile memory. The storage space recovery data structure may track for the storage controller which physical parts of the storage media are available for storage space recovery. For example the storage space recovery data structure may record which logical erase blocks LEB or parts of LEBs are available for data recovery. In one embodiment the deallocation module updates the storage space recovery data structure in response to or in conjunction with deallocating blocks by removing entries for the unused blocks in the logical to physical map.

The lock module maintains data integrity during changes to the logical to physical translation layer of the storage controller . In one embodiment the lock module obtains a lock on the logical to physical map data structure managed by the storage controller prior to updating the block map to include in flight storage operations. The lock module releases the lock on the logical to physical map subsequent to the storage controller deallocating the unused blocks. The lock module ensures that changes to the logical to physical map are synchronized so as to not cause errors or data failures from other processes accessing the logical to physical map. In one embodiment the lock module obtains the lock before the map combination module combines the block map and the in flight block map so that no other in flight operations modify the logical to physical map.

The method begins and the reference module references data block usage information for data blocks of a non volatile storage volume managed by a storage manager . The storage manager maintains the data block usage information which the reference module may reference through a storage API of the storage manager . In certain alternative embodiments reference module determines unused or unallocated data block information from the data block usage information and provides the unused or unallocated data block information to the synchronization module .

Next the synchronization module synchronizes data block usage information managed by a storage controller with the data block usage information maintained by the storage manager . The storage manager maintains the data block usage information separate from data block usage information managed by the storage controller . The synchronization module may synchronize the data block usage information based on a RAID configuration if the storage controller is a RAID storage controller . Then the method ends.

The method begins and the reference module references data block usage information for data blocks of a non volatile storage volume managed by a storage manager . Next the update module updates the data block usage information based on storage operations that modify the data block usage information. These in flight storage operations are those operations that are executed by the storage controller subsequent to referencing the data block usage information and executed by the storage controller prior to synchronizing the data block usage information.

The block determination module then determines one or more unused blocks from the data block usage information which includes the data block usage information from the in flight storage operations. The block determination module in certain embodiments may determine the unused blocks as those that are freed blocks versus those that are free blocks. If the deallocation module is configured to directly perform deallocation on the blocks the deallocation module deallocates used blocks identified by the storage controller corresponding to unused blocks identified by the data block usage information.

Alternatively the deallocation module sends a message directly to the storage controller directly managing the non volatile storage volume. The message indicates unused blocks identified by the data block usage information obtained from the storage manager and updated by the update module . The storage controller then deallocates the identified unused blocks in response to the message and the method ends.

The method begins and the reference module references a block map defining data block usage information for data blocks of non volatile storage media managed by a storage manager . The block map is maintained by the storage manager and may be referenced through functionality provided by the storage manager . Next the message module sends a message directly to a storage controller . The message includes unused block information indicating to the storage controller the unused blocks identified by the data block usage information of the block map . The message module may send one or more messages to one or more RAID storage controllers and or sub controllers based on a RAID configuration. Then the method ends. Depending on the type of message sent the storage controller may then determine whether to act on the unused block information in the message comply with the message and act and or comply with the message by performing a secure erase of the data on the media for the unused block information.

The method begins and the reference module selects a set of logical blocks for analysis. For example the reference module may select a set of logical blocks during a progressive scan of logical block addresses of a volume.

Then the reference module references a block map defining data block usage information for a set of data blocks of non volatile storage media managed by a storage manager . The non volatile storage media may be solid state storage media such as flash. The block map is maintained by the storage manager and may be referenced calling a function of a storage API of the storage manager .

Next the monitor module monitors storage operations on data blocks represented by the block map to detect in flight operations or operations executed by the storage controller subsequent to referencing the block map and executed by the storage controller prior to deallocating blocks for a storage volume. The record module then records in an in flight block map data block usage information for the monitored in flight storage operations that change unused blocks to used blocks.

Next the lock module obtains a lock on a logical to physical map or other address mapping index. In one embodiment the lock module obtains the lock on the logical to physical map to keep other in flight storage operations from simultaneously updating the logical to physical map and or the combined block map . The map combination module then combines the block map and the in flight block map into a combined block map to update the one or more unused blocks of the data blocks. As a result the data block usage information provided by the storage manager as a snapshot is updated to account for operations executed before the storage controller or deallocation module deallocates in accordance with the data block usage information.

The storage controller deallocates the unused blocks identified by the combined block map . The storage controller may deallocate the unused blocks in response to a message sent by the message module identifying the unused blocks. Alternatively the deallocation module may directly deallocate used blocks on the storage controller that correspond to the unused block information identified by determination module . The lock module releases the lock on the logical to physical mapping and the method ends. The method may be repeated for various sets of logical blocks during for example a progressive scan of logical block addresses in one or more volumes.

The present invention may be embodied in other specific forms without departing from its spirit or essential characteristics. The described embodiments are to be considered in all respects only as illustrative and not restrictive. The scope of the invention is therefore indicated by the appended claims rather than by the foregoing description. All changes which come within the meaning and range of equivalency of the claims are to be embraced within their scope.

