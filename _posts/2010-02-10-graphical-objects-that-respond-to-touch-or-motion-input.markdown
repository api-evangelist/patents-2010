---

title: Graphical objects that respond to touch or motion input
abstract: A first graphical object on a user interface of a device can be transformed to a second graphical object on the user interface. The second graphical object can be manipulated by a user on the user interface using touch input or by physically moving the device. When manipulated, the object can be animated to appear to have mass that responds to real-world, physical forces, such as gravity, friction or drag. The data represented by the second graphical object can be compressed or archived using a gesture applied to the second graphical object. Graphical objects can be visually sorted on the user interface based on their mass (size). The visual appearance of graphical objects on the user interface can be adjusted to indicate the age of data represented by the graphical objects.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08839150&OS=08839150&RS=08839150
owner: Apple Inc.
number: 08839150
owner_city: Cupertino
owner_country: US
publication_date: 20100210
---
This subject matter is generally related to user interactions with graphical objects displayed on a user interface of a device.

Conventional personal computers include operating systems that often provide a virtual desktop metaphor where users can manipulate and organize various graphical objects. This metaphor is easily understood by users because it is intuitive and relates to their real world physical environment. Modern computing devices such as smart phones often provide a large variety of applications. Some of these applications however provide interfaces that lack an equivalent of the desktop metaphor and as a result are more difficult to comprehend by the average user.

A first graphical object on a user interface of a device can be transformed to a second graphical object on the user interface. The second graphical object can be manipulated by a user on the user interface using touch input or by physically moving the device. When manipulated the object can be animated to appear to have mass that responds to real world physical forces such as gravity friction or drag. The data represented by the second graphical object can be compressed or archived using a gesture applied to the second graphical object. Graphical objects can be visually sorted on the user interface based on their mass size . The visual appearance of graphical objects on the user interface can be adjusted to indicate the age of data represented by the graphical objects.

In some implementations a first graphical object is presented on a user interface of a first device. Touch input is received through the user interface. In response to the touch input the first graphical object is transformed into a second graphical object. Motion input is received from one or more motion sensors onboard the first device. In response to the motion input the second graphical object is animated in the user interface so that it appears to respond to the motion input.

In some implementations a first graphical object is presented on a user interface of a device. A first touch input is received through the user interface. In response to the first touch input the first graphical object is transformed into a second graphical object. A second touch input is received when a user touches the second graphical object in the user interface. In response to the second touch input the second graphical object is animated in the user interface so that it appears to become smaller on the user interface and data associated with the second graphical object is compressed or archived.

In some implementations a user interface is presented on a display of a device. Graphical objects representing data are displayed on the user interface. A request is received to sort the graphical objects based on the relative sizes of the data the graphical objects represent. In response to the request the graphical objects are arranged in a hierarchy on the user interface according to the relative sizes of the data. The arranging includes animating the graphical objects in the user interface so that each object appears to have a mass that responds to real world physical forces.

In some implementations a user interface is presented on a display of a device. An object is displayed on the user interface. The object represents data having an age. The visual appearance of the object on the user interface is adjusted based on the age of the data.

In some implementations device includes touch sensitive display . Touch sensitive display can implement liquid crystal display LCD technology light emitting polymer display LPD technology or some other display technology. Touch sensitive display can be sensitive to haptic and or tactile contact with a user. In some implementations touch sensitive display is also sensitive to touch inputs received in proximity to but not actually touching display . In addition device can include a touch sensitive surface e.g. a trackpad or touchpad .

In some implementations touch sensitive display can include a multi touch sensitive display. A multi touch sensitive display can for example process multiple simultaneous points of input including processing data related to the pressure degree and or position of each point of input. Such processing facilitates gestures and interactions with multiple fingers chording and other interactions. Other touch sensitive display technologies can also be used e.g. a display in which contact is made using a stylus or other input tool.

A user can interact with device using various touch inputs e.g. when a user touches touch sensitive display . Gesture inputs can also be derived from multiple touch inputs e.g. where a user moves his or her finger or other input tool across touch sensitive display . An example gesture input is a swipe input where a user swipes his or her finger or other input tool across touch sensitive display . In some implementations device can detect inputs that are received in direct contact with display or that are received within a particular vertical distance of display e.g. within one or two inches of display . Users can simultaneously provide input at multiple locations on display . For example inputs simultaneously touching at two or more locations can be received.

In some implementations device can implement various device functionalities. As part of one or more of these functionalities device presents graphical user interfaces on touch sensitive display of device and also responds to touch input received from a user for example through touch sensitive display .

In some implementations graphical user interface presented on display can include one or more two dimensional graphical objects. In the example shown graphical objects are file icons representing files A and B and folder icon representing folder C. A user can interact with icons using various inputs. For example touching file icon can result in file A being opened in user interface . Similarly touching folder icon can result in folder C opening in user interface to expose the folder contents e.g. one or more folders . Graphical objects can represent any type of data or content including but not limited to files folders digital photos or videos audio files ebooks etc.

In step at a first instant of time a user can use a finger to draw a circle around icons to indicate that the icons are to be grouped together. For example a user can touch display at touch point and draw circle around icons without removing their finger from display . In some implementations a dashed line or other visual indicator can be displayed to show circle to visually indicate to the user that icons are selected for inclusion into a group.

In step at a second instant of time when the user removes their finger from display circle is automatically transformed into three dimensional graphical object which contains icons and . In the example shown graphical object is a ball or sphere that is detached or floating on user interface . Note that circle step and graphical object step are shown in as being in two different locations on user interface . This was for illustrative purposes only. In practice circle can be transformed into graphical object at the same location on user interface . In some implementations user interface also can be automatically transformed into a three dimensional user interface environment.

In this example multiple icons are associated or grouped together into graphical object . In other implementations however one or more user interface elements can be transformed into graphical objects.

In some implementations device includes onboard motion sensors e.g. accelerometer gyros which can detect motion of device . Graphical object can move freely about display in response to motion detected by onboard sensors. Graphical object can be animated so as to make graphical object appear to have mass which can appear to respond to virtual physical forces in user interface such as gravity friction or drag. Graphical object can bounce or reflect off boundaries of user interface or other graphical objects. Although graphical object is shown as a sphere or ball in this example other graphical objects can be used such as a cylinder wheel block or any other geometric shape.

In some implementations the size of graphical object is based on the size of data represented by graphical object . For example if graphical object is a ball then the radius of the sphere or ball will determine its size mass . When graphical object is manipulated on user interface the behavior of graphical object in response to touch or motion input can be based on its mass. Larger files more mass can be animated to move more slowly than smaller files less mass in accordance with Newtonian physics i.e. acceleration force mass.

In some implementations user interface can have a physical characteristic that can interact with the mass of graphical object . For example user interface can have a coefficient of friction or viscosity that can be set by the user. More friction would result in slowing graphical object as it moves about user interface .

When graphical object enters virtual opening a data transfer can be triggered where the files A and B and folder C represented by file icons and folder icon respectively are transferred to device over wireless communication link step . An example communication link can be a Radio Frequency RF link using known communication protocols e.g. Bluetooth WiFi RFID . Device can display graphical object on display step . In some implementations a user of device can manipulate graphical object on user interface in the same manner as graphical object can be manipulated on user interface of device . On device or a second touch input on graphical object or other input can be used to disassociate icons from graphical object so that the icons can be used to open the corresponding files or folder.

Referring to the final resting location of a given graphical object can provide a visual indication of the size of the data represented by the graphical object. In the example shown file icons representing data of similar size mass settled at level in user interface file icons representing data of similar size settled at level in user interface file icons and representing data of similar size settled at level in user interface file icons representing data of similar size settled at level in user interface and folder icons A B representing data of similar size settled at level in user interface . Since file icons settled at level top level the corresponding files were the smallest. Likewise since folder icons A B settled at level bottom level the corresponding folders were the heaviest. Thus icons e.g. desktop icons can be visually sorted by size where larger files fall to the bottom of user interface and the smaller files rise to the top of user interface . Animation can be applied to graphical objects to simulate the falling and rising actions in user interface under the force of gravity for example.

The line with arrows indicating file size shown in is for illustrative purposes only and may not be displayed in user interface .

In the example shown five states of a folder icon representing aging data is displayed on user interface . At a first time T icon is displayed in user interface with 0 transparency. At a second time T where T T icon is displayed with 25 transparency. At a third time T where T T icon is displayed with 50 transparency. At a fourth time T where T T icon is displayed with 75 transparency. And finally at a fifth time T where T T icon is displayed with 100 transparency. In this example the transparency of icon was reduced linearly over five time points as illustrated by curve . Accordingly a user can use simple visual inspection of file icons on user interface to determine the relative age of the files represented by the icons. Curve is shown in for illustrative purposes and may not be displayed in practice.

In some implementations other visual indicators can be used to indicate age of data other than transparency. For example icons representing data or files can change color based on age. Age of data can be indicated by adjusting color brightness hue and saturation of icons representing the data. Icons representing aging data can be animated to appear more active for newer data or files e.g. a fast jiggling icon than with older data or files e.g. a slow jiggling icon .

In some implementations icons representing data can be modified over time to look hot or cold. For example recently created edited or reviewed data or files can be represented by icons that include an animated flame and or be colored with varying shades of red to indicate that the corresponding data or files were recently created edited or reviewed. And icons representing older files can be animated to appear cold such as drawing frost on the icon and or coloring the icon with varying shades of blue to indicate that the corresponding data or files were created edited or reviewed in the past.

In some implementations a system can present a first graphical object on a user interface . The first graphical object can be for example an icon representing a file folder directory or other data. An input can be received through the user interface . In some implementations the first graphical object does not respond to physical motion of the device. For example the user interface can be presented on a touch sensitive display of a device. The touch input can be a single touch with a finger or stylus a multi touch input with two or more fingers or a gesture. The gesture can be a touch gesture or a physical gesture made by physically moving the device. Responsive to the input the first graphical object is transformed to a second graphical object that responds to motion input . For example the user can touch a two dimensional file icon resulting in the file icon transforming into a three dimensional ball. Motion input can be received from one or more onboard sensors of the device . For example an accelerometer or gyro onboard the device can sense accelerations or angular motion which can be received by process . The second graphical object can be animated in the user interface to appear to respond to the motion . For example if the second graphical object is a ball the user can move the device to make the ball roll on the user interface. The speed of the roll can be based on physical characteristics of the user interface or display environment such as friction drag gravity viscosity etc.

In some implementations a system presents a user interface on a display of a device . Graphical objects representing data can be displayed on the user interface . The graphical objects can be for example icons representing data files or folders. A request is received to sort the graphical objects based on the relative sizes of data represented by the graphical objects . The request can be a touch gesture or motion gesture. In one example the user can shake the device causing the graphical objects to move about the user interface resulting in a snow globe effect.

In response to the request the graphical objects can be automatically arranged in a hierarchy on the user interface according to the relative sizes of data represented by the graphical objects where the arranging includes animating the graphical objects in the user interface so that each graphical object appears to respond to real world physical forces . For example larger files have more mass than smaller files. The animation creates the appearance that the larger files heavier mass are sinking to the bottom of the user interface and the smaller files lighter mass are rising to the top of the user interface.

In some implementations a system presents a user interface on a display of a device . A graphical object is displayed on the user interface . The visual appearance of the graphical object on the user interface is adjusted based on the age of the data represented by the graphical object . In some implementations the transparency of the graphical object can be adjusted based on age. For example a file that is recently created edited or reviewed can be represented by an icon with zero or low transparency and a file that was created edited or reviewed in the past can be represented by an icon with a percentage of transparency to visually indicate its age. Other visible indications of aging data can be employed for example changing the color of icons representing data or files adjusting the brightness hue or saturation of colors to indicate age etc. In some implementations icons can be animated to appear more active for newer data or files e.g. fast jiggling icon than with older data or files e.g. slow jiggling icon .

In some implementations icons can be modified over time to look hot or cold. For example recently created edited or reviewed files can include an animated flame and or be colored with varying shades of red to indicate how hot or recent the data or files were created edited or reviewed. And older files can be animated to appear cold such as drawing frost on the icon and or coloring the icon with varying shades of blue to indicate how cold or how long ago the data or files were created edited or reviewed.

Operating system can provide an interface to the hardware layer e.g. a capacitive touch display or device . Operating system can include one or more software drivers that communicate with the hardware. For example the drivers can receive and process touch input signals generated by a touch sensitive display or device in the hardware layer. Operating system can process raw input data received from the driver s . This processed input data can then made available to touch services layer through one or more application programming interfaces APIs . These APIs can be a set of APIs that are included with operating systems e.g. Linux or UNIX APIs as well as APIs specific for sending and receiving data relevant to touch input.

Touch services module can receive touch inputs from operating system layer and convert one or more of these touch inputs into touch input events according to an internal touch event model. Touch services module can use different touch models for different applications for example depending on a state of the device.

The touch input events can be in a format that is easier to use in an application than raw touch input signals generated by the touch sensitive device. For example a touch input event can include a set of coordinates for each location at which a touch is currently occurring on a user interface. Each touch input event can include information on one or more touches occurring simultaneously.

In some implementations gesture touch input events can also be detected by combining two or more touch input events. The gesture touch input events can contain scale or rotation information. The rotation information can include a rotation value that is a relative delta in degrees. The scale information can also include a scaling value that is a relative delta in pixels on the display device. Other gesture events are possible.

All or some of these touch input events can be made available to developers through a touch input event API. The touch input API can be made available to developers as a Software Development Kit SDK or as part of an application e.g. as part of a browser tool kit .

Object management engine receives touch inputs from the touch services module and processes the input events for example as described above with reference to .

Sensors devices and subsystems can be coupled to peripherals interface to facilitate multiple functionalities. For example motion sensor light sensor and proximity sensor can be coupled to peripherals interface to facilitate various orientation lighting and proximity functions. For example in some implementations light sensor can be utilized to facilitate adjusting the brightness of touch screen . In some implementations motion sensor e.g. an accelerometer velocimeter or gyroscope can be utilized to detect movement of the device. Accordingly graphical objects and or media can be presented according to a detected orientation e.g. portrait or landscape.

Other sensors can also be connected to peripherals interface such as a temperature sensor a biometric sensor or other sensing device to facilitate related functionalities.

Location determination functionality can be facilitated through positioning system . Positioning system in various implementations can be a component internal to device or can be an external component coupled to device e.g. using a wired connection or a wireless connection . In some implementations positioning system can include a GPS receiver and a positioning engine operable to derive positioning information from received GPS satellite signals. In other implementations positioning system can include a magnetometer e.g. a magnetic compass and an accelerometer as well as a positioning engine operable to derive positioning information based on dead reckoning techniques. In still further implementations positioning system can use wireless signals e.g. cellular signals IEEE 802.11 signals to determine location information associated with the device Hybrid positioning systems using a combination of satellite and television signals such as those provided by ROSUM CORPORATION of Mountain View Calif. can also be used. Other positioning systems are possible.

Broadcast reception functions can be facilitated through one or more radio frequency RF receiver s . An RF receiver can receive for example AM FM broadcasts or satellite broadcasts e.g. XM or Sirius radio broadcast . An RF receiver can also be a TV tuner. In some implementations RF receiver is built into wireless communication subsystems . In other implementations RF receiver is an independent subsystem coupled to device e.g. using a wired connection or a wireless connection . RF receiver can receive simulcasts. In some implementations RF receiver can include a Radio Data System RDS processor which can process broadcast content and simulcast data e.g. RDS data . In some implementations RF receiver can be digitally tuned to receive broadcasts at various frequencies. In addition RF receiver can include a scanning function which tunes up or down and pauses at a next frequency where broadcast content is available.

Camera subsystem and optical sensor e.g. a charged coupled device CCD or a complementary metal oxide semiconductor CMOS optical sensor can be utilized to facilitate camera functions such as recording photographs and video clips.

Communication functions can be facilitated through one or more communication subsystems . Communication subsystem s can include one or more wireless communication subsystems and one or more wired communication subsystems. Wireless communication subsystems can include radio frequency receivers and transmitters and or optical e.g. infrared receivers and transmitters. Wired communication system can include a port device e.g. a Universal Serial Bus USB port or some other wired port connection that can be used to establish a wired connection to other computing devices such as other communication devices network access devices a personal computer a printer a display screen or other processing devices capable of receiving and or transmitting data. The specific design and implementation of communication subsystem can depend on the communication network s or medium s over which device is intended to operate. For example device may include wireless communication subsystems designed to operate over a global system for mobile communications GSM network a GPRS network an enhanced data GSM environment EDGE network 802.x communication networks e.g. Wi Fi WiMax or 3G networks code division multiple access CDMA networks and a Bluetooth network. Communication subsystems may include hosting protocols such that Device may be configured as a base station for other wireless devices. As another example the communication subsystems can allow the device to synchronize with a host device using one or more protocols such as for example the TCP IP protocol HTTP protocol UDP protocol and any other known protocol.

Audio subsystem can be coupled to speaker and one or more microphones . One or more microphones can be used for example to facilitate voice enabled functions such as voice recognition voice replication digital recording and telephony functions.

I O subsystem can include touch screen controller and or other input controller s . Touch screen controller can be coupled to touch screen . Touch screen and touch screen controller can for example detect contact and movement or break thereof using any of a number of touch sensitivity technologies including but not limited to capacitive resistive infrared and surface acoustic wave technologies as well as other proximity sensor arrays or other elements for determining one or more points of contact with touch screen or proximity to touch screen .

Other input controller s can be coupled to other input control devices such as one or more buttons rocker switches thumb wheel infrared port USB port and or a pointer device such as a stylus. The one or more buttons not shown can include an up down button for volume control of speaker and or microphone .

In one implementation a pressing of the button for a first duration may disengage a lock of touch screen and a pressing of the button for a second duration that is longer than the first duration may turn power to device on or off. The user may be able to customize a functionality of one or more of the buttons. Touch screen can for example also be used to implement virtual or soft buttons and or a keyboard.

In some implementations device can present recorded audio and or video files such as MP3 AAC and MPEG files. In some implementations device can include the functionality of an MP3 player such as an iPhone 

Memory interface can be coupled to memory . Memory can include high speed random access memory and or non volatile memory such as one or more magnetic disk storage devices one or more optical storage devices and or flash memory e.g. NAND NOR . Memory can store operating system such as Darwin RTXC LINUX UNIX OS X WINDOWS or an embedded operating system such as VxWorks. Operating system may include instructions for handling basic system services and for performing hardware dependent tasks. In some implementations operating system can be a kernel e.g. UNIX kernel .

Memory may also store communication instructions to facilitate communicating with one or more additional devices one or more computers and or one or more servers. Communication instructions can also be used to select an operational mode or communication medium for use by the device based on a geographic location obtained by GPS Navigation instructions of the device. Memory may include graphical user interface instructions to facilitate graphic user interface processing sensor processing instructions to facilitate sensor related processing and functions phone instructions to facilitate phone related processes and functions electronic messaging instructions to facilitate electronic messaging related processes and functions web browsing instructions to facilitate web browsing related processes and functions media processing instructions to facilitate media processing related processes and functions GPS Navigation instructions to facilitate GPS and navigation related processes and instructions e.g. mapping a target location camera instructions to facilitate camera related processes and functions and or other software instructions to facilitate other processes and functions e.g. security processes and functions device customization processes and functions based on predetermined user preferences and other software functions. Memory may also store other software instructions not shown such as web video instructions to facilitate web video related processes and functions and or web shopping instructions to facilitate web shopping related processes and functions. In some implementations media processing instructions are divided into audio processing instructions and video processing instructions to facilitate audio processing related processes and functions and video processing related processes and functions respectively.

Each of the above identified instructions and applications can correspond to a set of instructions for performing one or more functions described above. These instructions need not be implemented as separate software programs procedures or modules. Memory can include additional instructions or fewer instructions. Furthermore various functions of device may be implemented in hardware and or in software including in one or more signal processing and or application specific integrated circuits.

Devices and can also establish communications by other means. For example wireless device can communicate with other wireless devices e.g. other devices or cell phones etc. over wireless network . Likewise devices and can establish peer to peer communications e.g. a personal area network by use of one or more communication subsystems such as a Bluetooth communication device. Other communication protocols and topologies can also be implemented.

Devices or can for example communicate with one or more services over one or more wired and or wireless networks . These services can include for example animation service object management service and touch model service . Animation service generates the animations described above when graphical objects are moved deleted and securely deleted. Object management service determines how to process display graphical objects and their corresponding system graphical objects for example as described above with reference to . Touch model service provides the touch model features described above with reference to .

Device or can also access other data and content over one or more wired and or wireless networks . For example content publishers such as news sites RSS feeds web sites blogs social networking sites developer networks etc. can be accessed by Device or . Such access can be provided by invocation of a web browsing function or application e.g. a browser in response to a user touching for example a Web object.

The features described can be implemented in digital electronic circuitry or in computer hardware firmware software or in combinations of them. The features can be implemented in a computer program product tangibly embodied in an information carrier e.g. in a machine readable storage device for execution by a programmable processor and method steps can be performed by a programmable processor executing a program of instructions to perform functions of the described implementations by operating on input data and generating output. Alternatively or addition the program instructions can be encoded on a propagated signal that is an artificially generated signal e.g. a machine generated electrical optical or electromagnetic signal that is generated to encode information fro transmission to suitable receiver apparatus for execution by a programmable processor.

The described features can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from and to transmit data and instructions to a data storage system at least one input device and at least one output device. A computer program is a set of instructions that can be used directly or indirectly in a computer to perform a certain activity or bring about a certain result. A computer program can be written in any form of programming language e.g. Objective C Java including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as a module component subroutine or other unit suitable for use in a computing environment.

Suitable processors for the execution of a program of instructions include by way of example both general and special purpose microprocessors and the sole processor or one of multiple processors or cores of any kind of computer. Generally a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Generally a computer will also include or be operatively coupled to communicate with one or more mass storage devices for storing data files such devices include magnetic disks such as internal hard disks and removable disks magneto optical disks and optical disks. Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non volatile memory including by way of example semiconductor memory devices such as EPROM EEPROM and flash memory devices magnetic disks such as internal hard disks and removable disks magneto optical disks and CD ROM and DVD ROM disks. The processor and the memory can be supplemented by or incorporated in ASICs application specific integrated circuits .

To provide for interaction with a user the features can be implemented on a computer having a display device such as a CRT cathode ray tube or LCD liquid crystal display monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer.

The features can be implemented in a computer system that includes a back end component such as a data server or that includes a middleware component such as an application server or an Internet server or that includes a front end component such as a client computer having a graphical user interface or an Internet browser or any combination of them. The components of the system can be connected by any form or medium of digital data communication such as a communication network. Examples of communication networks include e.g. a LAN a WAN and the computers and networks forming the Internet.

The computer system can include clients and servers. A client and server are generally remote from each other and typically interact through a network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other.

One or more features or steps of the disclosed embodiments can be implemented using an Application Programming Interface API . An API can define on or more parameters that are passed between a calling application and other software code e.g. an operating system library routine function that provides a service that provides data or that performs an operation or a computation.

The API can be implemented as one or more calls in program code that send or receive one or more parameters through a parameter list or other structure based on a call convention defined in an API specification document. A parameter can be a constant a key a data structure an object an object class a variable a data type a pointer an array a list or another call. API calls and parameters can be implemented in any programming language. The programming language can define the vocabulary and calling convention that a programmer will employ to access functions supporting the API.

In some implementations an API call can report to an application the capabilities of a device running the application such as input capability output capability processing capability power capability communications capability etc.

A number of implementations have been described. Nevertheless it will be understood that various modifications may be made. For example elements of one or more implementations may be combined deleted modified or supplemented to form further implementations. As yet another example the logic flows depicted in the figures do not require the particular order shown or sequential order to achieve desirable results. In addition other steps may be provided or steps may be eliminated from the described flows and other components may be added to or removed from the described systems. Accordingly other implementations are within the scope of the following claims.

