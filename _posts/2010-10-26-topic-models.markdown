---

title: Topic models
abstract: Machine learning techniques may be used to train computing devices to understand a variety of documents (e.g., text files, web pages, articles, spreadsheets, etc.). Machine learning techniques may be used to address the issue that computing devices may lack the human intellect used to understand such documents, such as their semantic meaning. Accordingly, a topic model may be trained by sequentially processing documents and/or their features (e.g., document author, geographical location of author, creation date, social network information of author, and/or document metadata). Additionally, as provided herein, the topic model may be used to predict probabilities that words, features, documents, and/or document corpora, for example, are indicative of particular topics.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08645298&OS=08645298&RS=08645298
owner: Microsoft Corporation
number: 08645298
owner_city: Redmond
owner_country: US
publication_date: 20101026
---
Much of today s information is created and shared in an electronic format. In one example a user may upload and share a photo through a photo sharing website. In another example users may share articles music web pages and or ideas through social networking and or microblogging services. Unfortunately computing devices may lack the human intellect that may be useful to understand human generated content such as semantic meaning. Accordingly topic models inference algorithms and or other machine learning techniques have been developed to provide a mechanism for computing devices to learn how to understand human generated content. For example topic models may be used to discover topic structure e.g. content focusing on cars sports a specific natural disaster political debate etc. and determine probabilities that documents e.g. a text document an online text based article a blog etc. may relate to particular topics. However current inference algorithms for topic models may require multiple passes over a document corpus of documents thus making such algorithms ill suited for large scale document repositories such as web content which may also change rapidly. Additionally inference algorithms that consider merely the words contained in an article in their analysis may be unable to extract meaningful topics from large corpora of short and or semantically diverse documents. Since many online document corpora also comprise a considerable amount of additional metadata such as the identity of the author s time stamps etc. such information may be helpful in determining the topic structure of a corpus.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key factors or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

Among other things one or more systems and or techniques for training a topic model based upon sequentially processing documents and or their features are disclosed herein. Additionally one or more systems and or techniques for using a topic model to predict probabilities that words features documents and or document corpora are indicative of particular topics are disclosed herein. A document corpus may comprise a structure or grouping of a wide variety of documents e.g. social network data blog data feed data a database of photos a website of articles textual web page data a web service of microblogs a file folder of spreadsheets and or text documents XML files instant messages HTML files source code and or other documents or combinations thereof . Documents within the document corpus may be processed using a topic model in order to train the topic model. The trained topic model may be used by computing devices to understand and or predict topics of documents. During processing features of documents may be taken into account in order to enhance the processing of documents. In particular document features may provide helpful semantic information that may not be taken into account by standard topic models.

Accordingly as provided herein a topic model may be trained by processing documents and document features. Document features for example may comprise information regarding the author the geographical location of the author creation date or time document length social network membership of author document metadata web browsing history of the author source of the document type of document previous search queries of the author and or a wide variety of other features. In one example of training a topic model respective documents within a document corpus may be processed using the topic model. In particular for respective documents a document representation of a document and features of the document may be received. The document representation may comprise a frequency of word occurrences within the document for example.

The document representation and the features may be processed using the topic model. In particular during processing feature topic parameters may be specified for respective features of the document. A feature topic parameter may specify a distribution of probabilities of a feature being indicative of one or more topics which may be used to determine potential topics of a document associated with the feature. For example a feature topic parameters may comprise a distribution of probabilities that a feature author Dan may be indicative of topics such as Video Games Cars Vacations. Thus the feature topic parameter may be used to determine potential topics of documents authored by Dan.

It may be appreciated that the topic model may comprise current feature topic parameters based upon previously processed documents and features. During subsequent processing of documents current feature topic parameters may be updated with specified feature topic parameters during training of the topic model. Because current feature topic parameters may be adjusted during training an uncertainty measure may be associated with probabilities of a current feature topic parameter e.g. an uncertainty measure may comprise uncertainty values for respective probabilities within a distribution of probabilities of the feature topic parameter . Thus when a feature topic parameter is specified during processing a first uncertainty measure associated with a current feature topic parameter within the topic model may be taken into account. In one example a feature topic parameter may be specified based upon a current feature topic parameter within the topic model the first uncertainty measure and or other considerations.

Additionally during processing document word topic parameters may be specified for respective words within the document. A document word topic parameter may specify a distribution of probabilities of a word being indicative of one or more topics which may be used to determine potential topics of a document comprising the word. For example a document word topic parameter may comprise a distribution of probabilities that a word bike may be indicative of topics such as Sports Consumer Goods Health Money House etc. Thus document word topic parameters may be used to determine potential topics of documents comprising the word bike 

The topic model may be trained based upon the specified feature topic parameters and the specified document word topic parameters. For example current feature topic parameters and or current document word topic parameters within the topic model may be updated with the specified feature topic parameters and or specified document word topic parameters. Additionally the first uncertainty measure and or second uncertainty measure may be updated.

It may be appreciated that in one example a document corpus may be processed in a single iteration. Sequentially processing documents of a document corpus in a single iteration may mitigate processing time and or computational costs which may facilitate the processing of larger dynamic document corpora.

In one example a topic may comprise a grouping of words that may be descriptive of the topic. In another example a topic may be represented by a descriptive name of the topic. It may be appreciated that descriptive names of topics are used herein merely for the sake of illustrative purposes.

It may be appreciated that the topic model may be used to determine topic predictions for words documents features document corpora etc. In one example a document may be processed by a topic model to determine a distribution of probabilities of the document being indicative of one or more topics a document topic distribution prediction . In another example a feature may be processed by the topic model to determine a distribution of probabilities of the feature being indicative of one or more topics a feature topic distribution prediction . In another example a word may be processed to determine a distribution of probabilities of the word being indicative of one or more topics a word topic distribution prediction . In another example a document corpus may be processed to determine a distribution of probabilities of the document corpus being indicative of one or more topics a corpus topic distribution prediction . Such predictions may be used to predict topics of documents authors of documents groupings of similar documents etc.

To the accomplishment of the foregoing and related ends the following description and annexed drawings set forth certain illustrative aspects and implementations. These are indicative of but a few of the various ways in which one or more aspects may be employed. Other aspects advantages and novel features of the disclosure will become apparent from the following detailed description when considered in conjunction with the annexed drawings.

The claimed subject matter is now described with reference to the drawings wherein like reference numerals are used to refer to like elements throughout. In the following description for purposes of explanation numerous specific details are set forth in order to provide a thorough understanding of the claimed subject matter. It may be evident however that the claimed subject matter may be practiced without these specific details. In other instances structures and devices are illustrated in block diagram form in order to facilitate describing the claimed subject matter.

Today users generate share and interact with a wide variety of electronic documents such as images text files web content spreadsheets music etc. Unfortunately computing devices may not comprise the human intellect to understand such documents. Accordingly machine learning techniques have been developed in an attempt to train computing devices to understand a variety of documents such as their semantic meaning. For example topic models may be used as a mean of describing semantic similarities between documents in a low dimensional latent space based upon similarity of word frequencies. That is a topic model may provide probabilistic descriptions of documents as bags of words which may be generated from a mixture of discrete distributions. A document corpus may comprise a plurality of documents where a document may concern a mixture of topics such that respective words in the document may have been generated motivated by a particular topic e.g. there may be a 40 probability that the word bike was used within a document because the document relates to the topic of Sports .

Unfortunately contemporary inference algorithms for topic models use multiple iterations over a document corpus. Thus such inference algorithms topic models may be ill suited for large scale document corpora such as web content e.g. a social networking website with millions of dynamic messages a day . Additionally some document corpora may comprise a large number of short documents e.g. a status update message of a social networking website which may be ambiguous when analyzed using merely the words of the documents.

Accordingly as provided herein features such as document metadata and author information may be analyzed when processing documents using a topic model. For example an online repository of documents may provide document features other than the words of the documents which may provide helpful semantic information that may not otherwise be part of a standard topic model. Using such features may facilitate the extraction of meaningful topics from small documents that would otherwise be ambiguous because of their small word count. Using one or more of the methods and or systems described herein computation costs maybe linear because a document corpus may be processed in a single iteration where respective documents processing times are a linear function of the number of topics documents and features. Linear computational costs allow for processing of larger document corpora such as web based content. Moreover because of sequential document processing exponential forgetting techniques may be used to adjust for topic drifting. That is words and or features that are indicative of a topic may change drift over time. For example the topic Oil Spill may initially have a high association with the words wildlife Florida blame cleanup etc. However as time progresses the topic Oil Spill may instead have a high association with the words Texas litigation total cleanup cost etc. Thus as documents are processed topic drift may be taken into account when specifying parameters within a topic model.

In one example a topic may comprise a grouping of words that may be descriptive of the topic. In another example a topic may be represented by a descriptive name of the topic. It may be appreciated that descriptive names of topics are used herein merely for the sake of illustrative purposes.

One embodiment of training a topic model is illustrated by an exemplary method in . At the method starts. At for respective documents within a document corpus a document representation of a document and features of the document may be received at . It may be appreciated that the document corpus may comprise a plurality of documents of varying types e.g. a repository of billions of web page articles microbloggs HTML documents XML documents text documents social networking data etc. . A document representation for example may comprise a frequency of word occurrences within the document e.g. a vector comprising words occurring in the document and respective frequencies of occurrence of the words . A feature for example may comprise an author designation e.g. a user ID a cookie creator metadata tag a textual name etc. a geographical location e.g. creation location of document location of author etc. creation date and or time designation document length social network membership of the author web browsing history of the author previous search queries of the author document length document type e.g. spreadsheet image textual document HTML web page article source code XML file search results blogs microblogs social network data instant message etc. source of the document e.g. a personal computer a mobile device a website server a web page a web service a database etc. identity of users having accessed read the document and or other features of documents. It may be appreciated that several features of a document may be present for a single document. Hence the document may be described by a combination of the above. For example a document may be generated by several authors from different geographical regions marked as having been edited at several time points and or read by a number of people who are members of certain groups.

At the document representation and or the features of the document may be processed using the topic model. In a topic model a forward action and or a backward action may be iterated. The forward action may involve estimating the probability that the document belongs to respective topics using a generalized linear model in the document features and for respective words in the document estimating the probability that respective words belong to a particular topic which may be characterized by a distribution of words. In light of the actual words present in the document the parameters used during one or more forward actions may be adjusted using Bayesian inference during a backward action for example.

In particular feature topic parameters may be specified for respective features of the document at . In one example a feature topic parameter may specify a distribution of probabilities of a feature being indicative of one or more topics. A combination of features present in the document may be used to construct a distribution of probabilities for the feature combination to be indicative of one or more topics. For example a feature topic parameter for a geographical location feature of Florida may comprise parameters describing probabilities that a document associated with the geographical location feature of Florida may relate to particular topics and a feature topic parameter for an author feature of Dan may comprise other parameters describing probabilities that a document created by Dan may relate to certain topics with certain probabilities. It may be appreciated that the feature topic parameter for Florida and the feature topic parameter for Dan may be presented as two vectors of parameters which may be used to create a document topic distribution prediction for example. In particular the two vectors of parameters for the geographical location and the author may be summed together and a deterministic function may be used to construct probabilities for a document created in Florida by Dan to contain certain topics with certain probabilities and a certain uncertainty on those predicted topic distributions e.g. the distribution may specify that the average topic distribution such as a document topic distribution prediction for a document created by Dan in Florida is for 54 of the document s words to stem from the topic Oil Spill 28 of the words to stem from the topic Hurricane and 17 of the words to stem from the topic Vacations and a collective 1 of the words to stem from a variety of other topics. Additionally an uncertainty measure for the distribution prediction might be determined. For example the uncertainty measure may be very low indicating that the particular distribution prediction is highly likely. In contrast the uncertainty measure for the distribution prediction may be very large indicating that the document might also have a substantially different topic distribution. .

It may be appreciated that the topic model may comprise current feature topic parameters for features. However during sequential processing of document representations and or features one or more current feature topic parameters may be updated with specified feature topic parameters during training of the topic model. In this way feature topic parameters may change drift between the processing of one document representation and or feature to the next. An uncertainty measure of a current feature topic parameter within the topic model may be taken into account when determining how much a specified feature topic parameter may deviate from the current feature topic parameter in the topic model. In one example the feature topic parameter may be based upon a first uncertainty measure e.g. uncertainty based upon a Gaussian distribution in order to account for scenarios where for example probability values and or probability algorithms may have been used during processing of document representations and or features. If the first uncertainty measure comprises a high degree of uncertainty then the range of deviation in comparison with current probabilities of a current feature topic parameter within the topic model for the probabilities of the feature topic parameter may be high e.g. a specified feature topic parameter may comprise values that may be allowed to deviate substantially from a corresponding current feature topic parameter within the topic model . In contrast if the first uncertainty measure comprises a low degree of uncertainty then the range of deviation for the probabilities of the feature topic parameter may be low e.g. a specified feature topic parameter may comprise values that may be allowed to deviate slightly from a corresponding current feature topic parameter within the topic model . In this way current feature topic parameters having a low confidence of accuracy may be updated during training with specified feature topic parameters having a substantial allowed range of deviation if warranted whereas current feature topic parameters having a high confidence of accuracy may be updated during training with specified feature topic parameters having a small allowed range of deviation if warranted .

Additionally during processing document word topic parameters for respective words within the document may be specified at . In one example a document word topic parameter may specify a distribution of probabilities of a word being indicative of one or more topics. For example a document word topic parameter for the word bike in a specific document may comprise probabilities that in this document the word bike may relate to particular topics e.g. the distribution may specify a 40 probability that the word bike used in this document may have a topic of Sports a 20 probability that the word bike used in this document may have a topic of Consumer Product a 15 probability that the word bike used in this document may have a topic of Health etc. .

It may be appreciated that the topic model may comprise current document word topic parameters for words. However during sequential processing of document representations and or features one or more current document word topic parameters may be updated with specified document word topic parameters during training of the topic model. In this way document word topic parameters may change between the processing of one document representation and or feature to the next. An uncertainty measure of a current document word topic parameter within the topic model may be taken into account when determining how much a specified document word topic parameter may deviate from the current document word topic parameter in the topic model. In one example the document word topic parameter may be based upon a second uncertainty measure e.g. uncertainty based upon a Dirichlet distribution in order to account for scenarios where for example probability values and or probability algorithms may have been used during processing of document representations and or features. If the second uncertainty measure comprises a high value e.g. 90 uncertainty then the range of deviation in comparison with current probabilities of a current document word topic parameter within the topic model for the probabilities of the document word topic parameter may be high e.g. a specified document word topic parameter may comprise values that deviate substantially from a corresponding current document word topic parameter within the topic model . In contrast if the second uncertainty measure comprises a low degree of uncertainty then the range of deviation for the probabilities of the document word topic parameter may be low e.g. a specified document word topic parameter may comprise values that may be allowed to deviated slightly from a corresponding current document word topic parameter within the topic model . In this way current document word topic parameters having a low confidence of accuracy may be updated during training with specified document word topic parameters having a substantial allowed range of deviation if warranted whereas current document word topic parameters having a high confidence of accuracy may be updated during training with specified document word topic parameters having a small allowed range of deviation if warranted .

At the topic model may be trained based upon the specified feature topic parameters and or the specified document word topic parameters. For example current feature topic parameters and or current document word topic parameters within the topic model may be updated with corresponding specified feature topic parameters and or specified document word topic parameters. It may be appreciated that the topic model may comprise other parameters that may be used during the processing of document data and or during the training of the topic model. It may be appreciated that the processing of the document corpus may be performed within a single iteration thus facilitating the processing of large and or dynamic document corpora due to a linear computational cost.

It may be appreciated that the document corpus may be a dynamic repository of documents e.g. a social network comprising frequently updated social network messages . Thus an addition of a new document to the document corpus may be detected. A new document representation of the new document and new features of the new document may be processed using the topic model. In this way the topic model may be updated based upon the processing of the new document e.g. the topic model may be trained by updating current parameters with parameters specified during the processing of the new document representation and or the new features 

It may be appreciated that the trained topic model may be used to predict a variety of distributions relating to topics words feature document corpora groupings of similar documents and or other information. In one example a word topic distribution prediction for a word within a document may be determined based upon processing the document using the topic model. The word topic distribution prediction may specify a distribution of probabilities of the word being indicative of one or more topics for the document e.g. . For example the inclusion of the word bike within an article may be indicative of a variety of topics for the article e.g. the word topic distribution prediction may specify a 15 probability bike indicates the article concerns the topic Health a 20 probability bike indicates the article concerns the topic Consumer Product etc. . It may be appreciated that in another example a word topic distribution prediction may be determined for a word without reference to a particular document comprising the word. That is a word topic distribution prediction may be determined from processing the word as opposed to a document comprising the word with the topic model.

In another example a reference document representation of a reference document and or reference features of the reference document may be received for processing. A document topic distribution prediction of the reference document may be determined based upon processing the reference document representation and or the reference features using the topic model. The document topic distribution prediction may specify a distribution of probabilities of the reference document being indicative of one or more topics e.g. . For example an instant message document may concern a variety of topics in varying degrees e.g. the document topic distribution prediction may specify an average prediction specifying that 42 of the words of that instant message concern the topic Cars that 13 of the words of that instant message concerns the topic Racing etc. .

The distribution of probabilities of the document topic distribution prediction of the processed reference document may be compared to a distribution of probabilities of a document topic distribution prediction of an input document e.g. a document different than the reference document . In this way a probability that the input document may comprise similar topics as the processed referenced document may be specified.

In another example a feature topic distribution prediction for a feature combination may be determined based upon the individual features corresponding to individual feature topic parameters within the topic model. It may be appreciated that the feature topic distribution prediction may be determined based upon other information within the topic model. For example a document associated with the feature combination may be processed by the topic model when determining the feature topic distribution prediction. The feature topic distribution prediction may specify a distribution of probabilities of the features being indicative of one or more topics e.g. and . For example a geographical location feature of Florida may be indicative of a variety of topics e.g. the feature topic distribution prediction may specify a weight of 3.8 2.9 that the feature of Florida indicates that an associated document may concern the topic Retirement a weight of 0.2 0.02 for the feature of Florida indicating that the associated document may concern the topic Sports etc. and the feature Dan may specify weights of 0.8 1.3 and 2.9 0.01 for these to topics in associated documents respectively. The combination of merely the features Florida and Dan in one specific document would then sum to representations of 4.6 3.2 and 2.7 0.2 for the topics Retirement and Sports respectively. Depending on the values of the representations of other remaining topics these representations may be mapped by the algorithm to a topic distribution prediction which predicts the words of a document associated with merely the features Dan and Florida to on average stem to 63 from the topic Retirement and to 21 stem from the topic Sports and to smaller extend from other topics . The algorithm may also return uncertainties on this average prediction of topic distributions .

In another example a corpus topic distribution prediction for the document corpus may be determined based upon the topic model. The corpus topic distribution prediction may specify a distribution of probabilities of the document corpus being indicative of one or more topics e.g. . In one example the distribution of probabilities of the corpus topic distribution prediction may be compared with a distribution of probabilities of a document topic distribution of an input document to determine whether the input document concerns similar topics as the document corpus. In another example one or more documents within the document corpus having semantically similar topics may be grouped as determined by the topic model. At the method ends.

It may be appreciated that in one example a document topic distribution prediction for a document may comprise probabilities for respective words of the document to stem from a plurality of topics. Although document topic distribution predictions may be illustrated as average predictions it may be appreciated that document topic distribution predictions and or other predictions may be expressed in different manners as well such as probabilities over probabilities.

It may be appreciated that in one example a topic model may represent a probability distribution over probabilities. That is a document topic distribution prediction for a document may comprise finite probability mass assigned to respective finite sized range of probabilities of possible combinations of topics such that the overall assigned probability mass integrates to 1. For example a document topic distribution prediction for a document may comprise a first distribution 30 1 Hurricane 55 1 Vacation 10 1 Cars etc. which may be assigned a 0.3 probability. The document topic distribution prediction may comprise a second distribution 5 3 Oil Spill 55 2 Disney World 3 1 Cars etc. which may be assigned a 5 probability. In this way a document topic distribution prediction for a document may comprise one or more distributions of topic probabilities where a distribution of topic probabilities may be assigned a probability.

It may be appreciated that in one example a topic model may be associated with one or more variables. In one example of a variable variable pi d may comprise a distribution of a document over a plurality of topics. That is what faction of a document s words e.g. a percentage value may stem from respective topics. For example probabilities may be assigned to respective values with which pi d may take a probabilities prediction for pi d as opposed to comprising explicit values. In another example of a variable variable c di e.g. a document word topic parameter of a word may comprise a distribution of probabilities that respective topics may correspond to a word. Respective words within a document may be assigned a c di. In another example of a variable a variable theta k e.g. a feature topic parameter may comprise a distribution probability for respective topics over words within a vocabulary. For example a topic model may specify a probabilistic statement about the values of respective theta ks.

One embodiment of predicting topics of a document is illustrated by an exemplary method in . At the method starts. At a document representation of a document and features of the document may be processed using a topic model. In particular feature topic distributions for respective features of the document may be determined at . A feature topic distribution may specify a distribution of probabilities of a feature combination being indicative of one or more topics. At word topic distribution predictions for respective words within the document may be determined. A word topic distribution prediction may specify a distribution of probabilities of a word being indicative of one or more topics. At a document topic distribution prediction for the document may be determined based upon the feature topic distribution predictions and or the word topic distribution predictions. The document topic distribution prediction may specify a distribution or probabilities of the document being indicative of one or more topics. In this way an enhanced understanding of the document may be achieved.

The training component may process the document representation and or the features using the topic model . In particular the training component may retrieve e.g. retrieve parameters from the topic model current feature topic parameters current document word topic parameters a first uncertainty measure a second uncertainty measure and or other processing information. In one example the training component may specify feature topic parameters for respective features of the document. For example the training component may take into account the current feature topic parameters the first uncertainty measure and or other information within the topic model . A specified feature topic parameter may specify a distribution of probabilities of a feature being indicative of one or more topics. Additionally the training component may specify document word topic parameters for respective words within the document. For example the training component may take into account the current e.g. historical document word topic parameters the second uncertainty measure and or other information within the topic model .

The training component may be configured to train the topic model based upon the specified feature topic parameters and or the specified document word topic parameters. For example the training component may update the current feature topic parameters and or the current document word topic parameters within the topic model with corresponding specified feature topic parameters and or specified document word topic parameters. Additionally the first and or second uncertainty measures may be updated.

In one example the training component may be configured to group one or more documents within the document corpus having semantically similar topics as determined by the topic model .

In one example the prediction component may receive the word Bike . The prediction component may retrieve parameters e.g. a current document word topic parameter corresponding to Bike from the topic model . In this way the prediction component may determine the word topic distribution prediction where the word topic distribution prediction specifies a distribution of probabilities that the word Bike is indicative of one or more topics such as 40 indicative of Sports 20 indicative of Consumer Products 15 indicative of Health 17 indicative of Tour De France etc. Thus the appearance of the word Bike within a document may signify that the word Bike contributes a 40 indication that the document may relate to the topic Sports for example. It may be appreciated that probabilities may be represented through a variety of techniques e.g. percentages floating numbers normalized so that probabilities add up to 1 etc. .

In another example the prediction component may be configured to group one or more documents having semantically similar topics as determined by the topic model e.g. documents associated with similar document topic distributions .

Still another embodiment involves a computer readable medium comprising processor executable instructions configured to implement one or more of the techniques presented herein. An exemplary computer readable medium that may be devised in these ways is illustrated in wherein the implementation comprises a computer readable medium e.g. a CD R DVD R or a platter of a hard disk drive on which is encoded computer readable data . This computer readable data in turn comprises a set of computer instructions configured to operate according to one or more of the principles set forth herein. In one such embodiment the processor executable computer instructions may be configured to perform a method such as at least some of the exemplary method of and or exemplary method of for example. In another such embodiment the processor executable instructions may be configured to implement a system such as at least some of the exemplary system of for example. Many such computer readable media may be devised by those of ordinary skill in the art that are configured to operate in accordance with the techniques presented herein.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims.

As used in this application the terms component module system interface and the like are generally intended to refer to a computer related entity either hardware a combination of hardware and software software or software in execution. For example a component may be but is not limited to being a process running on a processor a processor an object an executable a thread of execution a program and or a computer. By way of illustration both an application running on a controller and the controller can be a component. One or more components may reside within a process and or thread of execution and a component may be localized on one computer and or distributed between two or more computers.

Furthermore the claimed subject matter may be implemented as a method apparatus or article of manufacture using standard programming and or engineering techniques to produce software firmware hardware or any combination thereof to control a computer to implement the disclosed subject matter. The term article of manufacture as used herein is intended to encompass a computer program accessible from any computer readable device carrier or media. Of course those skilled in the art will recognize many modifications may be made to this configuration without departing from the scope or spirit of the claimed subject matter.

Although not required embodiments are described in the general context of computer readable instructions being executed by one or more computing devices. Computer readable instructions may be distributed via computer readable media discussed below . Computer readable instructions may be implemented as program modules such as functions objects Application Programming Interfaces APIs data structures and the like that perform particular tasks or implement particular abstract data types. Typically the functionality of the computer readable instructions may be combined or distributed as desired in various environments.

In other embodiments device may include additional features and or functionality. For example device may also include additional storage e.g. removable and or non removable including but not limited to magnetic storage optical storage and the like. Such additional storage is illustrated in by storage . In one embodiment computer readable instructions to implement one or more embodiments provided herein may be in storage . Storage may also store other computer readable instructions to implement an operating system an application program and the like. Computer readable instructions may be loaded in memory for execution by processing unit for example.

The term computer readable media as used herein includes computer storage media. Computer storage media includes volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions or other data. Memory and storage are examples of computer storage media. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM Digital Versatile Disks DVDs or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by device . Any such computer storage media may be part of device .

Device may also include communication connection s that allows device to communicate with other devices. Communication connection s may include but is not limited to a modem a Network Interface Card NIC an integrated network interface a radio frequency transmitter receiver an infrared port a USB connection or other interfaces for connecting computing device to other computing devices. Communication connection s may include a wired connection or a wireless connection. Communication connection s may transmit and or receive communication media.

The term computer readable media may include communication media. Communication media typically embodies computer readable instructions or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal may include a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal.

Device may include input device s such as keyboard mouse pen voice input device touch input device infrared cameras video input devices and or any other input device. Output device s such as one or more displays speakers printers and or any other output device may also be included in device . Input device s and output device s may be connected to device via a wired connection wireless connection or any combination thereof. In one embodiment an input device or an output device from another computing device may be used as input device s or output device s for computing device .

Components of computing device may be connected by various interconnects such as a bus. Such interconnects may include a Peripheral Component Interconnect PCI such as PCI Express a Universal Serial Bus USB firewire IEEE 1394 an optical bus structure and the like. In another embodiment components of computing device may be interconnected by a network. For example memory may be comprised of multiple physical memory units located in different physical locations interconnected by a network.

Those skilled in the art will realize that storage devices utilized to store computer readable instructions may be distributed across a network. For example a computing device accessible via a network may store computer readable instructions to implement one or more embodiments provided herein. Computing device may access computing device and download a part or all of the computer readable instructions for execution. Alternatively computing device may download pieces of the computer readable instructions as needed or some instructions may be executed at computing device and some at computing device .

Various operations of embodiments are provided herein. In one embodiment one or more of the operations described may constitute computer readable instructions stored on one or more computer readable media which if executed by a computing device will cause the computing device to perform the operations described. The order in which some or all of the operations are described should not be construed as to imply that these operations are necessarily order dependent. Alternative ordering will be appreciated by one skilled in the art having the benefit of this description. Further it will be understood that not all operations are necessarily present in each embodiment provided herein.

Moreover the word exemplary is used herein to mean serving as an example instance or illustration. Any aspect or design described herein as exemplary is not necessarily to be construed as advantageous over other aspects or designs. Rather use of the word exemplary is intended to present concepts in a concrete fashion. As used in this application the term or is intended to mean an inclusive or rather than an exclusive or . That is unless specified otherwise or clear from context X employs A or B is intended to mean any of the natural inclusive permutations. That is if X employs A X employs B or X employs both A and B then X employs A or B is satisfied under any of the foregoing instances. In addition the articles a and an as used in this application and the appended claims may generally be construed to mean one or more unless specified otherwise or clear from context to be directed to a singular form.

Also although the disclosure has been shown and described with respect to one or more implementations equivalent alterations and modifications will occur to others skilled in the art based upon a reading and understanding of this specification and the annexed drawings. The disclosure includes all such modifications and alterations and is limited only by the scope of the following claims. In particular regard to the various functions performed by the above described components e.g. elements resources etc. the terms used to describe such components are intended to correspond unless otherwise indicated to any component which performs the specified function of the described component e.g. that is functionally equivalent even though not structurally equivalent to the disclosed structure which performs the function in the herein illustrated exemplary implementations of the disclosure. In addition while a particular feature of the disclosure may have been disclosed with respect to only one of several implementations such feature may be combined with one or more other features of the other implementations as may be desired and advantageous for any given or particular application. Furthermore to the extent that the terms includes having has with or variants thereof are used in either the detailed description or the claims such terms are intended to be inclusive in a manner similar to the term comprising. 

