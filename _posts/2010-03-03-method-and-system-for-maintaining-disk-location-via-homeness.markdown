---

title: Method and system for maintaining disk location via homeness
abstract: A method and system manages ownership information about disks in a storage network without the need for an emulated, partner mode system. The method and system provides for ownership information, including a current owner and a home owner for resources, such as disks, to be stored on each disk in a storage system, as well as to be stored in memory on each storage system node in the network. A further aspect of the invention is a disk homeness application program interface (API), which provides commands that can be utilized by an administrator at a host computer to set, modify and disco play ownership information about each disk in the cluster. Upon a takeover, any node in the network can takeover one or more disks by reading the ownership information stored on the disks or in the tables.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08041736&OS=08041736&RS=08041736
owner: NetApp, Inc.
number: 08041736
owner_city: Sunnyvale
owner_country: US
publication_date: 20100303
---
U.S. Pat. No. 7 613 947 issued on Nov. 3 2009 entitled SYSTEM AND METHOD FOR STORAGE TAKEOVER by Susan M. Coatney et al. which is presently incorporated by reference herein in its entirety and

U.S. Pat. No. 7 546 302 issued on Jun. 9 2009 entitled METHOD AND SYSTEM FOR IMPROVED RESOURCE GIVEBACK by Susan M. Coatney et al. which is presently incorporated by reference herein in its entirety.

The present application is a continuation of commonly assigned U.S. patent application Ser. No. 11 606 538 which was filed on Nov. 30 2006 now U.S. Pat. No. 7 711 683 by Steven S. Watanabe for a METHOD AND SYSTEM FOR MAINTAINING DISK LOCATION VIA HOMENESS and is hereby incorporated by reference.

The present invention relates to networked storage systems and more particularly to procedures for maintaining disk location information within such systems.

A storage system is a computer that provides storage service relating to the organization of information on writeable persistent storage devices such as memories tapes or disks. The storage system is commonly deployed within a storage area network SAN or a network attached storage NAS environment. When used within a NAS environment the storage system may be embodied as a file server including an operating system that implements a file system to logically organize the information as a hierarchical structure of data containers such as directories and files on e.g. the disks. Each ondisk file may be implemented as a set of data structures e.g. disk blocks configured to store information such as the actual data for the file. A directory on the other hand may be implemented as a specially formatted file in which information about other files and directories are stored.

The file server or filer may be further configured to operate according to a client server model of information delivery to thereby allow many client systems clients to access shared resources such as files stored on the filer. Sharing of files is a hallmark of a NAS system which is enabled because of semantic level of access to files and file systems. Storage of information on a NAS system is typically deployed over a computer is network comprising a geographically distributed collection of interconnected communication links such as Ethernet that allow clients to remotely access the information files on the file server. The clients typically communicate with the filer by exchanging discrete frames or packets of data according to pre defined protocols such as the Transmission Control Protocol Internet Protocol TCP IP .

In the client server model the client may comprise an application executing on a computer that connects to the filer over a computer network such as a point to point link shared local area network wide area network or virtual private network implemented over a public network such as the Internet. NAS systems generally utilize file based access protocols therefore each client may request the services of the filer by issuing file system protocol messages in the form of packets to the file system over the network. By supporting a plurality of file system protocols such as the conventional Common Internet File System CIFS the Network File System NFS and the Direct Access File System DAFS protocols the utility of the filer may be enhanced for networking clients.

A SAN is a high speed network that enables establishment of direct connections between a storage system and its storage devices. The SAN may thus be viewed as an extension to a storage bus and as such an operating system of the storage system enables access to stored information using block based access protocols over the extended bus . In this context the extended bus is typically embodied as Fibre Channel FC or Ethernet media adapted to operate with block access protocols such as Small Computer Systems Interface SCSI protocol encapsulation over FC FCP or TCP IP Ethernet iSCSI . A SAN arrangement or deployment allows decoupling of storage from the storage system such as an application server and some level of storage sharing at the application server level. There are however environments wherein a SAN is dedicated to a single server. When used within a SAN environment the storage system may be embodied as a storage appliance that manages access to information in terms of block addressing on disks using e.g. a logical unit number LUN in accordance with one or more block based protocols such as FCP.

One example of a SAN arrangement including a multi protocol storage appliance suitable for use in the SAN is described in United States Patent Application Publication No. US2004 0030668 A1 filed on Feb. 14 2004 entitled MULTI PROTOCOL STORAGE APPLIANCE THAT PROVIDES INTEGRATED SUPPORT FOR FILE AND BLOCK ACCESS PROTOCOLS by Brian Pawlowski et al. which is incorporated herein by reference in its entirety.

It is advantageous for the services and data provided by a storage system such as a storage node to be available for access to the greatest degree possible. Accordingly some storage systems provide a plurality of storage system nodes organized as a cluster with a first storage system node coupled to and cooperating with a second storage system node. Each storage system node is configured to takeover serving data access requests for the other storage system node if the other node fails. The storage nodes in the cluster notify one another of continued operation using a heartbeat signal exchanged over a cluster interconnect and a cluster switching fabric. If one of the storage system nodes detects the absence of a heartbeat from the other storage node over both the cluster interconnect and the cluster switching fabric a failure of the other node is assumed and a takeover procedure is initiated. The node failure is also usually confirmed by the surviving storage node using a mailbox mechanism of the other storage node to confirm that in fact a failure of the other storage node has occurred rather than simply a failure of the cluster node coupling.

Specifically the mailbox mechanism includes a set of procedures for determining the most up to date coordinating information through the use of one or more master mailbox disks. Such disks receive messages from the storage node with which they are associated in order to confirm that the node continues to be in communication with the disks and that the node continues to be capable of writing to other disks coupled to that node. Further details on the configuration and operation of the master mailbox disk are provided in commonly owned U.S. Pat. No. 7 231 489 of Larson et al. for a SYSTEM AND METHOD FOR COORDINATING CLUSTER STATE INFORMATION issued on Jun. 12 2007 which is presently incorporated by reference herein in its entirety.

Many such cluster configurations that have a plurality of storage system nodes is operate using the concept of partnering i.e. partner mode . Specifically each storage system node in the cluster is partnered with a second storage system node in such a manner that the partner storage system node is available to take over and provide the services and the data otherwise provided by the second storage system node upon a failure of the second node. That is upon such a failure the partner assumes the tasks of processing and handling any data access requests normally processed by the second storage system node. One such example of a partnered storage system cluster configuration is described in U.S. Pat. No. 7 260 737 entitled SYSTEM AND METHOD FOR TRANSPORT LEVEL FAILOVER OF FCP DEVICES IN A CLUSTER by Arthur F. Lent et al. issued on Aug. 21 2007 the contents of which are hereby incorporated by reference. It is further noted that in such storage system node clusters an administrator may desire to take one of the storage system nodes offline for a variety of reasons including for example to upgrade hardware etc. In such situations it may be advantageous to perform a voluntary user initiated takeover operation as opposed to a failover operation. After the takeover operation is complete the storage system node s data is serviced by its partner until a giveback operation is performed.

In such cases employing a partner mode additional infrastructure is often required. For example requests are tracked to determine whether they are partner requests and applicable data structures are duplicated. Separate data structures or tables describing the data such as for example a volume location database VLDB are maintained for the local disks and for the partner disks. In addition registry files which store options and configuration parameters are also maintained separately in a local registry file and a partner registry file. As will be apparent to those skilled in the art this results in additional code complexity in many systems. Moreover if a partner mode is not used it could be difficult for the takeover node or for an administrator to determine disks which have been assigned to a failed partner if the partner s ownership information is not available.

In some storage system architectures each storage node in the cluster is generally organized as a network element N module and a disk element D module . The N module includes functionality that enables the node to connect to clients over a computer is network while each D module connects to one or more storage devices such as the disks. The disks are arranged as one or more aggregates containing one or more volumes. A file system architecture of this type is generally described in U.S. Pat. No. 6 671 773 entitled METHOD AND SYSTEM FOR RESPONDING TO FILE SYSTEM REQUESTS by M. Kazar et al. issued on Dec. 30 2003 the contents of which are zo incorporated herein by reference in entirety .

Extensions to such architectures include the assignment of certain functionality to the D module that was previously performed by the N module. For example the N module is generally responsible for network connectivity while the D module performs functions relating to data containers and data access requests to those containers. In some designs the N and D module pairs are partnered in such a manner that during a failover the surviving N module and D module take over network addresses and perform other administrative tasks for the failed N and D modules. However in a cluster that does not have a one to one pairing between N and D modules and may have multiple nodes in a cluster there is not a readily available technique for identifying the resources e.g. disks that are to be taken over by one or more nodes and subsequently returned to the previously failed node that has been brought back into service. Some ownership information is stored on disk such as that described in commonly owned U.S. Pat. No. 7 650 412 of Coatney et al. issued on Jan. 19 2010 which is presently incorporated herein by reference. This on disk information however is not generally utilized for disk reassignment on takeovers send homes and disk topology reconfigurations.

There remains a need therefore for a multi node cluster system that is configured to provide ownership information about the disks served by the nodes in the cluster so that any of the D modules in the cluster can locate resources served by the cluster and further such that all or a portion of those resources can be assigned or reassigned to any other D module in the cluster or to more than one D module in the cluster.

The disadvantages of prior techniques are overcome by the present invention which provides a method and system which allows for ownership information pertaining to resources such as disks to be stored on each disk in a storage system cluster as well is as to be stored in memory on each storage system node in the cluster. A further aspect of the invention is a disk homeness application program interface API which provides commands that can be utilized by an administrator at a host computer to set modify and display ownership information about each disk in the cluster.

In accordance with the invention the ownership information is referred to herein to as homeness information and is also sometimes referred to generally herein as home owner information . Homeness information includes a current owner and a home owner. A disk s current owner is the storage system node that is assigned to handle data access requests directed to that disk. A disk s home owner is the node to which the disk is originally assigned and which owns the disk if the node is fully operational. Typically on initialization of a cluster the disk homeness API is used to set the home owner and current owner to the same node which node is originally assigned to handle data access requests for that disk upon initialization.

Once set by the administrator using the disk homeness API the ownership information is written to each disk in an ownership location on each disk. Illustratively the ownership location on each disk may be a sector or other portion of the media which is referred to herein as the ownership location. There may be more than one ownership location on a disk if for example a first node is assigned to handle data access requests for data on a first portion of the disk and a second node is assigned to handle data access requests to a second portion of the disk.

After the ownership information has been set a data structure e.g. an ownership table is created in the memory of each node. This table is maintained by an ownership module of the storage operating system of each node and illustratively on the D module. The ownership table stores the homeness information regarding each disk in the cluster. This homeness information includes fields which identify the current owner and the home owner. 

Upon takeover a disk iterator module on a takeover node checks the ownership table if available and passes appropriate homeness information to a takeover monitor which then issues the appropriate instructions to take over the disks which are assigned to the failed node based on that information. If the table is not available the takeover node checks the ownership location information on each disk in the cluster to identify the disks currently served by the failed node. Thus the takeover node does not need to be in a partner mode configuration with the failed D module in order to perform a takeover. Instead any D module in the cluster can access the homeness information either on disk or in core i.e. in a table in memory to obtain the ownership information about each disk associated with a failed D module. Similarly during a send home operation performed once the failed node is brought back into service the in core ownership table can be consulted or the on disk ownership location can be read to determine the proper homeness of a disk and to thus give back that resource to the intended D module.

Using this technique more than one D module can take over the disks of a failed D module such that the disks taken over can be distributed across multiple D modules for load balancing or for other reasons. In accordance with the disk homeness API a disk assign command can be used to update the owner information after a failed node is revived and the disks are sent home to it. In a maintenance mode the disk reassign command can modify current home owner information if necessary. A storage disk show command displays both current owner and home owner information for an administrator in accordance with the invention.

The nodes are also coupled across a cluster interconnect which provides an additional communication path between the nodes. The cluster interconnect may be Fibre Channel FC InfiniBand or another suitable medium. The cluster interconnect may be used to provide heartbeat signals heartbeats between the two nodes which signals are used monitor the active state of each node. The cluster heartbeats are also sent across the cluster switching fabric over which communications between an N module and D module are illustratively effected through remote message passing over the cluster switching fabric The death of a node is indicated by the loss of heartbeat from both the cluster interconnect and the cluster switching fabric. The cluster interconnect is sometimes also referred to as the storage takeover interconnect. That is as described further hereinafter if the heartbeat terminates i.e. times out then a takeover procedure is enabled.

The clients may be general purpose computers configured to interact with the nodes in accordance with a client server model of information delivery. That is is each client may request the services of the node and the node may return the results of the services requested by the client by exchanging packets over the network . The client may issue packets including file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of files and directories. Alternatively the client may issue packets including block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of blocks.

A management station sometimes referred to as an M host also communicates over the cluster switching fabric . This management station is operated and controlled by an administrator who may send instructions in a maintenance mode of the cluster whereby the administrator can assign disks reassign disks or otherwise modify the disk topology or other configuration of the cluster . The management station illustratively contains a graphic user interface or a command line interface CLI not shown whereby the user can interact with the software running on the management station in order to maintain configure and control the system .

During normal cluster operations the storage system node e.g. node that is connected to a set of disks is identified as the home owner of the disks . That storage system node is primarily responsible for servicing data requests directed to blocks on volumes contained on its set of disks. It is also the disk s current owner. For example the storage system node is primarily responsible for the volumes of the disk array which are represented as disks . Thus node is the home owner and the current owner of the disks . Similarly the storage system node is primarily responsible for the disks in the volumes represented as disk and thus node is the current owner and the home owner of the disk . As described in further detail herein a disk platter includes a portion of the media identified schematically by reference character in referred to herein as ownership location in which disk ownership information described herein is stored. As noted the clustered is storage system is configured such that any storage system node or or other nodes in a multi node cluster can take over data servicing capabilities for another storage system node in the event of a failure in the manner described further herein.

Each node is illustratively embodied as a dual processor storage system executing a storage operating system that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of named data containers such as directories files and special types of files called virtual disks hereinafter generally blocks on the disks. However it will be apparent to those of ordinary skill in the art that the node may alternatively comprise a single or more than two processor system. Illustratively one processor executes the functions of the N module on the node while the other processor executes the functions of the D module .

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data structures such as ownership table associated with the present invention. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system portions of which are typically resident in memory and executed by the processing elements functionally organizes the node by inter alia invoking storage operations in support of the storage service implemented by the node. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the invention described herein.

The network adapter comprises a plurality of ports adapted to couple the node to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Illustratively the computer network may be embodied as an Ethernet network or a FC network. Each client may communicate with the node over network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system executing on the node to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on the disks . The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC link topology.

Storage of information on each disk array is preferably implemented as one or more storage volumes that comprise a collection of physical storage disks cooperating to define an overall logical arrangement of a volume block number vbn space on the volume s . Illustratively the disks may be part of an aggregate that contains the volumes. Each logical volume is generally although not necessarily associated with its own file system. The disks within a logical volume file system are typically organized as one or more groups wherein each group may be operated as a Redundant Array of Independent or Inexpensive Disks RAID . Most RAID implementations such as a RAID 4 level implementation enhance the reliability integrity of data storage through the redundant writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of parity information with respect to the striped data once all of the disks in a given RAID group are assimilated. An illustrative example of a RAID implementation is a RAID 4 level implementation although it should be understood that other types and levels of RAID implementations may be used in accordance with the inventive principles described herein.

To facilitate access to the disks the storage operating system implements a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by disks . The file system logically organizes the information as a hierarchical structure of named data containers such as directories and files on the disks. Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the file system to further logically organize information as a hierarchical structure of data containers such as blocks on the disks that are exported as named logical unit numbers LUNs .

In the illustrative embodiment the storage operating system is preferably the NetApp Data ONTAP operating system available from Network Appliance Inc. Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such where the term WAFL is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.

In addition the storage operating system includes a series of software layers organized to form a storage server that provides data paths for accessing information stored on the disks of the node . As described further herein the software layers of the storage server are illustratively embodied as a D module . The storage server illustratively comprises a takeover monitor module that maintains a data structure such as the takeover monitor resource table . It should be understood that the takeover monitor resource table is not the only possible embodiment of the data structure but it is described herein for illustrative purposes. The takeover monitor resource table is configured to maintain information regarding a takeover procedure as described further herein. The file system module interacts in cooperating relation with a volume striping module VSM a RAID system module and a disk driver system module . The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with I O operations while the disk driver system implements a disk access protocol such as e.g. the Small Computer System Interface SCSI protocol. However it should be understood that processes other than the RAID system may in other embodiments perform such tasks while remaining within the scope of the present invention.

The VSM illustratively implements a striped volume set SVS and as such cooperates with the file system to enable storage server to service a volume of the SVS. In particular the VSM implements a Locate function to compute the location of data container content in the SVS volume to thereby ensure consistency of such content served by the cluster.

A disk ownership module manages ownership of the disks using for example one or more data structures such as tables including for example the disk ownership table . In particular the ownership module cooperates with the disk driver system to identify the appropriate D module for processing data access requests for particular volumes on the disk array and cooperates with the RAID subsystem which manages aggregates and with the file system which manages flexible volumes in the illustrative embodiment of the invention. To that end the ownership module consults disk ownership table which contains disk ownership information that is illustratively generated at boot up time and that is updated by disk homeness API as described in further detail herein of the storage operating system to reflect changes in ownership of disks. More specifically a disk iterator module and a disk migration is module cooperate to identify ownership information in the ownership layer and to change on disk reservations and ownership information in response to a takeover procedure or a send home procedure. In other words the disk ownership module includes program instructions for writing predefined ownership information at a proper location on each disk which as noted is illustratively identified herein as the ownership location such a sector on the disk such as the disk platter and which sector is a portion of the media identified schematically by reference character in referred to herein as ownership location . The disk ownership module also includes program instructions for asserting and eliminating SCSI reservation tags in response to commands received and generated by disk iterator and disk migration module .

A takeover or send home procedure is invoked and controlled by a takeover monitor process which follows a list of tasks to be performed for each procedure stored in an associated takeover monitor resource table of the takeover monitor process . These procedures are described in detail in the above cited U.S. patent application Ser. No. 11 606 727.

Initially the disk ownership table is generated upon boot up of a node . More specifically I O services of the disk driver system query all devices e.g. disks attached to the node. This query requests information as to the nature of the attached disks. Upon completion of the query the ownership module instructs the disk driver system to read the ownership information from each disk. In response the disk driver system reads the ownership information from the ownership location on each disk and creates the entries in the disk ownership table .

Subsequently the ownership module accesses the disk ownership table to extract the identification of all disks that are owned by the D module. The ownership module then verifies the SCSI reservations on each disk owned by that D module by reading the ownership information stored in on disk. If the SCSI reservations and on disk ownership location information do not match the ownership module changes the SCSI reservation to match the ownership location information. Once the SCSI reservations and ownership location information match for all disks identified as owned by the D module the ownership module passes the information to the file system and the RAID module which configure the individual disks into the appropriate RAID groups and volumes for the D module . Notably other embodiments of the invention may not include a RAID system in which case other suitable processes will perform such tasks as assimilating the disks into aggregates.

Referring again to the takeover monitor process operates in conjunction with a cluster fabric CF interface module to monitor the heartbeats between the node and the one or more other nodes in the cluster. If the absence of a heartbeat is detected the takeover monitor process initiates the takeover procedure. In addition the takeover monitor is responsive to a storage takeover command by e.g. an administrator. In response to lack of heartbeat or issuance of a storage takeover command the takeover procedure is enabled and takeover processing begins with the takeover monitor process invoking the takeover routines as defined by the table .

The file system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustratively embodied as e.g. a virtual disk vdisk module not shown and a SCSI target module . The vdisk module enables access by administrative interfaces such as a user interface of a management framework see in response to a user system administrator issuing commands to the node . The SCSI target module is generally disposed between the FC and iSCSI drivers and the file system to provide a translation layer of the virtualization system between the block LUN space and the file system space where LUNs are represented as blocks.

The file system is illustratively a message based system that provides logical volume management capabilities for use in access to the information stored on the storage devices such as disks. It is noted however that logical volume management capabilities are provided by both the file system and the RAID system . In addition to providing file system semantics the file system provides functions normally associated with a volume manager. These functions include i aggregation of the disks ii aggregation of storage bandwidth of the disks and iii reliability guarantees such as minoring and or parity RAID . The file system illustratively implements the WAFL file system hereinafter generally the write anywhere file system having an on disk format representation that is block based using e.g. 4 kilobyte kB blocks and using index nodes modes to identify files and file attributes such as creation time access permissions size and block location . The file system uses files to store metadata describing the layout of its file system these metadata files include among others an mode file. A file handle i.e. an identifier that includes an mode number is used to retrieve an mode from disk.

Broadly stated all modes of the write anywhere file system are organized into the mode file. A file system fs info block specifies the layout of information in the file system and includes an mode of a file that includes all other modes of the file system. Each logical volume file system has an fsinfo block. The mode of the mode file may directly reference point to data blocks of the mode file or may reference indirect blocks of the mode file that in turn reference data blocks of the mode file. Within each data block of the mode file are embedded modes each of which may reference indirect blocks that in turn reference data blocks of a file.

Operationally a request from the client is forwarded as a packet over the computer network and onto the node where it is received at the network adapter . A network driver of layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the write anywhere file system . Here the file system generates operations to load retrieve the requested data from disk if it is not resident in core i.e. in memory . If the information is not in memory the file system indexes into the mode file using the mode number to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system the logical vbn is mapped to a disk identifier and disk block number disk dbn and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client over the network .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the node may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by node in response to a request issued by client . Moreover in another alternate embodiment of the invention the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the node. It is expressly contemplated that the various processes architectures and procedures described is herein can be implemented in hardware firmware or software.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows XP or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment and a storage area network and disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present invention may be utilized with any suitable file system including a write in place file system.

In the illustrative embodiment the storage server is embodied as D module of the storage operating system to service one or more volumes of array . In addition the multi protocol engine is embodied as N module to i perform protocol termination with respect to a client issuing incoming data access request packets over the network as well as ii redirect those data access requests to any storage server of the cluster . Moreover the N module and D module cooperate to provide a highly scalable distributed storage system architecture of the cluster . To that end each module includes a CF interface module adapted to implement intra cluster communication among the N and D modules including D module to D module communication for data container striping operations.

The protocol layers e.g. the NFS CIFS layers and the iSCSI FC layers of the N module function as protocol servers that translate file based and block based data access requests from clients into CF protocol messages used for communication with the D module . That is the N module servers convert the incoming data access requests into file system primitive operations commands that are embedded within CF messages by the CF interface module for transmission to the D modules of the cluster . Notably the CF interface modules cooperate to provide a single file system image across all D modules in the cluster . Thus any network port of an N module that receives a client request can access any data container within the single file system image located on any D module of the cluster.

Further to the illustrative embodiment the N module and D module are implemented as separately scheduled processes of storage operating system however in an alternate embodiment the modules may be implemented as pieces of code within a single operating system process. Communication between an N module and D module is thus illustratively effected through the use of message passing between the modules although in the case of remote communication between an N module and D module of different nodes such message passing occurs over the cluster switching fabric . As noted the cluster switching fabric is also used as a second medium over which heartbeats between the nodes are transmitted and received. A known messagepas sing mechanism provided by the storage operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF application programming interface API . Examples of such an agnostic protocol are the SpinFS and SpinNP protocols available from Network Appliance Inc. The SpinFS protocol is described in the above referenced U.S. Patent Application Publication No. US 2002 0116593.

The CF interface module implements the CF protocol for communicating file system commands among the modules of cluster . Communication is illustratively effected by the D module exposing the CF API to which an N module or another D module issues calls. To that end the CF interface module is organized as a CF encoder and CF decoder. The CF encoder of e.g. CF interface on N module encapsulates a CF message as i a local procedure call LPC when communicating a file system command to a D module residing on the same node or ii a remote procedure call RPC when communicating the command to a D module residing on a remote node of the cluster . In either case the CF decoder of CF interface on D module de encapsulates the CF message and processes the file system command.

The VLDB is a database process that tracks the locations of various storage components e.g. striped volume sets SVSs volumes aggregates etc. within the cluster to thereby facilitate routing of requests throughout the cluster. In the illustrative embodiment the N module of each node accesses a configuration table that maps an identifier such as a striped volume set identifier of a data container handle to a D module that owns services the data container within the cluster. The VLDB includes a plurality of entries which in turn provide the contents of entries in the configuration table among other things these VLDB entries keep track of the locations of the volumes and aggregates within the cluster. Examples of such VLDB entries include a VLDB volume entry and a VLDB aggregate entry .

Notably the VLDB illustratively implements a RPC interface e.g. a Sun RPC interface which allows the N module to query the VLDB . When encountering contents of a data container handle that are not stored in its configuration table the N module sends an RPC to the VLDB process. In response the VLDB returns to the N module the appropriate mapping information including an ID of the D module that owns the data container. The N module caches the information in its configuration table and uses the D module ID to forward the incoming request to the appropriate data container. Thus after a takeover or send home process in accordance with the invention the N module is notified of the newly assigned D module when the VLDB is updated at the D module ID field of the VLDB aggregate entry .

The functions and interactions between the N module and D module are coordinated on a cluster wide basis through the collection of management processes and RDB library user mode applications. To that end the management processes have interfaces to are closely coupled to RDB . The RDB comprises a library that provides a persistent object store storing of objects for the management data processed by the management processes. Notably the RDB replicates and synchronizes the management data object store access across all nodes of the cluster to thereby ensure that the RDB database image is identical on all of the nodes . At system startup each node records the status state of its interfaces and IP addresses those IP addresses it owns into the RDB database.

As noted in accordance with the invention the ownership information for each disk is recorded in an ownership location on each disk . The actual location in the media may be determined in a particular application of the invention such as closest to the center of the disk towards a disk edge or in an area most likely nearest to the drive head. Illustratively the serial number of the D module of the current owner and the home owner is stored in an ownership location on each disk. Other ownership information may be stored in the ownership location such as an IP address or a world wide name of a device on the network such as a node with which the particular disk is associated. In addition ownership information is contained within ownership table of each node which includes one or more table entries having ownership information field that contains subfield which identifies a current owner for the disk and home owner field which indicates the home owner for that disk. Notably the ownership information is stored in the memory of the node in core for all disks attached to that node.

In accordance with a further aspect of the invention the disk homeness API is accessible by various layers of the storage operating system . Disk homeness API includes commands that can be used between modules layers of the system and commands that can be used by an administrator at management station . The disk homeness API includes commands to display set and modify the homeness information in the ownership location on each disk and in the table . The commands are illustratively sent as a RPC message over the cluster fabric interconnect as part of the CF protocol messaging described herein with reference to . It should be understood however that other communication methods can be used within the scope of the present invention. Other commands include a disk assign command which has an optional parameter wherein the home owner can be set to something different than the current owner. By default the home owner will typically match the current owner.

A disk reassign command is invoked when the storage system is in a maintenance mode and is controlled by an administrator through the management station . In maintenance mode the disk reassign command sets the current owner and home owner to the same value unless the administrator instructs otherwise. After a takeover as discussed further herein the disk reassign command is provided to update the homeness information to set the current owner as the takeover node. A storage disk show command results in a display of both the current owner and the home owner on the GUI at management station .

As another example the disk migration module may access the disk ownership table to determine current disk ownership. The disk ownership module continues to update the ownership table during operation of the D module. If the disk topology changes these changes are reported using the disk homeness API across the cluster fabric interconnect to the other D modules in the cluster. Each of the other Dmodules then updates its respective disk ownership table in accordance with the procedure described herein.

The disk iterator in the ownership module of each D module in the cluster extracts the current owner and the home owner information from the table and from the ownership location on the disks when this information is needed. Notably when a disk is first assigned ownership to a node the home owner is set to current owner. More specifically on boot up the disk ownership table is populated with the information from the ownership location of the disk a takeover node that reboots will have disks that show itself the takeover node as the current owner but the original node as the home owner in the homeness information.

The homeness information is used by the cluster takeover monitor process running on each D module during a takeover and during a send home and by the RAID subsystem in order to reassimilate the volumes into the aggregates after a takeover and this process is more fully described in the above cited U.S. patent application Ser. No. 11 606 538.

Once the SCSI reservations and on disk ownership location information match for all the disks identified as being owned by that D module the ownership module then passes the information to the RAID system for that module to configure the individual disks into the appropriate RAID groups and volumes for the cluster. The ownership information is thus configured and the node proceeds to serve data access requests that are directed to it. The procedure of illustrates that the next step is a takeover of another node s disks. If there is not a takeover there may be no change in the ownership information However the information is illustratively re verified when there is a disk add remove command and also verified more frequently if desired by rescanning the ownership information for all or a portion of the disks in the cluster. Though the table may not need to be fully recreated each time this re scan is advantageous in case another node in a shared storage system may have assigned new disks to this node since the prior scan. These newly assigned disks become apparent to the node upon scanning all the disks and looking for changes. Thus the figure depicts steps and as being repeated in a loop for purposes of illustration.

If there is a takeover as in step the procedure continues to step . The disk iterator of the takeover node reads ownership location information on each disk or from the in core ownership table and takes over disks assigned to the failed node and thus the current owner information is updated to reflect the takeover node as being the current owner of those disks. Notably due to this change further interaction with the aggregates on those disks is handled as local storage. As noted herein there is no need for a partner mode which is advantageous because it reduces code complexity and duplication. The takeover process using the homeness information is described in further detail in commonly owned U.S. patent application Ser. No. 11 606 727 of Coatney et al. filed on even date herewith.

In accordance with step when the failed node is revived a send home procedure is performed. More specifically the disks are reassigned to the previously failed node by changing the ownership location information on those disks to once again refer to its original node which is now a waiting node. Further details of the send home procedure are provided in commonly owned U.S. patent application Ser. No. 11 606 452 of Coatney et al. filed on even date herewith. This aspect of the inventive procedure ends at step .

As noted the disk ownership module continues to update the disk ownership table during the operation of the cluster. Thus when the disk topology changes as a result of a takeover maintenance mode drives being added to the configuration or otherwise i.e. for load balancing the disk homeness API can be used to reassign those disks.

It should be understood by those skilled in the art that the ownership information provided in accordance with the invention as well as the disk homeness API of the invention allow any D module in the cluster to access the disks directly to find home owner information. An administrator can also access the disk ownership table of a D module to determine the home owner information of the disks. Unlike prior techniques there is no need to require that failover is performed by a partner because that partner is the only other D module which has the information about the disks. Thus any D module or any group of D modules can be assigned to take over disks assigned to another D module. This allows for greater flexibility and load balancing within the cluster.

The foregoing description has been directed to particular embodiments of the invention. It will be apparent however that other variations and modifications may be made to the described embodiments with the attainment of some or all of their advantages. Specifically it should be noted that the principles of the invention may be implemented in a non distributed file system. Furthermore while this description has been written in terms of D and N modules the teachings of the present invention are equally suitable to systems in which the functionality of the N and D modules are implemented in a single system. Alternatively the functions of the N and D modules may be distributed among a number of separate systems wherein in each system performs one or more functions. Additionally the features of the present invention have been described with respect to a cluster which contains two nodes however it is equally applicable to clusters including a plurality of nodes which allow for an n way failover. Additionally the procedures processes and or modules described herein may be implemented in hardware software embodied as a computer readable medium having program instructions for one or a combination thereof. Therefore it is the object of the appended claims to cover all such variations and modifications as come within the true spirit and scope of the invention.

