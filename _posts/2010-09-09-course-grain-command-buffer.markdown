---

title: Course grain command buffer
abstract: A method for executing processes within a computer system is provided. The method includes determining when to switch from a first process, executing within the computer system, to executing another process. Execution of the first process corresponds to a computer system storage location. The method also includes switching to executing the other process based upon a time quantum and resuming execution of the first process after the time quantum has lapsed, the resuming corresponding to the storage location.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08743131&OS=08743131&RS=08743131
owner: ATI Technologies ULC
number: 08743131
owner_city: Markham, Ontario
owner_country: CA
publication_date: 20100909
---
This application claims the benefit of U.S. Provisional Application No. 61 241 230 entitled Course Grain Command Buffer filed Sep. 10 2009 which is incorporated herein by reference in its entirety.

During operation embedded programs within a processing unit such as a graphics processing unit GPU known as software schedulers read content such as data packets from the GPU s command buffers to perform graphics processing tasks. The performance of these tasks routinely requires the software to switch from reading of the contents of one command buffer to begin reading or executing content from another command buffer. Conventional GPUs however cannot efficiently switch from one process to another and then resume the earlier process. This switching inefficiency produces system latencies ultimately resulting in slower system performance and reduced user control.

What is needed therefore are methods and systems to efficiently enable software schedulers to determine which command buffers content should be read and to efficiently switch between these various buffers and processes. What are also needed are methods and systems to reduce system latency and increase the control of scheduling to a program running on the GPU.

The present invention meets the above described needs. For example an embodiment of the present invention provides a means to preempt the source stream on a GPU on a command boundary save that state away commence another process behind it and resume the original process later.

An embodiment of the present invention includes a method for executing processes within a computer system. The method includes determining when to switch from a first process executing within the computer system to executing another process. Execution of the first process corresponds to a computer system storage location. The method also includes switching to executing the other process based upon a time quantum and resuming execution of the first process after the time quantum has lapsed the resuming corresponding to the storage location.

Embodiments of the present invention provide mechanisms for efficiently tracking storing and restoring locations of processes occurring within a processing unit s command buffer. These mechanisms enable a command processor to essentially stop the execution of a process in the middle of a command buffer begin execution of a different process return to the same location in the command buffer and resume executing the earlier process. As a result the command processor can switch from one task such as processing a lower priority process to another task such as processing a higher priority process with significantly reduced pre emption times reduced system latency and greater user control.

Further features and advantages of the invention as well as the structure and operation of various embodiments of the invention are described in detail below with reference to the accompanying drawings. It is noted that the invention is not limited to the specific embodiments described herein. Such embodiments are presented herein for illustrative purposes only. Additional embodiments will be apparent to persons skilled in the relevant art s based on the teachings contained herein.

Embodiments of the present invention enable substantially improved utilization of resources in a processing unit such as a GPU. While the present invention is described herein with illustrative embodiments for particular applications it should be understood that the invention is not limited thereto. Those skilled in the art with access to the teachings provided herein will recognize additional modifications applications and embodiments within the scope thereof and additional fields in which the invention would be of significant utility.

Embodiments of the present invention can be used in any computer system or computing device having at least two processors such as a CPU that provides work items e.g. commands or command buffers and a GPU that processes work items provided by the CPU. For example and without limitation embodiments may include computers including laptop computers personal computers game platforms entertainment platforms personal digital assistants and video platforms.

In systems having a CPU and a GPU the utilization of the GPU is a significant factor in overall system performance. Ideally one would desire to have the GPU utilization at or near maximum. The CPU provides the instructions and data that are used in the GPU. In conventional systems the CPU provides substantially all of the instructions and data to the GPU via command buffers. The GPU receives the command buffers as one or more inputs and executes their content.

A command buffer is a data structure containing instructions or commands along with associated data. In conventional systems prioritization of the command buffers was performed by the CPU. The GPU simply executed the commands that were queued and prioritized by the CPU in the order specified by the CPU. Embodiments of the present invention facilitate greater GPU autonomy reducing the GPU s dependency on the CPU s prioritization of work to be executed.

For example embodiments of the present invention enable the GPU to perform the prioritization and scheduling of commands. More specifically the GPU can prioritize commands queued for execution by the CPU based on its local resource availability. This approach is more dynamic and efficient than CPU prioritization approaches. Furthermore the GPU can perform a second level of prioritizing beyond CPU specified command prioritization.

Embodiments of the present invention enable command processors within the GPU on a packet boundary to decide to preempt or change the source stream on a GPU. This preemption decision is based upon priorities. For example the software may recognize that a particular command buffer within the GPU has been running a particular process too long or that its processing time has expired. Under these conditions embodiments of the present invention provide an approach to switch between these processes within the GPU such that software within the GPU performs the scheduling.

To accomplish this the program writes to its own copy of the command buffers. The software scheduler then decides which command buffer of a particular program or process should be run next. An example is provided below.

In example of the command processor reads from the command buffer A in accordance with user instructions. In this arrangement the command processor may decide that it will cease reading from the command buffer A and instead transition to and begin reading from the command buffer B. To facilitate this transition the command processor is configured to track and monitor the precise command buffer it is processing e.g. buffer A . It can also track and monitor particular locations within the command buffer its processing. This tracking and monitoring enables the command processor to know where to begin upon resumption of its processing of the contents of buffer A.

To accomplish this the command processor saves state information of the GPU in a separate memory location to be able to recreate the state of the GPU or device upon resume. In this manner this process preserves precise knowledge of where to begin and knowledge of the state of the GPU or device. The save operation includes two aspects 

This hardware state save includes the pointers necessary to perform the saving or tracking mentioned above with reference to .

Another feature of the example of is an ability to flush all cached data in the GPU so that all command buffers in memory can be saved and restored upon resume.

With reference to the embodiment of the present invention illustrated in it was noted that a command processor within the GPU decides whether to preempt or change the source stream based upon priorities. The source of these priorities can originate in software i.e. software based scheduling . This software based scheduling routine discussed more fully below produces the input to switch from one process to another one ring to another one queue to another or one buffer to another.

In software based scheduling switching occurs when the scheduling routine determines that either a particular process has run long enough a particular time quantum i.e. timing threshold has lapsed or when other tasks now require attention. These are only a few of the factors that that can be considered. It will be appreciated by one of skill in the art however that switching can also occur because of other factors. When one or more of these factors occurs the scheduling routine instructs the command processor to switch between the command buffers A B and C. In this manner the embodiment of is an illustration of software controlled switching between processes within the GPU.

As noted above embodiments of the present invention provide mechanisms for efficiently tracking storing and restoring locations of processes occurring within a command buffer. These mechanisms enable a command processor to stop the execution of one process e.g. in the middle of a command buffer begin execution of a different process return to the same location in the command buffer and resume execution of the earlier process. More specifically the command processor can switch for example from processing a lower priority process to processing a higher priority process with significantly reduced pre emption times.

In the example of a ring buffer A includes individual command buffers CB to CBn. Ring buffer A is included within the GPU s system memory. Command buffer CB for example stores packets e.g. packets through M for processing.

The GPU also includes execution units such as a command processor ring buffer controller a save restore area and a software scheduler . The command processor among other tasks well known to those of skill in the art fetches ring packets such as the packets M which can point to a command buffer such as the command buffer CB . The command processor also records driver written state information to the save restore area .

The ring buffer controller informs the command processor which ring buffer to process. In the example of the command processor is illustrated as it processes contents from the ring buffer A . The command processor could also process contents from additional ring buffers not shown . The ring buffer controller receives its instructions from the software scheduler .

The exemplary command processor of includes a command buffer index a ring buffer index a preemption index a preamble begin index and a preamble end index . The command processor records the command buffer index the ring buffer index the preamble begin index and the preamble end index continuously and automatically.

An arrow from the ring buffer index to the ring buffer A indicates the beginning of an area within the command buffer CB that has not yet been read. More specifically the ring buffer index cannot advance from the beginning of the command buffer CB i.e. the unread area to CB until the processing of the contents of CB has concluded. To facilitate this advancement the command processor is configured to be able to return to the correct location within the ring buffer A e.g. CB if the reading of the ring buffer A ceases for some reason.

The command processor processes CB until the ring buffer controller requests that it to stop processing the ring buffer A . This process is known as preemption and is facilitated by the preemption index . Preemption can occur for example when the command processor is requested to process a task having a priority higher than the task currently being processed. When the command processor reports that all processing up to the point of preemption has concluded it informs the ring buffer controller .

By way of example an arrow from the command buffer index register points to a location within the command buffer CB currently being processed. There is also an input from the ring buffer controller to the preemption index indicating the need to switch from the ring buffer A to another ring buffer or to another command buffer within the ring buffer A .

When processing within the command processor ceases the ring buffer controller reads the related indexes as indicated by arrows from ring buffer controller to save restore area . In the exemplary embodiment of when the command processor resumes operations it restores these operations from the save restore area .

The illustration of represents an embodiment of the present invention where software provides control via the software scheduler to the ring buffer controller . Thus in the embodiment of the ring buffer controller is more or less a slave making few decisions on its own. In an alternative embodiment discussed in greater detail below a hardware scheduling mechanism is provided enabling the ring buffer controller to perform more of a master role.

As noted above the ring buffer controller controls switching from one process to another i.e. one ring buffer to another . The ring buffer controller reads registers within the command processor and writes this content back to the command processor at restore time. This process enables the command processor to specifically know what ring buffer and where within that particular ring buffer to read from. More specifically when the command processor recognizes that the command buffer index associated with the command buffer it was just reading e.g. CB is a non zero value it knows that its reading of this buffer has not concluded. It therefore resumes reading from the next location within this buffer upon resume.

Also included in the command processor is the preamble begin index and the preamble end index . These preamble indexes are also referred to as set up indexes or registers.

By way of background commands stored within the command buffers typically include preambles that can be used for example for set up. Preamble or set up indexes are used because the GPU s software drivers place multiple software clients or processes within the same command buffer. Each of these clients and processes has a unique preamble including a beginning portion and an ending portion. The preamble begin index and the preamble end index track the beginning and ending of these various preambles.

In the example of CB of a preamble associated with a first software client at the beginning of packet can include a set up representing draws or other specific operations to perform. Packet for example can include a different preamble for a new software client. The ring buffer controller dynamically tracks the beginning and ending of the preambles for packet and packet in real time using the preamble begin index and the preamble end index . The real time availability to dynamically track multiple preamble beginnings and preamble endings provides the flexibility of handling multiple process owners within a single command buffer. In this example the command processor remembers when a final draw from the CB has finished based upon information within the command buffer index see arrow pointing from the command buffer .

As known by those of skill in the art conventional GPU systems e.g. command processors have an ability to perform rolling preamble begin and preamble end execution but not tracking. That is these conventional systems can execute sets of preamble begin ends as they occur within the command buffer. These systems can also track and record preamble begin and ends associated with a second set when that second set is processed. Embodiments of the present invention however not only execute but also track the initial preamble of the command buffer along with subsequent preamble begin and ends within the command buffer.

Embodiments of the present invention as illustrated in can apply to many different types of preamble categories. One particularly relevant category is illustrated in the example of a draw call associated with a specific software client. In this first category there is a particular initialization state that this software client desires the GPU have as a base. Then prior to each of these draw calls the command buffer may signal that it desires to assign one value to a first register and another value to a different register. Thus associated with this particular draw call there is a delta from the base or initialization or an incremental state. There is also a preamble associated with this draw call that indicates that this is what the base of the entire GPU should resemble. This preamble provides an ability to incrementally change the state for each draw call.

Referring back to the example of the ring buffer controller reads from preamble begin index and the preamble end index and writes those out to the save restore area . The ring buffer controller then reprograms the command processor to fetch from another ring buffer or another command buffer with a particular ring buffer based upon other information included within the save restore area .

The system includes a CPU a system memory a graphics driver GPU and communication infrastructure . A person of skill in the art will appreciate that system may include software hardware and firmware components in addition to or different from that shown in the embodiment shown in .

CPU can be any commercially available CPU a digital signal processor DSP application specific integrated processor ASIC field programmable gate array FPGA or a customized processor. CPU can comprise of one or more processors coupled using a communication infrastructure such as communication infrastructure . CPU can also include one or more processors that have more than one processing core on the same die such as a multi core processor.

In the embodiment of CPU can be a dual core processor having processing cores core and core . CPU executes an operating system not shown and one or more applications and is the control processor for system . The operating system executing on CPU controls and facilitates access to devices in system . One or more applications executing on CPU including user applications cause CPU to coordinate the use of various devices of system including GPU and system memory to accomplish the tasks.

System memory includes one or more memory devices. Typically system memory can be a dynamic random access memory DRAM or a similar memory device used for non persistent storage of data. In some embodiments system memory can include a memory device such as a flash memory device and or static RAM SRAM device. During execution of system in an embodiment system memory can have residing within it one or more memory buffers through which CPU communicates commands to GPU .

Memory buffers through which CPU communicates commands to GPU can be implemented as ring buffers or other data structure suitable for efficient queuing of work items. In the following memory buffers are referred also to as ring buffers . Commands from CPU to GPU can include instructions and data. In some embodiments data structures having instructions and data are input to a ring buffer by an application and or operating system executing on CPU . CPU or an application and or operating system executing on CPU can specify a priority associated with one or more ring buffers . Commands may be added to a ring buffer based on a determined priority level of each command. For example CPU may define one ring buffer each for a high priority commands low priority commands and low latency commands.

A set of indirect buffers may be used to hold the actual commands e.g. instructions and data . For example when CPU communicates a command buffer to the GPU the command buffer may be stored in an indirect buffer and a pointer to that indirect buffer can be inserted in the ring buffer of the corresponding priority level. It should be noted that indirect buffers can be implemented to enable either a single level of indirection or multiple levels of indirection.

Ring buffer work registers can be implemented in system memory or in other register memory facilities of system . Ring buffer work registers provide for example communication between CPU and GPU regarding commands in ring buffers . For example CPU as writer of the commands to ring buffers and GPU as reader of such commands may coordinate a write pointer and read pointer indicating the last item added and last item read respectively in ring buffers . Other information such as list of available ring buffers priority ordering specified by CPU can also be communicated to GPU through ring buffer work registers .

Graphics driver can comprise software firmware hardware or any combination thereof. In an embodiment graphics driver is implemented entirely in software. During the execution of system graphics driver software can reside in system memory . Graphics driver provides an interface and or application programming interface API for the CPU and applications executing on CPU to access GPU . Generally when system comes up the operating system initializes the graphics driver as appropriate for the particular GPU .

GPU provides graphics acceleration functionality and other compute functionality to system . GPU can include a plurality of processors such as single instruction multiple data SIMD processors including processing elements such as arithmetic and logic units ALU . Having multiple SIMD processors in general makes GPU ideally suited for execution of data parallel tasks such as is common in graphics processing. For example when rendering an image on a display the same or substantially the same instructions are executed on each pixel that is rendered on the display.

GPU can also be used for tasks other than graphics operations such as various compute intensive tasks that can benefit from parallel execution of data streams. In the description below graphics applications are used for ease of description. A person of skill in the art will however recognize that the teachings herein are applicable to numerous other tasks that can be executed on a graphics processor. Also as will be understood by those of ordinary skill in the art GPU could be logic embedded in another device such as CPU a bridge chip such as a northbridge southbridge or combined device or the like.

GPU comprises components including a GPU memory a 3 dimension compute shader complex 3D CS complex a ring list controller RLC and command processor . GPU memory provides a local memory for use during computations in GPU and may include DRAM or such memory device. In an embodiment GPU includes a plurality of context save areas CSA . Each CSA provides a memory area for saving the context of work items that are swapped out of execution in GPU before completion as described below.

3D CS complex is the main computation component within GPU and comprises of a plurality of SIMD processors that facilitates computations including computations on parallel data streams. 3D CS complex for example can include vertex shaders pixel shaders geometry shaders unified shaders and other components necessary for data computation in GPU . In embodiments described below 3D CS complex can be considered as comprising 3D computation components compute shader components and low latency computation components. The commands sent to the GPU from CPU are implemented using the 3D CS complex.

Ring list controller RLC includes functionality to coordinate the access to memory buffers such as ring buffers . In an embodiment RLC determines the list of ring buffers that is to be processed in GPU receives any priority ordering of ring buffers specified by CPU more specifically a process or operating system executing on CPU and determines the scheduling of the ring buffers on GPU in a manner that optimizes the utilization of processing resources in GPU . For example RLC together with command processor can schedule the ring buffers received from CPU in a manner that keeps each SIMD processor in 3D CS complex at or near maximum utilization.

Command processor controls the processing within GPU . Command processor receives instructions to be executed from CPU and coordinates the execution of those instructions on GPU . In some instances command processor may generate one or more commands to be executed in GPU that corresponds to each command received from CPU . In an embodiment command processor together with RLC implements the prioritizing and scheduling of commands on GPU in a manner that maximizes the utilization of GPU resources.

Logic instructions implementing the functionality of the command processor and RLC can be implemented in hardware firmware or software or a combination thereof. In one embodiment command processor is implemented as a RISC engine with microcode for implementing logic including scheduling logic.

Communication infrastructure provides coupling to devices and components of system . Communication infrastructure can include one or more communication buses such as Peripheral Component Interconnect PCI Advanced Graphics Port AGP and the like.

A subset may be selected based on the criteria specified by CPU of . For example the CPU can identify subset as having commands ready to be executed on GPU . After enqueuing one or more commands to each ring buffer and CPU can update one or more memory locations such as a location in ring buffer work registers which is read by GPU .

In another embodiment upon writing one or more commands to one or more ring buffers CPU can directly write into a register within GPU notifying GPU that command buffers are available for processing.

GPU periodically monitors the ring buffers in system memory ring buffer work registers in system memory and or other register locations that are updated by CPU to determine if any ring buffers have command buffers that are ready to be processed by GPU . Upon detection that one or more ring buffers have command buffers ready to be executed GPU can receive the command buffers for execution. In an embodiment GPU may use direct memory access DMA or the like to receive the ring buffers specified by the CPU into GPU local memory or into a set of general purpose registers GPR . The RLC may perform the monitoring of the ring buffers and control the transfer of the ring buffers to GPU memory and or GPR.

Having determined the set of ring buffers to be executed on GPU RLC determines the allocation of the ring buffers to GPU the prioritization of the ring buffers and prioritizations of the command buffers within the ring buffers. In some embodiments the determination of prioritizations is performed by RLC in coordination with command processor .

For example in the subset of ring buffers received for execution on GPU a priority ordering as shown of ring buffer as priority level ring buffer as priority level and ring buffers and as priority level may be determined based on the prioritizations determined by CPU and prioritization determined by GPU .

Aspects of the present invention can be stored in whole or in part on a computer readable media. The instructions stored on the computer readable media can adapt a processor to perform the invention in whole or in part.

The present invention has been described above with the aid of functional building blocks illustrating the implementation of specified functions and relationships thereof. The boundaries of these functional building blocks have been arbitrarily defined herein for the convenience of the description. Alternate boundaries can be defined so long as the specified functions and relationships thereof are appropriately performed.

For example various aspects of the present invention can be implemented by software firmware hardware or hardware represented by software such as for example Verilog or hardware description language instructions or a combination thereof. After reading this description it will become apparent to a person skilled in the relevant art how to implement the invention using other computer systems and or computer architectures.

It should be noted that the simulation synthesis and or manufacture of the various embodiments of this invention can be accomplished in part through the use of computer readable code including general programming languages such as C or C hardware description languages HDL including Verilog HDL VHDL Altera HDL AHDL and so on or other available programming and or schematic capture tools such as circuit capture tools . This computer readable code can be disposed in any known computer usable medium including semiconductor magnetic disk optical disk such as CD ROM DVD ROM and as a computer data signal embodied in a computer usable e.g. readable transmission medium such as a carrier wave or any other medium including digital optical or analog based medium . As such the code can be transmitted over communication networks including the Internet and intranets. It is understood that the functions accomplished and or structure provided by the systems and techniques described above can be represented in a core such as a GPU core that is embodied in program code and can be transformed to hardware as part of the production of integrated circuits.

It is to be appreciated that the Detailed Description section and not the Summary and Abstract sections is intended to be used to interpret the claims. The Summary and Abstract sections may set forth one or more but not all exemplary embodiments of the present invention as contemplated by the inventor s and thus are not intended to limit the present invention and the appended claims in any way.

