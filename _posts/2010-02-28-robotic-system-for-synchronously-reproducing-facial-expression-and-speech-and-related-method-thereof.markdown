---

title: Robotic system for synchronously reproducing facial expression and speech and related method thereof
abstract: A robotic system and a related method for reproducing a real person's facial expression and speech simultaneously and synchronously is provided herein. The robotic system comprises at least a robotic head which in turn comprises a speaker, a plurality of face actuators, and a computing engine. The robotic head drives the speaker and the face actuators synchronously based on a speech segment and a sequence of time-stamped control vectors so that the robotic system could mimic a real person's facial expression and speech. The speech segment and the sequence of time-stamped control vectors are retrieved from a storage device of the robotic system, or from an external source via an appropriate communication mechanism.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07904204&OS=07904204&RS=07904204
owner: 
number: 07904204
owner_city: 
owner_country: 
publication_date: 20100228
---
This is a division of U.S. application Ser. No. 11 311 614 filed Dec. 19 2005 now U.S. Pat. No. 7 738 997 issued Jun. 15 2010 which is incorporated herewith by reference.

The present invention generally relates to robotic systems and more particularly to a robotic system and a related method for reproducing a real person s facial expression and speech synchronously and simultaneously.

Recent robotic researches have shifted from traditional autonomous robots designed to operate as independently and remotely as possible from humans to humanoid robots that can communicate in a manner that supports the natural communication modalities of humans such as facial expression body posture gesture gaze direction voice etc.

One such humanoid robot currently under development is the Kismet robot by the Robotics and Artificial Intelligence Laboratory of Massachusetts Institute of Technology. Kismet has a 15 degree of freedom robotic head whose ears eyebrows eyelids lips jaw etc. are driven by actuators to display a wide assortment of facial expressions. For example Kismet has four lip actuators one at each corner of the mouth so that the mouth can be curled upwards for a smile or downwards for a frown. Similarly each eyebrow of Kismet can be lowered and furrowed in frustration or elevated upwards for surprise. More details about Kismet could be found in the article Toward Teaching a Robot Infant using Emotive Communication Acts by Breazeal C. and Velasquez J. in Proceedings of 1998 Simulation of Adaptive Behavior workshop on Socially Situtated Intelligence Zurich Switzerland pp. 25 40 1998.

Another similar research is the Tokyo 3 robot by the Hara Laboratory of Tokyo University of Science. The Tokyo 3 robotic head has a facial skin made of silicone so its facial expression is more resembling to that of real human. The actuators of Tokyo 3 robotic head drive 18 characteristic points of the facial skin to imitate various human expressions such as happiness anger sadness resentment surprise horror etc. More details about the Tokyo 3 robot could be found in the article Artificial Emotion of Face Robot through Learning in Communicative Interactions with Human by Fumio Hara JST CREST International Symposium on Robot and Human Interactive Communication Kurashiki Ivy Square Kurashiki Okayama Japan Sep. 20 2004.

The focus of these foregoing researches is to engage the robot into natural and expressive face to face interaction with human. To achieve this goal the robot usually perceives a variety of natural social cues from visual and auditory channels and in response to these sensory stimuli delivers social signals to the human through gaze direction facial expression body posture and vocal babbles autonomously. On the other hand researches in seemingly unrelated areas such as pattern recognition and computer animation and modeling suggest an interesting application of the humanoid robotic head. For example Pighin et al. in the article Synthesizing Realistic Facial Expressions from Photographs by Pighin F. Hecker J. Lischinski D. Szeliski R. and Salesin D. in SIGGRAPH 98 Conference Proceedings pp. 75 84 ACM SIGGRAPH July 1998 presents a technique for creating highly realistic face models and natural looking animations. Pighin et al. generates a 3D face model of a person by deriving feature points on several 2D images of the person s face from different viewpoints and using the feature points to compute the positions of the remaining face mesh vertices. Separate face models corresponding to the person s different facial expressions could be produced in this way. Pighin et al. then create smooth transitions between different facial expressions by 3D shape morphing between these different face models. It should be obvious that the technique of Pighin et al. could be readily adapted to the humanoid robotic head for example by locating the feature points at where the face actuators is positioned and using 3D shape morphing to guide the operation of the actuators. The result would be a humanoid robotic head instead of generating generically human like expressions but actually reproducing a specific real person s facial expression in very high degree of resemblance. Many similar facial expression interpretation techniques such as using neural networks multiple point integrations etc. could be found in the literature.

Besides facial expressions another social signal delivered by the humanoid robotic heads of recent researches is the voice. For example Kismet is equipped with a synthesizer that models the physiological characteristics of human s articulatory tract. By adjusting the parameters of the synthesizer Kismet is possible to convey speaker personality as well as adding emotional qualities to the synthesized speech. Despite that the humanoid robotic heads by recent researches are still made to deliver generically human like voice not a specific real person s voice. Following the thought of making a humanoid robotic head to reproduce a specific person s facial expression it would make an even more interesting application if the person s own voice is pre recorded and then played synchronously along with the humanoid robotic head s delivery of the person s facial expression.

Following up the recent progress in the robotic heads as described above the present invention provides a robotic system and a related method for reproducing a real person s facial expression and speech synchronously and simultaneously.

The robotic system of the present invention comprises at least a robotic head which in turn comprises a speaker a plurality of face actuators and a computing engine. The robotic head drives the speaker and the face actuators synchronously based on a speech segment and a sequence of time stamped control vectors so that the robotic system could mimic a real person s facial expression and speech. The speech segment and the sequence of time stamped control vectors are retrieved from a storage device of the robotic system or from an external source via an appropriate communication mechanism.

The robotic system could further comprise a recording device and an interpretation device which prepare the speech segment and the sequence of time stamped control vectors. The recording device comprises at least a camera and a microphone with which a person s facial expression and the person s speech could be recorded simultaneously over a period of time. The recorded speech and video are then processed by the interpretation device to obtain the speech segment and the sequence of time stamped control vectors. The speech segment and the sequence of time stamped control vectors are then uploaded into the storage device of the robotic head or are retrieved by the robotic head so that the robotic head could play the speech segment and in the mean time drive the face actuators according to the control vectors at appropriate times. As such the robotic head is able to mimic a real person s speech and facial expression such as telling a joke narrating a story singing a song or any similar oral performance. In addition to the system described above a process for obtaining the speech segment and the sequence of time stamped control vectors is also provided herein.

The foregoing and other objects features aspects and advantages of the present invention will become better understood from a careful reading of a detailed description provided herein below with appropriate reference to the accompanying drawings.

The following descriptions are exemplary embodiments only and are not intended to limit the scope applicability or configuration of the invention in any way. Rather the following description provides a convenient illustration for implementing exemplary embodiments of the invention. Various changes to the described embodiments may be made in the function and arrangement of the elements described without departing from the scope of the invention as set forth in the appended claims.

The present invention is about making a robotic head to reproduce a real person s facial expression and speech synchronously. The robotic head of the present invention unlike those used in the Kismet and Tokyo 3 projects is not an autonomous one but purely driven by pre prepared information to mimic a real person s facial expression and speech. A robotic head according the present invention should contain at least 1 a speaker 2 a plurality of face actuators and 3 a computing engine to drive the speaker and the face actuators.

The computing engine of the robotic head could be a conventional or similar computing device. For example the computing engine contains among other components a processor an random access memory RAM and a read only memory ROM connected to the processor via a system bus a storage device and an input output I O interface and a driving interface connected to the processor via the I O bus . The storage device could be a hard disk or laser disk drive for storing information such as the operating system and controlling programs for the robotic head . The I O interface could contain a network connector for connecting to a local area network a USB universal serial bus port for accessing external storages and appropriate ports for connecting to a keyboard a mouse and a display. There is also a driving interface for connecting to the vocalization device and the expressive motor device of the robotic head .

Please note that this is a very simplified view of the computing engine with a number of details such as controllers power supply etc. omitted and this is by no means to restrict the embodiments of the computing engine in any way. The major characteristic of the computing engine is that it has the intelligence and the computing power to via an appropriate driving interface control the vocalization device and the expressive motor device which in turn drive their speaker and face actuators respectively based on the information delivered to the computing engine via an appropriate I O interface in real time i.e. an interactive mode of operation or stored in its storage device i.e. a batch mode of operation . Please note that the vocalization device and the expressive motor device could actually be integral parts of the computing engine .

Assuming that the robotic head has n n 1 actuators the driving of the actuators from the computing engine could be modeled as delivering a vector V A A . . . A from the computing engine to the expressive motor device which in turn based on the parameter A 1 i n drives the ith actuator . Depending on the characteristics of the corresponding ith actuator the parameter Adescribes for example how the actuator moves in its various dimensions i.e. degrees of freedom and to what extent at each dimension. Since the actuators are not identical for example the actuator for the eyebrow may have only one degree of freedom while the actuator for the ear may have two degrees of freedom the parameters A A . . . Amay contain different amount of information and may be in different formats. However they are modeled as such for simplicity.

Accordingly to deliver continuously changing facial expression basically all that is required is to store an appropriate sequence of the vectors V V V . . . V m 1 on the storage device of the computing engine and then to have the computing engine to read this sequence of vectors and deliver them one vector at a time to the expressive motor device . In an alternative embodiment the sequence could also be retrieved from an outside source by the computing engine via a network interface and the computing engine delivers the sequence to the expressive motor device . However to mimic a real person s facial expression the computing engine would require timing information about when to issue a particular vector and for how long to wait before issuing the next vector. Accordingly the vector sequence could be extended to be a time stamped vector sequence as follows 

On the other hand the speech segment could be a segment of pre recorded speech or it could be a sequence of time stamped synthesis commands derived from human voice. The segment of pre recorded speech could be in an appropriate analog or digital such as WAV MP3 etc. format. The computing engine retrieves the speech segment from the storage device converts it into a format required by the vocalization device and delivers it to the vocalization device which in turn plays it out via its speaker . Similarly in an alternative embodiment the speech segment could also be retrieved from an outside source to the computing engine via a network interface and the computing engine delivers the speech segment to the vocalization device . If the speech segment is a sequence of time stamped synthesis commands the computing engine actually issues the sequence of commands based on their time stamps to the vocalization device to produce synthesized voice just like how the actuators are driven to deliver facial expression. Please note that in order to play the speech segment or to issue the sequence of synthesis commands and to deliver the vector sequence simultaneously the computing engine requires some parallel mechanism. For example the computing engine has a real time clock not shown in for timing and it also has the interrupt mechanism associated with the real time clock to trigger the delivery of a specific vector or the issuance of a specific synthesis command at the time specified by the time stamp. The real time clock and the interrupt mechanism are common in conventional computers.

Therefore a major characteristic of the present invention is about the generation of the speech segment along with a sequence of control vectors so that a robotic head could by playing out the speech via its speaker and generate continuously changing facial expressions by its face actuators . The relationship between the speech segment and the vector sequence is illustrated in . As illustrated the time stamp binds a vector to a specific time during the speech segment so that the facial expression controlled by the specific vector is corresponding to the emotion of the speaker at that particular time. Where the vectors are denser together it is the period of time that the speaker is more emotional and has more abrupt changes in terms of his or her facial expression.

To generate the speech segment and the synchronized actuator vector sequence a robotic system according to an embodiment of the present invention as illustrated in further contains a recording device and an interpretation device . The recording device contains one or more cameras and a microphone for recording both a speaker s facial expression and his or her speech over a period of time. If there are multiple cameras they are usually positioned so that the person s performance is recorded from different viewing angles.

The most important function of the recording device is to obtain recording s of the facial expression and speech. The facial expression and speech could be recorded into a single recording e.g. a single audio video clip or into separate recordings. It is also possible that the facial expression and the speech are recorded separately at different times. It is also possible that the facial expression and the speech are performed by different persons. However the most common scenario is that a same person s facial expression and speech are captured simultaneously and synchronously. Therefore the following explanation focuses on this most common scenario first.

The recording device could be as simple as a camcorder which records both the facial expression and speech into a single audio video clip on a removable media such as a tape or a laser disc. The recording device could also be a computing device such as a desktop computer installed with an appropriate recording application program with a camera and microphone built in or externally connected and the speech and the facial expression are recorded into a single audio video file or into separate files. As can be imagined there are various possible implementations for the recording device . Please note that the speech recorded could be in a form already suitable for playing on the robotic head or it requires further processing by the interpretation device . Besides using a removable media as a communication mechanism the recording device and the interpretation device could have a direct link mechanism such as a local area network. In this way the recording device could actually capture the facial expression and speech and transmit the captured information to the interpretation device via the local area network almost if not entirely simultaneously. Please also note that in some embodiment the robotic system of the present invention does not contain a recording device where the recording is conducted somewhere else and the recording s are brought to the interpretation device for further processing. There are also embodiments where the recording device only captures facial expressions while the speech is recorded somewhere else. There are also embodiments where the recording device only captures speech while the video is recorded somewhere else. Please note that if the facial expressions and the speech are not recorded simultaneously they have to be aligned or synchronized in time which will be conducted by the interpretation device.

The interpretation device is usually a computing device with an interpretation application program. The computing device is equipped with the appropriate mechanism for accessing the removable media where the recorded speech and the facial expression are stored. In some embodiment the interpretation device and the recording device are actually integrated into a single computing device. In this embodiment the interpretation application program simply accesses the file s generated by the recording application program. The interpretation program has appropriate knowledge about the actuators on the robotic head and their characteristics. Then by employing a technique according to Pighin et al. as mentioned earlier or other similar approaches the interpretation program can extract face mesh points from the images recorded in the video file s and based on how the face mesh points change in terms of space e.g. how and in what direction they move and time e.g. how fast the interpretation program is able to transform the information into the parameters for controlling the actuators of the robotic head as shown in . is a schematic diagram showing roughly how this is achieved. Assuming that the ith face actuator is located at a location P in the 3D model based on the algorithm the mesh point at location P changes from its current location at time tto a new location P at time t. Then the control parameter A could be derived based on the space vector PP i.e. the dashed arrow in and the time difference t t . The technique or algorithm of the interpretation device or the interpretation program is not part of the present invention and many such algorithms are already available from the related arts in the academic arena. However one point has to be pointed out is that if the speech and the facial expression are recorded simultaneously the processing of the video file s and the production of the actuator vectors could be conducted independently from the production of speech segment. However if the speech and the facial expression are recorded separately a user has to align the recorded speech and the facial expression first on the interpretation device before the interpretation device could begin its processing. Alignment could be achieved in various ways. For example a user could chop off some recorded speech or video or assign a start time of the speech relative to the start of facial expressions or vice versa .

The processing of the recorded speech by the interpretation device could be as simple as converting it into a format suitable for playing by the vocalization device . If the speech segment has to be a sequence of time stamped synthesis commands the interpretation device would derive the sequence of time stamped synthesis commands for the vocalization device from the recorded speech. Again if the speech and the facial expression are recorded simultaneously the processing of the recorded speech and the production of the speech segment could be conducted independently from the production of actuator vector sequence. If the speech and the facial expression are recorded separately they have to be appropriately aligned.

Optionally the interpretation device could further contain an editing application program for a user to simulate the reproduction of the speech segment and the synchronized actuator vectors on a 3D model using the same technique as described in Pighin et al. or similar approaches but applying it in reverse i.e. using the actuator vectors to regenerate the 3D model s face mesh points instead of the other way around . Then during the simulation a user could choose to delete extraneous vectors as the facial expression remains unchanged or insert additional vectors by interpolation as the facial expression undergoes abrupt change or modify existing vectors. The reason for this editing is that as the interpretation program does not have any knowledge regarding how the facial expression varies along with time and the images retrieved from the video file s are treated equally the interpretation program usually generates the vectors at fixed time intervals as shown in . As illustrated the time stamps t t t . . . t of vector sequence V V V . . . V have fixed intervals. This inevitably leaves out some important details or keeps too much unnecessary information and a user is therefore required to step in to make up the inefficiency of the interpretation program. After the editing a vector sequence similar to that shown in would be obtained with more vectors in a shorter interval t tand less vectors in a longer interval t t.

The resulting speech segment and the appropriate time stamped vector sequence are then delivered to the computing engine of the robotic head . Depending on uploading or communication mechanism provided by the interpretation device and the robotic head this can be achieved via a removable media or via a direct link such as a local area network or via any other appropriate means.

Based on the foregoing description a similar process for obtaining and performing the speech segment and the vector sequence is shown in . As shown the process starts with the step in which a person s performance such as telling a joke making a speech singing a song or reporting an event is recorded. As noted earlier the video and speech portion of the recording could be stored separately or together in a single file or a single audio video clip. Please note that the speech could be recorded directly in a form suitable for playing on a robotic head or it requires further processing. Then in step the recorded speech and the images of the recorded video are optionally aligned and processed according to an algorithm such as the one used by Pighin et al. and based on the knowledge of the robotic head s vocalization device and expressive motor device to obtain a speech segment for playing on the robotic head and a series of time stamps control vectors for driving the face actuators of the robotic head at appropriate times. An optional step allows a user to view and simulate the effect of the speech segment and the vector sequence delivered simultaneously and during this process the user is allow to delete modify and insert vectors to fine tune their performance. Finally in the step the resulting speech segment and vector sequence are delivered to the robotic head over an appropriate mechanism and performed by the robotic head accordingly.

Various variations to the foregoing process could be implemented. For example as mentioned earlier the robotic system of the present invention could have no recording device and obtain the recorded speech and facial expressions from somewhere else. Therefore the step would become to obtain the recorded speech and facial expressions. Also some details are omitted in the foregoing process for simplicity. For example two types of speech segment could be produced by the step one is the recorded voice in an appropriate analog or digital format and the other one is a sequence of time stamped synthesis commands.

Although the present invention has been described with reference to the preferred embodiments it will be understood that the invention is not limited to the details described thereof. Various substitutions and modifications have been suggested in the foregoing description and others will occur to those of ordinary skill in the art. Therefore all such substitutions and modifications are intended to be embraced within the scope of the invention as defined in the appended claims.

