---

title: Memory management and recovery for datacenters
abstract: A system including a plurality of servers, a client, and a metadata server is described herein. The servers each store tracts of data, a plurality of the tracts comprising a byte sequence and being distributed among the plurality of servers. To locate the tracts, the metadata server generates a table that is used by the client to identify servers associated with the tracts, enabling the client to provide requests to the servers. The metadata server also enables recovery in the event of a server failure. Further, the servers construct tables of tract identifiers and locations to use in responding to the client requests.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08181061&OS=08181061&RS=08181061
owner: Microsoft Corporation
number: 08181061
owner_city: Redmond
owner_country: US
publication_date: 20100419
---
Large scale network based services often require large scale data storage. For example Internet email services store large quantities of user inboxes each user inbox itself including a sizable quantity of data. This large scale data storage is often implemented in datacenters comprised of storage and computation devices. The storage devices are typically arranged in a cluster and include redundant copies. This redundancy is often achieved through use of a redundant array of inexpensive disks RAID configuration and helps minimize the risk of data loss. The computation devices are likewise typically arranged in a cluster.

Both sets of clusters often suffer a number of bandwidth bottlenecks that reduce datacenter efficiency. For instance a number of storage devices or computation devices can be linked to a single network switch. Network switches are traditionally arranged in a hierarchy with so called core switches at the top fed by top of rack switches which are in turn attached to individual computation devices. The Top of rack switches are typically provisioned with far more bandwidth to the devices below them in the hierarchy than to the core switches above them. This causes congestion and inefficient datacenter performance. The same is true within a storage device or computation device a storage device is provisioned with disks having a collective bandwidth that is greater than a collective network interface component bandwidth. Likewise computations devices are provisioned with an input output bus having a bandwidth that is greater than the collective network interface bandwidth.

To increase efficiency many datacenter applications are implemented according to the Map Reduce model. In the Map Reduce model computation and storage devices are integrated such that the program read and writing data is located on the same device as the data storage. The Map Reduce model introduces new problems for programmers and operators constraining how data is placed stored and moved to achieve adequate efficiency over the bandwidth congested components. Often this may require fragmenting a program into a series of smaller routines to run on separate systems.

In addition to bottlenecks caused by network bandwidth datacenters also experience delays when retrieving large files from storage devices. Because each file is usually stored contiguously the entire file is retrieved from a single storage device. Thus the full bandwidth of the single storage device is consumed in transmitting the file while other storage devices sit idle.

Also datacenter efficiency is often affected by failures of storage devices. While the data on a failed storage device is usually backed up on another device as mentioned above it often takes a significant amount of time for the device storing the backed up data to make an additional copy on an additional device. And in making the copy the datacenter is limited to the bandwidths of the device making the copy and the device receiving the copy. The bandwidths of other devices of the datacenter are not used.

Additionally to efficiently restore a failed storage device the storage device and its replica utilize a table identifying files stored on the storage device and their locations. Failure to utilize such a table requires that an entire storage device be scanned to identify files and their locations. Use of tables also introduces inefficiencies however. Since the table is often stored at a different location on the storage device than the location being written to or read from the component performing the reading writing and table updating must move across the storage device. Such movements across the storage device are often relatively slow.

Systems described herein include a plurality of servers a client and a metadata server. The servers each store tracts of data a plurality of the tracts comprising a byte sequence and being distributed among the plurality of servers. The client provides requests associated with tracts to the servers and identifies the servers using a locator table. The locator table includes multiple entries each pairing a representation of one or more tract identifiers with identifiers of servers. Also the client determines whether a byte sequence associated with a write request is opened in an append mode or a random write mode and performs the write request accordingly. The metadata server generates the locator table and provides it to the client. The metadata server also enables recovery in the event of a server failure by instructing servers storing tracts that are also stored on the failed server to provide those tracts to additional servers. Further the servers construct tables of tract identifiers and tract locations by scanning server memory for the tract identifiers and noting the locations where tracts associated with the identifiers are stored. In some implementations rather than scanning an entire server memory a server scans only a part of the memory marked out of date in the table updates the entry marked out of date based on the scan and marks an entry associated with another part of memory out of date to enable writes to that other part of memory without having to update the table.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

Described herein are servers clients and metadata servers as well as systems including combinations of multiple servers at least one client and at least one metadata server. Such systems are used in datacenters for data storage and input output operations. For example a system described herein could be a datacenter for a word processing service. Each document of the word processing service corresponds to a byte sequence. Byte sequences whether corresponding to documents or some other sort of data are comprised of tracts of data. The tracts each have a predetermined same size such as one megabyte and represent the smallest unit of data that can be read from or written to a storage unit that maximizes performance. For example on a mechanical device such as a disk the tract size would be large enough to avoid giving up performance due to the lost opportunity of reading more data for free after a seek or rotational delay. As a second example on a medium such as flash the tract size would be calculated based on the chip bandwidth and characteristics of the flash storage medium. To make full use of the bandwidth of storage nodes implementing the servers the tracts of a byte sequence are distributed across the servers thus enabling the client to read from and write to multiple servers simultaneously when reading from or writing to a byte sequence.

The tracts are distributed among the servers by a locator table. The locator table is generated by the metadata server and provided to the client and the servers. The table indexes the servers storing the tracts by associating the servers with representations such as bit patterns. For example the representations could each have a bit length of three allowing for eight possible representations. Each of these eight representations is associated with one or more servers. Further each representation could correspond to a prefix included in a translation to a fixed length of a tract identifier. Each prefix matches one of the eight representations and the tract associated with that prefix is assigned to the servers associated with the matching representation. These translations are calculated by the client using a client library. In one implementation the translations are hashes of tract identifiers calculated using a hash algorithm. An effect of the translation process is that two adjacent tracts in a byte sequence which have similar tract identifiers will have dissimilar translations. For example each tract identifier may share the same first three bits such as 001 but their translations may have different first bits such as 110 and 010. Because of the translation process and the manner of assignment in the locator table two adjacent tracts of a byte sequence are assigned to different servers resulting in a distribution of tracts among the servers such as a uniform distribution. When the client reads from or writes to the byte sequence the client now does so at the combined bandwidth of the multiple servers storing the multiple tracts of the byte sequence.

In addition to enabling distribution of the tracts among the servers the locator table also enables recovery in the event of a server failure. When a server fails the metadata server is notified of the failure and identifies the representations associated with the failing server. The metadata server also identifies other servers associated with those representations and instructs one of the other servers for each representation to write the tracts associated with that representation to an additional server. The effect of these instructions is to create a replica of each tract stored on the failed server through use of multiple other servers writing to multiple additional servers at the combined bandwidth of those other servers.

The client library also serves further purposes. For example the client library enables the client to determine if a byte sequence is opened in an append mode or in a random write mode. If opened in an append mode the client library requests allocation of memory for a next available tract of data for a byte sequence and writes to that tract. For example if tracts one through three have been written to the client library would request allocation of tract four. Once allocated to the requesting client other clients cannot write to tract four. If the byte sequence has instead been opened in a random write mode the client simply attempts to write to the next available tract and relies on a locking mechanism associated with the byte sequence to ensure data integrity.

The servers comprising the datacenter are also configured to operate more efficiently. Each server stores tracts contiguously in its storage and reserves the last bits of the tract to store the tract identifier. Because tracts have the same length as one another and tract identifiers have the same length as one another the memory can be more efficiently scanned based on these lengths to generate a table of tracts stored on the server that includes tract identifiers and associated tract locations.

To further improve efficiency the tables stored at the servers are constructed incrementally. When a write request is first received the server marks entries in a table as out of date. These entries correspond to a part of the storage of the server that is large enough to store multiple tracts. The server then writes tracts to that part of the storage until it is full. Once full the server updates the entries marked out of date with the tract identifiers and tract locations of the tracts written to the part of the storage. The server then marks additional entries of another part of the storage as out of date and proceeds as before. If the server fails during this process it only needs to scan the part of storage corresponding to the entries marked out of date in recovery to arrive at an up to date table. By updating the table after multiple write operations rather than after each write the server reduces the number of times that a storage unit component e.g. a head of a disk storage unit travels across the storage unit. Such movements across a storage unit are often relatively slow and can be a major factor in inefficient operation of a storage unit.

In some implementations the servers are implemented on storage nodes belonging to storage clusters of the datacenter and the clients are implemented on computation nodes belonging to computation clusters of the datacenter. Within each node the bandwidth of the node s network interface components and the bandwidth of the node s other components are proportioned to one another to avoid bottlenecks associated with the bandwidth of the network interface components.

The following paragraphs further describe the servers client and metadata server and make reference to figures illustrating the servers client and metadata server a number of their aspects and their operations.

As is also shown in the metadata server includes a distribution module for generating the locator table and a recovery module for managing recovery in the event that one of the servers fails.

The client includes a file system interface enabling a user or application such as application to interact with a client library . The client library enables the client to formulate and transmit read and write requests to the servers and the receive responses in return. As is also shown the client includes a locator table to enable the client library to identify the servers to transmit the requests to. The client and servers receive the locator table from the metadata server which generates and also stores a copy of the locator table . The metadata server also receives notifications of failure of a server in some embodiments triggering the recovery module to perform recovery operations.

In various embodiments each of the servers the client and the metadata server is implemented in a separate computing device. The computing device may be any sort of computing device such as a personal computer PC a laptop computer a workstation a server system a mainframe or any other computing device. In one embodiment one of the servers client and metadata server is a virtual machine located on a computing device with other systems. In some embodiments rather than implementing each server client and metadata server on a separate computing device two or more of the server client and metadata server are implemented on a shared computing device as separate virtual machines or otherwise. For example a server and metadata server could be implemented on a single computing device. Also multiple ones of the servers may be implemented on a single computing device with one server for each storage unit memory of the computing device. Thus if a single computing device includes both storage unit memory and storage unit memory that computing device would implement both server and server . Example computing devices implementing the servers client and metadata server are illustrated in and are described in greater detail below with reference to those figures.

In some embodiments the computing devices implementing the servers client and metadata server are connected by one or more switches not shown . These switches can also comprise one or more networks such as wide area networks WANs local area networks LANs or personal area networks PANs . The network may also be a private network such as a network specific to a datacenter. In such an implementation the switches comprise or are connected to routers and or devices acting as bridges between data networks. Communications between the computing devices through the switches and routers may utilize any sort of communication protocol known in the art for sending and receiving messages such as the Transmission Control Protocol Internet Protocol TCP IP and or the Uniform Datagram Protocol UDP .

In various embodiments as mentioned above each server may include a locator table received from the metadata server . The locator table includes entries for representations associated with tract identifiers and servers associated with each representation. For example each row in the locator table may be one entry including a representation and the servers associated with that representation. The locator table could then include a column for representations and a column for servers . Each representation may be a bit pattern of a determined bit length such as a length of three . The number of representations and thus the number of entries is a function of that bit length with one representation for each possible bit pattern. Thus if the bit length of each representation is three there will be eight representations and eight entries in the locator table . In one embodiment the locator table includes additional entries that are specific to tracts where the representations are the full translations of the tract identifiers. Such additional entries may be included for frequently accessed tracts. In other embodiments rather than storing the entire locator table each server stores only the representations that it has been associated with by the locator table . For instance if server has been associated with the representations 000 011 and 110 server would store only those representations rather than the entire locator table . The servers obtain their associated representations by querying the metadata server for those representations. In some embodiments the locator table or representations are stored in the storage unit memories of the servers or in other memories such as caches or random access memories RAM of the computing devices implementing the servers . Further details regarding the locator table are included in the following description and an example locator table is illustrated in and is described with reference to that figure.

In addition to the locator table or representations each server includes a storage unit memory . The storage unit memory could be any sort of storage component such as a disk drive a permanent storage drive random access memory an electrically erasable programmable read only memory a Flash Memory a miniature hard drive a memory card a compact disc CD a digital versatile disk DVD an optical storage drive a magnetic cassette a magnetic tape or a magnetic disk storage. Each storage unit memory includes a memory table storing identifiers of the tracts stored in the storage unit memory and locations where those tracts are stored. Such a memory table is illustrated in and is described in greater detail with reference to that figure.

In various embodiments each storage unit memory stores tracts of data from multiple byte sequences. For example storage unit memory stores the first tract from each of three byte sequences including sequence tract sequence tract and sequence tract . Storage unit memory stores the second tract from the three sequences including sequence tract sequence tract and sequence tract . Storage unit memory stores the third tract from the three sequences including sequence tract sequence tract and sequence tract . Each tract of the tracts as well as any other tracts stored by the storage unit memories has the same length such as a length of one megabyte sixty four kilobytes or some length in between. The storage unit memory may store tracts contiguously one after another with the last bytes of each tract being reserved for the tract identifier. In some embodiments the tract identifier is a combination of a byte sequence identifier and a tract sequence number indicating the place of the tract within the sequence. When the sequence number is combined with the tract size a byte offset can be calculated for the tract. For example the byte sequence identifier may be a globally unique identifier GUID and may have a one hundred twenty eight bit length. The tract sequence number may be represented by sixty four bits creating a one hundred ninety two bit length tract identifier. In other embodiments rather than storing the tract identifier the storage unit memory stores a hash of the tract identifier or some other translation to a fixed length. Such a hash translation may have a short length than the tract identifier such as a length of one hundred sixty bits. The storage unit memory is illustrated in and is described in greater detail with reference to that figure.

In addition to the components shown in each server may include logic enabling the server to read from and write to its storage unit memory to construct and update its memory table to acquire the locator table or the representations with which it is associated in the locator table to provide the locator table to client and to determine if failure is imminent and to notify the metadata server of the imminent failure. The logic may acquire the locator table as part of a locator table invalidation process. Such logic may include processes threads or routines and may be stored in the storage unit memory or in additional memory such as cache memory or RAM of the computing device implementing the server . Operations performed by such logic are described in greater detail below.

As illustrated in and mentioned above the metadata server includes a distribution module . The distribution module generates and updates the locator table . In some embodiments generating the locator table includes selecting a bit length of the representation. The distribution module performs this selecting based at least in part on the number of servers . The distribution module further determines the of the locator table based at least in part of the number of servers . For example if there are eight servers the distribution module could select a bit length of three. After selecting the length the distribution module generates a locator table with an entry for each possible representation associating multiple servers with each possible representation to ensure redundancy. In some embodiments the metadata server ascertains or is programmed with the knowledge that one of the tracts stored by one of the servers is frequently accessed. The metadata server adds an entry to the locator table for that tract including as the representation the full translation of the tract s identifier. In one embodiment the locator table accommodates the inclusion of new tracts without requiring a refresh or update of the locator table . Because the locator table includes all possible representations there is no need to update the locator table in response to the creation of a new tract as that new tract will correspond to one of those possible representations.

The distribution module then updates the locator table upon detecting the failure of a server the addition of a new server or to re balance the load between the servers . The generated or updated locator table is then provided to the client and in some embodiments to the servers . The locator table can be transmitted in any format recognizable by the client and servers . In other embodiments the distribution module processes requests for representations associated with a given server and provides those associated representations to that server . In various embodiments the distribution module includes processes threads or routines. The operations of the distribution module are illustrated in and are described further below in reference to that figure.

In various embodiments the metadata server also includes a recovery module to manage recovery in response to a failure of a server . The recovery module processes notifications received from a server that has failed or is about to fail. In response to processing a notification or detecting a failure in some other manner the recovery module determines the representations associated with the failing server by examining the locator table . The recovery module then also determines other servers associated with those representations and instructs at least one of the other servers per representation to write tracts associated with those representations to additional servers . The additional servers may be unutilized servers or servers currently associated with different representations. In various embodiments the recovery module includes processes threads or routines. The operations of the recovery module are illustrated in and are described further below in reference to that figure.

As illustrated in the metadata server stores the locator table after generating or updating the locator table . By storing the locator table the metadata server is enabled to provide the locator table to requesting clients and to use the locator table in recovery operations. The metadata server may also provide the locator table to servers as part of a locator table invalidation process to enable more efficient distribution of an updated locator table .

Turning now to the client the client is shown as including a file system interface . The file system interface is a process thread or routine capable of representing a byte sequence as a file to be requested by a user or an application such as application . In some embodiments the file system interface is a part of an operating system OS of the computing device implementing the client . The file system interface may further present a file system structure such as a hierarchy of folders storing files. The user or application interacts with the byte sequences as it would interact with files or other known data structures. The file system interface maps or translates a requested file to a byte sequence. To do so the file system interface may make use of a table or other structure storing a mapping of a file name to a byte sequence identifier such as the byte sequence GUID described above. In other embodiments the file system interface requests a byte sequence identifier from a central store the central store located on the computing device implementing the client or on some other computing device.

The application is any sort of application. For example the application may be a word processing service or a process thread or routine of such a service. The application interacts with the other components of the client as it would interact with an OS and components of any computing device. As mentioned above the application makes a request related to a file such as a read or write request. The request is received by the file system interface and mapped or translated into a request for a byte sequence. In other embodiments the client does not include a file system interface and the application is configured to request a byte sequence rather than a file.

In some embodiments the client library is a component configured to be utilized by other processes threads or routines such as a dynamic link library. The client library may be utilized by the file system interface by the application or by the OS of the computing device implementing the client . The client library provides interfaces to the file system interface the application or the OS such as application programming interfaces APIs and performs actions based on requests received at those interfaces. When the client library receives a request for a byte sequence it identifies servers associated with tracts comprising that byte sequence by referencing the locator table . In some embodiments the client library also determines whether the requested byte sequence has been opened in an append mode or a random write mode. If the byte sequence has been opened in an append mode the client library send a request for allocation of a tract. The client library then provides a read or write request to the identified server and receives the response . In some embodiments the client library sends multiple requests to multiple servers associated with the multiple tracts comprising a requested byte sequence. In return the client library receives multiple responses simultaneously or substantially simultaneously. These and other operations of the client library are illustrated in and are described further below in reference to those figures.

As illustrated in the computing device implementing the client stores the locator table . The client receives the locator table from the metadata server when the metadata server generates the locator table . In some embodiments the client library is configured to request the locator table from the metadata server from a server or from another client in response to an event such as a failure or reboot of the client .

In various embodiments the locator table also includes a locator table version number such as a sixty four bit number. Each time a new locator table is generated or updated the metadata server increments the version number and stores the incremented version number in the new locator table . The version number enables clients and servers receiving a new locator table to determine whether they have the most recent locator table . The version number also enables the metadata server to determine whether a requesting client or server has the most recent locator table as such requests include the version number of the locator table stored on the client or server . For example if a client requests a locator table from a server or other client it is possible that the locator table the client receives will not be as up to date as a locator table that the client already has. The client can ascertain which locator table is most up to date simply by comparing version numbers .

In various embodiments the memory table is stored in a reserved part of the storage unit memory such as the start or the end of the storage unit memory the start and end corresponding to the lowest or highest byte locations within the storage unit memory . The memory table has a known size such as ten megabytes thus enabling the server to scan the tract identifiers in the storage unit memory without scanning the memory table .

As mentioned above when the memory table is initially generated one or more entries for tracts are marked out of date. Once the tracts corresponding to those one or more entries are written to their identifiers and locations are stored in the entries and an additional one or more entries are marked out of date. Other available entries in the memory table are set to null. In one embodiment the number of entries available in the memory table is proportioned to the number of tracts the storage unit memory is capable of storing. In some embodiments the memory table is constructed by scanning the tract identifiers stored in the storage unit memory . Because the tracts each have a same size e.g. one megabyte and the tract identifiers each have a same size e.g. one hundred ninety two bits the server can scan only the tract identifiers thus more efficiently scanning the storage unit memory . The constructing and updating of the memory table are illustrated in and are described further herein in reference to those figures.

As is further shown in the storage unit memory also stores tracts and and tract identifiers and . Tract identifier identifies tract tract identifier identifies tract and tract identifier identifies tract . Each tract identifier may be stored within the memory allocated for its corresponding tract as shown or may be stored contiguously with a tract following it or preceding it. If stored within the memory allocated for its tract the tract identifier may be stored at the beginning or end of the allocated memory. In addition to tracts and identifiers the storage unit memory also includes unallocated memory .

In each entry corresponds to two tracts. The first entry stores identifiers XYZ corresponding to identifiers and for the first two tracts tracts and and locations ABC for the first two tracts. The second entry is marked out of date because the second entry is to store the third and fourth tracts but only the third tract tract has been written. The third entry is set to null because the second entry has not been updated with identifiers and locations.

At block the client library is invoked to receive the request for the byte sequence from the file system interface . The request includes the byte sequence identifier a designation as to whether the request is to be a read or write request and if a write request the data to be written. Also the request may include information about the byte sequence such as a size of the byte sequence. In one embodiment the client library receives the request via an interface of the client library that was invoked by the file system interface .

At block the client library calculates translations of tract identifiers for one or more of the tracts comprising the byte sequence to fixed lengths. If the request received via the file system interface is a read request the client library calculates translations of tract identifiers for each tract comprising a byte sequence. If the request received via the file system interface is a write request however then the client library identifies a tract or tracts to write to and calculates translations of tract identifiers for those identified tracts. The identifying of tracts to write to is described further below in reference to .

In some embodiments the calculating makes use of a hash algorithm such as a SHA 1 hash algorithm in translating the tract identifiers. As mentioned above each tract identifier includes the byte sequence identifier and a tract sequence number. In one embodiment the tract identifiers are generated by the client library based on the specified tract size and the size of the byte sequence. For example if each tract is one megabyte and the byte sequence is forty megabytes the client library generates forty tract identifiers for the forty tracts that make up the byte sequence. As also mentioned above the byte sequence identifier portion of each tract identifier is one hundred twenty eight bits and the tract sequence number is sixty four bits combining to form a one hundred ninety two bit tract identifier. Each hash or translation of a tract identifier may be a shorter length such as one hundred sixty bits. One result of the calculation is that the translations for two very similar tract identifiers e.g. the identifiers for the first and second tracts in a byte sequence are very different from one another. While the first one hundred twenty eight bits of two sequential tract identifiers are the same the first two three or four bits of the translations of those tract identifiers are likely to be different from one another.

At block the client library then looks up prefixes included in the translations in the locator table . As described above the locator table distributes the tracts among the plurality of servers . Because the locator table associates servers based on the first bits of a translation of a tract identifier and because any two sequential tracts have different first bits of in their translated tract identifiers those two sequential tracts are associated with different servers .

In various embodiments the client library compares each translation to representations stored in the locator table such as the representations in column of . Because the representations may have a smaller bit length than the translations the client library may determine the length of the representations and determine the first N bits of each translation N being the determined length of the representations. These first N bits are referred to herein as a prefix that is included in a translation. The client library looks up these prefixes in the locator table and notes the servers associated with each prefix.

At block the client library identifies a server . Among the noted servers for each prefix the client library identifies a server to which the client library will transmit a request for a corresponding tract. For example if servers A B and C are associated with a prefix the client library may select A as the identified server since the server A is listed first. In some embodiments the client library looks up and identifies a server for each tract of the requested byte sequence. Also in some embodiments after identifying a server the client library uses the server identifier included in the locator table to retrieve additional information about the server that may have been provided with the locator table such as a port that the server is listening on for requests .

At block the client library next transmits the requests associated with the tracts to the identified servers . The client library may transmit the requests one after another as each server is identified or may wait until all servers have been identified and then transmit all of the requests simultaneously. Each request identifies the server that it is being sent to e.g. via an IP address of the server whether the request is a read request or a write request the tract identifier associated with the request the version number of the locator table and if the request is a write request the data to be written to the tract. In some embodiments the transmission may include encapsulating the request in a packet such as a TCP IP packet or a UDP packet and transmitting the request across a network.

At block the client library receives responses to the requests in parallel. Because the client library may transmit multiple requests to multiple servers responses to those requests may be received at substantially the same time. The responses may provide a requested tract provide an indication of whether a write operation was successful provide notification that a requested tract is not stored by the server or include an updated locator table having a more recent version number than the version number that was included in the request . If the response indicates that the requested tract is not stored by the server the client library identifies another server to which the client library will transmit the request . For example the client library may have looked up servers A B and C for the tract identified server A and transmitted the request to server A. If the response indicated that server A did not have the tract then the client library may identify server B and transmit the request to server B. If none of the servers have the tract then the client library requests an updated locator table from the metadata server . Upon receiving the updated locator table the client library repeats the operations illustrated at blocks for the unlocated tract.

At block the client library may receive a tract of an empty length in response to a request for an unwritten to tract. If a server determines that it is associated with a tract but the tract has not been written to the server returns a tract of an empty length in response to a read request.

Upon receiving a response the client library may in turn respond to the file system interface . If the request received from the file system interface was a read request the client library may respond with all the tracts for a byte sequence. If the request received from the file system interface was a write request the client library may respond with an indication of whether the write operation was successful.

At block the client library next determines whether the byte sequence has been opened in an append mode or a random write mode. If the byte sequence is new the client library may simply select the mode. Otherwise the client library determines the mode in which the byte sequence has been opened by querying a server or computing device. This may be the same server or computing device as the one performing the coordinating or may be a different server or computing device. The client library then receives a response to the query indicating whether the byte sequence is opened in an append mode or a random write mode.

At block in response to determining that the byte sequence has been opened in append mode the client library requests allocation of the next available tract in the byte sequence which may or may not be the last tract in the byte sequence. In some embodiments the same server or computing device that coordinates how a byte sequence is opened allocates tracts for the byte sequence. Such a server or computing device may store a mode for each byte sequence and a count of the tracts already allocated for that byte sequence. For example if a client library requests a tract and six tracts have been allocated the server or computing device may allocate the seventh tract in response to the request. The server or computing device does not check whether the client library transmits the write request for the allocated tract however which may result in a number of the allocated tracts being empty. Once the client library has been allocated a tract the client library performs the calculating looking up and identifying described above. The client library then transmits the data to be written to the tract to the identified server and at block the identified server writes the data to the allocated tract.

In response to determining that the byte sequence has been opened in random write mode the client library first queries the same server or computing device that coordinates how a byte sequence is opened to determine which tract is the next available tract. The server of computing device may keep a count of the tracts that have been written to. At block the client library then attempts to write data to the tract. This may involve performing the calculating looking up identifying and transmitting described above. In some embodiments the identified server upon receiving a write request for a tract checks the memory table for the tract. If no tract identifier for the tract is in the memory table the identified server applies a locking mechanism to enable the write to proceed. If the memory table does include the tract identifier or if the tract is subject to a locking mechanism the server responds to the client library informing the client library that the tract cannot be written to. The client library then proceeds to query the server or computing device for the next tract and repeats the operations associated with the writing.

At block the metadata server determines the representations to include in the locator table . As mentioned above the metadata server includes each possible combination of bits of a given length the length set by the metadata server in the locator table . To calculate the number of representations the metadata server utilizes the function 2 where N is the length set by the metadata server . For example if the length is three the metadata server will determine that eight representations should be included in the locator table and that those representations will be 000 001 010 011 100 101 110 and 111. In one embodiment the size of the locator table is based at least in part on the number of available servers.

In addition to those representations the metadata server determines if any frequently access tracts should be associated with their own sets of servers . If so the metadata server retrieves the tract identifiers for those tracts and calculates the translations of the tract identifiers. The metadata server then includes the full translations as representations to be associated with servers .

At block the metadata server effects a distribution of tracts among servers by associating the servers with the representations. In associating servers with representations the metadata server determines a number of servers to associate with each representation and apportions the servers in some manner to the representations. For example if the metadata server determines that three servers should be associated with each representation and servers A Z are available for assignment the metadata server may associate servers A B and C with a first representation servers D E and F with a second representation and so on. Once each server has been associated with a representation the metadata server may cycle through the servers again associating each server to a second representation. In some embodiments the associating effects a distribution of the tracts among the servers by associating different representations with different servers . As mentioned above each representation corresponds to a prefix included in a translation of a tract identifier. The translating is performed in such a manner that sequential tracts of a byte sequence have different prefixes. Thus by associating the different prefixes of tracts of a byte sequence to different servers the metadata server ensures that the tracts of a given byte sequence are distributed among a plurality of servers .

At block in performing the associating the metadata server associates multiple servers with each representation to ensure redundancy. The number of servers associated for redundancy may be a function of the number of servers available and the information security desired. The number of servers may also be a function of a target recovery duration i.e. the more servers assigned to a representation the faster the recovery in the event of server failure . In the example above three servers are associated ensuring two copies of tracts associated with a representation are replicated in the event that one server fails.

At block the metadata server generates the locator table based on the representations and servers . The locator table may include columns for the representations and server respectively as shown in . The locator table may then further include a row for each representation and its associated servers . In other embodiments the locator table includes a row for representations a row for servers and a column for each representation and its associated servers . In addition to the representations and servers the locator table includes a version number such as locator version number . In generating the locator table the metadata server increments the version number and includes the incremented version number in the locator table . The resulting locator table may be represented in any format such as a text file an extensible markup language XML file or a database file such as a Microsoft Access database file. In some embodiments upon generating the locator table the metadata server stores the locator table in memory of the computing device implementing the metadata server .

At block the metadata server then notifies servers of representations that the servers are associated with. The metadata server may perform the notifying by transmitting the locator table to the servers or by looking up for each server representations associated with that server in the locator table and transmitting those representations to the server .

At block the metadata server provides the locator table and metadata describing the servers to the clients . Such metadata can include ports listened on by the servers . The clients may each be registered with the metadata server thereby informing the metadata server of their identities. In other embodiments the metadata server retrieves a list of clients from another server or computing device. In yet other embodiments the metadata server only provides the locator table to clients in response to requests from the clients for the locator table . In the requests the clients may indicate the version numbers of locator tables stored on those clients . If the current version number shows that the locator table has since been updated the metadata server provides the locator table in response to the requests. If the clients already have the current locator table then the metadata server simply indicates to the clients that their locator tables are current.

At block the metadata server updates the locator table in response to the failure or addition of a server or to re balance the load among the servers . The metadata server is made aware of the failure or addition by the failed added server . The failed added server transmits a notification that the server is about to fail or a join message indicating that the server has been added as an available server . In one embodiment metadata server or another server infers the failure of the failed server because the failed server has failed to follow a protocol. In response to receiving such a notification or join message the metadata server updates the locator table by repeating some or all of operations shown at blocks .

At block the metadata server determines that a server failure has occurred noting the server that the notification was received from. In other embodiments the metadata server determines failure in some other manner. For instance a server may fail before having an opportunity to transmit a failure notification . The metadata server may nonetheless determine the occurrence of the failure in other ways. In one embodiment each server periodically sends a heartbeat message indicating that the server is operational. If the metadata server does not receive such a message from a server the metadata server may determines that the server has failed. In other embodiments the metadata server receives a message from a client or a different server indicating that a server is unreachable. In response to such a message the metadata server determines that the server has failed.

At block the metadata server determines representations in the locator table that are associated with the failed server . To determine the representations the metadata server utilizes a name or identifier of the failed server that is used in the locator table to look up entries in the locator table that include that name or identifier. Each of those entries includes a representation that the failed server is associated with.

At block the metadata server determines other servers that are associated with the determined representations. This determining includes selecting one other server for each representation. For example if A is the failed server and is associated with representations 000 011 and 110 the metadata server may select one other server for each of representations 000 011 and 110. If 000 is associated with A B and C the metadata server may select server B as the other server for 000. In one embodiment the metadata server caches a list of the selected other servers . When presented with a choice of other servers to select the metadata server references the cached list and selects as the other server a server not appearing in the list or appearing fewer times. The metadata server thereby selects as many different other servers as possible thus increasing the collective bandwidth made available for recovery.

At block the metadata server then selects a number of additional servers to store tracts associated with the determined representations. In selecting the additional servers the metadata server maximizes the number of additional servers that are to receive tracts. In one embodiment the metadata server maximizes the number of additional servers by associating each possible server to a determined representation before associating the same server twice. Also the metadata server makes sure that the additional server selected for a determined representation is not already one of the servers associated with that representation. The result of the determining and the selecting is a maximized number of other servers writing tracts and additional servers receiving tracts thereby sending and receiving data at the collective bandwidths of the other servers and additional servers .

At block the metadata server then instructs the other servers to write tracts associated with the determined representations to the additional servers at least two of the other servers performing the writing in parallel. The instructions provided to one of the other servers include the representation associated with tracts stored by that other server as well as an identifier or name of the additional server such as an IP address of that additional server . Upon receiving the instructions the other servers each determine which tracts stored by the other servers are associated with the representations. The other servers may determine the tracts by examining the memory table for tract identifiers that are stored as translations and comparing the representations to the prefixes in the translations. In other embodiments the other servers calculate translations for each tract identifier stored in the memory table in the same manner as described above with regard to the client and compare the calculated translations to the representations. After determining the tracts that are subject to the instructions the other servers write the tracts to the additional servers .

At block the metadata server generates an updated locator table removing the failed server and including the additional servers in its place. Once the updated locator table is generated the metadata server provides the updated locator table to the clients and servers .

At block because the memory table storing locations and identifiers of tracts stored in the storage unit memory of the server may now be out of date the server next scans the storage unit memory for tract identifiers noting the tract identifiers discovered and their locations within the storage unit memory . Also since the tracts each have the same size and the tract identifiers each have the same size the server can scan only the tract identifiers stored in the storage unit memory skipping the tracts stored between the tract identifiers.

At block while scanning the tract identifiers the server caches the tract identifiers and the locations where they are stored. The identifiers and locations are cached in other memory of the server such as cache memory or RAM of the server .

At block the server then constructs the memory table based on the cached identifiers and locations adding an entry to the memory table for each identifier and location pair. Such entries may either be rows or columns of the memory table as illustrated in and described above. In some embodiments rather than storing locations of the tract identifiers the servers utilizes the locations of the tract identifiers as well as the sizes of tract identifiers and tracts to calculate locations of the tracts such as the starting or ending locations of the tracts and stores those calculated locations with the tract identifiers.

In some embodiments the memory table constructed by the server also includes the representations associated with that server by the locator table . At block these representations are received or retrieved by the server from the metadata server . Such receiving or retrieving may occur in connection with the construction of the memory table . At block upon receiving the representations the server stores the representations either in memory table or in another location in the storage unit memory .

At block the server marks entries in the memory table for the first part of the memory as out of date. The server may perform the marking in response to receiving a first write request . If the memory table includes an entry for each tract then the server determines the number of tracts capable of being stored in the first part of the memory and marks that many entries an entry for each tract to be stored in the first part of the memory as out of date. In other embodiments the memory table may be constructed with one entry per part of memory that entry capable of storing one out of date marker or multiple identifiers and locations for multiple tracts. In such other embodiments the server would only mark the one entry out of date. 

In some embodiments before during or after receiving the write request and marking the entries the server receives read requests . At block upon receiving the read requests the server queues the read requests. At block the server performs the read requests when a writing component is within a predetermined distance of a requested tract. For example if writing component is writing to a first memory location and the read request is for an adjacent memory location the server may then perform the queued read request . By queuing read requests and performing the read requests when the writing component is adjacent to or within a predetermined distance of the requested tract the server minimizes the number of trips the writing component makes across the storage unit memory .

Further once entries for the first part of the memory have been marked the server may write tracts received in write requests to the first part of the storage unit memory . At block the server first determines which location in the first part of the storage unit memory to write the tract to based on read requests . For example if the server has queued a read request for one memory location and an adjacent memory location belongs to the first part of the storage unit memory and has not been written to the server selects that adjacent memory location to write the tract to. At block the server then writes the tract to the first part of the storage unit memory . Unless the first part of the storage unit memory is full however the writing component stays in the vicinity of the first part of the storage unit memory after writing the tract and does not move across the storage unit memory to update the memory table .

At block the server caches the identifier of the written tract as well as the location in the storage unit memory that the tract was written to. The cached identifier and location are stored by the server in cache memory or RAM of the server until the writing component moves across the storage unit memory to update the memory table .

At block while the server is idle e.g. not receiving requests the server may update the memory table with the cached identifiers and locations. After updating the memory table the writing component returns to the vicinity of the first part of the storage unit memory to write tracts received in further write requests.

In various embodiments the server then receives additional write requests . At block upon receiving an additional write request the server determines whether the first part of the storage unit memory has been filled. The server may perform the determining by scanning the first part of the storage unit memory for available memory locations or by examining the cached identifiers and locations to count the number of tracts that have been written. If the server determines that the first part of the storage unit memory is not full then the server repeats the operations shown at blocks for the received write request .

At block if the first part of the storage unit memory is full then the server updates the entries in the memory table marked out of date with the cached identifiers and locations writing the cached identifiers and locations to the memory table in order of their memory locations i.e. in order of the byte location within the storage unit memory that they are associated with . The updating may include writing an identifier and location to each one of multiple entries of the memory table or writing all of the identifiers and locations to a single group entry. Upon completion of the updating none of the entries in the memory table is marked out of date. 

At block the server marks another one or more entries corresponding to a second part of the storage unit memory as out of date After marking the other one or more entries out of date and receiving another write request the server repeats the operations shown at blocks .

At block the server detects that a failure has occurred. Such a failure may be a reboot due to a power outage or some other cause. While the server is shown in as detecting the failure after performing the marking the server may detect the failure before during or after any of the operations shown at blocks .

At block upon detecting a failure the server scans any part of the storage unit memory marked out of date in the memory table . Because this limits scanning to only a part of the storage unit memory the time to recovery from the failure is reduced. The results of the scan identifiers and locations are then used to update the entries marked out of date. 

Computer system may also include additional data storage devices removable and or non removable such as for example magnetic disks optical disks or tape. Such additional storage is illustrated in by removable storage and non removable storage . Removable storage and non removable storage may represent the storage unit memory if the computer system implements a server . Computer readable storage media may include volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. System memory removable storage and non removable storage are all examples of computer readable storage media. Computer readable storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can be accessed by computer system . Any such computer readable storage media may be part of the computer system .

In various embodiment any or all of system memory removable storage and non removable storage may store programming instructions which when executed implement some or all of the above described operations of the servers client or metadata server . In some embodiments the programming instructions include instructions implementing one or more of the distribution module the recovery module the file system interface the application or the client library .

Computer system may also have input device s such as a keyboard a mouse a touch sensitive display voice input device etc. Output device s such as a display speakers a printer etc. may also be included. These devices are well known in the art and need not be discussed at length here.

Computer system may also contain communication connections that allow the device to communicate with other computing devices . The communication connections are implemented at least partially by network interface components.

In various embodiments the storage node computation node and node are connected by one or more switches . In three switches connect the nodes one switch being directly connected to each of the storage node computation node and node . Any number of switches however may connect the nodes to one another. The switches may be any sort of switches. The switches also each include network interface components such as incoming and outgoing network interface components each network interface component having a bandwidth. For example a switch may have a number of incoming Ethernet ports and an incoming wireless port as well as outgoing Ethernet and wireless ports. In some embodiments the incoming bandwidth of network interface s of a switch that serve devices below the switch in the network hierarchy is proportioned to the outgoing bandwidth of that switch up to core switches. For instance the collective incoming bandwidth of the incoming network interface components of the switch may be ten gigabytes per second and the collective outgoing bandwidth of the outgoing network interface components may also be ten gigabytes per second. By proportioning the incoming and outgoing bandwidths of a switch the datacenter avoids introduction of bottlenecks associated with the switches . Such switches with proportioned bandwidths are described in further detail in MONSOON APPLICATION .

In some embodiments as described above the switches may comprise one or more networks such as WANs LANs or PANs. In such embodiments the switches comprise or are connected to routers and or devices acting as bridges between data networks.

In various embodiments the storage node includes a storage unit such as the storage device described above and a network interface component . Each server comprises one storage unit . Each storage node however may have multiple server and storage unit combinations. Also while only one network interface component is shown each storage node may have multiple network interface components . The storage unit may be the same storage device as the storage unit memory shown in and described in greater detail above. The network interface component may be any sort of network interface component such as a network interface card a modem an optical interface or a wireless interface.

As shown in the storage unit and network interface component have proportioned bandwidths and respectively. The proportioned bandwidths and match or are within a predefined tolerance of one another. For example the proportioned bandwidth of the storage unit could be nine tenths of a gigabyte per second and the proportioned bandwidth of the network interface component could be one gigabyte per second. In proportioning the bandwidths and to one another the storage node can be provisioned with a network interface component of a given bandwidth based on the bandwidth of the storage unit or can be provisioned with a storage unit of a given bandwidth based on the bandwidth of the network interface component . If the storage node includes multiple storage units or network interface components the collective bandwidth of the storage units or network interface components is proportioned to the bandwidth of the other. If the storage node includes both multiple storage units and multiple network interface components the collective bandwidths of both multiple sets are proportioned to one another.

In various embodiments the computation node includes an input output I O bus and a network interface component . The client is shown as comprising both the I O bus and the network interface component as a single client is associated with each computation node . The I O bus is any sort of I O bus connecting components of the computation node such as the network interface component and any sort of processor memory or storage not shown of the computation node . The network interface component may be any sort of network interface component such as a network interface card a modem an optical interface or a wireless interface. While only one network interface component is shown as being included in the computation node the computation node may have multiple network interface components .

As is further shown in the I O bus and network interface component have proportioned bandwidths and respectively. The proportioned bandwidths and match or are within a predefined tolerance of one another. For example the proportioned bandwidth of the I O bus could be four gigabytes per second and the proportioned bandwidth of the network interface component could also be four gigabytes per second. In proportioning the bandwidths and to one another the computation node can be provisioned with a network interface component of a given bandwidth based on the bandwidth of the I O bus . If the computation node includes multiple network interface components the collective bandwidth of the network interface components is proportioned to the bandwidth of the I O bus .

By implementing the servers and client in nodes and with proportioned bandwidths the datacenter avoids bottlenecks associated with network bandwidth in performing the read and write operations of the client and servers . Data is written to and read from the storage unit of a server at the full bandwidth of the storage unit and requests transmitted and responses received by the client are processed at the full bandwidth of the I O bus .

The implementation shown in and described herein is shown and described in further detail in Ser. No. 12 766 726.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described. Rather the specific features and acts are disclosed as exemplary forms of implementing the claims.

