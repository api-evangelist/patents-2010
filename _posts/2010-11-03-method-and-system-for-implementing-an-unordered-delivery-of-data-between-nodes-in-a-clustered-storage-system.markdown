---

title: Method and system for implementing an unordered delivery of data between nodes in a clustered storage system
abstract: Described herein is a novel technique for implementing an unordered delivery (UOD) of write logs between nodes in a cluster to optimize processing resources during log mirroring operations. A mirroring entry may be generated for each write log in a local log cache constituting the write log and an order indicator for the write log. The order indicator may be, for instance, a storage location of the write log in the local log cache. The mirroring entry may then be forwarded across a network from the local node to the remote node, where the mirroring entry may be stored at a next available location of an interim cache at the remote node independent of the write log storage location in the local log cache.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08751598&OS=08751598&RS=08751598
owner: NetApp, Inc.
number: 08751598
owner_city: Sunnyvale
owner_country: US
publication_date: 20101103
---
The present invention relates to clustered storage systems and particularly to a technique for optimizing processing resources during log cache mirroring operations by implementing an unordered delivery of data between nodes in the clustered storage system.

A storage system typically comprises one or more storage devices into which information may be entered and from which information may be obtained. The storage system includes a storage operating system that functionally organizes the system by invoking storage operations in support of a storage service implemented by the system. The storage system may be implemented in accordance with a variety of storage architectures including but not limited to a network attached storage NAS environment a storage area network SAN and a storage device assembly directly attached to a client or host computer. The storage devices are typically disk drives organized as a storage array although other solid state devices including flash memory may also constitute the storage array.

The storage operating system may implement a high level module or layer of abstraction e.g. file system to logically organize the information as a hierarchical structure of data containers such as volumes directories and files. For example each on disk file may be implemented as set of data structures e.g. disk blocks configured to store information such as the actual data for the file. The disk blocks may be organized within a volume block number vbn space that is maintained by the file system which may also assign each disk block in a file a corresponding file offset or file block number fbn . Sequences of fbns are typically assigned by the file system on a per file basis whereas vbns are assigned over a larger volume address space. In addition the file system may organize the disk blocks within the vbn space as a volume such that each volume may be although is not necessarily associated with its own file system.

The storage system may be configured to operate according to a client server model of information delivery to allow many clients to access data containers stored on the system. In this model the client may comprise an application such as a database application executing on a computer that connects to the storage system over a computer network such as a point to point link shared local area network LAN wide area network WAN or virtual private network VPN implemented over a public network such as the Internet. Each client may request the services of the storage system by issuing access requests a read or write request as file based and block based protocol messages to the system over the network.

Multiple storage systems may be interconnected to provide a clustered storage system cluster configured to service many clients where each storage system may be a node of the cluster. Advantages of a clustered architecture include the nodes being configured to communicate with one another to act collectively to increase performance of or to offset any single node failure within the cluster. For instance one node of the cluster local node may have a predetermined failover partner node remote node that may take over or resume storage services of the local node upon failure at the local node. When a failure occurs access requests intended for the local node may be re directed to the remote node for servicing. For ease of explanation components residing on the local node may be referred to as a local component whereas components residing on the remote node may be referred to as a remote component.

Preferably data access services may be provided by the cluster using shared storage of the cluster constituting a set of storage devices commonly accessible by the nodes. Clients may then connect to a node of the cluster to submit read or write requests that are received and serviced by the node accessing the shared storage. With write requests the node may further implement a write log in local non volatile storage local log cache for aggregating write requests and writing data to shared storage at a later point in time. In one instance deferring write operations may be desirable to optimize system resources during peak access request periods by clients.

When configured to protect against a node failure the remote node of the cluster may implement a remote log cache which mirrors the local log cache. The remote log cache may then be accessed by the remote node to carry out any remaining write requests on the shared storage. A consistent view between log caches is thus desirable to ensure that write operations not executed by the local node are executed by the remote node in a failover. Similarly if write logs in the local log cache become corrupt or lost the remote node may access the remote log cache to carry out any remaining write operations on behalf of the local node.

Known techniques for mirroring log caches between nodes include an out of order delivery OOD of data across multiple connection paths between the nodes to enforce a particular ordering of write logs at the remote node. In contrast with in order delivery IOD where data is transmitted to the remote node across a single connection in the same order as it is received at a local node data may now arrive at the remote node in any order due to the multiple connection paths between nodes. The local node must thus manage the timing of data transmission to enforce the same ordering constraints at the remote node. As an example data may be transmitted to the remote node in the form of data sets where multiple data sets e.g. W X and Y may be grouped together and associated with a metadata set Z . Metadata set Z may describe each of the related data sets and for instance indicate the number of data sets in the group i.e. 3 and the order of transmission of the data sets i.e. W X Y and then Z to enable a consistent view between log caches.

In certain instances however corruption and inconsistency may occur if metadata set Z is received by the remote node and written to storage before data sets W X and Y. To avoid this problem the local node may implement strict data set ordering constraints by maintaining a request queue for managing the data sets already sent to the remote node. Once confirmation of a successful mirroring operation is returned by the remote node the local node may remove the completed request from the queue and then transmit the related metadata set. Avoidance of data corruption and inconsistency may thus be achieved since data sets are stored at the remote node prior to transmission of its related metadata set thereby corresponding to the transmission order by the local node.

One known disadvantage with the conventional techniques involves the additional processing required at the local node in enforcing ordering constraints on the data and metadata sets. For OOD as well as IOD the remote node may further be heavily involved in processing incoming write requests from clients. To avoid remote processing overhead specialized data placement protocols such as remote direct memory access RDMA may be implemented to facilitate a consistent view of log caches between nodes. With RDMA log writes may be transmitted from the local log cache to the remote log cache and stored directly to a specified location in the remote log cache without consuming processing overhead at the remote node.

An RDMA implementation involves however highly specialized interconnects and data placement semantics which may be complex and costly to implement. As storage demands grow specialized components and semantics are an impractical approach to a flexible and scalable cluster since such configurations are typically vendor specific and require a tradeoff between high data availability e.g. protection against node failures and cost. As such there is a need for an improved method for mirroring log cache data while optimizing performance and scalability of the cluster.

Described herein is a novel technique for implementing an unordered delivery UOD of write logs between nodes in a cluster to optimize processing resources during log mirroring operations. A mirroring entry may be generated for each write log in a local log cache constituting the write log and an order indicator for the write log. The order indicator may be for instance a storage location of the write log in the local log cache. The mirroring entry may then be forwarded across a network from the local node to the remote node where the mirroring entry may be stored at a next available location of an interim cache at the remote node independent of the write log storage location in the local log cache.

Advantageously mirroring entries may be stored to a next available location in the remote interim cache to optimize processing overhead of the remote node during log mirroring operations. A network component at the remote node may store incoming mirroring entries to the next available location in the interim cache for promptly storing mirroring entries upon receipt. At a later predetermined time write logs from entries in the remote interim cache may be organized and stored to the remote log cache at a location indicated by the order indicator to achieve a consistent view between log caches.

Performance of the cluster may in this way be optimized since write logs may be mirrored to the remote node without requiring the local node to maintain a particular transmission order of write logs. Since the local node need not monitor completion of a log mirroring operation prior to sending a next write log processing and network resources of the cluster may be optimized during such mirroring operations. Deferral of processing by the remote node to organize the write logs from the mirroring entries further allows system resources to be optimized during peak request periods. By mirroring write logs across the network the need for specialized RDMA components and semantics may be avoided to thereby provide a cluster with increased scalability and flexibility.

In the following description numerous details are set forth for purpose of explanation. However one of ordinary skill in the art will appreciate that embodiments described herein may be practiced without the use of these specific details. In other instances well known structures and devices are shown in block diagram form in order not to obscure the description with unnecessary detail.

It should be noted that although disks are used in some embodiments described below any other type of storage device may be used in accordance with the teachings of the present invention. For example a solid state storage device may be used instead including flash memory non volatile storage device NVRAM Magnetic Random Access Memory MRAM Phase Change RAM PRAM etc. In other embodiments storage devices other than those mentioned here may also be used.

It should also be noted that while there is shown an equal number of N and D blades in cluster there may be differing numbers of N and or D blades and or different types of functional components implementing node in accordance with various embodiments. For example there may be multiple N blades and or D blades interconnected in cluster that does not reflect a one to one correspondence between the N and D blades of nodes . As such the description of node comprising only one N blade and one D blade should be taken as illustrative only. For instance node A may also have one N blade and a plurality of D blades a plurality of N blades and one D blade or a plurality of N blades and a plurality of D blades. In addition the functional components of the N and D blades may be implemented in other components of node so the novel techniques are not limited to the illustrative embodiment discussed herein.

Clients may be general purpose computers configured to communicate with nodes in accordance with a client server model of information delivery. That is each client may request the services of a node by e.g. submitting a read or write request and the node may return the results of the services requested by the client by exchanging information packets over network . Client may submit access requests by issuing packets using file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of files and directories. Alternatively client may submit access requests by issuing packets using block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of blocks.

In some embodiments the totality of storage space provided by disks constitute shared storage of cluster which may be commonly accessed by nodes . Shared storage may be accessible by each D blade of node for instance to provide a high availability of service to clients by protecting against a failure of one of nodes or increasing performance of cluster . As an example a write request from the client may be received by any of nodes and carried out on storage array by virtue of each D blade having access to shared storage .

To optimize system resources during periods where a high volume of access requests are serviced by node each node may implement a log cache A B constituting one or more write logs. A write log may for instance include information provided by client in a write request such as a volume identifier and the actual data to be written to disk of shared storage . Write logs may then be grouped in log caches for a predetermined period of time and then stored to disk . For example the predetermined time may be during an off peak request period for cluster .

Preferably one of the nodes local node A may be configured with one or more predetermined failover partner nodes e.g. node B to provide a high availability of service to clients . When node A fails node B may automatically resume e.g. take over the storage services provided by node A. To that end client requests sent to node A may automatically be re directed to node B for servicing to thereby ensure a high availability of service to clients despite a node failure. Note that a node failure may occur unintentionally or intentionally e.g. where a node is taken offline for servicing in certain instances.

When configured to protect against a failure node A may further mirror the write logs in local log cache A to remote log cache B for guaranteeing completion of write requests on disks . When node A fails node B may then access remote log cache B to execute any remaining write operations uncompleted by node A. Consistency between log caches is thus desirable to enable completion of write requests on disks despite a node failure.

Illustratively novel logging engines A B may be operative at nodes to optimize resources of cluster while mirroring log writes across multiple connection paths between the nodes. Advantageously logging engine may implement an unordered delivery UOD technique across cluster switching fabric to produce a consistent view of log caches . To that end a data structure such as a mirroring entry may be implemented by logging engines to transfer write logs from node A to node B. The mirroring entry may include the write log as well as an order indicator constituting e.g. a storage address of the write log in local log cache A. The order indicator may thus indicate an organization of the write log with respect to other write logs in local log cache A. In this way the write log may be transmitted between nodes using any connection path via cluster switching fabric while avoiding the need for node A to enforce a strict arrival and storage order of write logs at node B.

When received node B may store the mirroring entry to a next available storage location in a remote interim cache e.g. cache of . Node B may further defer processing of the mirroring entries to a later predetermined time such as during an off peak request period or upon a failover event. In one embodiment the processing of mirroring entries may involve extracting the write logs from the mirroring entries and organizing the extracted write logs according to storage locations indicated by the order indicator. The resultant set of ordered write logs may then be stored to log cache B to result in a consistent view with log cache A.

Network adapter comprises a plurality of ports adapted to couple node to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. Network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Illustratively computer network may be embodied as an Ethernet network or a Fibre Channel FC network. Each client may communicate with node over network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

Cluster access adapter may comprises a plurality of ports adapted to couple node to other nodes of the cluster through cluster switching fabric . In the illustrative embodiment Ethernet is used as the clustering protocol and interconnect media although it will be apparent to those skilled in the art that other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternate embodiments where the N and D blades are implemented on separate storage systems or computers cluster access adapter may be utilized by the N D blade for communicating with other N D blades in the cluster . In yet other embodiments more than one processor may implement node such that one processor executes the functions of N blade while another processor executes the functions of D blade .

Node is illustratively embodied as a storage system executing a storage operating system that preferably implements a high level layer of abstraction e.g. file system to logically organize the information as a hierarchical structure of data containers such as volumes directories and files on the disks. Each on disk file for instance may be implemented as a set of data structures e.g. disk blocks configured to store information such as the actual data for the file. Disk blocks may further be organized as a volume where each volume may be although is not necessarily associated with its own file system.

Storage adapter cooperates with storage operating system executing on node to access information requested by clients . The information may be stored on any type of writable storage media such as disk drives magnetic tape flash memory electronic random access memory or any other media adapted to store information. However as illustratively described information is preferably stored on disks configured as storage array . Storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to disks over an I O interconnect arrangement such as a conventional high performance FC link topology. Storage array may further constitute shared storage of cluster which may be accessed by multiple nodes for increasing cluster performance and protecting against a failure of any single node.

Information on storage array is preferably organized as one or more volumes that comprise a collection of e.g. disks cooperating to define an overall logical arrangement of volume block number vbn space on the volume s . The disks within a volume file system are typically organized as one or more groups wherein each group may be operated as a Redundant Array of Independent or Inexpensive Disks RAID . Most RAID implementations enhance the reliability integrity of data storage through the redundant writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of parity information with respect to the striped data.

Memory illustratively comprises storage locations that are addressable by the processor and adapters for storing program instructions and data in some embodiments. The processors and adapters may in turn comprise processing elements and or logic circuitry configured to execute the instructions and manipulate the data stored in memory . In some embodiments memory may comprise a form of random access memory RAM comprising volatile memory that is generally cleared by a power cycle or other reboot operation.

Storage operating system portions of which are typically resident in memory and executed by the processing elements functionally organizes node by invoking operations in support of the storage services implemented by the node. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the invention described herein. In some embodiments the storage operating system comprises a plurality of software layers including logging engine that are executed by processor .

A user console may be implemented by node to allow a user e.g. a storage administrator to interface with node to supply inputs to or receive outputs from node . Preferably user console may include a keyboard for receiving e.g. command line interface CLI inputs from the storage administrator and a monitor for displaying outputs generated by node . As an example the storage administrator may interface with user console when configuring node to communicate with a failover partner for mirroring write logs between the nodes. It will be appreciated that other devices and or components may implement user console such as a computer remotely located and networked to node not shown .

One or more non volatile storage devices may implement a request cache for locally storing information relating to access requests from clients . Preferably request cache may be implemented to store write logs corresponding to write requests from clients in the event of a system shutdown or other unforeseen problem. To that end write logs may be generated by storage operating system upon receipt of a write request from client for carrying out the requested write operation at a later time. All or a predetermined portion of request cache may thus be implemented as a log cache for storing write logs for deferred write requests. Note that request cache may include disks flash memory non volatile random access memory NVRAM Magnetic Random Access Memory MRAM Phase Change RAM PRAM or any other type of media or device suitable for storing instructions and or data thereon in.

Illustratively novel logging engine may be implemented as program instructions stored in memory and executed by processor for optimizing resources of node during a log cache mirroring operation. To protect against a failure at node log cache may be mirrored by logging engine to a remote node e.g. node B of via cluster access adapter without node managing the transmission order of the write logs. When implemented at node constituting a local node logging engine may generate mirroring entries including write logs and order indicators for the write logs. In one embodiment the order indicators may constitute storage address of the write logs in log cache . The mirroring entries may then be supplied by logging engine across cluster access adapter to node .

A remote logging engine may be implemented at node to receive the mirroring entries and store the entries to a next available storage location in a remote interim cache e.g. cache of . In one example the interim cache may be accessed by a remote cluster access adapter of node for storing entries received from node based on an arrival order at node . Storage locations in the interim cache may be accessed by adapter sequentially to automatically store mirroring entries to the next available storage location of the interim cache. Illustratively sequential storage locations addressed by adapter may be updated periodically by the remote storage operating system of node to address new sequential storage locations in the interim cache as currently addressed locations are occupied with mirroring entries.

Advantageously logging engine may be operative to process the mirroring entries in the interim cache at a later predetermined time to produce a consistent view between log caches at the nodes while conserving processing resources during peak request periods. For instance a later predetermined time may involve a predetermined off peak request period such as during evening or weekend hours. In other instances the later time may involve a predetermined event such as a failover from node to node . To that end logging engine may organize the write logs in the mirroring entries of the interim cache based on the order indicators and store the ordered write logs in a remote log cache for access by node during a failover.

To facilitate access to the shared storage storage operating system may implement a file system that cooperates with one or more virtualization modules to virtualize the storage space provided by the shared storage. The file system logically organizes the information as a hierarchical structure of named directories and files on the storage devices e.g. disks constituting the shared storage. Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the file system to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers luns .

In the illustrative embodiment storage operating system is preferably the Data ONTAP software operating system available from NetApp Inc. Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system may be implemented for use in accordance with the inventive principles described herein. As such where the term WAFL is employed it should be taken broadly to refer to any storage operating system that may be otherwise adaptable to the teachings of this invention.

A file system protocol layer provides multi protocol file access and to that end may include support for the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the Hypertext Transfer Protocol HTTP protocol . A VI layer may implement the VI architecture to provide direct access transport DAT capabilities such as RDMA as required by DAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the node. The FC and iSCSI drivers provide FC specific and iSCSI specific access control to the blocks and thus manage exports of luns to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing the blocks on node .

In addition storage operating system includes a series of software layers organized to form a storage server D blade that provides data paths for accessing information stored on disks . To that end storage server includes a file system module a storage RAID system layer and a storage device e.g. disk driver system module . RAID system layer manages the storage and retrieval of information to and from e.g. volumes on disk in accordance with I O operations while disk driver system implements a storage device access protocol such as e.g. the SCSI protocol.

File system implements a virtualization system of storage operating system through the interaction with one or more virtualization modules embodied as e.g. a virtual disk vdisk module not shown and a SCSI target module . SCSI target module is generally disposed between the FC and iSCSI drivers and file system to provide a translation layer of the virtualization system between the block lun space and the file system space where luns are represented as blocks.

File system is illustratively a message based system that allocates storage space for itself in a storage device array e.g. array and controls the layout of information on array . File system further provides volume management capabilities for accessing information stored on disk . These volume management capabilities may include aggregation of disk space aggregation of storage bandwidth of the disks and reliability guarantees such as mirroring and or parity RAID . Preferably file system implements the WAFL file system having an on disk format representation that is block based using e.g. 4 kilobyte kB blocks and using index nodes Modes to identify files and file attributes such as creation time access permissions size and block location . Illustratively a data container such as an Mode file may be used to describe the layout of file system and may be retrieved from disk using a file handle i.e. an identifier that includes an Mode number.

All inodes of the write anywhere file system may be organized into the Mode file. A file system fs info block specifies the layout of information in the file system and includes an Mode of a data container e.g. file that includes all other Modes of the file system. Each volume has an fsinfo block that may be stored at a fixed or variable location within e.g. a RAID group. The Mode of the Mode file may directly reference point to data blocks of the Mode file or may reference indirect blocks of the Mode file that in turn reference data blocks of the Mode file. Within each data block of the mode file are embedded Modes each of which may reference indirect blocks that in turn reference data blocks of a file.

Operationally an access request read write request from client is forwarded as a packet over network and to node where it is received at network adapter . A network driver of layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to file system . Here file system produces operations to load retrieve the requested data from disk if it is not resident in core i.e. in memory . If the information is not in memory file system indexes the requested data into the Mode file using the Mode number to access an appropriate entry and retrieve a logical vbn. File system then passes a message structure including the logical vbn to RAID system whereby the logical vbn is mapped to a disk identifier and disk block number disk dbn and sent to an appropriate driver e.g. SCSI of disk driver system . Disk driver system then accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the access request node and storage operating system returns a reply to client over network .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the node may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by node in response to a request issued by client . Moreover in another alternate embodiment of the invention the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the node. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of node implement data access semantics of a general purpose operating system. Storage operating system may also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows NT or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present invention may be utilized with any suitable file system including a write in place file system.

In some embodiments storage server is embodied as D blade of storage operating system to service one or more volumes of array . In addition multi protocol engine is embodied as N blade to perform protocol termination with respect to client issuing incoming data access request packets over network as well as redirect data access requests to another storage server of cluster . Moreover N blade and D blade cooperate to provide a highly scalable distributed storage system architecture of cluster . To that end each blade includes a cluster fabric CF interface module adapted to implement intra cluster communication among the blades e.g. communication between blades of the same node or communication between blades of different nodes using CF protocol messages.

For example the protocol layers e.g. the NFS CIFS layers and the iSCSI FC layers of N blade may function as protocol servers that translate file based and block based access requests from clients into CF protocol messages used for communication with D blade . In some embodiments N blade converts the incoming client access requests into file system primitive operations commands that are embedded within CF protocol messages by the CF interface module for transmission to D blade of cluster . Notably CF interface modules cooperate to provide a single file system image across all D blades in cluster . Thus any network port of an N blade that receives a client request can access any data container within the common file system image located on any D blade of cluster .

In some embodiments N blade and D blade are implemented as separately scheduled processes of storage operating system . In other embodiments N blade and D blade may be implemented as separate software components code within a single operating system process. Communication between N blade and D blade in the same node is thus illustratively effected through the use of CF messages passing between the blades. In the case of remote communication between an N blade and D blade of different nodes such CF message passing occurs over cluster switching fabric .

A known message passing mechanism provided by the storage operating system to transfer information between blades processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF application programming interface API . Examples of such an agnostic protocol are the SpinFS and SpinNP protocols available from NetApp Inc.

The CF interface module implements the CF protocol for communicating file system commands messages among the blades of cluster . Communication is illustratively effected by the D blade exposing the CF API to which an N blade or another D blade issues calls. To that end the CF interface module is organized as a CF encoder and CF decoder. The CF encoder of e.g. CF interface on N blade encapsulates a CF message as a local procedure call LPC when communicating a file system command to a D blade residing on the same node or a remote procedure call RPC when communicating the command to a D blade residing on a remote node e.g. node B of cluster . In either case the CF decoder of CF interface on D blade de encapsulates the CF message and processes the file system command. As used herein the term CF message may be used generally to refer to LPC and RPC communication between blades of the cluster.

Illustratively novel logging engine may be implemented as layer of storage operating system to implement an unordered delivery of write logs when performing log cache mirroring operations. Logging engine may generate data structures at a local node e.g. node A such as mirroring entries to store the write logs from a local log cache e.g. log cache A and order indicators e.g. storage addresses of the write logs in log cache A which indicate an organization order of write logs in log cache . In one embodiment a new mirroring entry may be generated each time storage operation system generates a write log in response to a write request from client .

Multi protocol engine may forward and receive mirroring entries between the nodes e.g. via cluster switching fabric and further store received mirroring entries to a next available location in an interim cache e.g. cache of of a remote node e.g. node B . At a predetermined time or upon a predetermined event e.g. failover logging engine may retrieve the mirroring entries from the interim cache and organize the write logs stored therein using the order indicator. The ordered set of write logs may then be stored to a remote log cache e.g. log cache B thereby producing a consistent view between log caches A B.

Upon a failover event in which node B may take over the services of node A node B may access log cache B to determine the remaining write requests to be executed on disk . The storage operating system operative a node B may thus carry out write operations associated with the write logs in log cache B. In particular layers of storage operating system such as a remote RAID system and remote disk driver system may access log cache B to carry out write operations deferred as write logs in log cache A.

Preferably log cache A may be partitioned into portions such that layers of the storage operating system may access dedicated portions of log cache A for storage and retrieval of write logs generated by such respective layers. In one embodiment the partitioning may be implemented by storage operating system during initialization of log cache A by configuring each layer to address log cache A at an offset value corresponding to a storage location in log cache A. In the exemplary embodiment a base layer may be configured to manage write requests from client and store corresponding write logs at a portion starting at offset W. In contrast RAID system may access the portion starting at offset X. Other layers of the storage operating system e.g. Misc WAFL may access portions starting at other offset values e.g. offset Y and Z for grouping write logs corresponding to write operations requested by such other layers.

As an example a first write log write log 0 generated by the base layer may be stored to a first available storage location starting at offset W. A second write log write log 1 generated by RAID system may be stored to a first available storage location starting at offset X. A third write log write log 2 generated by yet another layer of the storage operating system may be stored to a first available storage location at offset Y. Additional write logs may similarly be stored to portions of log cache A associated with specific layers of the storage operating system. Write operations may then be executed by each layer accessing log cache A at the preconfigured offset and requesting write operations for write logs stored therein.

It should be appreciated that log cache A may constitute different or additional partitions than those discussed herein. In one embodiment a portion of log cache A may serve as a backup for log cache B such that write logs may be mirrored from log cache B to log cache A to protect against a failure at node B. Accordingly write operations grouped in log cache A may be carried out on disk by one layer of the storage operating system at a point in time independent from write operations executed by another layer by virtue of the partitioning of log cache A.

To optimize the mirroring of write logs between log caches write logs stored in log cache A may be mirrored to node B using an unordered delivery of write logs. Advantageously an unordered delivery of write logs may involve logging engine sending the write log in a data structure such as a mirroring entry to node B as each write log is generated and stored to log cache A. The mirroring entry may also include an order indicator e.g. storage address of the log write in log cache A to enforce an ordering constraint at node B while avoiding the need to manage the transmission order of write logs at node A.

In the present example mirroring entries generated at node A and forwarded to node B may include a first mirroring entry mirroring entry 0 for write log 0 constituting a tuple such as . Subsequent mirroring entries may similarly constitute tuples including the write log and the order indicator of the write log such as and . Each mirroring entry may then be forwarded by node A to node B across any of the multiple connection paths between the nodes e.g. via cluster switching fabric .

Notably a first mirroring entry mirroring entry 0 sent before a second mirroring entry mirroring entry 1 may arrive at node B after the arrival of mirroring entry 1 as a result of the entries being transmitted across different connection paths. As such an arrival order of mirroring entries at node B may differ from a transmission order of such entries by node A. With conventional techniques node A must manage the transmission and arrival order to enforce a consistent view between log caches . Using the novel techniques however node A need not enforce a particular order of arrival at node B since an organization of write logs may be generated by node B based on the order indicators provided in the mirroring entries.

To optimize storage operations at node B incoming mirroring entries may be stored to a next available storage location in an interim log cache of request cache B. In one embodiment interim cache may be implemented as a logical portion of request cache B. In other embodiments interim cache may be implemented as physically separate memory from request cache B and configured by the storage operating system during initialization of request cache B.

Illustratively a mirroring entry received from node A may be stored to a next available location in interim cache based on its arrival at remote node B. To that end mirroring entry 2 constituting may arrive at node B after mirroring entry 3 constituting despite being transmitted at a point in time prior to the transmission of mirroring entry 3. Upon arrival mirroring entry 3 may be stored to interim cache prior to mirroring entry 2 based on an arrival order at node B. As such write logs in interim cache may be stored in a different order than the order of the mirrored write log in log cache A. To achieve a consistent view between log caches interim cache may be processed by node B at a later predetermined time to organize e.g. sort the write logs stored therein. The resulting organized set of write logs may then be stored to log cache B which constitutes a mirror of log cache A.

A resultant set of ordered write logs may then be stored to log cache B to mirror the order of write logs at node A. As an example processing of mirroring entry 0 constituting may result in write log 0 being stored to location offset W in log cache B. Since write logs stored in log cache B are not accessed by node B until a failover a consistent view of log caches need not exist until an actual failover from node A to node B. In certain instances however earlier processing of mirroring entries in interim cache may be desired to avoid competing resource needs during a failover. To that end a storage administrator interfacing at node B may input a predetermined time for processing interim cache such as during a predefined off peak request period e.g. night time weekends . In other instances node B may organize write logs from the mirroring entries in interim cache upon taking over services of node A. Layers of the operating system at node B configured to access predefined storage locations in log cache B may then retrieve the write logs in log cache B at predefined offset values and execute write operations on disks .

As depicted in logging engine may include components such as an entry generator a mirroring module and a deferred processing engine . Entry generator may be operative at a local node e.g. node A to generate data structures such as a mirroring entry . Mirroring entry may include fields such as a write log and an order indicator . Preferably order indicator may involve a storage address e.g. offset value of write log in a local log cache e.g. log cache A . In one example write log may be generated by the storage operating system when a write request is received by node A from a client.

In further detail when write log is generated and stored to log cache A entry generator may generate mirroring entry to include write log . Write log may include data received from the client targeted for a volume on disk e.g. disk or generated by a layer of the storage operating system intended for disk in certain embodiments. In addition entry generator may request the address of write log in log cache A from e.g. the storage operating system and store the address to order indicator . Order indicator may thus include an offset value corresponding to a location in log cache A which may be configured into partitions by the storage operating system when initializing functional layers of the storage operating system e.g. RAID system .

Once generated mirroring entry may be forwarded by mirroring module across a cluster switching fabric e.g. cluster switching fabric to a remote node e.g. node B in the cluster. Advantageously mirroring entry may be forwarded via a cluster access adapter e.g. adapter across a network connection to avoid the need for specialized RDMA components. In addition mirroring entry may be transmitted to node B via any of the multiple communication paths between the nodes e.g. cluster switching fabric to increase bandwidth of the cluster during a log mirroring operation.

Mirroring module may be operative at node B to receive mirroring entry via a remote cluster access adapter and store mirroring entry to a next available location in an interim cache e.g. interim cache . In one embodiment the remote cluster access adapter may implement e.g. a circular buffer for addressing sequential locations in interim cache . In one embodiment when initializing the remote cluster access adapter mirroring module may map locations of the circular buffer e.g. X X to a set of sequential locations in the interim cache Y Y by requesting such locations from the storage operating system. The buffer mapping may be stored as a data structure in memory e.g. memory and accessible by mirroring module to address and store mirroring entry to sequential storage locations in interim cache .

Mirroring entries received at the remote cluster access adapter may automatically target the circular buffer which results in storing a mirroring entry to a corresponding location in interim cache based on the buffer mapping. Preferably the adapter increments through storage addressed of the buffer which implements a circular scheme. Incrementing from the last storage location in the buffer e.g. X thus causes the adapter to target the initial buffer storage address e.g. X . To avoid overwriting mirroring entries stored in interim cache when incrementing through the circular scheme of the buffer mirroring module may update the buffer mapping by requesting a new set of storage locations of interim cache e.g. Y Y to correspond to locations X Xprior to incrementing to the initial location again in one embodiment.

To achieve a consistent view between log caches in the nodes deferred processing engine may be operative at node B to write log in mirroring entry . In one embodiment the processing involves a sorting operation on write log using order indicator . As an example deferred processing engine may retrieve write log from mirroring entry and store write log at a location in log cache B indicated by order indicator . Advantageously deferred processing engine may organize mirroring entries at a later predetermined time such as during an off peak request period or following a failover event. In one instance an administrator of the cluster may input the predetermined time at which deferred processing engine may carry out the processing of mirroring entry . In another instance deferred processing engine may be configured to process entries stored upon receiving a notification e.g. from the storage operating system of node B that a failover has occurred.

Returning to the example in processing of mirroring entry 0 may be carried out by extracting write log 0 from mirroring entry 0 and storing write log 0 to the location of offset W in log cache B. Write log 3 may be extracted from the next mirroring entry mirroring entry 3 and stored to the location of offset Z 1. Notably write log 3 may be sent by node A in a particular time order i.e. after write log 1 but may be received by node B in a different time order i.e. before write log 1. Deferred processing engine may therefore enforce an order on the write logs at node B by processing mirroring entry in interim cache to produce an order of write logs in log cache B constituting a consistent view with log cache A.

When carrying out write operations on disks during a failover layers of the storage operating system at node B may access the preconfigured partitions in log cache B to execute remaining write operations. For example modules of node B such as a remote RAID system and a remote WAFL layer may be configured to access predetermined partitions of log cache B. Notably layers of the storage operating system operative at respective nodes may implement the same addressing scheme on respective log caches to ensure the completion of write operations on disk despite a failover.

Preferably as write logs are carried out by node A mirroring module may also forward such write log updates from log cache A to log cache B. Updates may be carried out by for instance entry generator generating an update constituting mirroring entry having an empty or null value as write log and order indicator constituting the storage address of the executed write log. When the update is received by node B mirroring module may store the update to interim cache at a next available storage location. When processed at a later predetermined time deferred processing engine may delete or overwrite the previously stored write log in the storage location indicated by order indicator . It will be appreciated that a value or indicator other than an empty or null value may be implemented as the update so the invention is not so limited to the illustrative embodiment.

Using the novel techniques performance of the cluster may be optimize since the local node need not manage the time order transmission of write logs to achieve a consistent view with the remote log cache. Instead write logs may be stored at the remote log cache based on an arrival order without the local node continually querying the remote node for a completed mirroring operation on the remote log cache. Network bandwidth may in this way be optimized for handling client requests rather than carrying out log mirroring operations. Processing resources at the remote node may similarly be optimized for servicing client requests since operations executed to achieve a consistent view of log caches may be deferred until a later time.

At step a local node e.g. node A may receive a write request from a client which may include data to be stored to a volume on disk e.g. disk of shared storage e.g. shared storage . Alternatively the operating system of node A or a layer thereof may generate a request to write data to shared storage. For example a RAID system e.g. RAID system may generate data to be written to disk e.g. disks when a parity value is calculated on a stripe of data.

Upon receipt or generation of data intended for disk a write log associated with the requested write operation may be generated by the storage operating system of node A and stored to a log cache e.g. log cache A of node A step . Preferably the storage operating system may store the write log in a partition of log cache A based on the source of the request. In the illustrative embodiment of if the write request is received from a client a base layer of the storage operating system may store the write log to the portion located at offset W in log cache A. If the write request is generated by the RAID system the write log may be stored to the portion located at offset X in log cache A. Other write logs may be stored to other portions of log cache A having different preconfigured offset values.

Once generated the write log may be forwarded by the storage operating system to the logging engine. The order indicator e.g. offset value may also be forwarded by the storage operating system to the logging engine. At step an entry generator e.g. entry generator of the logging engine may generate a mirroring entry e.g. mirroring entry to include the write log and the order indicator provided by the storage operating system. For instance the write log may be stored to a write log field e.g. write log and the order indicator may be stored to an order indicator field e.g. order indicator of the mirroring entry. The entry generator may then forward the mirroring entry to the remote node e.g. node B at step . Advantageously the mirroring entry may be forwarded to node B across the cluster switching fabric e.g. cluster switching fabric having multiple network connection paths between the nodes for increasing bandwidth of the cluster and avoiding the need for specialized data placement components.

At step a mirroring module of the logging engine e.g. mirroring module may be operative at node B to receive the mirroring entry and store the mirroring entry based on its arrival order to a next available location of an interim cache of node B e.g. interim cache . The mirroring entry may for instance be received by a cluster access adapter e.g. adapter of node B and stored to a next available location of interim cache . The mirroring entry may be stored to a next available location in interim cache based on a buffer mapping generated by the mirroring module for instance. In the exemplary embodiment the buffer mapping may be generated by the mirroring module during initialization of the buffer and interim cache or through mapping updates carried out by the mirroring module.

To optimize cluster resources during peak request periods a deferred processing engine e.g. deferred processing engine of the logging engine may be operative at node B to produce a consistent view between log caches . In short the deferred processing engine may process the mirroring entries in interim cache to produce an ordered set of log writes based on the order indicators at a later predetermined time. Thus at step the deferred processing engine may extract the write logs from the mirroring entries in interim cache . The write logs may then be stored to locations in log cache B based on the order indicator associated with the write log in the mirroring entry step .

Advantageously the deferred processing engine may process the mirroring entries at an off peak request period to conserve processing resources during peak request periods. To that end the deferred processing engine may be operative at a predetermined time supplied input by the storage administrator in one example. In another example the deferred processing engine may be operative upon the occurrence of a failover. In each case the deferred processing engine may process the mirroring entries to result in the storage of an ordered set of write logs to the remote log which may be accessed when executing uncompleted write operations on disk on behalf of a failed local node. Thus at step log cache B may be accessed by node B to carry out remaining write operations on disks of the cluster. When a failover from node A to node B occurs write logs in log cache B may be accessed by node B to carry out write operations not completed by node A.

When received the entry generator of the logging engine may generate the update entry using the received address at step of the completed write log. In one embodiment the storage address may be stored to the order indicator field e.g. order indicator of the update entry leaving the write log field e.g. write log as an empty or null value or other value constituting an update indicator. At step the entry generator may forward the update entry via the cluster access adapter from node A to node B across any of the multiple network connection paths between the nodes.

At node B the update entry may be stored by the mirroring module of a remote logging engine to a next available storage location in the interim cache e.g. interim cache at step . In detail the mirroring entry may be received via the remote cluster access adapter of the remote node and stored to the next available storage location in interim cache based on a buffer mapping accessed by the adapter.

At a predetermined time or event the deferred processing engine of the logging engine may process the update entry in interim cache step . In one embodiment processing the update entry may involve the deferred processing engine implementing an order on write logs extracted from updated and or mirroring stored in interim cache to produce an ordered set of write logs. The ordered write logs may be stored to log cache B to thereby produce a consistent view between log cache B and log cache A. When processing an update entry with an update indicator e.g. null value in the write log field the deferred processing engine may delete overwrite or otherwise remove data e.g. write log at the address indicated by the order indicator step . In this way a write log previously stored at the indicated storage location of log cache B may be updated to avoid duplicate processing of such write logs by the remote node.

Using the novel techniques write logs may be mirrored between nodes in a cluster to provide a consistent view of log caches while optimizing network bandwidth and processing resources. Write logs may be sent from a local node to a remote node using an unordered delivery of data to avoid the need for the local node to coordinate the transmission and arrival order of write logs. Complex and costly components such as RDMA may also be avoided since network resources may be implemented to carry out log mirroring operations. Aspects of the novel technique in this way overcome the disadvantages of the prior art.

Some embodiments may be conveniently implemented using a conventional general purpose or a specialized digital computer or microprocessor programmed according to the teachings herein as will be apparent to those skilled in the computer art. Some embodiments may be implemented by a general purpose computer programmed to perform method or process steps described herein. Such programming may produce a new machine or special purpose computer for performing particular method or process steps and functions described herein pursuant to instructions from program software. Appropriate software coding may be prepared by programmers based on the teachings herein as will be apparent to those skilled in the software art. Some embodiments may also be implemented by the preparation of application specific integrated circuits or by interconnecting an appropriate network of conventional component circuits as will be readily apparent to those skilled in the art. Those of skill in the art would understand that information may be represented using any of a variety of different technologies and techniques.

Some embodiments include a computer program product comprising a computer readable non transitory storage medium media having instructions stored thereon in and when executed e.g. by a processor perform methods techniques or embodiments described herein the computer readable medium comprising sets of instructions for performing various steps of the methods techniques or embodiments described herein. The computer readable medium may comprise a storage medium having instructions stored thereon in which may be used to control or cause a computer to perform any of the processes of an embodiment. The storage medium may include without limitation any type of disk including floppy disks mini disks MDs optical disks DVDs CD ROMs micro drives and magneto optical disks ROMs RAMs EPROMs EEPROMs DRAMs VRAMs flash memory devices including flash cards magnetic or optical cards nanosystems including molecular memory ICs RAID devices remote data storage archive warehousing or any other type of media or device suitable for storing instructions and or data thereon in.

Stored on any one of the computer readable medium media some embodiments include software instructions for controlling both the hardware of the general purpose or specialized computer or microprocessor and for enabling the computer or microprocessor to interact with a human user and or other mechanism using the results of an embodiment. Such software may include without limitation device drivers operating systems and user applications. Ultimately such computer readable media further includes software instructions for performing embodiments described herein. Included in the programming software of the general purpose specialized computer or microprocessor are software modules for implementing some embodiments.

Those skilled in the art would further appreciate that the various illustrative logical blocks modules circuits techniques or method steps of embodiments described herein may be implemented as electronic hardware software executing computer firmware or combinations thereof. To illustrate this interchangeability of hardware and software executing processor various illustrative components blocks modules circuits and steps have been described herein generally in terms of their functionality. Whether such functionality is implemented as hardware or software executing processor depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application but such implementation decisions should not be interpreted as causing a departure from the embodiments described herein.

The various illustrative logical blocks modules and circuits described in connection with the embodiments disclosed herein may be implemented or performed with a general purpose processor a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or other programmable logic device discrete gate or transistor logic discrete hardware components or any combination thereof designed to perform the functions described herein. A general purpose processor may be a microprocessor but in the alternative the processor may be any conventional processor controller microcontroller or state machine. A processor may also be implemented as a combination of computing devices e.g. a combination of a DSP and a microprocessor a plurality of microprocessors one or more microprocessors in conjunction with a DSP core or any other such configuration.

While the embodiments described herein have been described with reference to numerous specific details one of ordinary skill in the art will recognize that the embodiments can be embodied in other specific forms without departing from the spirit of the embodiments. Thus one of ordinary skill in the art would understand that the embodiments described herein are not to be limited by the foregoing illustrative details but rather are to be defined by the appended claims.

