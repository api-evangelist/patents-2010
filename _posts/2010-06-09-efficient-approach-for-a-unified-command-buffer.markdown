---

title: Efficient approach for a unified command buffer
abstract: A method for providing two or more processors access to a single command buffer is provided. The method includes receiving instructions in the command buffer from a central processor, at least one of the instructions being designated for a particular one of the two or more processors. The method also includes sending the at least one instruction to only the particular processor.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08675002&OS=08675002&RS=08675002
owner: ATI Technologies, ULC
number: 08675002
owner_city: Markham, Ontario
owner_country: CA
publication_date: 20100609
---
The present invention generally relates to processing systems. More specifically the present invention relates to enhancing efficiencies and performance in video processing systems configured for multi processor unit operation and capable of performing 3D rendering.

Recent developments in conventional graphics technology have created the ability to produce extraordinarily realistic video images. In most video systems a specialized processor is responsible for configuring these images for display on a monitor. As the realism of these images has increased so has the amount and complexity of the information required to produce the images. Correspondingly the performance demands on these processors has also increased. To meet these increasing performance demands display activity is no longer handled by the system s central processing unit CPU . Instead display activity processing is now handled by intelligent graphics cards including a coprocessor known as a graphics processing unit GPU also called a video processing unit VPU .

At the crux of the aforementioned developments in graphics technology is the ability to convert information stored in a processing system s memory to video signals for output to the monitor. One device commonly used to perform this conversion is known as a display adapter. In short the display adapter creates a pipeline for the real time conversion of graphics patterns stored in a GPU s memory frame buffers into the video signals output to the monitor. Additional improvements in graphics technology however have created the ability to combine the processing power of two or more GPU s multi GPUs operating simultaneously to produce even more realistic and more complicated images. Multiple GPUs for example are especially beneficial for rendering different portions of an image to respective portions of a monitor.

Interfaces have been developed to connect two or more display adapters together from two or more GPUs in a multi GPU system for faster graphics rendering on the monitor. These interfaces for example enable the execution of complicated programs such as 3 dimensional 3D rendering applications by multiple GPUs simultaneously. One such interface is known as CrossFire.

Even further developments have provided the ability to balance loads between these multiple simultaneously operating processors to more efficiently and more quickly render these complicated images. As performance demands have continued to increase several shortcomings have emerged with respect to these multi GPU rendering and load balancing solutions.

On a more technical level existing video or graphics processing systems include the capability to drive multiple GPUs as noted above. Using the current solutions however each of these multiple GPUs points to its own unique command buffer. GPU operation is driven by command buffers containing instructions that specify how the GPU is to render a scene. These buffers can be quite large particularly on complex scenes running on powerful GPU s. Current solutions require that each GPU have its own unique command buffer which results in large sections of duplication between these command buffers. This requires that the CPU perform at least twice as much work in order to create the unique command buffers for each GPU. As a result the command buffers in whole or in part are unnecessarily duplicated. That is when display activity commands are sent from the system s CPU the commands are sent to multiple GPUs and or multiple buffers requiring at least twice the work.

Additionally conventional graphics processing systems are significantly limited in their ability to dynamically and efficiently distribute rendering loads across multiple GPUs. Particularly these conventional systems are unable to distribute the load in a manner that matches each GPU s capabilities to the demands of scenes displayed on respective portions of the monitor.

By way of example consider images associated with the display of a flight simulator program. for example is an exemplary illustration of a screen shot from a popular flight simulator video game. In this example a bottom portion of the screen shot includes dials and controls along with other 2 dimensional 2D static images. A top portion of the screen shot however includes a 3D rendered world consisting of many rapidly changing images. The top portion therefore will require more GPU power to render than bottom portion because the bottom portion is less complex. Conventional graphics processing systems cannot efficiently distribute the load across multiple GPUs to render the top portion of the screen shot in the manner discussed above.

Additionally the conventional multi GPU systems require specifically designed multi GPU aware drivers. This awareness extends throughout the entire driver stack increasing code complexity and development cost.

What is needed therefore are methods and systems to eliminate or reduce the need for duplicate command buffers in multi GPU systems. Also needed are methods and systems that more efficiently distribute rendering loads across multiple GPUs. Additional methods and systems are needed to facilitate greater compatibility with existing multi GPU system products.

The present invention meets the above described needs. For example a first exemplary embodiment of the present invention provides an approach to submit identical command buffers to multiple GPU s without the need to explicitly modify the contents of the command buffer per GPU. Since command buffers are relatively large this approach provides a significant performance enhancement since it essentially eliminates the need to write out multiple buffers for each GPU in which significant portions of each buffer are largely duplicated. Eliminating multiple buffer write outs applies not only to the initial CPU write but also to command buffer patching in the kernel for each buffer and during associated cache flushes ultimately resulting in huge cost reductions.

A second exemplary embodiment of the present invention provides techniques to improve the performance of multi GPU rendering by providing dynamic feed back generated load balancing. More specifically this embodiment provides a mechanism to improve the performance of multi GPU rendering by dynamically adjusting the scissor orientation and coverage or rendering ratio based on different types of feedback. This exploits the fact that performance can often be improved by fine tuning these parameters based on images scenes that are currently being rendered. Dynamic feed back generated load balancing includes among other things the integration and aggregation of a number of different optimization components.

One optimization component includes measuring the performance of each GPU over a time window to permit fine tuning of the balance of work being distributed to each renderer. Since different areas of the screen can have differing render loads e.g. static area on portion of screen and heavy shader based rendering in another direct measurement of render time makes it possible to dynamically fine tune the configuration by redistributing the rendering load to achieve the highest level of performance on any given scene.

Another optimization component includes using predefined optimal configurations scissor orientation coverage render ratio etc for different applications. These predefined configurations can be utilized as a starting point if the currently running application e.g. a video game can be determined. This is useful because it is common for different applications to have differing areas of the screen with variable render complexity.

Yet another component for achieving dynamic feed back load balancing is the use of static region analysis for determining an initial starting configuration. Another aspect to this component includes scene change analysis. Scene change analysis determines dramatic scene changes. This in turn allows for a quicker response when substantial adjustments to the rendering profile are necessary. Scene change analysis can also be used to reset to a known optimal configuration.

Finally a history buffer is provided. The history buffer tabulates dynamic configuration changes permitting the intelligent creation of an optimal profile for a given application even when one has not been predefined. The history buffer can be analyzed for large screen regions of relatively stable consistent patterns. The results of this analysis forms the basis for new rendering configurations.

A third exemplary embodiment of the present invention provides a system for achieving seamless integration of multi GPU rendering. As an example this embodiment provides a mechanism whereby neither the 3D driver nor the 3D application has specific knowledge of whether multi GPU rendering is occurring. This enables the 3D driver to behave the same regardless of whether multi GPU rendering is enabled or not. More specifically this embodiment allows the use of conventional 3D drivers that are not specifically multi GPU aware. In the present embodiment a kernel layer is provided that abstracts this away from these conventional drivers and sends the appropriate commands to each GPU device driver. This in turn contributes to reduced costs and complexity.

Further features and advantages of the invention as well as the structure and operation of various embodiments of the invention are described in detail below with reference to the accompanying drawings. It is noted that the invention is not limited to the specific embodiments described herein. Such embodiments are presented herein for illustrative purposes only. Additional embodiments will be apparent to persons skilled in the relevant art s based on the teachings contained herein.

Embodiments of the present invention enable substantially improved utilization of video processor resources. While the present invention is described herein with illustrative embodiments for particular applications it should be understood that the invention is not limited thereto. Those skilled in the art with access to the teachings provided herein will recognize additional modifications applications and embodiments within the scope thereof and additional fields in which the invention would be of significant utility.

As noted above the present invention provides a number of techniques for enhancing efficiencies and performance in multi processor systems. These techniques are especially beneficial in multi GPU systems that are capable of performing complex image processing such as 3D rendering.

The application is an end user application that requires video processing capability such as a video game application flights simulations or other uses. The application communicates with application programming interface API . By way of example the API can be any one of the available graphics or video or 3D or compute APIs including DirectX from Microsoft OpenGL from Khronos and OpenCL from Khronos .

The API communicates with a driver . The driver translates the standard code received from the API into a native format understood by the GPU components.

The system also includes a GPU A and GPU B . The GPU A and GPU B can be on one or more video cards each including a video processor and other associated hardware. As understood by those of skill in the art more than one GPU can be resident on one card or board.

GPU A and GPU B receive commands and data from the driver through respective ring buffers A and B . The commands instruct GPU A and GPU B to perform a variety of operations on the data in order to ultimately produce a rendered frame for a display .

The driver has access to a shared memory . The shared memory or system memory is memory on a computer system that is accessible to other components on the computer system bus but the invention is not so limited.

The shared memory GPU A and GPU B all have access to a shared communication bus and therefore to other components on the bus . The shared communication bus can be for example a peripheral component interface express PCIE bus but the present invention is not so limited.

GPU A and GPU B communicate directly with each other using for example a peer to peer protocol over the bus . There can also be a direct dedicated communication mechanism between GPU A and GPU B . Local video memory and can be shared.

GPU A and GPU B each have a local video memory and respectively. By way of example one of the GPUs functions as a master GPU and the other GPU functions as a slave GPU but the invention is not so limited. The multiple GPUs could be peers under central control of another component. GPU A can act as a master GPU and GPU B acts as a slave GPU.

Various coordinating and combining functions are performed by an interlink module IM that is resident on a same card as GPU A . This is shown as IM enclosed with a solid line. In such an embodiment GPU A and GPU B communicate with each other via the bus for transferring inter GPU communications e.g. command and control and data. For example when GPU B transfers an output frame to IM on GPU A for compositing the frame is transferred via the bus .

The IM is not resident on a GPU card but is an independent component with which both GPU A and GPU B communicate. GPU A and GPU B perform at least some communication through an IM connection . For example GPU A and GPU B can communicate command and control information using the bus and data such as frame data via the IM connection .

As noted above embodiments of the present invention can be implemented in an environment such as the video system . The present invention however is not limited to this particular environment. The embodiments of the present invention include approaches for providing a unified command buffer. Embodiments of the present invention also include dynamically balancing rendering loads and seamlessly integrating multi GPU rendering into existing video processing systems. Each embodiment is addressed in greater detail below.

Aspects of the present invention can be implemented in the kernel driver layer of system memory without requiring knowledge of a multi GPU render configuration in either the application or a client side 3D driver layer. Additionally this implementation uses feedback of render performance to calculate the orientation and render coverage or ratio for each GPU. The present invention however is not limited to this particular approach.

The present invention provides methods and systems directed at eliminating the need for duplicate command buffers in multi processing environments such as multi GPU systems. is an illustration of one exemplary embodiment.

The system includes a system memory that includes its own unified command buffer . A first GPU A and a second GPU B are configured to access the unified command buffer . The first GPU A and the second GPU B respectively include command ring buffers and . Each of these GPUs accesses the unified command buffer .

By way of background in conventional multiple GPU systems separate commands are provided to each GPU. These separate commands instruct each of the GPUs to perform a particular task e.g. draw a particular triangle . To perform these tasks in these conventional systems each of the GPUs must point to and accesses a separate local memory or command buffers within a larger system memory.

The system can be configured to operate as a conventional multi processor system. When configured conventionally the GPU A points to a local memory A and the GPU B points to a separate local memory B . This need to point to separate memories exists because there are differences in where things are stored within the shared memory . This conventional approach requires more memory and therefore greater chip real estate. This approach also increases memory access times decreasing system performance.

The exemplary embodiment of the present invention as illustrated in increases the efficiency of multi processor configurations such as the multi processor arrangement in system of .

By way of example the command buffer of includes the use of conditionals e.g. conditional executions . These conditionals include predications i.e. predicated instructions . The predications loosely anticipate whether instructions received within the command buffer are associated with i.e. intended for or matched with GPU A or on the other hand are associated with GPU B .

The predications also facilitate the use of the same command buffer across multiple GPUs. The predications can stipulate for example if particular received instructions are associated with GPU A then a first set of corresponding commands are fetched to perform a first set of tasks. On the other hand if particular received instructions are associated with the GPU B then a second set of corresponding commands are fetched to perform a second set of tasks.

The predications provide a type of tagging for each instruction within the command buffer . This tagging indicates whether the instruction is designated for the GPU A or the GPU B . In this manner although all of the instructions may be received by each processor only the processor tagged to receive the particular instruction e.g. GPU A or GPU B will actually read the instruction. Although illustrates the use of two GPUs the present invention is not so limited. The present invention is also not limited to GPUs. Instead of multiple GPUs the processor function could be implemented as CPUs or CPU GPU combinations.

The exemplary system of can be implemented within the kernel driver layer of the system memory . The kernel driver ensures that the objects located in VRAM are located identically for all GPUs with the exception of the frame buffer and the address remapping table. All allocations frees reserves done on master GPUs are duplicated on slave GPUs.

Reserving specific ranges in VRAM reserves the same ranges in all GPUs. For example location 0 10 in N of GPUs have the same array. Each 4 bytes i.e. each integer in each location represents a particular GPU. In the example of GPU A is associated with location 0 GPU B is associated with location 1 etc. The memory of all other CPUs at that location would be 0. The predications discussed above utilize this process. Within the kernel driver ranges of VRAM are allocated one for each GPU e.g. GPU A and GPU B . Each range contains a boolean array of size number of GPUs participating N called GPU 0 . . . N 1 such that only one position in the array is true 1 and all the other places are false 0 . That is GPU i 1 if and only if the GPU index is i i 0 . . . N 1 I represents the GPU index .

Reserving specific VRAM ranges also ensures the same VRAM map for all GPUs and provides portions of the code that only a particular GPU will execute. Consider the example of a video game that includes a wall scene textured with a specific pattern of stones. The textured pattern will be stored in the same virtual memory locations across all GPUs. Therefore if a command is submitted to GPU A it goes to this same memory location and accesses the same texture. This process of reserving VRAM ranges reduces the need to store duplicate content in multiple command buffers.

An additional aspect of the unified command buffer technique of the present invention relates to remapping table entries. More specifically remapping table entries are mapped to the same locations in the remap tables on all the GPUs. The remap table points to the system memory and not to the local memory of the GPU.

A remap table discussed in greater detail below is a system memory view of each of the GPUs. By way of example if GPU A is configured to read a texture from remap table address 10000 the address 10000 in the remap table points to a physical address of the system memory e.g. value 0x1234ABCD . If the same command is executed on GPU A and GPU B GPU B also sees a command that is trying to read a texture at address 10000 in the remap table. In this case the same content of the entry 10000 must be placed in the GPU B i.e. the value 0x1234ABCD . If this content is not also placed in the GPU B GPU B will read a different texture.

Another feature of the command buffer techniques provided in the present invention includes sharing command buffer retirement information between all GPU instances. This process is aided by synchronized commands. As understood by those of skill in the art synchronized commands include the concept of command retirement meaning that completed commands are time stamped to indicate time of their completion. In the present invention before a command buffer can be reused command retirement time stamps are examined to determine whether the commands have been completed.

Desirably all GPUs receive the same command timestamp values with the same command buffer being submitted to all GPUs. By way of one example this can be achieved when a GPUi writes its retirement timestamp to retirement buffer entry i and b a client will not retire the command buffer until all timestamps have retired i.e. all GPU s are finished with the command buffer even when using a single GPU instance.

In the present invention as noted above at least two different GPUs can reference the same unified command buffer. This unified command buffer in turn references the same underlying resources such as textures etc. Each GPU however does not have to reference the same physical resource i.e. memory space . The virtual to physical page mapping configuration of is one exemplary approach for submitting a single command buffer with a single virtual address with the underlying physical resources being located at different physical addresses.

The exemplary virtual to physical page mapping configuration of includes a GPU 222 coupled to a memory controller . When the GPU 222 makes a memory request the memory request travels through the memory controller and through a respective virtual mapping mechanism such as a virtual mapping table . The virtual mapping table includes virtual entries V0 Vn . Each of the virtual entries V0 Vn are mapped to a respective page P0 Pm of a physical memory . The physical memory can be associated with a physical command buffer or can be associated with some other physical resource.

The pages P0 Pm of the physical memory however need not be contiguous or in the same order as the virtual entries V0 Vn . In the exemplary configuration the virtual mapping mechanism returns the physical address of the requested memory to the memory controller . This physical address can then be used to access a respective page P0 Pm of the physical memory .

The VM scenario includes VM table and VM table . The VM table is associated with a GPU 0 and the VM table is associated with a GPU 1. Each of the VM tables and include virtual addresses that can refer to any resource. In the exemplary illustration of however each of the virtual addresses in the VM tables and refers to at least one physical memory resource. Exemplary physical memory resources include command buffer B and texture A . By way of example texture A can be an image that is being referenced somewhere else within the command buffer B such as an instruction to load the texture at this address. Other physical objects illustrated as random X and random Y in can also be referenced by the VM tables and . Random X and random Y can include for example vertex buffers other textures or command buffers or any other resource.

As an example entry i in VM table points to the same physical memory object as entry i in the VM table . Any address that is referenced in the command buffer B is desirably mapped in the same way on the VM table and the VM table . Addresses in either of the VM table or the VM table that are not referenced in the command buffer B can point to other locations.

In the same address Virt i on VM tables and points to the same command buffer B . Virt k on each of the VM tables points to the same texture A . However Virt m which is not used by the command buffer B can be used by GPU 0 and GPU 1 to point to different physical resources.

In the embodiment of the present invention illustrated in the resources pointed to by the VM tables and are not required to be the same physical memory location. They can be different physical memory locations as long as the content is the same. For example it is sufficient that the same texture is loaded in local memory on two different GPUs as long as Vi for example points to its own unique instance of that texture.

Also shown in is an indirect command buffer ring . Indirect command buffers such as the command buffer ring are buffers that include indirect references to other command buffers. For example the indirect command buffer includes an instruction list which includes direct commands cmd0 cmdn. The instruction list also includes indirect references ind0 ind1. The indirect references ind0 ind1 further point to indirect command buffers and each including other indirect references allowing for multiple levels of indirection. The concept of having an indirect buffer is one approach for simplifying implementation of unified command buffer discussed above.

During operation GPU 1 and GPU 2 fetch from buffer rings and respectively. Additionally GPU 1 has its own read and write pointers RPTR1 and WPTR1 respectively. GPU 2 also has its own read and write pointers RPTR2 and WPTR2 respectively. As the system CPU not shown fills a ring with new commands it updates the respective write pointer. As GPU 1 and GPU 2 fetch from their respective buffer rings and they update their associated read pointers. Each GPU runs asynchronously to the other and may have differing workloads even when presented with the same command buffer due to predication. Therefore it is desirable that each GPU maintains its own read pointer.

In when using indirect commands different entries in command buffer rings and can actually point to the same resource within the system memory such as resource . As discussed above with reference to it is sufficient if the memory content at different physical locations is identical. In this manner it is not required that different entries point to the same physical location. This approach is utilized when implementing a unified command buffer in embodiments of the present invention.

A second embodiment of the present invention provides techniques to improve the performance of multi processor rendering e.g. GPUs by dynamically adjusting scissor orientation and coverage or rendering ratio based upon different types of feedback. This second embodiment exploits the notion that performance can be improved by fine tuning and adjusting multi GPU rendering profiles based upon parameters from scenes currently being rendered.

In a multi GPU system the CPU determines how to efficiently distribute the task of rendering an image i.e. rendering load to each of the individual GPUs in the system. is an illustration of a conventional approach for performing load rendering in a multi GPU system.

More specifically includes a conventional scheme for distributing the rendering load across four GPUs 1 4 configured for multi GPU rendering. The conventional scheme represents a video screen displaying an image with each of the GPUs 1 4 rendering a respective portion of the image to a corresponding portion of the screen.

In the scheme for example the rendering task has been divided into four chunks distributed equally across GPUs 1 4. Here the term equally denotes the positioning of respective vertical and horizontal rendering boundaries and which collectively determine the portion of the rendering load each GPU performs to render the image on the screen. Each of the GPUs 1 4 in the scheme renders an equal portion of load i.e. processes an equal portion of the image for display on the screen . One goal of a multi GPU system such as the 4 GPU system in the scheme would be that by efficiently distributing the rendering task across the four GPUs the rendering process would be four times faster. Conventional systems however rarely achieve this level of efficiency.

The second embodiment of the present invention provides an improved technique for distributing the rendering load across N number of GPUs at significantly higher levels of efficiency than achieved in conventional systems. This embodiment also provides an approach to fine tune the rendering boundaries. This fine tuning enables the boundaries to be established based more upon the availability capability of particular ones of the GPUs instead of merely dividing the load evenly as performed in the scheme of .

In the present invention load balancing efficiency is increased by measuring the performance of each GPU over a time window allowing for fine tuning of the balance of work being distributed to each renderer e.g. each GPU . Since different areas of the screen or monitor can have differing render loads e.g. static area on portion of screen and heavy shader based rendering in another direct measurement of render time makes it possible to dynamically fine tune the rendering scheme to achieve the highest level of performance on any given scene.

In exemplary rendering boundaries and are shown. The CPU determines the most optimal load distribution among the GPUs 1 4 to render a complete scene to the screen. The exemplary rendering boundaries and reflect this distribution.

In the scheme of for example GPU 1 renders a portion of the scene defined by the rendering boundary . As an example the portion might represent a hallway portion of a scene in a video game. GPU 2 renders a portion of the scene defined by the rendering boundaries and . The portion might represent for example a static right side wall portion scene from the same the video game. Similarly GPU 3 renders a portion of the scene defined by the rendering boundaries and . The portion can represent a static left side wall portion scene from the same video game. GPU 4 renders a portion of the scene defined by the boundaries and .

FIG. B provides a more detailed graphical illustration of the various screen views referenced above in relation to . FIG. B includes an exemplary screen from a commercial video game to illustrate the various demands placed upon individual GPUs in a multi GPU environment. In FIG. B portions and correspond to the screen portions and of respectively. By way of example the screen portion rendered by the GPU 1 is a more static i.e. less complicated portion of the screen.

By contrast the screen portion of FIG. B rendered by the GPU 2 is changing and more complicated to render including many moving images. Thus in rendering the video game scene GPU 2 has a heavier work load than GPU 1. Similarly the screen portion rendered by GPU 3 includes a first person shooter perspective which is changing and more complicated than the screen portion . The screen portion is similar to the screen portion in that it is also a less complex static portion of the screen.

Embodiments of the present invention have the ability to distribute and dynamically adjust the rendering work load across the various GPUs 1 4 based upon changing scene requirements such as those illustrated in FIG. B . Many other factors however can influence the rendering load distribution. For example GPU 4 could be inherently more powerful than GPU 1 and therefore better suited to render more complicated scene portions.

Several exemplary techniques can be used to implement the dynamic feedback load balancing aspects of embodiments of the present invention mentioned above. One such approach includes the use of predefined optimal configurations. Optimal configurations e.g. scissor orientation coverage render ratio etc. can be established for example where a screen portion remains static for substantial periods of time.

One optimal GPU configuration to efficiently distribute the rendering workload across multiple GPUs can be used where largely static views such as the bottom portion are used for substantial portions of the scene. These largely static views present a significantly smaller GPU rendering load than more active and complicated images such as the top portion of . A different approach discussed in greater detail below is used to establish an optimal GPU rendering configuration for rendering the top portion .

Therefore in the case of optimally configuring N number of GPUs to render the screen shot of the smaller GPU load required to render the bottom portion can be considered. For this particular flight simulator application for example one optimal configuration i.e. partitioning scheme for N number of GPUs to render all of the screen shots might apportion a less powerful GPU to render the bottom portion each time the bottom portion appears. This less powerful GPU would essentially be dedicated to rendering the more static bottom portion along with any other less complicated screen shots. This approach is particularly useful where different applications occupy differing areas of the screen with variable render complexity.

In the present invention the optimal partitioning configuration for rendering the portion can be dynamically adjusted based upon either the number of GPUs available or the power of individual one of these GPUs. Additionally this particular example configuration can become the baseline for this fight simulator with a predefined profile that is dynamically adjustable based upon real time feedback.

In yet another embodiment of the present invention a static region analysis can be performed to determine the optimal rendering configuration when N number of GPUs are used. A static region analysis is a technique for analyzing a screen currently displaying an image. This analysis can be used to determine screen portions that may be more active e.g. the portion from screen portions that may be more static e.g. the portion . Although used to distinguish active from static static region analysis is used primarily to more quickly identify static screen regions. Thus static region analysis is one more tool that can be used to dynamically distribute the rendering load across N of GPUs in embodiments of the present invention. Static region analysis can be used to determine a starting point or baseline for establishing a rendering configuration in the absence of predefined optimal configurations.

In a region contains more changes representing a larger GPU rendering load. A region contains fewer changes representing a smaller GPU rendering load. A number of techniques known to those of skill in the art are available for performing the actual analysis of the regions and of . One such technique is known as the sum of absolute differences SAD which is widely used to perform motion estimation for video processing. SAD entails comparing blocks of one or more original pixels from a video frame with blocks of one or more pixels from an ensuing frame. An absolute value of the comparison is used to assess the amount of change within the particular portion of the screen related to the block of pixels. In the case of the present invention SAD can be used for example to identify screen regions with the lower amounts of change.

In for example pixels from a current frame of the region are compared or subtracted with pixels from one or more ensuing frames of the region . The more similar they are the closer the delta is to 0. The absolute of this delta for each pixel in the block is determined and all the values are summed. This sum represents the level of change for this block. The more pixels that deviate and the larger the deviation the larger the resultant sum. In other words the higher the absolute value number the greater the degree of movement or change within the region .

Pixels from one frame of the region are compared with pixels from one or more ensuing frames of the same region . The sum of absolute differences for this region is also calculated.

In the example of the region might represent the image of a control panel within a video game with very little movement. The region might represent a dynamic section of the screen within respect to movement such as the scene of FIG. B representing a first person shooter. In therefore the SAD from the region will be higher than the SAD from region indicating very little movement. Therefore by using static region analysis the conclusion can be drawn that the region is relatively static.

Knowledge of the static regions within the exemplary scene for example can be used to select from a predefined set of rendering profiles having at least one profile that matches a scene of interest. In the absence of a predefined rendering profile a suitable profile can be determined on the fly based upon the results of the static region analysis. By way of illustration for the example screen of GPU0 may be defined to render the first 6 rows of the frame and GPU1 may be defined to render Pixels 7 through 16.

Another technique that can be used in conjunction with static region analysis in multi GPU rendering load balancing is scene change analysis. Scene change analysis is also well known to those of skill. In embodiments of the present invention however scene change analysis is used as another tool to efficiently distribute the rendering load in a multi GPU environment across each of the GPUs. Scene change analysis is a statistical analysis of pixel data related to a scene taken from the same vantage point but at different times. This information is used to determine whether objects in one frame of a scene are present in ensuing frames of the same scene. An absence of these objects in the ensuing scenes represents a complete change in the scene.

With respect to scene change analysis consider the exemplary scene of a first person shooter in a video game running down a corridor. Next as the shooter reaches the end of this corridor assume they leap from a window inside of the corridor into a helicopter. When the scene changes from a shooter traveling down a corridor i.e. lots of movement and changes to a helicopter cockpit i.e. more static a substantial portion of the bottom half of the screen becomes a static control panel see e.g. the bottom portion of . In this example since the scene has totally changed the GPU rendering load therefore has also changed. The challenge at this point becomes dynamically reconfiguring the GPU rendering load distribution to match the load rendering requirements of the newly changed scene.

By way of review to reconfigure the GPU rendering load distribution it will become necessary to adjust the rendering boundaries between the GPUs see e.g. the boundaries and of . As noted above these boundaries correlate specific areas of the screen with respective GPUs for purposes of rendering. In the example above the use of scene change analysis makes this adjustment and correlation process more efficient.

More specifically scene change analysis helps avoid merely incrementally moving i.e. fine tuning the rendering boundaries when a scene has substantially changed as in the case of the example above with the first person shooter and the helicopter. In embodiments of the present invention when substantial scene changes occur the current GPU load rendering scheme can be abandoned in favor of a more optimized or predefined rendering profile.

One additional optimization technique used in embodiments of the present invention includes a configuration history table to track the changes and adjustments that occur to the GPU load rendering profile in real time. That is as the rendering boundaries such as the boundaries and are adjusted up down left right etc. to accommodate changes in the GPU rendering load distribution these changes are tracked and tabulated.

By way of example when a video game is running such as the first person shooter helicopter example above the rendering profiles are dynamically created and or adjusted to more efficiently distribute the rendering load across the multiple GPUs. As these rendering profiles are created and or adjusted they are tracked and tabulated in a history buffer. As the video game continues to run this history table is updated with the new and or updated rendering profiles.

Following some predetermined period of time entries to the history table can be reviewed to determine whether any of the tabulated rendering profiles were more prevalent or dominant during execution of the video game than any of the other rendering profiles. More specifically the history table can be analyzed to determine which if any of the tabulated profiles were used more than others. The results of this analysis can be applied to future uses of the video game.

For example it can be helpful to know whether the rendering load was split equally across all of the GPUs 90 of the time. It may also be of use to know whether a particular one of the GPUs was performing at a particular level or in a predictable manner. This information can be saved in the history table and upon exit from the game a new and more efficient rendering profile can be developed based upon this tabulated information.

Additionally each user may have their own style of playing the game. Using the history table feature of embodiments of the present invention each time a user plays the video game the system learns and capitalizes on that user s style and experience correlating this information into adjustments to the rendering profile for subsequent uses of the video game by the same user.

In step of the method a determination is made as to whether a pre defined optimal configuration has already been established. If a predefined configuration has not been established a static region analysis or scene change analysis is performed in step to determine an efficient initial GPU rendering configuration. If on the other hand a predefined rendering configuration has been established this predefined rendering configuration can be applied in step to a displayed scene based upon user defined priorities.

In step render statistics are gathered in real time and stored to assist in determining an appropriate rendering load for each GPU within the multi GPU configuration. These statistics can include by way of example frames per second e.g. of flips GPU Load i.e. how much of the time the GPU is loaded GPU Idle time per second i.e. the amount of time the GPU stayed Idle or performs no work number of commands retired per second i.e. the number of commands the GPU can complete etc. The render statistics of step can be used to apply a new partitioning profile.

Step represents the application of an additional rendering profile optimization tool. In step another static region or scene change analysis can be used to determine whether more substantial scene changes than those identified by the statistics in step have occurred. If substantial changes have occurred in the presently displayed scene as indicated in step a new partitioning configuration is developed and applied see e.g. step . If the static region or scene change analysis of step determines that substantial scene changes have not occurred fine tune adjustments are made to the rendering scheme as indicated in step and this information is saved in the history table as shown in step .

If the video games has finished as indicated in step the process exits and the history table is checked to determine whether a dominant configuration existed for a large duration of the game. If a profile can be identified this profile is saved as a predefined profile as shown in step . If the vide game has not finished the process returns to gathering statistics at step .

As noted above a third embodiment of the present invention provides techniques for performing seamless integration of multi processor rendering. More specifically a mechanism is provided whereby neither the 3D driver nor the 3D application have specific knowledge of multi GPU rendering which permits the use of turn key drivers.

In conventional systems application programs such as video games write to a render engine requesting the performance of one or more tasks. These tasks can include for example a request that a particular object be drawn a particular 3D scene be built or to render a specific scene in a particular location. These requests are typically forwarded to a hardware driver and then to a single GPU. The driver converts the requests to commands the GPU can understand and then forwards the commands to that GPU.

In embodiments of the present invention this driver functionality is largely abstracted so that instead of sending the commands to a single GPU the commands are sent to N number of GPUs. The present application refers to this process as seamless integration of multi GPU rendering.

One conventional approach for performing multi GPU rendering includes designing the driver itself to be multi GPU aware. In this manner the driver is able to recognize the need to render one thing to one part of one GPU and to render the other thing to another GPU. Embodiments of the present invention however avoid the need to redesign or reconfigure the drivers to perform multi GPU rendering. Instead embodiments of the present invention permit the use of less complicated off the shelf i.e. turn key drivers for multi GPU rendering.

In the system for example the intelligence designed into conventional multi GPU capable drivers is instead moved to an abstraction layer . Consequently each of the GPUs in the system e.g. GPU 1 GPU 2 . . . GPU n believes that it is the only GPU rendering a particular scene. That is GPU 1 is unaware that GPUs 2 . . . GPU n are also rendering the same scene. This approach avoids the need to build this level of multi GPU intelligence into the drivers. As a result the complexity of the driver can be reduced by not building in special paths to accommodate multi GPU rendering.

In the exemplary system the abstraction layer includes a module to perform an initial set up based upon an predefined profile. By way of example this predefined profile might stipulate for a particular video game that the screen should be split in half. In this example the initial setup module helps ensure that memory maps in the GPUs are substantially identical. The initial set up process can also be used to support scissoring as illustrated in .

Through scissoring each of the GPUs and actually renders the entire image. However the initial set up module of enables the image to be masked such that only half of the image gets written out from each of the GPUs and . Each of the GPUs and however believes that it is solely rendering and writing the entire image . The abstraction layer masks the presence of each of the GPUs from the other GPUs as indicated above. This approach provides a substantial performance improvement in writing the image data and also saves time.

In embodiments the present invention scissor commands can be placed in the command buffer such that when the same command stream is executed on GPU i it sets the appropriate scissor for that particular GPU instance. In order for this mechanism to work the command processor should be able to handle conditionals in the command stream. As an example the scissor can look as follows 

The coverage percentage for each GPU instance can vary based upon optimal load. Only one of the above conditions will holds true for any given GPU instance as each GPU has a unique boolean identifier.

In embodiments of the present invention scissoring can also be used to set the GPUs to perform different ratios. For example one GPU may be able to render faster than others. Thus the faster GPU can be directed to render two frames while a slower GPU renders one frame. This approach for example is similar to a technique known to those of skill in the art as alternative frame ratio AFR . In embodiments of the present invention however scissoring can be used to determine how the GPU frames will be rendered in order for the GPUs to efficiently run in parallel.

Referring back to the abstraction layer also includes an application profile module and a scene analysis module which are respectively used to develop rendering profiles and perform scene change analysis as described above. Also included is a render statistics module used to compile render statistics as noted in relation to . Finally an optimize configuration module is included to process data received from the application profile module the scene analysis module and the render statistics module . The optimize configuration module analyzes this data to dynamically create the optimized rendering profiles discussed above. The optimized rendering profiles are ultimately used to form the GPU boundaries e.g. and of .

To achieve optimal performance the driver in the system for example dynamically creates a measuring window period in which the driver determines optimal scissor parameters for each GPU. Exemplary parameters are 

During the measurement period the driver incrementally adjusts the parameters and record the GPU s performance feedback i.e. gathered statistics . At the end of the measurement period the results are analyzed and the optimal scissor ratio configuration is set before another round of performance data gathering is conducted.

The search for optimal configurations can be optimized and modified in multiple ways to eliminate the search time required to update the performance score table s . One optimization is to use the previous optimal setting and only perform a limited search around these settings. Another possible optimization is to use the current application knowledge to limit the search to specific parameters or to start with a predefined set.

Embodiments of the present invention have been described above with the aid of functional building blocks illustrating the implementation of specified functions and relationships thereof. The boundaries of these functional building blocks have been arbitrarily defined herein for the convenience of the description. Alternate boundaries can be defined so long as the specified functions and relationships thereof are appropriately performed.

For example various aspects of the present invention can be implemented by software firmware hardware or hardware represented by software such as for example Verilog or hardware description language instructions or a combination thereof. After reading this description it will become apparent to a person skilled in the relevant art how to implement the invention using other computer systems and or computer architectures.

It should be noted that the simulation synthesis and or manufacture of the various embodiments of this invention can be accomplished in part through the use of computer readable code including general programming languages such as C or C hardware description languages HDL including Verilog HDL VHDL Altera HDL AHDL and so on or other available programming and or schematic capture tools such as circuit capture tools .

This computer readable code can be disposed in any known computer usable medium including semiconductor magnetic disk optical disk such as CD ROM DVD ROM and as a computer data signal embodied in a computer usable e.g. readable transmission medium. As such the code can be transmitted over communication networks including the Internet and intranets. It is understood that the functions accomplished and or structure provided by the systems and techniques described above can be represented in a core such as a GPU core that is embodied in program code and can be transformed to hardware as part of the production of integrated circuits.

It is to be appreciated that the Detailed Description section and not the Summary and Abstract sections is intended to be used to interpret the claims. The Summary and Abstract sections may set forth one or more but not all exemplary embodiments of the present invention as contemplated by the inventor s and thus are not intended to limit the present invention and the appended claims in any way.

