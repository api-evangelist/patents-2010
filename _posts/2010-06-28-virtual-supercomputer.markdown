---

title: Virtual supercomputer
abstract: The virtual supercomputer is an apparatus, system and method for generating information processing solutions to complex and/or high-demand/high-performance computing problems, without the need for costly, dedicated hardware supercomputers, and in a manner far more efficient than simple grid or multiprocessor network approaches. The virtual supercomputer consists of a reconfigurable virtual hardware processor, an associated operating system, and a set of operations and procedures that allow the architecture of the system to be easily tailored and adapted to specific problems or classes of problems in a way that such tailored solutions will perform on a variety of hardware architectures, while retaining the benefits of a tailored solution that is designed to exploit the specific and often changing information processing features and demands of the problem at hand.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08271259&OS=08271259&RS=08271259
owner: 
number: 08271259
owner_city: 
owner_country: 
publication_date: 20100628
---
This application is a Continuation of U.S. patent application Ser. No. 10 821 582 entitled Virtual Supercomputer and filed Apr. 9 2004 which claims priority to the provisional application 

This application incorporates by reference in whole as a partial embodiment of some of the elements of the virtual supercomputer the prior filed co pending application 

The present invention is generally directed to the field of computing and more specifically to the field of information processing and even more specifically to the field of high demand high performance and supercomputing.

The ability to use computers to process extremely complex or large amounts of stored information and derive new useful information from that information previously stored has assumed an important role in many organizations. The computer aided methods used to store and derive information vary but the vast majority of such methods depend on a general purpose computing hardware architecture. Accordingly the potential to craft an optimal solution to a computing problem is limited by a static hardware architecture that is optimized not for any specific computing task but rather to provide an acceptable level of processing ability over a wide and disparate range of computing tasks.

Attempts to create optimal solutions to solve specific and complex information processing tasks have focused on creating hardware architectures designed to exploit various features of the information to be processed so that such processing can be performed in an optimal manner. Hardware devices containing specialized vector processing units are one such example. Software written for such hardware formats the information into a form that takes advantage of the hardware s specialization thus creating a computing environment that is tailored to the specific problem at hand. Such tailored solutions are usually implemented on high end supercomputing hardware architectures with specialized software. Consequently this approach is prohibitively expensive for most organizations often costing millions of dollars. Additionally once created tailored solutions of this type are only suitable for a specific problem or class of problems.

The software written to solve specific high performance computing problems is necessarily constrained by the features of the hardware upon which such software runs. When such software is in machine readable form it is tightly coupled to a specific architecture upon which it will run. Further the underlying hardware machine architecture is almost always static and only reconfigurable and only partially so in a few non commercial machines not widely available. Even the so called grid or network or large cluster computing approaches which rely on large numbers of interconnected physical or virtual machines are still constrained by running on a few different types of conventional processors. While the topology of the network can be configurable in such approaches the architecture of the underlying processors is static and thus not tailored to the problem at hand.

The concept of a virtual supercomputer addresses these shortcomings. The virtual supercomputer provides a conceptual reconfigurable hardware architecture for high performance machine readable software. The conceptual hardware architecture masks the actual underlying hardware from the machine readable software and exposes to the software a virtual machine reconfigurable for the problem at hand. The virtual supercomputer thus provides to the software the operations needed for optimal processing unconstrained by the overhead associated with those hardware operations of the underlying machine that are not relevant to the task. This not only speeds the computing required for the problem but also importantly it dramatically speeds the process of software application development as the developer can write program code to a machine that directly processes operations specifically optimized for the problem to be solved.

The virtual supercomputer translates the software instructions from the format for the virtual machine into a format that a particular underlying hardware architecture can process. Each specific hardware architecture must have a specific virtual machine associated with it. Thus software for the virtual supercomputer can run on a wide variety of hardware architectures because the virtual machine for each specific hardware architecture provides the same conceptual hardware architecture for software developers. Therefore a large investment in a supercomputer or supercomputing cluster with attendant maintenance and obsolescence issues is avoided. Further unlike a grid or conventional network computing system which increases power in a brute force manner by simply adding more processors each virtual machine in a virtual supercomputer network has an internally configurable architecture thus magnifying the power of the virtual supercomputer to provide a tailored solution.

One embodiment of some portions of the virtual supercomputer is described in the pending application Knowledge based e catalog procurement system and method listed in the CROSS REFERENCE section of this application.

The present invention solves the previously mentioned disadvantages as well as others. In accordance with the teachings of the present invention a computer implemented method apparatus and system is provided for crafting high performance information processing solutions that are able to be tailored to specific problems or classes of problems in a way that such tailored solutions will perform on a variety of hardware architectures while retaining the benefits of a tailored solution that is designed to exploit the specific information processing features and demands of the problem at hand.

The present invention provides a reconfigurable virtual machine environment upon which a tailored solution to a specific problem including a class of problems is crafted. Additionally an operating system for such a virtual machine is included. The information to be processed for a problem is encoded into a solution space or manifold of nodes where a node can be any kind of data structure and the nodes may be independent point clouds or connected in any kind of topology such as an acyclic directed graph ACG structure a balanced tree or other suitable data representation. This data representation is specifically constructed to closely match the architecture of the problem to be solved and the information to be processed. By exploring the data representation the information comprising the problem is processed and various possible solutions to the problem are generated and evaluated. The identified solution is not necessarily the optimal solution to the problem but is sufficiently accurate and robust to be useful. The exploration of the data representation is performed in a controlled manner to locate a solution.

In one embodiment of the present invention the virtual supercomputer operates on a single hardware processor and provides a software environment in which tailored solutions to multiple problems and or problem classes can be created. In another embodiment of the present invention the virtual supercomputer operates on a distributed interconnected network of hardware processors. Such processors may or may not be all of the same type. In this second embodiment the advantages of additional computing resources and concurrent processing can be exploited to find a solution in a highly efficient manner.

The accompanying drawings which are incorporated in and form part of the specification illustrate an embodiment of the present invention and together with the detailed description serve to explain the principles of the invention.

In a preferred embodiment of the present invention shown in the virtual supercomputer is a system apparatus and method composed of the NVSI Virtual Machine VM which is the actual reconfigurable virtual hardware processor an associated operating system NVSI OS a virtual machine assembler NVCL Assembler an application programming interface NVSI API Platform Drivers and a Platform Assembler.

A problem domain specific application requests specific processing tasks be performed for it by the virtual operating system running on the NVSI virtual machine VM . These processing requests take the form of function calls that are defined by the virtual supercomputer s application programming interface API . The architecture does allow for an embodiment in which direct calls to the VM are made by the Domain Application.

The virtual operating system NVSI OS is composed of multiple layers containing a plurality of sub components. The uppermost layer contains the OS managers. The managers coordinate various aspects of the creation of the solution space and the operation of the virtual supercomputer. Managers manage various engines and can invoke the operation of any set of engines to accomplish a task. The next layer contains engines daemons and a toolbox. The engines implement low level machine instructions to send to the virtual machine and generate code that will activate the virtual machine. Daemons are background processes responsible for such tasks as reconfiguring the data representation garbage collection and memory recapture. An example would be pruning of unused or outdated branches in a tree manifold by the navigation engine see below . The toolbox is a collection of routines that are frequently called by the managers. To accomplish certain frequently preformed tasks a manager has the option of issuing an instruction to an engine or instead making a call to the toolbox.

The solution space is the collection of nodes or other data formats that are interconnected in such a way as to construct a data representation or manifold with input data encoded into its topology. One possible embodiment for such a data representation is an acyclic directed graph. Other possible embodiments include but are not limited to independent point clouds ordered sets of points cyclic graphs balanced trees recombining graphs meshes lattices and various hybrids or combinations of such representations. Each node represents one point in the data representation that is implemented using a data structure. The topology of the data representation is determined by the interconnections among the data structures. A node contains data in various forms depending on the particular problem to be solved. Choices from among possible data representations are made based upon the attributes of the particular problem to be solved. Data contained in a node can be in the forms of numeric tags character tags boolean flags numeric values character values objects IDs database record IDs simple arrays variable density multidimensional arrays symbolic functions mathematical functions connection pointers to other nodes function pointers lookup table list pointers linked lists or even pointers to other solution spaces or data representations.

The instantiation engine IE provides instructions for the instantiation unit IU that creates and deletes nodes the IU and other machine units are shown in . The population engine PE provides instructions for the population unit PU that stores data into nodes and the arithmetic and logic unit ALU that emulates a more traditional hardware implemented ALU. The navigation engine NE provides instructions for the navigation unit that reads selected nodes. The evolution engine EE provides instructions for updating the contents of the IU and the PU. The configuration engine CE provides instructions for the solution space configuration unit SCU which allocates memory for the data nodes and the node index. The SCU also stores configuration parameters for every aspect of the architecture.

The configuration engine CE modifies a data representation s to create a topology tailored to the problem at hand. When creating this topology the CE chooses from among a plurality of available topologies and modifies a chosen topology or topologies to suit the given problem. The CE then stores the chosen data representation parameters and hardware configuration parameters into the SCU.

The virtual operating system including its component parts interacts with the VM via the virtual assembler. The virtual assembler is analogous to a conventional assembler or compiler in that it converts function calls written in a high level programming language into commands that the machine can understand and process. In this case the commands are in a format the virtual machine can process.

The NVSI virtual machine VM interacts with the platform drivers. The platform drivers allow the virtual machine to interact with the operating system resident on the host computer. The platform drivers interact with one or more underlying hardware platform CPUs via a platform assembler which converts commands from virtual machine level function calls to commands that the platform specific operating system and hardware can understand and process.

The virtual operating system has the ability to create multiple threads to perform tasks concurrently. When a new thread is created a new virtual central processing unit VCPU is created along with the thread. Newly created VCPUs are not complete copies of the entire virtual machine. VCPUs contain only the components necessary for their respective processing tasks such as the IU PU and NU. Certain components of the VM such as the index memory data memory the configuration unit and the network control unit comprising the backbone core of a CPU are not typically duplicated in threads. The resources and services provided by such components are shared among the other components of the virtual supercomputer.

A functional block diagram of the components and interconnections within the virtual machine NVSI Virtual Machine as denoted by the bold bordered box in is shown in .

The Solution space Configuration Unit SCU contains the index base register IBR stack the index memory allocation and data memory allocation registers IMAR DMAR the node data structure definition register DDR the data definition array DDA memory the field type FT table the number of nodes register NNR the data definition count register DDCR and the parameter configuration register PCR . The SCU is where the configuration structures of the virtual machine are stored. The PCR contains information that defines various attributes of the virtual machine by defining the structure of the data words used. This configuration can be altered by users or an application program at startup or even during runtime execution.

The IBR stack is a set of IBRs that provides an indexed addressing system for memory access. Each virtual register stores a base address that specifies a boundary for a segment of the virtual supercomputer s memory space. Offsets may be calculated by taking this base address value and adding to it the value from the virtual node address VNA field of the node index word IW stored.

The DDA is a table of data structure definition words DDW each identified by a unique integer data definition number DDN . When a store data definition word instruction is executed if the DDN indicates that the DDW is new the word is written into an available free memory location in the DDA. If the DDN indicates the word is not new the previous version of the DDW is overwritten with the new version. The DDW to write into the table is located in the DDR.

The FT table in the SCU stores a list of preset data word field types such as tag flag character integer fixed point floating point function pointer node pointer and list pointer. This table defines the valid field types that may be contained in a DDW and may be extended via settings in the PCR .

The NNR is a virtual register that stores the current count of non null nodes. It assists in the numbering and creation of new nodes as such nodes are instantiated and serves effectively as a measure of the size of the solution space.

The DDCR is a virtual register contained within the SCU that contains the total count of active data definitions. This information is used for DDA operations.

The PCR stores the basic parameters that define all configurable and thus alterable elements of the virtual supercomputer CPU. Such configurable elements include maximum field lengths maximum register lengths the number of registers in a stack or the sizes of arrays and tables.

The Instantiation Unit IU creates the nodes and makes space for the nodes in the data representation. The IU contains one node index word IW in the node index word register IWR . The IW contains a null flag that is set when a delete instruction is executed for a specified node. The next field contains the DDN. Following the DDN is a field that specifies the length of the data word. Next is the VNA followed finally by an application defined field ADF . The ADF can be used for special purposes defined by the specific application.

The node counter NC register is a virtual register containing a pointer to the next node. The NC contains a node number that is a non negative integer specifying the relative address of the corresponding IW in the node index memory NIM .

The Population Unit PU contains a node data word DW stored in a virtual register labeled the node data word register DWR P . A DW may be fixed length or variable length. A fixed length flag indicates the type of a particular DW. The DW stored in the PU is used when populating the solution space Node Data Memory NDM with nodes. The PU evaluates the data structure for a given node. The results of such evaluation are stored into a data word whose location in the NDM is pointed to by the index word. Every data word has a unique address in the NDM that is the VNA.

The navigation unit NU like the PU contains a node data word DW stored in a virtual register labeled the node data word register DWR N . This value is used when navigating through the solution space.

The node index memory NIM contains the node free index NFI and the next free node registers NFNR . The NFI stores the current intervals of free nodes in the node index space. The NFNRs are loaded from the NFI and store the beginning and the end of a range of free nodes. The primary use of the NFNRs is during instantiation operations where unused node index words are overwritten.

The arithmetic and logic unit ALU is a software implementation of some functions that are often implemented in hardware. It contains an adder multiplier a logic evaluator an arithmetic register AR stack a lookup table index a function index and an ALU memory. It allows as well for pass though of arithmetic operations to the underlying hardware CPU.

The physical memory controller PMC operates between the NIM and the NDM. The PMC controls the use of physical memory devices such as random access memory RAM disk drives optical storage drives and other physical memory devices which may be available to store data.

The network control unit NCU handles the low level details of sending out data and processes to be processed. It in turn is controlled by a network manager. These two units handle the tasks of separating tasks to be run concurrently load balancing and other network and concurrency related management tasks.

The CE store configuration parameters in the PCR and also creates and stores data definition words DDWs in a manner depicted by the pseudocode in . The engine begins by entering a loop. This loop executes once for each of a specified number of data word architectures in the domain solution space modeled within the data representation. Within each iteration of the loop the CE creates a DDW in register DDR according to the parameters specified by the domain application program. The CE next stores the DDR into the DDA in the configuration unit. The CE then continues its processing by executing the next iteration of the loop. The CE finishes its execution when it has executed the loop the specified number of times.

The IE creates nodes in a manner depicted by the pseudocode in . The engine begins by entering a loop. This loop executes once for each of a specified number of nodes to be created in the domain solution space modeled within the data representation. Within each iteration of the loop the IE creates an IW in register IWR in the IU. The IE next stores the IWR into index memory at a node number indicated by the node counter. The IE then allocates space in data memory at a virtual node address VNA calculated by the IM internal memory manager based upon parameters in the corresponding DDW word. The IE then continues its processing by executing the next iteration of the loop. The IE finishes its execution when it has executed the loop the specified number of times.

The population engine PE evaluates and stores nodes in a manner depicted by the pseudocode in . The PE begins by entering a loop. This loop executes once for each of a number of nodes. The PE reads an IW from index memory NIM at the specified node address. The PE next reads the DDW pointed to by the DDN in the IW. The PE then evaluates all fields in the data word according to the corresponding DDW. The PE then creates a data word in the data word register DWR P in the population unit. If the length of the data word has changed then the internal memory manager computes a new VNA stores the new VNA into the corresponding IW and updates the VNA in the IWR and stores the DWR P into data memory NDM at the new VNA. If the length of the data word has not changed the PE stores the DWR P into data memory at the old VNA.

The navigation engine NE finds and reads a node data word in a manner depicted by the pseudocode in . The NE gets the selected node number from the domain application program. The NE then reads the IW from index memory at the position specified by the node counter. The NE reads the data word at the corresponding VNA into the DWR N.

The evolution engine EE adds deletes or modifies nodes in a manner depicted by the pseudocode in . The EE begins execution by getting a selected node number from the domain application program. The EE then gets the evolution condition from the domain application program. The evolution condition specifies whether the EE is to add a new node delete an existing node or modify an existing node. If the condition specifies that the EE is to add a new node the EE calls the instantiation procedure for the specified node number. The EE then calls the population procedure for the same node according to parameters specified by the domain application program. If the condition specifies that the EE is to delete an existing node the EE calls the instantiation procedure in delete mode for the specified node and updates the NFI. If the condition specifies that the EE is to modify an existing node the EE calls the navigation procedure for the specified node number. The EE next modifies fields in the DWR P as specified by the domain application program. The EE then calls the population procedure for the specified node number. When the activities required by the given condition are completed the EE completes its execution.

NVSI Netcentric Virtual Supercomputing Infrastructure is a technology that provides a software solution for a broad range of computationally demanding problems in a myriad of commercial domains that would normally require a dedicated supercomputer or large scale special purpose one off software. The core of NVSI is a system architecture that blends key ideas in computing some rather novel and some imported and modified from discrete mathematics analog and digital computing theory high performance computing and hardware supercomputers. The most unique aspect of NVSI is the design philosophy derived from a biological perspective in which analogs of evolution and growth provide computational structures and representations that are dynamic flexible adaptive and work well enough under the constraints of imperfect and incomplete information.

As analogy the human brain is not the ideal solution that a computer engineer would construct from scratch. The brain is in effect a hodge podge of accumulated structures and algorithms that includes the biological equivalent of dead buggy bloated contradictory redundant expensive unstructured inelegant non optimal and apparently useless code and yet it solves an enormous range of changing situational problems. It doesn t always or even usually provide perfect results in an ideal sense but the brain does yield workable and often novel moment to moment solutions that accomplish the task at hand.

Fundamentally NVSI is a substrate that embeds biological notions in a system architecture a virtual machine enabling construction of high performance applications that provide superior solutions to certain classes of problems arising in a variety of industries. In particular NVSI creates a common computing foundation that applies to widely disparate domains where the shared theme is the rapid response to complex and fluid user demands not requiring the real time analysis of a massive flood of streaming data and for which the solutions may be imprecise and incomplete but sufficiently accurate and robust to relieve the burden on brute force processing and to dramatically enhance the responsiveness of the system.

The innovations of NVSI beyond the leading one of creating a virtual supercomputer in software include the merging of existing high performance computing techniques and designs with the employment of novel approaches to computation such as the separation of pre computation from navigation to dramatically reduce real time overhead biologically inspired good enough solutions and the use of evolving data structures and adaptive computational architecture. In particular key aspects of NVSI are 

NVSI is a virtual computer composed of an operating system and a quasi general purpose reconfigurable multiprocessing network based asynchronous RISC machine that happens to be created in software only and is designed to provide high performance over a large class of specific business computing problems. NVSI can be implemented on a mosaic of platforms and an increase in power of the underlying physical CPUs simply enhances the overall performance of the system.

One may ask how can a layer of software be more powerful than writing code directly to the underlying physical platforms The answer of course is that fundamentally it can not. However by acting essentially as an alternative computing architecture NVSI is designed to expose to the system developer a landscape of primitive that is elementary operations essential to the kinds of problems for which the features of NVSI are optimum. Thus applications can be quickly developed for the relevant domains without a business having to employ extensive labor resources to create a special purpose dedicated one off solution at enormous cost. Further the internal NVSI operations use as few and the fastest platform CPU instructions as possible thus maximizing performance and avoiding the bloat and wasted power inherent in general purpose microprocessors. And last NVSI bypasses the platform OS and makes direct calls to the platform CPU thus eliminating one software layer that applications would typically be forced to use.

The result is that NVSI squeezes maximum performance out of the underlying CPUs while providing to the developer a set of operations optimally suited to the problem at hand. With computational power proportional not only to speed and storage but also to utility and inversely proportional to capital and human resources consumed NVSI outperforms both hardware supercomputers and custom vertical applications in power per dollar when applied to the problems for which it is designed.

As in any computer there are CPUs that include functional units memory stores and dedicated registers all of which execute machine instructions coded by system programmers. The core of the NVSI System is composed of the NVSI Virtual Machine VM the hardware and the NVSI Operating System OS . The NVSI OS is an integrated collection of managers engines and Toolbox macros all written in an NVSI control language NVCL which is assembly like and makes direct calls to the virtual hardware.

A typical system configuration for an NVSI installation is shown in which illustrates one NVSI System installed on one physical machine server platform . The actual NVSI processor VM is shown in more detail in the functional block diagram of .

Solution Space the collection of points or nodes in a state space representing the entire landscape of data structures that is allocated instantiated computed populated explored navigated and modified evolved . The atomic element of the solution space is a node which represents a point in the solution space to which is attached a set of data structures. The way in which nodes are connected or not determines the topology of the solution space sometimes termed a manifold and this topology together with the associated node data structures implements a map of the problem domain. A spectrum of basic topologies for the solution space is briefly described below.

Node the elemental entity in a solution space. Each node is represented by two parts an Index Word IW that corresponds to the node number and includes a pointer to the associated Data Word DW which is the second part that comprises a node. For some applications an Index Word can serve as the Data Word as well in which case a node is then represented by only Index Words that also contain some data fields. The DW is an array of fields that can be of various data structure types including numeric or character tags boolean flags numeric or character values arrays connection pointers to other nodes function pointers lookup table list pointers linked lists or even pointers to entire other manifolds. There can be many different kinds of data structures in a solution space and each distinct data word architecture is defined and stored in the Solution Space Configuration Unit SCU see below .

Unit a functional component of the VM that performs a specific category of operations on nodes. Some typical operations are configuring and reconfiguring the NVSI architecture for the problem domain allocating configuring the index and data spaces creating deleting populating evaluating nodes navigating exploring the node data space solution space or manifold modifying evolving nodes and or connections performing optimized arithmetic getting lookup table values for common mathematical operations and generating function values and distributing data and or tasks over a network.

Register an internal part of a Unit used to transfer data between memory and NVCL variables. The configuration of each register corresponds to an associated memory word and is thus implicitly composed of sequential fields of specifiable length. Each field in a register is ultimately bound one to one to a specific persistent variable appropriately named to reflect the field description in the relevant engine NVCL program. However an important design aspect of the VM hardware is that to maintain the utility that is fastest possible performance of loading a register in a real machine which is a parallel operation and thus takes only one hardware cycle the actual transfer of contents between registers and Index Data Memory is done as a single binary encoding of the entire word. That is when NVSI is running in compact mode only bit strings are loaded into and from Index and Data registers and are stored into or read from Index and Data memory words. Thus a register is loaded from external operands by first encoding the field variables into binary and then compacting the bits into a single register operand. Conversely to extract the individual fields into NVCL bound variables the register contents are assigned to a whole register variable in NVCL and this bit string is then parsed by the program code into the corresponding variables. The VM can also be run in non compact mode typically for creating application prototypes in which registers and memory words are simply stored as mixed type arrays of fields and thus no binary encoding extraction is performed. Of course this severely degrades performance in both space and time but it does allow for an easier development process.

The functional units of the VM are all dedicated to manipulating nodes their data architecture and their connection topology. Each unit is controlled directly by the corresponding engine s in the OS which send instructions to the unit. The NVSI OS is both netcentric and multitasking and thus machine programs can not only be sent to multiple VM CPUs but can also execute in each of the functional units of a CPU separately in tandem.

The units and their basic functions follow. For reference the NVSI Register Memory Word Specifications are shown below and the NVSI Machine Instruction Set is shown in Table 1 both are discussed in more detail in later sections .

The various engines are low level modules in the NVSI OS that generate the actual machine code sent to and executed by the Units of the VM. Thus the Instantiation Engine for example runs procedures to create nodes the Population Engine procedures fill evaluate store nodes and so forth. Calls to the engines are made from the OS managers or perhaps the application directly as tasks such as Instantiate nodes with associated parameters passed to the engines. Some typical engine procedures are shown in .

The Toolbox is simply a collection of generic engine programs that have wide utility both for low level OS tasks and application level functions that are generic across domains. The Toolbox enlarges as experience with various application domains yields engine code that is used repeatedly.

Daemons are autonomous programs used for concurrent dedicated processes that operate in the background to collect garbage prune trees condense redundancies process edit queues etc.

While the Engines implement various tasks the NVSI OS Managers coordinate among tasks. These include network thread data distribution multiprocessing managers as well as engine managers that handle the coordination of all the different tasks for a particular engine in a higher level manner accessible to the NVSI API.

OS engine and or manager programs are written in NVCL. In subsequent versions of NVSI there may be provisions for each unit to maintain its own program counter and thus its own flow of control.

For all example code fragments the Embedding Language EL is a form of pseudo code in which boldface indicates a keyword italic denotes a variable name and underline indicates a named constant. For virtual machine instructions shown in monotype operands appear in sequence after the operation separated by spaces. True values are coded as 1 False as 0 . A null value codes a non operative variable or operand.

The following description advances and details the concept of implementing a novel software architecture on a heterogeneous network of conventional computational platforms to create a Netcentric Virtual Supercomputer Infrastructure NVSI . In order to provide a full appreciation for the commercial importance of this breakthrough enabling technology the description also describes one possible application of the NVSI named RiskScape. The RiskScape application addresses a particular computationally intensive problem in financial risk management known as Portfolio Stress Testing.

We begin by discussing why this technology represents a breakthrough and we then enumerate the several innovations that we have brought to bear in order to achieve this breakthrough. We next discuss the nature of how NVSI accomplishes its radical level of performance without special purpose hardware. We then review the reasons why we believe no one has heretofore taken this approach to computer system design.

The description continues with a mathematical treatment of the RiskScape design in order to demonstrate the power of the NVSI technology. In this section we outline the assumptions we have made to make the problem computationally tractable as well as discuss several of the optimization techniques that the NVSI platform provides to the application developer.

Finally we make a comparison of the NVSI technology with hardware based supercomputers. This comparison addresses both the classes of problems to which NVSI is suited as well as a discussion of those to which it is not. The last comparison is in price performance. Here we show the estimated computational throughput of the NVSI system in terms of VFLOPS Virtual Floating point Operations per Second and compare this with two CRAY RESEARCH machines the T90 and the T3E.

Our findings conclude that for certain classes of problems for which pre computation is a viable methodology the NVSI RiskScape solution is capable of about 250 Virtual GigaFLOPS. This performance is comparable to that of the CRAY T3E. Most importantly from a commercial perspective we estimate that the cost of an NVSI RiskScape implementation will be on the order of 50 to 100 times less than a comparable hardware based solution.

Netcentric computing is the new paradigm for building efficient cost effective computer systems to solve numerous business problems. Many commercial enterprises have invested substantial sums in computer hardware only to discover that they realize a fraction of the total CPU power. This is because the operating costs of a piece of hardware are identical whether the machine is running at its peak capacity or sitting idle. The problem is compounded by the fact that while one machine is sitting idle another in the same office is so overloaded that it has slowed to a crawl. What is needed is a way to allow the power of idle or underutilized machines to automatically augment the capacity of those that are over burdened. Such a solution would allow businesses to add hardware in an incremental fashion rather than having to continuously upgrade expensive servers and mainframes.

This description describes a breakthrough software based enabling technology that transforms a network of conventional PCs workstations and servers into a virtual supercomputer designed for optimal performance over a wide class of commercial domains. Quite general in nature this virtual supercomputer can be used to solve many although certainly not all computationally intensive problems that are found in a number of different businesses. These include telecommunications switching investment portfolio valuation e commerce order processing high demand query caching and fraud detection to name a few.

So that our proposed technology does not begin life as a solution in search of a problem we have applied virtual supercomputing to the problem of Financial Risk Management. Specifically we have addressed Portfolio Stress Testing an area that has a significant need for a cost effective solution and for which our new technology is well suited.

However it must be appreciated from the outset that while we describe a specific application this should in no way imply that the underlying technology is limited to this application. Our sole purpose in blending the description of the risk management application with the enabling technology in this paper is to demonstrate the vast cost performance benefit.

It is clear that many commercial problems such as Financial Risk Management need the level of computational power associated with conventional hardware based supercomputers. Unfortunately unlike mission critical projects for military and government operations the commercial world is highly constrained by economic considerations. In the main businesses cannot justify the expenditure of tens if not hundreds of millions of dollars on computer hardware that will become obsolete in three to five years. This is true even though rapid and accurate risk management can spell the difference between business success and catastrophic failure.

What is needed for commerce is a viable solution that transforms the substantial hardware investment already made by a firm into a computational platform capable of supporting the necessary time critical decision process. In this description we outline our design for such a computer system termed Netcentric Virtual Supercomputer Infrastructure or NVSI and then continue to describe one commercial application RiskScape that together can provide the level of performance financial institutions require in order to manage their risk.

Our current best estimate of the sustained Virtual FLoating point OPerations per Second VFLOPS obtainable by our risk management application supported by our proposed NVSI technology is between 5 and 250 GigaFLOP billion floating point operations per second depending on the number of CPUs available to the system. This meets the computational power of a CRAY RESEARCH T3E the highest end commercially available supercomputer.

This translates as the ability to evaluate a portfolio consisting of one million instruments including portfolios of derivatives on both debt and equity instruments across three million scenarios in under one hour.

It is important to note that the solution we propose is software based and requires only an incremental amount of additional hardware. From an economic and commercial standpoint we estimate that a full NVSI RiskScape implementation will be between 5 and 10 million dollars. A hardware based solution with comparable performance would cost in excess of 50 to 100 million. Moreover unlike a hardware solution the NVSI system will not become obsolete with advances in computer technology. Indeed performance only improves with evolution in platform and network capability.

Computing portfolio risk PR is a problem taking center stage in investment risk management. Managing risk is not trivial. Banks and other financial institutions are in the business of taking risks to generate increased revenues and profits but they are also required to protect shareholder value and prevent catastrophic losses from occurring. To date evaluation of PR has mostly been confined to macroscopic aggregate measures such as Value at Risk VaR which estimates PR as an expected loss derived from a weighted sum of volatilities in the individual securities in the portfolio based on small statistically derived market moves. Current risk management systems thus do an effective job of characterizing the expected loss in linear portfolios operating in normal markets. Indeed some form of full Monte Carlo VaR is the state of the art for both market and credit risk measurement.

Yet VaR indicates only the maximum expected loss that could occur over some time interval the portfolio holding period within some confidence level usually 2 or standard deviations about 97.5 . VaR has nothing to say about discontinuous or extreme 3 market events such as the Russian Sovereign Debt and Japanese and Emerging Market currency crises. That is VaR ignores the fat tails in the distribution of portfolio values within which lurk the dragons of risk the unexpected large moves in financial variables that can cause substantial losses as have been suffered by a number of leading financial institutions over the last several years such as LTCM . And given that the financial markets are not normally distributed they are log normal such events happen with considerable and distressing regularity. To cover the possibility of extreme events financial institutions implement large safety factors as the absence of specific risk data for the tails induces great caution. The result is that excess capital is lying dormant an inefficient solution at best. Moreover if the VaR method is pushed to achieve a greater range of application severe computational limitations arise. Finally even setting this consideration aside VaR calculations still do not really address the What if scenario questions.

In light of these limitations the method of stress testing has evolved as a complement to VaR. Also known as scenario analysis this approach attempts to address the weaknesses in VaR by subjectively generating scenarios that simulate large variance events. This enables the handling of nonlinear positions and certainly fills in some of the gaps. But current implementations of stress testing are flawed because of the small number of scenarios that can be examined due to computational constraints thus forcing necessarily subjective choices about which extreme changes to evaluate. The method also considers movements in only one or few variables and correlations are virtually ignored. And most glaring of all the defects in current stress analysis of PR is the inability to forecast path dependent scenarios several time steps into the future again due to limits on computational resources.

Even if the appropriate predictive tools existed such as comprehensive scenario analysis systems that accessed rich historical data combined with mark to future states the sheer data problem is enormous. Further the integrity of the data is crucial for assuring the validity and integrity of the risk management process results. Real issues of analytical model fidelity and accuracy compound these challenges. New financial products are finding their way to market credit derivatives synthetic financial products etc. at an accelerating pace. As the worlds of market risk and credit risk begin to merge there is an accelerating pace in transaction volume growth. It is increasingly clear that the standard approaches to managing risk are not keeping pace with the problem domain. The approach to problem solving in the risk management marketplace today is gated by problems of computing power database transaction processing throughput application design application scalability and user interface technology.

Of course this is not news. It is well known that the ideal approach would be to simulate the detailed price trajectory for all the instruments in a portfolio over a broad range of path dependent scenarios using the best perhaps several pricing models available or Monte Carlo simulation finite difference methods numerical integration or tree expansion where closed form models do not exist all calibrated by accumulations of historical data to provide correlation coefficients scaling factors and transition probabilities for variations in financial parameters. The difficulty has been that such an approach requires an enormous amount of computing power on the order of leading edge supercomputers at a cost that is daunting to even the most resource rich investment banks. So we settle for VaR and a lot of theoretical modeling and projection. But risk analysis must be essentially data driven not a theory driven exercise performed in a data vacuum.

This description outlines a new technology that allows for the first time a practical solution to the problem of calculating future risk for large scale portfolios. We present a novel computing architecture essentially a software based virtual supercomputer that supports on demand access to projected prices over portfolios of O 1M securities for a full range of path dependent scenarios that entail large 3 moves in financial variables. Termed NVSI Netcentric Virtual Supercomputer Infrastructure the system is a suite of highly optimized system kernels and user applications designed to emulate the key aspects of supercomputer architecture tools techniques and algorithms running on off the shelf workstation and network hardware for about 1 100the overall cost of a dedicated supercomputing system.

How does it work In brief by continuous off line background computing the RiskScape NVSI constructs a daily updated landscape of projected portfolio values for a broad range of scenarios along with associated scenario probabilities. This multidimensional state space hyperspace can then be queried to yield near real time answers to questions such as which scenarios if any could result in catastrophic loss to my portfolio in a week ninety days and six months out and with what likelihood 

By using supercomputing techniques emulated in software model optimization massive non swappable RAM and distributed processing over commercial networks of existing workstations the NVSI first populates a hyperspace of up to 10 billion nodes with state vectors that contain the pricing information and other moments for the entire range of instruments in a portfolio of up to 10 million securities. Once the hyperspace is fully populated with pricing vectors the portfolio can then be marked to any future state scenario desired simply by looking up or navigating to the address of that state and applying the pricing vectors to each instrument. The computational overhead of this second phase is minimal as the vectors have been pre computed off line. Clearly the combinatoric nature of the problem requires that the solution space be properly constrained. To do this correctly the optimal granularity and distribution of the state vectors in the hyperspace must be determined to ensure that the problem domain is fully bracketed.

If the set of state vectors is chosen appropriately the entire landscape of scenarios can be searched for the extreme events. In fact the application can be programmed to find the boundary conditions for catastrophic loss. In other words the NVSI system can actively search future state space to determine what combination of market and or credit conditions would cause the portfolio or institution to fail. In addition the probability of these conditions obtaining and the likely amount of time required for the conditions to obtain as well as the transition states through which the world must pass to reach the final state can be determined.

Such information would allow strategic management to see problematic conditions in advance and take appropriate action. Problematic financial and non financial holdings can be analyzed understood and unwound before potential financial meltdowns occur.

Why hasn t this been done before The answer is because the computing demand is enormous the price performance ratio for available technology has just recently come within practical range and no one has heretofore applied a blend of software solutions using virtual supercomputer architecture off the shelf network hardware background computing a spectrum of numerical optimization techniques and domain specific tricks to make the computational problems more tractable.

Large scale computing problems have been around since the Manhattan Project and indeed Los Alamos was the necessity that mothered the invention of DBM colloquially named Dah Big Machine . Until recently development of DBMs now known as supercomputers has been driven by military and civilian government needs and hence contracts. Typical application domains have been weather forecasting code breaking signal and image processing intelligence evaluation aerospace engineering and nuclear weapons design. The resulting machines were designed and built with cost as no object and their prices reflect that history. Spending 50 M 150 M on a dedicated number cruncher is an expense difficult to justify for an investment bank especially when that glistening rocket ship will become a burdensome dinosaur in about three years.

The alternative has been to scale up existing software that calculates trajectories for individual instruments such as options and other derivatives using pricing models like Black Scholes. These programs work perfectly well and provide a flexible platform for future improvements except that they provide single instrument answers for one input vector in time frames on the order of a minute or at best several seconds. This is fine for the trader negotiating a position but when this performance is expanded to include a family of trajectories arising from a wide range of scenarios and then further multiplied by up to a million instruments in a large global portfolio even brave souls pale. Waiting a hundred days or more to get an answer somewhat moots the requirement for daily update and response.

The application of the NVSI techniques in coordination with the recent convergence of several other factors enables the construction of an affordable virtual supercomputing system that can usefully meet the need for large scale near real time portfolio risk analysis. These recent factors are 

In essence no one has demonstrated a software emulated supercomputer because the performance would be too slow for the real time government applications listed above and providing a one off solution for near real time commercial needs would require massive construction of custom software at a cost nearly as burdensome as buying a big machine.

Instead we have determined that it is possible to build a suite of leading edge system programs and software applications running on networks of off the shelf workstations that gives dedicated near supercomputer performance without requiring much or eventually any specialized hardware. Such an approach can evolve with improvements in hardware and software technology. When presented as a retail solution to potential client financial institutions at a reasonable cost 1 100that of a dedicated supercomputer the NVSI system becomes a viable product.

What is new here What is not new is netcentric computing of large scale problems. This has also been termed Metacomputing by the NCSA National Center for Supercomputing Applications and its affiliated institutions and is being actively pursued as an alternative to single box supercomputing using wide area networks of high end mainframes. The innovation is in building integrated highly optimized software that emulates the kit of hardware supercomputing tools and techniques to create a hardware independent virtual supercomputer optimized to solve a wide class of problems that require large scale evaluation of independent state functions in an unbounded hyperspace of multidimensional inputs and outputs. Indeed our architecture could be used to solve a range of similar problems in other domains such as credit risk query caching and transaction processing for global compliance management on the internet which are decomposable into separate processes of background pre computation and real time demand query navigation of a large state space. NVSI would not be suitable for a domain that required the system to keep up with a data stream from the world arriving in real time such as cryptographic analysis.

The following innovations and or breakthroughs in RiskScape NVSI are detailed in subsequent sections.

The functional block diagram of the NVSI illustrates the essential aspects of the design. The core of the NVSI system is a suite of powerful state of the art applications built around a large 128 GB dedicated non swappable MassRAM that in later versions will be implemented via shared RAM from network resources or even hard storage based virtual memory . The particular structure of the data words and hyperspatial connectivity are implemented from domain specific parameters set by the client user via the RiskScape application. The Instantiation Manager then configures the design by creating a set of tables in CPU RAM containing metadata such as data word definitions tree structures pointer parameters network sharing and problem domain specifications indexes. The state vectors hyperspace nodes are then computed and stored in MassRAM by the Population Manager which calculates all function model values for each scenario. The Population Manager works continually in background distributing the burden across the shared Network CPUs. On demand mark search via queries from RiskScape of the state space is then handled by the Navigation Manager in concert with the Interpolation Manager and its associated Extended MassRAM EMR for finer grained exploration of selected node neighborhoods . More detail is presented in subsequent sections.

For each entity in the collection each instrument type in a portfolio for risk applications the NVSI constructs a path dependent tree a rooted ordered unidirected graph also more generally termed a metastructure or scenario tree. The branches edges represent variations Iin input parameter Iwith fanout . Every node vertex is a state vector s containing the value output of one or more model vectors V for the given instrument allowing for multiple models to value the same instrument and the probability P s associated with that node derived from the conditional probability of the particular input parameter variation that led to the current state for each timestep tin a sequence of chosen intervals. An example tree for fanout 9 is shown in .

For each node i a point in the state space H there is an associated state s defined by a 7 tuple state vector where 

A particular state is defined by computing scalar values for all the variables or functions v defined in the state vector for the particular node in the tree.

Note that is just the total number of variations over I. The way that is mapped onto I i.e. how the available granularity in variations is distributed across the input variables is defined by the client user.

This enables fast calculation of pointers saves an enormous amount of storage and wastes virtually no memory space. And because the trees are fixed until the state space is recreated for a new domain and a new set of scenarios there is very little shuffling or garbage collection required.

The total number of nodes or states in the hyperspace H is constrained by the total amount in bytes of dedicated MassRAM denoted by Mem available and the characteristic length in bytes of the data words. Thus 

A scenario is the path of events or conditional changes that leads to a given state. Thus each node represents the result of one scenario. The entire set of scenarios yields a scenario tree which is exactly one metastructure. Note that typically different metastructures are defined on the same landscape of events and thus share the same set of scenarios. The total number of separate trees is therefore constrained by both H and the chosen fanout derived from the desired granularity in input variation and the total number of time steps desired in the analysis .

For balanced trees the number of nodes M in a tree is given by the sum of a geometric progression of base 

The NVSI along with the RiskScape Intelligent Financial Risk Application was originally designed to optimize the computation of future portfolio values over a range of scenarios designed to uncover dragons the potential catastrophes that can result from less likely events changes in relevant financial variables that lie in the fat tail of quasi lognormal kurtotic distributions presumed to underlie most financial variations. Typically large global portfolios are composed of 100 000 to 10M instruments securities and include a wide range of types stocks equities bonds futures currencies swaps convertible debt interest rate instruments options on all of these and other exotics. Various models of these instruments exist and the output variables include price and the various moments the greeks of the model function. Typical input variables which are changed systematically in a simulation to construct a scenario tree include p price of the underlying asset the asset volatility interest rate i and interest rate volatility . Some models have closed form solutions that are relatively straight forward to evaluate such as Black Scholes . Others such as those for interest rate options require some form of stochastic simulation such as Monte Carlo finite difference methods or trinomial tree expansions for determining term structure such as Black Derman Toy and are thus far more cumbersome to compute.

In the prototype version designed for use in financial risk analysis the data structure has elements that are key for future risk computation. For portfolio evaluation the conditional probabilities P I I correspond to the actuarial transition probabilities derived from tables of historically calibrated movements in financial variables or extracted from relevant time series of changes in those variables. For risk analysis the risk adjusted probabilities are also important so the RiskScape interface allows the user to choose either or both. If both are specified one of the alternative probabilities is stored in one of the n variables v.

With a fanout of only 12 16 and four input variables for example there is not enough granularity to allow for joint transitions such as 4 p 0.5 . Thus in the prototype branches typically denote orthogonal moves that is changes in one input variable at a time. However implicit nonorthogonality can be approximated by using historically derived correlations between financial variables. This approach relies on the fact that with large moves correlations between financial variables become tighter. Alternatively can be increased to allow for joint transitions consonant with available resources.

To make the computation tractable for large portfolios 1M instruments and congruent with the theme of future risk analysis which is to look at possible catastrophes dragons arising from large moves in the input variables only changes of I 3 are typically evaluated. Although available tables of transition probabilities do not always contain such data the required probabilities can be easily extrapolated as the distributions or at least the first moments are known.

Even with 128 GB RAM and modest tree size 12 and 6 for large heterogeneous portfolios an unaugmented NVSI can support no more than 100 1000 metastructures separate scenario trees depending on the word length . To compensate one of the innovations in the architecture is therefore to use virtual or proxy instruments state vectors with V composed of generic model functions for all the securities based upon the same underlying asset or base index . The actual instrument prices can then be calculated during navigation by evaluating the corresponding function and converting back using standard scaling and correlation coefficients. When a proxy state is computed during the hyperspace population phase the various prices and moments of a given model are calculated over a range of values thus creating a parametric space that brackets the range of possible magnitudes for the portfolio instruments. During the navigation phase the actual instrument prices and other moments are then calculated via extraction of values from the pre computed parametric space using the state dependent input parameters stored in the proxy state vector.

For example suppose the actual instrument is a standard European option defined by free parameters K strike price S p r i div yield T time to expiration and volatility . If the model of choice is Black Scholes the option price can be obtained from a normalized space defined by only three parameters K S r T T. During the population phase the parametric pricing space is pre computed creating a parametric hypercube and mapped to the proxy and the values of the input vector I are stored in s for each node in the scenario tree. Then during navigation the price p of the actual portfolio option instrument is calculated marked to scenario simply by extracting a virtual price pfrom the associated parametric hypercube using the input values K S r T and then transforming the result using known scaling and correlation factors. That is 

Recall that each different proxy represents instruments with different underlying assets and each proxy is evaluated over one scenario tree. Thus the number N of available metastructures determines the number of available proxies onto which the portfolio can be mapped.

Some types are much more cumbersome to compute as indicated above. For example some of the exotics require complex use of input parameters such as interest rate derivatives that use a path dependent interest rate curve or actually an interest rate vector . Models such as Heath Jarrow Morton and Black Derman Toy are valued with state dependent trees which the NVSI architecture already supports.

The proxy state vector must contain models or parametric references for as many types of instruments as there are in the portfolio. For example a proxy might contain variables for cash futures standard options all Type 1 sovereign debt Type 2 and interest rate options Type 6 . All instruments of Type 1 2 6 in the portfolio that are also based upon the same underlying asset or base index such as the S P 500 are linked to this proxy. Thus there need to be as many proxies as there are underlying indexes in the portfolio typically in the hundreds .

The structure and size of a data word the bit wise representation of a state vector is specified by the Data word Definition Table see MetaTables which is constructed by the Instantiation Manager according to specifications received from the client user. For each proxy the Population and Navigation Managers carry this table around so to speak as a part of their operation.

The back pointer j indexes the branch of from the parent node that a state occupies. The prototype allows for a maximum fanout of 64 so the length of j in bits is 

Probabilities must reflect likelihoods derived from large move events and need be no more accurate than 1 part in 1000 three significant digits . Thus 

Strictly n is redundant because the data word definition table created by the Instantiation Manager specifies the number of variables in the state vector. Yet placing n in the state vector promotes data integrity and its storage penalty is small.

Finally the variables in V can represent function outputs parameter ratios input variables statistical distribution points partial derivative values or parameter reference pointers. In general the magnitude ranges are known and specified in the data word definition. Thus the only part stored in the state vector is the integer mantissa again typically to three significant digits for dragon hunting. In those cases where the range is arbitrary or infinite an integer exponent to accommodate 10 5 bits is also stored. The default then is 

For the complex proxy described above which we shall use as a conservative benchmark the state vector would contain one function variable for a cash index one variable for each of several say four futures four variables from the input vector I p i for the Type 1 options perhaps an additional four different input variables for the Type 2 options and an interest rate vector of say eight variables that represents the path dependent interest rate curve for the Type 6 options. We can also imagine that some of the exotics have models so complex that the parametric hypercube yields a family of curves or perhaps an instrument type is so new that its model has not yet been parameterized. In either case variables say ten representing curve selection values or five slope intercept pairs for linearized segments of the curves need to be stored.

Then number of variables 1 4 4 4 8 10 or 5 2 1 32. If most of the price related variables cash futures curve segments require stored exponents then len 15 15 17 10 395 bits. Accounting for the length of V the time dual of V this yields len len 2 len 36 2.395 826 bits. The actual word length in bytes for our benchmark proxy state vector is then len 8 104 bytes. This will be used to calculate relevant performance parameters. MetaTables

The user specifies the nature of the problem domain and all relevant data via the RiskScape Interface using a Scenario Description Language SDL . The Instantiation Manager then creates and configures a set of tables to implement the specifications. Typical tables include 

Some of the tables are quite large such as the Portfolio Data 1 million records or more and the Node Index a billion records . All are stored on hard disk but some are also resident in CPU RAM not MassRAM which holds only the hyperspace H such as the Data word Definition Proxy Definition Probability Transition Input Variable and Scenario Definition tables.

In this section boundary conditions and typical mid range values for spatial storage requirements and temporal performance are derived.

Thus for the prototype MassRAM of size 128 GB 137 10bytes and a typical for our benchmark proxy of 104 bytes 137 10 104 10 1 billion nodes or states available in the hyperspace.

Note that with a 50 bytes for very simple Type 1 only proxies to 200 bytes for very complex proxies representing instrument Types 2 5 6 the result is still about the same.

For the same size scenario tree as used earlier with 12 allowing three gradations or moves for each of four input variable and 6 allowing for six time steps such as 1 day 3 days 1 week t 20 days 3 months 6 months then

The number of proxies available to map the portfolio onto is therefore about 300 for 12 and 6. Can a large global portfolio be mapped to only 300 proxies that is only 300 base indexes Absolutely. There are only six types of proxies and linking these to 300 underlying assets indexes would cover most of the developed world. To bound this result consider a scenario tree with a larger of 16 

Suppose that we don t need six time steps but only four 1 week t 1 month 3 months 16 65 53615 000. Thus reducing the number of time steps significantly increases the number of proxies that can be created. If we wish to increase the granularity of input variations perhaps allowing for joint transitions like p then can be increased to 32 or 64. If a client user simply wanted to value a portfolio at one timestep 1 t and with very high precision in the scenario mesh 64 then 10 64 16 million. At this level a sizable portfolio can be evaluated for future risk directly without the need for proxies. The realistic spectrum for N is thus 1

Since many portfolios require less than 100 base indexes then with a typical N of 300 available for our benchmark tree of 12 and 6 the effective fanout can be increased thus increasing the input variation granularity by indexing an array of proxies to one instrument class each identical except that the scenario trees use different I increments. For example one tree could assign each branch to variations of 0 3 another tree have branches for 8 and another for 12 .

As we show in the temporal performance calculations the virtual effective throughput for navigation is high enough that it will be possible to implement MassRAM not only as a shared network resource but eventually as virtual memory using hard sequential storage. That is because the hyperspace trees are fixed and space filling for a given problem domain the RAM transaction volume is low with little random access. Thus by using JINI technology for example over shared network resources NVSI v2.0 can use FIFO paging from optimized disk storage performing a look ahead page fetch in 16 GB or larger segments while still not slowing the Navigation Manager. Under such an operating system Mem is virtually unbounded and we could realistically process 10nodes which still only requires a 40 bit address or more.

Performance of the NVSI is partitioned into the two primary phases the time Tto populate compute and fill all the state vectors in the state space and the time Tto navigate the space evaluate the domain collection such as a financial portfolio at each scenario and flag selected nodes for states that meet the criteria .

The key to the NVSI idea is that the apparent effective throughput in response to a user query is driven by the navigation time as the population time reflects background off line computing.

As a reference performance parameters are derived for the RiskScape problem domain using the benchmark proxy already described.

3. A recurring for each daily update as the portfolio is aged recalculation of all values in V for every node in H and

4. Background I O and network sharing overhead which varies according to the amount of real world data capture and the size of H.

It is the third component that is most characteristic of T. For our benchmark proxy the calculation requirements for each of the s values is 

For V the calculation times are highly varied. The cash variable requires 1 the futures each require 3 1 lookup 1 and the alternate probability 1 1 . Each of the input parameters involves 1 to calculate and similarly for the interest rate values. The curve selectors and or slope intercept pairs require 2 . Each of the 32 variables values takes 1 to store in the state vector. Thus the total recurring time to populate one state vector for our benchmark is 1 1 INT 4 3 FLOP 1 INT 4 1 FLOP 4 1 FLOP 8 1 FLOP 10 2 FLOP 1 1 INT 1 FLOP 32 1 INT 38 INT 4 FLOP 49 FLOP 53 FLOP. The total FLOPs F 3 to populate all nodes is then 3 53 53 1053

We take as a baseline reference CPU a typical mid range stand alone workstation with a computational throughput of R 10 FLoating point Operations Per Second 10 M. Then 3 53 10 10 sec 5300 seconds 1.5 hours.

To calculate the pre computation time for the various pricing model parametric hypercubes we require the characteristic computation time for typical models and a choice of granularity in the parameter space. For Black Scholes options computing one price and the associated moments vega takes 70 one barrier or lookback option moments 1000 and one exotic derivative such as Black Derman Toy or Heath Jarrow Morton 2 000 20 000

One of each type is needed for the benchmark proxy. Assuming three parameters for each actually six is more appropriate for exotic derivatives but the hypercube becomes enormous so each point is made a vector that embeds a family of curves and assuming a granularity of 100 increments per parameter dimension then

The total computation to create a Type 1 hypercube is thus 70 10 and for a Type 2 hypercube 1000 10 and a Type 6 20000 10

Thus the amount F 2 of parametric hypercube precalculation is dominated by the exotics 2 s Type 6 s Type 2 2.1 1021 For the 10MFLOP CPU this yields 2 2100 seconds hour.

If we estimate the Instantiation time T 1 at about the same to fill all the tables some with millions of entries hour and that system overhead category 4 doubles all the other times then we have once 2 1 2 3 5 hours and recurring 2 3 3 hours. That is the entire hyperspace H of states can be repopulated daily allowing for real time aging of the portfolio.

Navigation of H is the process of evaluating pricing fully the entire portfolio over the entire scenario tree and flagging the nodes states that satisfy the client user query on both probabilities and values . The Navigation time T is thus dominated by the time required to price the portfolio which entails pricing each instrument and summing over the entire collection. Pricing an instrument involves calculating the virtual price and all moments from the proxy and transforming with known multiplier coefficients scaling a and correlation . Doing this in turn requires taking each value stored in V usually a set of input variables I combining it with the free parameters of the instrument such as X T for a Black Scholes option and calculating the relevant selection parameters used to either access a parametric hypercube or for newer instruments or open form models to calculate the final function output directly.

The process of evaluating the portfolio price for one scenario that is for one node state in the scenario tree we term mark to scenario m t s . Evaluation of the portfolio over all scenarios we term mark to landscape m t l .

One of the essential aspects of the NVSI architecture is that because the function spaces are pre computed during the populate phase the time to extract the price values is nearly independent of model complexity. Instead evaluation time is dependent on the number of parameters required to calculate for extraction of the proxy price.

Of course the cash and futures values are stored in s directly so pricing their corresponding instruments is trivial. Thus a more realistic estimate is obtained by calculating the Tfor the options all of which are comparable to each other.

For the Type 1 option the three parameters for extracting the price and any other moment are K S r T T. Each parameter requires about 1 2 FLOP to calculate for a triplet total of 4 . All of the moments are obtained with the same parameters at the same time from the parametric hypercube so the time to obtain the entire proxy price is just 4 . To value the instrument price each proxy price moment is then multiplied by a combined factor which adds 1 to the process.

Thus to price all eight moments p and 7 greeks for advanced models of an actual option instrument the amount of computation required in s F is given by 4 8 12

For a 1M 10 instrument portfolio the s to mark the entire portfolio to all scenarios for our benchmark is then 

On the reference CPU the time required is then 39 10 10 39 10seconds 1100 hours. At this point two key components of the NVSI come into play 

Recall that the client user measures performance in terms of the response to query that is on demand access to H over all scenarios. Thus we calculate an effective virtual throughput for this computation optimized for portfolio risk analysis with 100 shared reference CPUs in terms of virtual or as Risk Domain 100 39 158 250 A bounded spectrum for performance in units of G is then 5 1 

For comparison if one were to simply scale up existing instrument valuation software and run it on the best mainframe dedicated hardware supercomputer available then a similar calculation to the above would involve very different computation times for different instruments. If we assume a typical global portfolio with 1M instruments and a mix of 50 Type 1 40 Type 2 and 10 Type 6 then the total computational demand is given by 0.5 1000 0.4 70 0.1 20 000 10 10 3 10 7587 10 7587

A CRAY T3E Supercomputer has a peak burst throughput of 2.2 T and a sustainable rate which we estimate is appropriate for the I O requirements of this problem space of R 250Gs. Therefore even on a T3E the Risk Domain problem would require CRAY 3 7587 10 250 10 sec 30 000 sec 8.4 hours.

On a more affordable and obtainable for a financial institution CRAY T90 for which we estimate in this problem space a sustainable R 18 G the problem would take nearly 5 days and on a conventional high end business mainframe about 100 days.

The most straightforward measure of price performance is simply cost R. The projected cost of version 1.0 of NVSI is 10M. The cost of a fully populated CRAY T90 is 20M and a fully populated CRAY T3E 100M. Thus NVSI 10 10 250 10 0.00004 0.004 90 20 10 18 10 0.00111 3 100 10 250 10 0.00040 Therefore the NVSI is about 10 times more cost effective than the only other machine that can solve the problem in reasonable time. And this does not even include the extended costs for the CRAY machine of software development including staff machine room support depreciation.

In contrast all of our NVSI calculations have been conservative in MassRAM use problem complexity structural requirements estimated calculation times and network sharing resources.

With these factors considered we project that the cost of an NVSI implementation will be 50 100 times less than any comparable hardware based commercial solution.

