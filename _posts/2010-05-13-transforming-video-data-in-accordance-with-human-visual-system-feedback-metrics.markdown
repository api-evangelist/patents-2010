---

title: Transforming video data in accordance with human visual system feedback metrics
abstract: In general, techniques are described for transforming video data in accordance with human visual system feedback metrics. For example, an apparatus comprising a transformation module, a parameter discovery module and a human visual system (HVS) feedback module implements these techniques. The parameter discovery module configures the transformation module to generate three-dimensional (3D) video data in accordance with parameters defining capabilities supported by a 3D display device. The transformation module transforms video data to generate the 3D video data. The HVS feedback module determines, while the transformation module transforms the video data, one or more metrics using an HVS model that reflects a quality of 3D visualization of the generated 3D video data with respect to a human visual system and reconfigures the one or more modules based on the determined one or more metrics to refine the generation of the 3D video data.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08629899&OS=08629899&RS=08629899
owner: QUALCOMM Incorporated
number: 08629899
owner_city: San Diego
owner_country: US
publication_date: 20100513
---
This application claims the benefit of U.S. Provisional Application No. 61 231 925 filed Aug. 6 2009.

 Transforming Video Data in Accordance with Three Dimensional Input Formats filed concurrently herewith assigned to the assignee hereof and expressly incorporated by reference herein 

 Preparing Video Data in Accordance with a Wireless Display Protocol having filed concurrently herewith assigned to the assignee hereof and expressly incorporated by reference herein and

 Encapsulating Three Dimensional Video Data in Accordance with Transport Protocols having filed concurrently herewith assigned to the assignee hereof and expressly incorporated by reference herein

The disclosure relates to video data processing and more particularly delivery of video data for presentation to viewers.

Video display devices present video data for viewing by a user. Typically the video data presented by a display device comprises a sequential series of video frames that are intended for playback at a given rate e.g. 29.97 frames per second as set forth in National Television Systems Committee NTSC standards. As this video data does not contain any depth information the video data is characterized as two dimensional 2D video data. Display devices that present this 2D video data are often referred to as 2D displays. 

Currently three dimensional 3D display devices are being developed to present three dimensional 3D video data. These so called 3D displays may require additional viewing accessories such as shutter glasses polarized glasses or bi color glasses e.g. glasses with one red and one green colored lens to properly view the presented 3D video data. 3D display devices that require additional viewing accessories are often referred to as stereoscopic 3D displays. Other 3D display devices referred to as auto stereoscopic 3D displays are capable of presenting 3D video data that is viewable by viewers without the need for any additional viewing accessories.

Whether stereoscopic or auto stereoscopic 3D displays of different manufacturers typically require 3D video data that complies with a vendor or manufacturer specific input file format. For example one proposed 3D video data format comprises 2D video data plus depth information and is referred to as 2D plus depth. A 2D plus depth display device may only present 3D video data provided in the 2D plus depth 3D video data format. Other types of 3D displays may require 3D video data in a multi view 2D stream format. The multi view 2D stream format packages a plurality of 2D streams where the 2D streams are each acquired from the same scene with different capture elements e.g. cameras at the same time ideally synchronized . As a result of these different and typically proprietary 3D video data formats a given 3D display device from one manufacturer may only present 3D video data that is formatted in accordance with that manufacturer s proprietary 3D video data format.

In general techniques are described for enabling cross platform three dimensional 3D video data playback. The term platform generally refers to the software and hardware computing framework of a particular video display device and or any supporting devices such as an audio video receiver and the limitations and functionality of this framework with respect to video decoding and playback. In various aspects the techniques may transform video data whether two dimensional 2D or 3D video data into 3D video data in a manner that enables playback on different 3D video playback platforms. In this respect the techniques may in various aspects promote cross platform 3D video playback.

In one aspect a method comprises configuring a video processing device to generate three dimensional 3D video data in accordance with parameters defining capabilities supported by a 3D display device and transforming with the configured video processing device video data to generate the 3D video data. The method further comprises forwarding the 3D video data to the 3D display device while transforming the video data determining one or more metrics using a Human Visual System HVS model that reflects a quality of 3D visualization of the generated 3D video data with respect to a human visual system and while transforming the video data reconfiguring the one or more modules based on the determined one or more metrics to refine the generation of the 3D video data.

In another aspect an apparatus comprises a transformation module and a parameter discovery module that configures the transformation module to generate three dimensional 3D video data in accordance with parameters defining capabilities supported by a 3D display device wherein the transformation module transforms video data to generate the 3D video data. The apparatus also includes at least one interface that forwards the 3D video data to the 3D display device and a human visual system HVS feedback module that determines while the transformation module transforms the video data one or more metrics using an HVS model that reflects a quality of 3D visualization of the generated 3D video data with respect to a human visual system and again while the transformation module transforms the video data reconfigures the one or more modules based on the determined one or more metrics to refine the generation of the 3D video data.

In another aspect a computer readable storage medium comprises instructions that cause a processor to configure a video processing device to generate three dimensional 3D video data in accordance with parameters defining capabilities supported by a 3D display device transform with the configured video processing device video data to generate the 3D video data forward the 3D video data to the 3D display device while transforming the video data determine one or more metrics using a Human Visual System HVS model that reflects a quality of 3D visualization of the generated 3D video data with respect to a human visual system and while transforming the video data reconfigure the one or more modules based on the determined one or more metrics to refine the generation of the 3D video data.

In another aspect an apparatus comprises means for configuring a video processing device to generate three dimensional 3D video data in accordance with parameters defining capabilities supported by a 3D display device means for transforming with the configured video processing device video data to generate the 3D video data means for forwarding the 3D video data to the 3D display device means for determining while transforming the video data one or more metrics using a Human Visual System HVS model that reflects a quality of 3D visualization of the generated 3D video data with respect to a human visual system and means for reconfiguring while transforming the video data the one or more modules based on the determined one or more metrics to refine the generation of the 3D video data.

The details of one or more aspects of the techniques are set forth in the accompanying drawings and the description below. Other features objects and advantages of the techniques will be apparent from the description and drawings and from the claims.

This disclosure is directed to techniques that promote cross platform three dimensional 3D video playback. The term platform generally refers to the software and hardware computing framework of a particular video display device and or any supporting devices such as an audio video receiver and the limitations and functionality of this framework with respect to video decoding and playback. A two dimensional 2D display device typically provides a platform by which to receive decode and present video data formatted in accordance with Moving Pictures Experts Group MPEG standard part two which is commonly known as MPEG 2. Other hybrid display devices capable of presenting both 2D and three dimensional 3D video data may provide a hybrid platform capable of receiving decoding and presenting video data formatted in accordance with the MPEG 2 standard and a particular 3D video data format such as a proprietary manufacturer specific format. Examples of proprietary formats include a 2D plus depth format which may be referred to as a 2D plus z format where the z stands for depth a 2D plus depth occlusion and global effects in which transparency is a particular type of global effect and a multiview 2D stream format. A 3D only display device typically provides a platform by which to receive decode and present 3D video data formatted in accordance with one of the manufacture specific 3D formats.

The techniques promote cross platform 3D video playback by enabling 3D video playback over a number of different 3D video platforms. Currently no one manufacturer specific proprietary or even open source or other free 3D video formats have been standardized or gained industry wide acceptance. Instead manufacturers associated with each of these various formats are attempting to promote such standardization in the marketplace. Moreover due to the competition between different formats none of these 3D video formats support cross platform 3D video playback. Cross platform 3D video playback generally refers to the ability of one platform to playback 3D video data formatted for a different platform. Consequently 3D video data formatted in one format typically cannot be displayed by a 3D display device providing a platform that receives decodes and presents 3D video data formatted in another different format. In this respect the techniques of this disclosure may promote cross platform 3D video playback by transforming 3D video data from one format to another format.

Cross platform 3D video playback also involves transforming 2D video data to 3D video data for playback on a 3D display device. The term cross platform therefore also may include 2D platforms as well as 3D platforms where the techniques may comprise receiving 2D video data and transforming this 2D video data to 3D video data formatted in accordance with a 3D video data format supported by a particular 3D display device or hybrid 2D 3D display device. A hybrid 2D 3D display device comprises a display device capable of both 2D and 3D video decoding and playback.

The techniques may promote cross platform 3D video playback in a number of aspects. In one aspect the techniques may facilitate video display interrogation to determine a 3D video data format supported by a video display device under interrogation. This interrogation may occur automatically or without user input other than initial user input to select one or more 3D display devices for interrogation. After automatically determining this 3D video data format of a 3D display device the techniques may involve automatically e.g. without any user input configuring one or more modules that convert 2D video data into 3D video data so as to generate 3D video data in accordance with the automatically determined 3D video data format. The configured modules then receive 2D video data and transform or otherwise convert this 2D video data into 3D video data that complies with the automatically determined input format of the display device. This 3D video data is sent to the 3D display device which proceeds to decode and present this 3D video data for viewing by a user.

In another aspect the techniques may promote cross platform 3D video playback by reconfiguration of one or more of the modules that transform the video data to 3D video data based on monitored parameters that reflect a quality of 3D visualization of the 3D video data during playback of the 3D video data by the particular 3D display device. This reconfiguration may occur dynamically during the transformation of the video data. The reconfiguration of the modules refines the generation of the 3D video data to dynamically improve the quality of 3D visualization produced by the 3D display device. The reconfiguration of the modules may also serve the purpose of reducing the processing complexity for an acceptable 3D video quality. The processing complexity may be reduced by disabling the execution of some of the modules or selecting simpler processes to perform the same functionality according to the reconfiguration parameters. A simpler process may induce a reduced 3D video quality that should still be considered acceptable according to the user defined criteria. Reducing the processing complexity may reduce power consumption or may speed up the execution of the functionality of the modules. Notably the monitoring of the parameters and reconfiguration of the modules used to transform the video data either 2D or 3D video data into 3D video data formatted for a specific 3D display device may occur in near real time or while streaming the 3D video data to the 3D display device.

In another aspect the techniques may promote cross platform 3D video playback for a certain class of 3D display devices that provide wireless interfaces. A device that implements this aspect of the techniques may comprise a first module to store video data and a wireless display host module that determines one or more display parameters of a display device external from the portable device. These display parameters may comprise a display resolution of the display device file formats supported by the display device video data encoder decoder techniques so called video codecs supported by the display device audio codecs supported by the display device whether the display device supports 3D video data playback and other parameters concerning capabilities or additional aspects of the display device. The device that implements this aspect of the techniques also may include a third module that prepares the video data to generate video data for playback on the external display device based on the determined display parameters and a wireless module that wirelessly transmits the 3D video data to the external display device.

Notably various aspects of the techniques may be implemented by a portable device including a wireless cellular handset which is often referred to as a cellular or mobile phone. Other portable devices that may implement the various aspects of the techniques include so called smart phones extremely portable computing devices referred to as netbooks laptop computers portable media players PMPs and personal digital assistants PDAs . The techniques may also be implemented by generally non portable devices such as desktop computers set top boxes STBs workstations video playback devices e.g. a digital video disc or DVD player 2D display devices and 3D display devices. Thus while described in this disclosure with respect to a mobile or portable device the various aspects of the techniques may be implemented by any computing device capable of receiving and forwarding video data to an external display device.

Display device generally represents any device capable of video playback via a display. Display device may comprise a television TV display which may be referred to as a 2D video display device a 3D video display device or a hybrid 2D 3D video display device depending on whether display device supports 2D 3D or a combination of 2D and 3D video data playback. Display device may alternatively comprise any other device with a display such as a laptop a personal media player PMP a desktop computer a workstation a PDA and a portable digital media player such as a portable DVD player . For purposes of illustration display device is assumed to represent a wireless television that communicates with mobile device wirelessly. The techniques of this disclosure should not however be limited to wireless televisions.

Source device includes a storage module that stores one or more of 2D video content and 3D video content . Storage module may comprise memory either volatile or non volatile memory including random access memory RAM static RAM SRAM dynamic RAM DRAM Flash memory read only memory ROM programmable ROM PROM erasable PROM EPROM and electrically erasable PROM EEPROM . Storage module may alternatively comprise a storage device such as a hard drive an optical drive a tape drive and a disk drive. Storage module may in some instances comprise a combination of one or more memories and one or more storage devices.

2D video content represents 2D video data formatted in accordance with a particular 2D video data file format. An example 2D video data file format may include an MP4 file format defined by a Moving Pictures Expert Group MPEG 4 part 14. The MP4 file format is a container file format that is typically used to store digital audio and digital video streams. Other container file formats comprise a simplified version of the MP4 file format referred to as 3GP an Advanced Systems Format ASF an Advanced Video Interleave AVI file format a DivX Media Format DMF an Enhanced Video Object EVO file format and a Flash video file format. File formats may in this aspect or other aspects also refer to file formats used with respect to particular transport and or application layer protocols such as a Real time Transport Protocol RTP and a Stream Control Transmission Protocol SCTP .

3D video content represents coded 3D video data formatted in accordance with a particular 3D video data file format. Exemplary 3D video data formats comprise a 2D plus depth which is commonly referred to as 2D plus z format a 2D plus depth occlusion and global effects or multiview 2D stream file format. In this respect a video data file format generally refers to a standard way of encapsulating video data whether encoded or not. Thus a video data file format may define a way of generally encapsulating video data or portions thereof to facilitate the storage and or transmission of the video data. The video data may be encoded using a variety of codecs such as MPEG 2 or any other codec including those defined in the International Telecommunication Union Standardization Sector ITU T H.264 MPEG 4 Part 10 Advanced Video Coding AVC standard hereinafter H.264 MPEG 4 AVC standard . The video data may also be encoded in some instances using a codec referred to as H.265 or by its moniker next generation video coding NGVC which is under development by the ITU T Video Coding Experts Group VCEG . The term video content is used in this disclosure to refer to coded video data encapsulated in accordance with a particular video data file format.

While not shown in for ease of illustration source device may include additional modules such as a video capture module for capturing 2D video content and 3D video content . Alternatively source device may act as an archive or repository for storing content . In some instances source device may receive content wirelessly via an interface included within source device . That is source device includes interface by which to communicate wirelessly with external devices. In some instances these external devices may interface with source device via interface to store content to storage module .

Display device which as noted above may represent a wireless television display for illustrative purposes includes an interface a file format module a video decoder and a display . Interface similar to interface represents an interface by which devices external to display device may communicate with display device . In this example it is assumed that each of interfaces and represents wireless interfaces. File format module represents a hardware or combination of hardware and software module that implements one or more of the file formats described above. Typically file format module performs decapsulation to remove file format headers that encapsulate the coded video data and thereby output the coded video data.

Video decoder may represent a hardware or combination hardware and software module that implements one or more video codecs for decoding the coded video data. Notably the term codec is used regardless of whether video decoder implements both the encoding i.e. compression and the decoding i.e. decompression aspects of a given codec. Hence video decoder may be construed to implement a codec by implementing only the decompression aspects of that codec. In this respect video decoder may implement codecs whether or not video decoder implements both the compression and decompression aspects of the codec or only the decompression aspects of the codec.

Although not shown in the example of display device may also include other modules that represent hardware or a combination of hardware and software that implement one or more audio codecs. In some instances the video and audio modules may be combined in the same module which is commonly referred to as an audio video A V decoder. Display may comprise any type of display including a organic light emitting diode OLED display a light emitting diode LED display a plasma display and a cathode ray tube CRT display.

In the example of mobile device also includes one or more interfaces that are substantially similar to interfaces and of respective source and display devices and . Mobile device also includes a control unit that implements one or more aspects of the techniques described in this disclosure. Control unit may comprise one or more processors not shown in that execute software instructions such as those used to define a software or computer program stored to a computer readable storage medium again not shown in such as a storage device e.g. a disk drive or an optical drive or memory e.g. a Flash memory random access memory or RAM or any other type of volatile or non volatile memory that stores instructions e.g. in the form of a computer program or other executable to cause a programmable processor to perform the techniques described in this disclosure. Alternatively control unit may comprise dedicated hardware such as one or more integrated circuits one or more Application Specific Integrated Circuits ASICs one or more Application Specific Special Processors ASSPs one or more Field Programmable Gate Arrays FPGAs or any combination of the foregoing examples of dedicated hardware for performing the techniques described in this disclosure.

Control unit includes a transformation module a parameter discovery module and a human visual system HVS feedback module HVS feedback module . Transformation module represents one or more configurable hardware modules or a combination of one or more hardware and software configurable modules that transform either one or both of 2D video content and 3D video content to video data acceptable to or compatible with display device in accordance with various aspects of the techniques described in this disclosure. Video data may be compatible with display device when the resulting transformed video data is encoded in accordance with a video codec supported by video decoder and formatted in accordance with a file format supported by file format module .

Parameter discovery module represents one or more hardware modules some of which may execute software that interface with display device to discover one or more parameters of display device including one or more file formats supported by display device in accordance with one or more aspects of the techniques described in this disclosure. These formats typically comprise one or more file formats supported by file format module of display device . Often formats imply one or more types of codecs supported by video decoder and therefore parameter discovery module may also discover a particular codec supported by video decoder .

In addition to file formats parameters may include supported resolutions a current resolution supported contrast a current contrast a display or screen size audio and video codecs supported by display device a list of interfaces a current sharpness a supported sharpness a supported color temperature a current color temperature a supported brightness a current brightness supported display format a current display format supported color settings current color settings supported input formats a display type a display manufacturer a supported depth range a current depth range a supported locations for the convergence plane a current location of the convergence plane a supported degree of smoothness of background objects a current smoothness of background objects a supported eye distance configuration a current eye distance configuration a supported dominant eye configuration a current dominant eye configuration a supported number of views a current number of views a supported viewing distance a current viewing distance a supported viewing angle a current viewing angle a supported display location of the 3D video content within a screen and a current location of the 3D video content within a screen and any other parameters related to display device and its capabilities.

In any event parameter discovery module discovers formats which may include both file formats and coding formats or techniques such as supported codecs. For example discovering the 3D file format referred to as 2D plus depth implies that video decoder supports a 3D video codec capable of decoding the coded 2D video data portion of 2D plus depth file format coded in accordance for example with the MPEG 2 standard while also decoding the depth portion of the 2D plus depth file format to render 3D video data for playback via display device .

HVS feedback module represents one or more hardware modules or a combination of hardware and software modules that analyze qualitative and quantitative metrics concerning display of 3D video data by display device . HVS feedback module may analyze qualitative and quantitative metrics and then reconfigure transformation module to improve transformation of the input video data e.g. 2D video content or 3D video content to video data that complies with an input format discovered by parameter discovery module in accordance with various aspects of the techniques described in this disclosure. HVS feedback module may interface with display device to retrieve video data output by video decoder of display device which it then analyzes to determine these qualitative and quantitative metrics. Through this reconfiguration of transformation module HVS feedback module may promote better quality 3D video data playback.

In accordance with one aspect of the techniques described in this disclosure mobile device automatically determines a 3D input file format of a 3D display device and transforms 2D video content into 3D video data so as to comply with the 3D input file format. For purposes of illustration it is assumed that display device comprises a hybrid 2D 3D wireless television capable of presenting both 2D and 3D video content and . Parameter discovery module of mobile device may interface with display device via wireless communication channel to interrogate file format module in order to determine at least one input file format supported by file format module to receive 3D video data. As described above file formats may comprise one or more 3D file formats such as 2D plus depth or multiview 2D stream format as well as one or more codecs implemented by video decoder to decode 3D video data which may be implied by the determined file format.

Notably display device may implement a first file format while other display devices not shown in may implement a plurality of different file formats different from the first file format. The plurality of different file formats may prevent cross platform playback of 2D video data but more particularly of 3D video data as 3D video file formats have not been standardized or generally adopted within the industry. By discovering the display file format and then transforming any input or source video data into the discovered file format may overcome cross platform playback issues that may arise when attempting to decode and present 3D video data formatted in a first file format on a display device that accepts a second different file format.

Initially a user or other operator of mobile device may interface with a user interface presented by user interface module not shown in for ease of illustration to select or otherwise discover display device . The user interface may present a list of devices located within a given range of mobile device or connected to a network such as an 802.11x wireless network a ultra wideband UWB network and or a Bluetooth network to which mobile device is likewise connected. The operator may interface with the user interface to select a device such as display device whereupon the user interface module informs parameter discovery module of the selected display device .

While manual selection may occur in this manner parameter discovery module may automatically interface with each device in a list to discover parameters such as a file form for each device on the list. The user interface may then present the list and receive input from the operator or other user selecting one of the devices. In other words parameter discovery module may automatically discover parameters including formats in response to a user selecting a given device on a list of devices to which mobile device may interface or without waiting for such selection. The discovery may be automatic in the sense that the discovery occurs without any further user input. Parameter discovery module may interface with display device for example interrogate file format module and receive formats supported by display device as one of parameters without requiring any user input.

Parameter discovery module interfaces with display device via an appropriate one of interfaces that is coupled to interface of display device via communication channel . Parameter discovery module may implement an interrogation protocol by which to communicate with file format module . For example parameter discovery module may implement various aspects of a display interface protocol or standard such as a high definition multimedia interface HDMI protocol or wireless HDMI WHDMI protocol that provides a communication channel by which to determine various parameters which may be characterized as capabilities and characteristics of display device . While no specific standard has yet been set forth that defines WHDMI looking to the wired version of HDMI as set forth in a specification entitled High Definition Multimedia Interface Specification Version 1.3A dated Nov. 10 2006 which is hereby incorporated by reference in its entirety there exists a display data channel DDC by which to interrogate or otherwise determine parameters of display device including supported file formats.

While described in this disclosure with respect to HDMI and WHDMI the techniques may be implemented in accordance to open standards such as non proprietary standards as well as other specifications not specifically supported or as of yet incorporated into any standard either proprietary or open. For example parameter discovery module may implement a mobile display digital interface MDDI which is an interface defined in an open standard supported by the Video Electronics Standards Association. Alternatively or in conjunction with MDDI parameter discovery module may implement the wireless extension to MDDI referred to as wireless MDDI WMDDI . Moreover the techniques may be implemented with respect to a mobile industry processor interface MIPI . The techniques should not therefore be limited in this respect to any one standard interface such as HDMI but may be implemented with respect to

After discovering formats in this manner parameter discovery module then interfaces with transformation module to configure transformation module which as described above represents one or more modules that convert both 2D and 3D video data into 3D video data that complies with determined input format . Parameter discovery module may determine configuration data and load this configuration data into transformation module so as to configure transformation module to transform video content such as 2D video content into 3D video content formatted in accordance with input format .

The user interface described above but not shown in to ease illustration may also present another list of devices which may be referred to as a source list to differentiate this list from the display list which may be characterized as a destination list that includes sources for video content. Mobile device may discover these devices via interfaces similar to discovering the devices of the destination list. Mobile device as one example discovers source device via interfaces and communication channel where the user interface presents a source list that included source device . Assuming the operator selects source device control unit may instruct interfaces to interface with source device to determine content and stored to storage module of source device . The user interface of control unit may then present filenames images or other data representative of determined 2D video content and 3D video content . The operator may select either 2D video content or 3D video content for display on the selected destination device e.g. display device .

Assuming the operator selects 2D video content for display on the selected display device in 3D transformation module begins receiving 2D video content which may be formatted in one of the above describe 2D file formats and coded in accordance with a first codec. Configured transformation module may dynamically convert possibly in real time or near real time 2D video content which comprises file formatted and coded video data into 3D video data that is coded and formatted in a file format that complies with automatically determined input format . Transformation module then forwards the dynamically generated 3D video data that is coded and formatted in a file format that complies with automatically determined input format which is shown in as 3D video content to display device via communication channel and interfaces .

File format module of display device receives 3D video content that comprises 3D video data coded in accordance with a codec supported by video decoder and then formatted in accordance with a 3D file format supported by file format module . File format module due to the compliant file formatting decapsulates 3D video content to generate coded 3D video data . Video decoder then decodes coded 3D video data using the appropriate codec to generate 3D video data which display presents for consumption by a viewer e.g. the operator of mobile device .

In this manner this aspect of the techniques facilitate cross platform 3D video playback by dynamically configuring transformation module to generate 3D video data from 2D video content such that the 3D video data complies with a format supported by display device . Often such 2D video content may be limited to 2D platforms and while it may be assumed that hybrid 2D 3D display device could utilize the 2D portion of the hybrid 2D 3D platform to present 2D video content the viewer would be denied a 3D viewing experience without the intervening or intermediate transformation capabilities of mobile device performed in accordance with the techniques described in this disclosure. In this respect the techniques of this disclosure enable mobile device to promote cross platform video playback by facilitating the playback of 2D video content on a 3D video playback platform.

While transformation module transforms 2D video content to generate 3D video data in the format supported by display device shown in as 3D video content and forwards the transformed 3D video content to display device HVS feedback module may determine one or more metrics using an HVS model that reflect a quality of 3D visualization of the generated 3D video content with respect to a human visual system. More particularly in some examples HVS feedback module interfaces with display device to determine 3D video data and analyzes 3D video data using the HVS model to determine the one or more metrics.

The one or more metrics may comprise a size and or a depth metric for each of one or more objects depicted by 3D video data where in some instances the depth metric may be expressed as a function of time a shadow metric for each of one or more shadows depicted by 3D video data a background contrast metric a sharpness metric a spatial frequency metric as well as a wide variety of other metrics each of which is described in more detail below. In addition HVS feedback module may interface with parameter discovery module to receive one or more parameters determined by parameter discovery module such as supported resolutions a current resolution supported contrast a current contrast a display or screen size file formats or any other parameters related to display device and its capabilities. HVS feedback module may base the reconfiguration of transformation module at least partially on these parameters .

HVS feedback module may also store user preference data that defines one or more user preferences such as a desired sharpness a desired contrast a desired display format a desired color temperature desired color settings a desired brightness a desired maximum depth range a desired location of a convergence plane a desired degree of smoothness of background objects a desired eye distance configuration a desired dominant eye configuration a desired number of views a desired viewing distance a desired viewing angle a desired display location and resolution of the 3D video content within the screen or any other preference related to 2D or 3D visual display by a display device such as display device . HVS feedback module may again also reconfigure transformation module based at least partially on these user preferences. That is HVS feedback module may comprise an HVS model which is configured in accordance with the user preference data so as generate configuration data used to reconfigure transformation module based at least partially on these user preferences.

In this way HVS feedback module interfaces with transformation module while transformation module is currently transforming 2D video content or in alternative instances 3D video content to reconfigure transformation module based at least on the metrics determined through analysis of 3D video data to refine the generation of 3D video content . In this respect HVS feedback module may dynamically reconfigure transformation module to automatically refine and likely improve the perceived visual qualities of 3D video content . If such refinement is based on user preference data in addition to the metrics determined from 3D video data HVS feedback module may automatically reconfigure transformation module to refine the perceived quality of 3D video content for a specific user or on a per user basis. Accordingly the techniques may not only facilitate cross platform video playback but also promote dynamic refinement of any transformation required for cross platform video playback to improve the viewing experience potentially on a per user basis.

In some instances mobile device interfaces with display device via a wireless communication medium. Given the above assumption that display device represents a hybrid 2D 3D wireless display device mobile device interfaces with display device wirelessly. When interfacing with display device wirelessly parameter discovery module may be characterized as a wireless display WD host that determines one or more display parameters of display device . Parameter discovery module then configures transformation module in a manner such that transformation module prepares the video data so as to generate 3D video content based on the display parameters.

This preparation may involve formatting or more specifically encapsulating 3D video data in accordance with a transport protocol. The transport protocol defines encapsulation of each of the video data audio data and depth data segments in a different one of a plurality of packets. In some instances the video audio and depth data may be sequentially stored within a single packet. The transport protocol module adds metadata for enhancing playback of the 3D video content within an optional data field in a header of one of the plurality of packets. This metadata may provide hints or other indications that facilitate playback on devices having specific capabilities. The metadata may also define tips or specifications for generating or otherwise transforming the 3D video data to enable playback on different 2D or 3D display devices with specific capabilities or parameters.

The transport protocol unit that performs this preparation may comprise a real time transport protocol RTP unit comprising hardware or a combination of hardware and software which is an application layer and is also known as a layer 7 or L7 protocol unit. The term layers refers to layers within the Open Systems Interconnection reference model OSI model . Typically transport protocols are considered to fall within the transport layer which is also referred to as layer 4 or L4 of the OSI model. RTP relies on a transport layer protocol referred to as universal datagram protocol UDP to provide an enhanced transport protocol using application layer functionality. In this sense RTP may be considered a transport protocol despite the fact that RTP resides at the application layer rather than the transport layer. The term transport protocol therefore should not be limited to transport layer protocols but may include any protocol of any layer in the OSI model that provides transport layer functionality.

After preparing 3D video content in the manner described above transformation module forwards the content to a wireless one of interfaces which transmits the packets to display device . Display device receives the packets un formats the packets to decapsulate the encoded audio encoded video and depth data as well as the metadata decodes the encoded audio and encoded video data in accordance with the metadata to generate audio and enhanced video data and presents the audio and enhanced video data via audio playback module not shown in and display for consumption by a viewer. The video data is enhanced in that the metadata may improve decoding and playback of the resulting video data that was decoded with benefit of the metadata.

In this respect the various aspects of the techniques promote a cohesive cross platform video playback system. In one aspect file formats are discovered and used to configure a transformation module to improve playback across platforms in the manner described above. In another aspect the techniques facilitate the quality of playback through dynamic feedback determined using a HVS model so as to refine during the transformation the quality of 3D video playback. In yet another aspect parameters are automatically determined and used to prepare delivery of 3D video data for playback by a 3D display device. This preparation may involve embedding metadata into packets used to transmit the 3D video content wirelessly to the display device where this metadata may define parameters that facilitate decoding and further transformation of the 3D video content. In some aspects this metadata may be embedded such that it is transparent to displays that do not support 3D video playback such as 2D only display devices.

While shown as separate devices source device and mobile device may comprise a single device that incorporates the functionality of source device and the functionality of mobile device . In some instances mobile device may incorporate the functionality of source device . In this respect the techniques should not be limited to the example shown in .

Moreover although described above with respect to a mobile device the techniques may be implemented by any device capable of video transformation consistent with the techniques described in this disclosure. Such devices may be referred to herein generally as video processing devices. Again the techniques should not be limited in this respect to the example shown in .

Mobile device also includes a video capture device a local storage module and an internal display . Video capture device represents one or more hardware modules some of which may execute software that implement a stereoscopic 3D video camera for recording 3D video data or a 2D video camera for recording 2D video data. Local storage module represents a hardware or combination of hardware and software module for storing locally data including 2D and 3D video data or if coded and formatted content. Local storage module may comprise static or dynamic memory and or storage devices such as any of those listed above with respect to control unit . Internal display represents a hardware or combination of hardware and software module that presents video data and image data for consumption by a viewer of internal display . Internal display may comprise any of the displays noted above with respect to display of display device .

Each of video capture device local storage module and internal display couple to control unit which has been illustrated in further detail in . In the example of control unit comprises transformation module parameter discovery module and HVS feedback module which is the same as shown with respect to . These modules however have been shown in further detail in the example of to include a number of sub modules and data. For example transformation module includes pre processing module 2D to 3D processing module post processing module rendering module and display format module .

While each of these modules are described in greater detail with respect to briefly pre processing module represents one or more hardware modules some of which may execute software that perform pre processing to determine information for performing conversion of video data from one platform to another platform. 2D to 3D processing module represents one or more hardware modules some of which may execute software for performing as the name suggests conversion of 2D video data to 3D video data based on the information determined by pre processing module where this information may be referred to as pre processing information. Post processing module represents one or more hardware modules some of which may execute software that modify or otherwise adjust the generated 3D video data such as a depth map to refine the quality of 3D video playback. Rendering module represents one or more hardware modules some of which may execute software that model 3D scenes generate additional views and otherwise further refine the 3D video playback. Display format module represents one or more hardware modules some of which may execute software that format the generated and subsequently refined 3D video data according to a given file format.

As another example HVS feedback module may include a qualitative evaluation module and a quantitative evaluation module . Qualitative evaluation module represents one or more hardware modules some of which may execute software that performs qualitative analysis of 3D video data decoded by a display device external from mobile device such as video decoder of external display device to determine one or more of metrics that define in part a perceived quality of a quality of 3D visualization of 3D video data with respect to a human visual system. Quantitative evaluation module performs a quantitative analysis of 3D video data generated by various sub modules of transformation module such as post processing module rendering module and display format module . This analysis may result in determining additional ones of metrics . These metrics may then form the basis for subsequent modification or re configuration of one or more of modules that form transformation module to refine 3D video content generated by transformation module . This refinement may lead to improved playback of 3D video data in terms of both qualitative and quantitative metrics .

Control unit also includes two additional modules an offline processing module and a user interface module not previously shown with respect to the example of . Offline processing module represents one or more hardware modules some of which may execute software that generally perform statistical analysis and modeling of descriptors of images or video frames with respect to properties such as a type and direction of illumination object reflectance texture effects and atmospheric effects. These descriptors may comprise descriptors that comply with an MPEG 7 standard sometimes referred to as Multimedia Content Description Interface that defines a standard for multimedia content descriptors. In any event descriptors may generally represent data defining descriptions of the visual features depicted by video data. Often descriptors describe elementary characteristics of these visual features such as the shape color texture or motion associated with these features.

User interface module represents one or more hardware modules or a combination of hardware and software modules that present a user interface with which an operator of mobile device interacts to input and receive data from mobile device . Often user interface module presents a graphical user interface GUI to internal display which may provide the GUI to the user. In some instances internal display may comprise a so called touch screen by which a user may interact with internal display to input data defining selections of various items presented by the GUI. In this sense internal display may be considered as a part of user interface module contrary to the exemplary depiction of internal display separate from user interface module in the example of .

Initially an operator of mobile device may interface with a user interface presented by user interface module to select one or more sources that includes video content that the operator desires to display via one or more destination displays. These sources may include video capture device and local storage module as well as source devices communicatively coupled to mobile device via wireless and wired interfaces A B interfaces such as source device shown in the example of which is communicatively coupled to interfaces via wireless communication channel . Destination displays may comprise internal display and one or more destination display devices communicatively coupled to interfaces such as destination display device shown in the example of that is communicatively coupled to interfaces via wireless communication channel .

After selecting one or more sources the operator may also interface with this or another user interface presented by user interface module to select the particular content stored by the selected source. For example the operator may select source device as the source of video content. In response user interface module may interface with source device to determine content and stored by source device within storage module . User interface module may then update the current user interface or present another user interface by which to display content available for playback by a destination. The operator may interact with the presented user interface to select one or more of 2D video content and 3D video content . It is assumed for purposes of illustration that the operator selects 2D video content .

After selecting 2D video content the operator may interface with the current user interface a previous user interface or another user interface to select one or more destinations. Again for purposes of illustration it is assumed that the operator interacts with this destination user interface to input data selecting display device . Upon selecting display device as the destination for the selected 2D video content stored by source device to storage module user interface module receives this data indicating the selections and instructs the appropriate interfaces to begin the receipt selected 2D video content and transfer of 3D video content generated through the transformation of 2D video content by transformation module . User interface module may also interface with parameter discovery module to indicate the selected destination to parameter discovery module so that parameter discovery module may interface with the appropriate destination to determine parameters in accordance with the techniques described in this disclosure.

The operator of mobile device may also interface with a previous current or possibly a different user interface presented by user interface module to input data specifying one or more user preferences user prefs . User preferences may include a dominate eye preference a contrast preference a sharpness preference a color hue preference a color temperature preference or any other preference related to display of video data. User preferences may also include audio preferences such as a surround sound preference and volume preference and the techniques should not be limited to video only user preferences. User interface module forwards these user preferences to HVS feedback module which may employ these preferences when configuring or re configuring transformation module in accordance with the techniques described in this disclosure.

In any event after selecting source device and destination display device and potentially one or more of content and stored to source device parameter discovery module may interface with the selected destination e.g. display device to determine parameters . Parameter discovery module may for example interface with display device via a wired interface that complies with the HDMI standard noted. Using various aspects of the HDMI standard parameter discovery module may determine parameters such as an input file format supported by file format module for receiving 3D video content. Other parameters include those noted above related to various capabilities or parameters of display such as supported resolutions current resolution display size supported color temperatures current color temperature or any other parameter related to display and video decoding including codecs supported by video decoder .

As described in more detail below parameter discovery module may also be characterized as a wireless display WD host that hosts a session by which these parameters are discovered wirelessly. Often when implementing these wireless display host techniques parameter discovery module implements a form of HDMI referred to as wireless HDMI or WHDMI. 

After discovering parameters parameter discovery module may interface with display format module of transformation module to configure display format module so as to properly format 3D video data in accordance with a file format supported by file format module of selected destination display device . Parameter discovery module may perform both the discovery of parameters which includes the input file format and the configuration of display format module automatically.

Use of the term automatically as noted above typically indicates that no operator intervention is required to perform those actions denoted as occurring automatically. Use of this term however is not meant to suggest that the operator or user input may not be required to initiate the noted automatic operations. To illustrate consider the above example in which the operator interacts with the various user interfaces to select a source content and a device. The interaction with the user interfaces does not detract from the automatic nature of determining parameters and display format module . The operator may be unaware of these automatic operations as the user in the above example is not required to provide any input or data specifying that parameter discovery module determine these parameters and then configure display format module . In this respect automatic operations may be construed to occur transparently from the operator s perspective inasmuch that the operator has not actively indicated that these operations are to be performed.

Parameter discovery module may configure display format module via an application programming interface API call by which parameter discovery module may specify the determined input file format of display device . Prior to receiving 2D video content HVS feedback module may also interface with pre processing module 2D to 3D processing module post processing module rendering module and display format module to configure these modules based on user preferences . For example one of user preferences may define a preferred sharpness which HVS feedback module may utilize when configuring 2D to 3D processing module . HVS feedback module may utilize the preferred sharpness as an indication of the accuracy at which 2D to 3D processing module computes depth values at sharp image discontinuities. In another example one of user preferences may define a preferred location of the convergence plane or a preferred depth range which HVS feedback module may utilize when configuring rendering module . HVS feedback module may utilize the preferred convergence plane location or depth range to adjust the rendering parameters of module .

In any event both parameter discovery module and HVS feedback module may configure one or more of modules of transformation module . Once configured transformation module may receive 2D video content which may comprise single view or multi view 2D video data. Single view 2D video data may comprise a single view shot from a single video capture device. Multi view 2D video data may comprise multiple views shot from multiple video capture devices. Typically multi view 2D video data enables playback of any one of the multiple views and often the viewer can switch between the multiple views during playback however a 2D display device typically does not present two or more of the multiple views simultaneous to one another.

Pre processing module typically receives 2D video content . Offline processing module does not receive the 2D video content but rather supports pre processing module in the manner described below with respect to . Briefly offline processing module typically implements various algorithms by which to as noted above perform statistical analysis and modeling to generate models for use by pre processing module .

Pre processing module receives 2D video content and determines global information concerning the various images or frames that form 2D video content . This global information may pertain to a single image or frame or several images or frames of 2D video content . For example the global information may define atmospheric effects e.g. information indicating the presence of rain snow wind etc. . Pre processing module may also determine local information for a given image or frame of 2D video content. Such local processing to determine the local information may involve determining information relating to the location and intensity of edges classification of edges segmentation of objects detection of illumination properties and detection of regions of interest. After pre processing and determining the global and local information pre processing module forwards 2D video content and the global and local information to 2D to 3D processing module .

2D to 3D processing module processes the received 2D video content to extract depth information. 2D to 3D processing module may extract depth information based on the global and local information determined by pre processing module . For example 2D to 3D processing module may extract depth information from geometric linear information such as edge information defined by the information determined by pre processing module . 2D to 3D processing module may implement a number of depth extraction algorithms to for example extract depth information or values from geometric structure and motion focus defocus shading and shadow and the above noted geometric linear information. 2D to 3D processing module may merge the depths extracted via one or more of these algorithms to generate a depth map. The depth map assigns a depth value to each pixel of every image or frame of 2D video content thereby generating 3D video data which it forwards to post processing module .

Post processing module receives this 3D video data generated by 2D to 3D processing module and modifies the 3D video data to refine the depth map. For example post processing module may modify the depth map to improve a quality of visualization of the resulting transformed 3D video content when displayed via display device . This modification may involve globally smoothing the depth map or selectively modifying the depth information for certain regions of interest in a frame that corresponds to the depth map. After refining the depth map in this manner post processing module forwards the refined 3D video data to rendering module .

Rendering module receives the refined 3D video data and models 3D scenes for instances where one or more views are required such as is the case when the determined input format for display device is a multi view streams format or where display device otherwise supports multiple 2D plus depth z views. After this view rendering rendering module forwards the potentially multi view 3D video data to display format module which proceeds to format the multi view 3D video data in accordance with the configured input file format supported by display device to generate transformed 3D video content . Display format module then forwards transformed 3D video content to display device for presentation to a viewer.

While transforming 2D video content in the manner described above HVS feedback module interfaces with display device to retrieve 3D video data output by video decoder of display device . HVS feedback module then performs qualitative and quantitative evaluation of decoded 3D video data . More specifically qualitative evaluation module performs the qualitative evaluation related to a quality of visualization of decoded 3D video data to determine one or more of metrics which may be referred to as qualitative metrics . This quality of visualization may also include comfort of a given viewer as defined by user preferences when viewing 3D video data . To illustrate consider that a right eye dominant viewer favors his right eye when viewing 3D video data . In this case qualitative evaluation module may analyze 3D video data to ensure that the right eye stream is favored over a left eye stream. That is if the viewer is right eye dominant qualitative evaluation module may weight right eye stream metrics over left eye stream metrics when determining an overall metric for a given frame or group of pictures.

Quantitative evaluation module may perform quantitative analysis of 3D video data to determine one or more of metrics which may be referred to as quantitative metrics . For example quantitative evaluation module may evaluate a relationship between a depth and a size of an object in one or more frames of 3D video data generated by one or more of post processing module rendering module and display format module . Quantitative metrics may include this relationship between the depth and size of the object as one metric. Other quantitative metrics may include those related to depth image based rendering such as a filter efficacy metric a visible discontinuity metric and an interpolation efficacy metric. Quantitative metrics may also include those useful in normalization of depth. Quantitative metrics may therefore include at least one metric related to a relationship between a depth and a size of an object defined by the 3D video data a relationship between a depth discontinuity and a mean region color discontinuity of the 3D video data a filter efficacy metric an interpolation efficacy metric metrics useful in normalization of depth and metrics related to measuring discontinuities of depth along time. Qualitative evaluation module may further measure discontinuities of depth along time of the video data generated by modules as well as perform a wide number of other forms of quantitative analysis.

Using qualitative evaluation module and or quantitative evaluation module HVS feedback module may perform a wide array of analysis. This analysis may as noted above in part involve measuring and adjusting the relationship between depth and size of any object in a given frame or image of 3D video data either generated by one or more of modules or received from display device as 3D video data . The analysis may also involve measuring discontinuities of depth along time shadow or shading effects background contrast and spatial frequency.

Based on these metrics HVS feedback module may then determine configuration data based on metrics and interface with one or more of modules of transformation module to re configure these modules so as to refine a perceived visual quality of transformed 3D video content . In addition to determining configuration data on metrics HVS feedback module may also determine configuration data based on parameters which parameter discovery module may forward to HVS feedback module . Moreover HVS feedback module may determine or otherwise tailor configuration data to accommodate user preferences . Configuration data may therefore be determined in accordance with a wide variety of metrics parameters and preferences.

As one example HVS feedback module may tailor configuration data to rectify focus cues for display devices that lack eye tracking functionality. Focus cues comprise data describing a clarity of an image across a given frame for different focal lengths. In this example parameters may indicate the lack of eye tracking functionality of display device . To rectify focus cues which may comprise cues in 3D video content by which the HVS interprets focus in this context HVS feedback module may determine configuration data for 2D to 3D processing module and or rendering module so as to constrain depth extraction performed by 2D to 3D processing module and or constrain depth interpretation for multi view rendering by rendering module .

HVS feedback module may generate this configuration data based on parameters that indicate a size of display of display device as focus may depend on a field of view and standard viewing distance both of which can be derived from the size of the display. Focus may also depend on eye separation which is typically defined as the distance between the left and right eye and user preferences may store a standard eye separation preference. HVS feedback module may therefore access both user preferences and parameters to generate configuration data so as to rectify focus miscues in display device having no eye tracking mechanism by which to provide the above feedback e.g. actual viewing distance actual eye position and actual eye separation .

If eye tracking is available through display device HVS feedback module may receive this feedback via the same interface by which HVS feedback module receives 3D video content . This mechanism may be an external device coupled to display device . Alternatively this eye tracking mechanism may be integrated within display device . The name eye tracking may be a misnomer in some examples as the mechanism need not track eye movements but may more generally track a position of each user in which case viewing distance and eye position may be derived.

In some instances HVS feedback module may generate configuration data to rectify focus cues differently for stereoscopic and auto stereoscopic displays. Whether a display device is stereoscopic a display device that requires additional viewing apparatus to properly present 3D video data such as shutter glasses or auto stereoscopic a display device that does not require additional viewing apparatus to properly present 3D video data may be determined by parameter discovery module and stored as one of parameters .

Consider a specific type of auto stereoscopic display referred to as a volumetric display which again may be indicated via one of parameters . A volumetric display may be incapable of presenting a true light field for multiple simultaneous viewpoints where each viewpoint is for a different viewer . As a result these volumetric displays usually cannot correctly present view dependent lighting such as occlusions specularities and reflections when used as auto stereoscopic displays. HVS feedback module may configure various modules of transformation module to correct a viewing position so as to improve stereo perception for a volumetric display.

Other differences between stereoscopic and auto stereoscopic displays may warrant different configurations and HVS feedback module may tailor configuration data based on these parameters . For example there are usually differences in depth perception and depth performance between auto stereoscopic and stereoscopic displays. HVS feedback module may tailor configuration data differently based on these device capabilities or parameter differences.

As noted above HVS feedback module may tailor configuration data to constrain focus cues based on different form factors of display device and more particularly the size of display . Internal display is usually of a much smaller form factor than display of external display device and these different form factors may imply differences in how stereo images are built and displayed. Moreover the form factor may indicate a potential size of a viewing audience where smaller form factors may suggest a single viewer while larger displays may suggest multiple viewers. HVS feedback module may generate configuration data so as to account for these different form factors.

Specifically HVS feedback module may in one example leverage aspects known of the HVS to generate configuration data based on the form factor. In one aspect optimal voxel distribution is dictated by the spatial and focus resolutions of the human eye. Given this known limitation of the human eye HVS feedback module determines specific spatial and focus resolutions based on the type of display stereoscopic or auto stereoscopic including different sub classes of these two types of 3D display technologies and size of the display. HVS feedback module may then generate configuration data for 2D to 3D processing module to modify depth processing.

HVS feedback module may also refine or potentially optimize spatial frequency as indicated by one of metrics based on a particular visual axis or viewing angle of the viewer with respect to display device . This visual axis or viewing angle may be provided with 3D visual data or parameter discovery module may interface with display device to discover this visual axis or viewing angle as one of parameters in that it relates to an eye or viewer tracking apparatus integrated within display device . HVS feedback module may also utilize one or more of parameters specifying a current display resolution as this display resolution may impact focus cues. HVS feedback module may then generate configuration data based on these parameters .

In another example HVS feedback module may generate configuration data to refine or potentially optimize depth image based rendering DIBR . HVS feedback module may analyze 3D video data and 3D video data generated by post processing module rendering module and display format module to refine various aspects of DIBR. In one instance HVS feedback module analyzes this video data or that generated by one or more of modules to ensure proper filtering in the depth dimension to minimize and possibly potentially eliminate visual artifacts. Often depth filtering refinement revolves around a particular display technology or capability which again is discoverable by parameter discovery module and therefore known via one or more of parameters . To illustrate consider that visible discontinuities due to non depth filtered rendering are potentially much greater in subtractive displays because these displays may make visible direct illumination from a back light. HVS feedback module may therefore generate configuration data to adjust depth filtering so as to account for different display capabilities.

HVS feedback module may also generate configuration data to refine various aspects of DIBR based on metrics related to filtering for the purpose of filling holes e.g. interpolation which may arise when adjust resolutions of 2D video content to accommodate the current resolution . The filtering may comprise Gaussian filtering or edge based filtering. HVS feedback module may also generate configuration data to refine DIBR based on metrics related to particular file formats such as a file format referred to as P3D so as to force a depth value for particular pixels to zero where occlusions are expected to occur in a given image or frame.

Also with respect to refining DIBR HVS feedback module may generate configuration data based on metrics related to shadow effects and shading so as to increase shadow effects shading in regions using visible neighbor pixels. HVS feedback module may also augment edges to refine DIBR through high frequency enhancement HFE to minimize blurring due to depth filtering which tends to smooth out edge discontinuities. Additionally HVS feedback module may refine DIBR by generating configuration data to facilitate dynamic range reshaping of depth maps whereby this configuration data expands higher depth values and compress lower depth values. In some instances HVS feedback module may modulate the range of expansion and compression based on perceptibility or sensitivity of a given viewer s HVS as defined by one of user preferences .

This DIBR refinement may also occur by way of HVS feedback module generating configuration data that adjusts DIBR based on one or more of user preferences related to eye preference or dominance. To illustrate consider that for any one view a viewer may be left or right eye dominant and the viewer may indicate this dominance as one of user preferences . Notably the viewer may undergo a test presented via a user interface to determine dominance or may merely select the dominance. In any event HVS feedback module may generate configuration data that adjusts the various views based on those of user preferences related to eye dominance sensitivity or perceivability difference such that views are fined for 20 degrees or up to 50 degrees along visual axis for good voxel resolution of up to 120 color pixel density cpd . Where appropriate HVS feedback module may generate configuration data to adopt an asymmetric quality so as to improve depth perception where this asymmetric quality denotes that a perceived quality of left and right eye view are asymmetric or not equal.

In yet another example HVS feedback module may generate configuration data to normalize depth based on one or more of user preferences related to eye sensitivity as well as metrics related to content type illumination level and the like. Normalization of depth by HVS feedback module may also depend on a max depth which is determined based on video data and metrics related to scene changes content type and constitution e.g. if there are objects that are at infinite focal length . Normalization of depth by HVS feedback module may also depend on parameters such as a display type in addition to quantitative metrics related to a rendering method.

In another example HVS feedback module may generate configuration data to refine 3D visualization so as to minimize stress e.g. eye fatigue and nausea . In some aspects HVS feedback module may generate configuration data to optimize 3D visualization so as to minimize stress e.g. eye fatigue and nausea . HVS feedback module may constrain depth to minimize stress perform selective depth extraction based on type of objects in the scene perform selective depth rendering for specific depth extraction for specific object of interest in the scene or only extract depth values above a certain confidence level. In this manner HVS feedback module may generate configuration data based on metrics parameters and user preferences and re configure one or more modules of transformation module so as to refine if not possibly improve or even optimize transformation of 2D video content to 3D video content .

Re configured modules of transformation module may then continue to transform 2D video content into 3D video content in accordance with configuration data . Transformation module may then begin to forward refined 3D video content to display device after this dynamic reconfiguration whereupon display device may present this refined 3D video content via display for consumption by one or more viewers. This feedback mechanism in which HVS feedback module may continue during the entire transformation of 2D video content to 3D video content so as to continually refine or in some aspects optimize this video data for playback on the particular platform selected by the operator e.g. display device . In this respect HVS feedback module may enable transparent form the viewers perspective real time or near real time dynamic feedback that refines and potentially improves if not optimizes 3D video content in a manner that tailors 3D video content for playback on a specifically selected display device e.g. display device to a particular viewer as defined by user preferences . Set forth below is an example in which HVS feedback module generates configuration data and reconfigures various ones of modules of transformation module in accordance with configuration data . HVS feedback module may also enable transparent form the viewers perspective real time or near real time dynamic feedback that can be used to re configure modules such that computational complexity is reduced while preserving an acceptable 3D video quality according to user preferences.

Pre processing module also include local processing units A C that process input video data or for a given region of neighboring pixels within a single frame at a time which is considered localized or local processing to detect regions of interest ROI edges and contrast and or illumination respectively. In the example of these local processing units A C are shown as ROI detection unit A edge detection unit B and contrast illumination detection unit C contrast illum detection unit C . ROI detection unit A represents one or more hardware modules some of which may execute software that detect ROI such as faces or human bodies. Edge detection unit B represents one or more hardware modules some of which may execute software that locates and classifies edges e.g. classifying edges as bounding real objects as defining shadows boundaries or as defining shading effects. This classification may also involve determining an intensity of edges. Contrast illumination detection unit C represents one or more hardware modules some of which may execute software that detect illumination properties.

While not shown in for ease of illustration purposes pre processing module may include additional global or local processing units. For example pre processing module may include another local processing unit that performs segmentation of objects based on chroma components of a given image or frame the color components of the image or frame or both the chroma and color components of the image or frame. The techniques set forth in this disclosure should therefore not be limited to the example shown in but may include additional global and local processing units. The various information extracted by units A C and A C may be forwarded to 2D to 3D processing to facilitate depth extraction.

2D to 3D processing module includes a number of units to perform both multi frame and single frame depth extraction techniques. Multi frame depth extraction units include camera modeling unit A and moving object modeling unit B. Camera modeling unit A represents one or more hardware modules some of which may execute software that models geometric structure and motion e.g. models a camera capturing geometric structure and motion to extract depth values. Moving object modeling unit B represents one or more hardware modules some of which may execute software that segment a background from independent moving objects to extract depth values.

Single frame extraction units include lens modeling unit A occlusion modeling unit B illumination modeling unit C and geometric modeling unit D. Lens modeling unit A represents one or more hardware modules some of which may execute software that extract depth values based on focus and defocus cues detected in a given frame or image. Occlusion modeling unit B represents one or more hardware modules some of which may execute software that model or otherwise detect occlusions between various objects in a given frame or image and extracts depth values based on these occlusions. Illumination modeling unit C represents one or more hardware modules some of which may execute software that extract depth values for a single frame or image based on detected shadows and shading effects. Geometric modeling unit D represents one or more hardware modules some of which may execute software that extract depth values for a single frame or image based on modeling of geometric linear perspectives within the single frame or image.

As noted above many of these units A B and A D rely on information extracted by units A C and A C of pre processing module . For example illumination modeling unit C may base the extraction of depth on contrast illumination information determined by contract illumination detection unit C and edge detection unit B. As another example occlusion modeling unit B may model occlusions using edge information detected by edge detection unit B. As yet another example lens modeling unit A may base depth extraction on edge information determined by edge detection unit B. As a further example geometric modeling unit D may extract depth values based on edge information determined by edge detection unit B. Camera modeling unit A and moving object modeling unit B may extract depth values based on information determined by atmospheric effects detection unit A scene change detection unit B and camera motion detection unit C. In this manner 2D to 3D processing module may extract depth values from both 2D and 3D video content and .

Notably for multi view streams transformation module may include multiple 2D to 3D processing module e.g. multiple 2D to 3D processing modules may be invoked to process each frame of a different stream that was acquired or captured at the same time from multiple capture systems. Even for single view streams transformation module may comprise multiple 2D to 3D processing modules by which to process two or more consecutive frames or images. In this single view stream instance each of the multiple 2D to 3D processing modules will extract feature points and descriptors for these points. These descriptors will then be used to set the correspondence between the feature points in different frames where the location of correspondent points will be used along with the projective geometry camera model implemented by camera modeling unit A to extract the depth values.

With respect to multi view streams where each view has been acquired at the same time with different camera units each camera located at a different point in space with the same or different camera parameters 2D to 3D processing unit may include multiple cameras modeling units similar to lens modeling unit A. Each of these lens modeling units A process a frame of a different multi view where each of these frames are captured at the same time. For single view streams 2D to 3D processing unit may invoke camera modeling unit A to process a single image or frame of the single view stream at a time.

After determining depth values using one or more of multi frame processing units A B and single frame processing units A D 2D to 3D processing module may merge depth values or otherwise integrate these depth values from various multi frame and single frame processing units A B and A D to create a composite depth map for each frame of a given view stream or in the case of multi view streams each of the view streams . The depth map assigns a depth value to each pixel of a given frame or image. With respect to the 2D plus depth file format as one example the depth map for each frame is represented as a grey scale image where each 8 bit pixel value of the grey scale image defines a depth value for a corresponding pixel in the frame.

HVS feedback module may generate configuration data to configure 2D to 3D processing module so as to constrain depth extraction by one or more of multi frame and single frame depth extraction processing units A B and A D. As one example HVS feedback module may generate configuration data to configure camera modeling unit A such that linear system of equations used to model the camera system are disambiguated and sped up by using results from other units e.g. units B and A D to constrain a set of valid corresponding points.

HVS feedback module may also generate configuration data to affect the merge of depth maps generated by multi frame and single frame processing units A B and A D. For example 2D to 3D processing module may merge these depth maps using a weighted average merge function that assigns a different weight to each of the determined or extracted depth maps. HVS feedback module may generate configuration data to modify or configure these weights to adjust the resulting composite depth map. This adjustment may improve the quality of visualization of transformed 3D video content or otherwise reduce stress associated with viewing transformed 3D video content .

After processing 2D video content in this manner 2D to 3D processing module forwards 2D video content and the generated one or more as in the case of multi view streams composite depth maps to post processing module . Post processing module includes a depth perception unit that modifies the resulting one or more depth maps to refine the quality of visualization of transformed 3D video content . In some instances depth perception unit modifies the resulting one or more depth maps to improve the quality of visualization of transformed 3D video content . Depth perception unit may introduce a unit in a pipeline of modules representative of transformation module whereby HVS feedback module may intercede to refine the depth maps. To illustrate depth perception unit may represent an interface with which HVS feedback module may interact to load configuration data such that HVS feedback module may intercede in the pipeline to perform post processing of depth maps.

In one example HVS feedback module may generate configuration data for depth perception unit so as to configure depth perception unit to perform operations that globally smooth the depth map across multiple frames or the entire 3D video data and or selectively modify depth information for certain regions of interest identified by ROI detection unit A. In another example HVS feedback module may generate configuration data based on parameters that configure depth perception unit to perform operations that modify and constrain a range of depth values such that those values are tailored for the particular display e.g. display device . In a further example HVS feedback module may generate configuration data based on metrics that configure depth perception unit to perform operations that dynamically modify the depth maps to facilitate quantitative improvement of transformed 3D video data.

After refining the one or more depth maps post processing module forwards the 2D video content and the one or more refined depth maps to rendering module . Rendering module comprises a 3D modeling unit an illumination modeling unit illum modeling unit and an interpolation unit . Each of 3D modeling unit illumination modeling unit and interpolation unit represents one or more hardware modules some of which may execute software that model various aspects of 3D scenes so as to enable generation of one or more additional views from 2D video content .

3D modeling unit may utilize depth maps from post processing module to facilitate the generation of 3D scenes and subsequent generate of one or more additional views. Illumination modeling unit may utilize illumination information determined by illumination modeling unit C and or contrast illumination detection unit C to facilitate the generation of 3D scenes and subsequent generate of one or more additional views. Interpolation unit may utilize pixel information of the 2D video data to facilitate the generation of 3D scenes and subsequent generate of one or more additional views. In this respect rendering module generates additional views so as to enable generation of 3D video content in a multi view stream format from single view 2D video content .

Display format module although not shown as comprising additional units similar to modules for ease of illustration may invoke various units to format the 3D video data received from rendering module in accordance with an input file format determined by parameter discovery module . In one example display format module invokes a color interleaving unit to interleave different views in different color components as required for example by an anaglyph file format . As another example display format module invokes a spatial interleaving unit to interleave different views in different pixel location. When multiple view are generated display format module may invoke a multi view unit that temporally multiplexes the various views for frame switched displays which may be indicated by configuration data and learned via one of parameters . In addition to display format requirements HVS feedback module may also generate configuration data based on metrics and or user preferences that configure display format module to emphasize filter or generally modify the 3D video stream.

Offline processing module may generally perform statistical analysis and modeling of certain descriptors of the various frames or images of a training set with respect to various properties of training set. The various properties may include a type and direction of illumination object reflectance texture and atmospheric effects. Offline processing module provides support to pre processing module by creating one or more models for use by the various units A C and A C of pre processing module .

Offline processing module includes a machine learning unit to perform the statistical analysis and modeling. Machine learning unit may define and model a descriptor as one example that accounts for the variation in intensity across edges that are created due to shading or a descriptor in another example that accounts for the various in intensity across edges that are caused by a shadow. In a further example machine learning unit may define and model a descriptor that accounts for reflectance properties of an object in the scene under certain illumination conditions.

Machine learning unit models the statistical behavior of each descriptor off line using a training set not shown in . This training set may consist of a set of images that are obtained under all potential variations of the properties that affect each descriptor. The model created by machine learning unit after undergoing training using the training set therefore captures the probability of the descriptor to be a certain value for a given scenario condition e.g. a given direction of the illumination . The model of the statistical behavior is then used online by the units of pre processing module . Pre processing module utilizes these models to determine the maximum likelihood that for a particular value for a given descriptor obtained from the one or more images or frames that pre processing module is currently processing the conditions of the scenes be a particular one e.g. a certain direction of illumination .

These models can be extended to not only illumination related scenarios but also atmospheric effects in the scene haze rain snow etc. or texture variations. The descriptors can also be local if they capture information within a neighborhood of pixels or global if they capture information of the whole image . The models may capture statistical behavior of the relationship between several descriptors using Markov random fields. In this respect offline processing module may support pre processing module with machine learning unit generating models for use by pre processing module . Notably machine learning unit may receive new training sets that may enable improved modeling and subsequent transformations.

User interface module may also present the same or another user interface with which the user may interact to select one or more source devices and one or more destination devices. User interface module may then receive data selecting one or more source devices e.g. source device shown in the example of and one or more destination devices e.g. external display device . Based on these selection user interface module may interface with various ones of wireless interfaces A and wired interfaces B to establish communicative links or channels and with the selected one or more source devices e.g. source device and the selected one or more destination devices e.g. external display device respectively. After ensuring that these channels and are established user interface module may then interface with parameter discovery module to initiate discovery of parameters concerning the capabilities of display device .

Parameter discovery module interfaces with display device to determine parameters via established communication channel in the manner described above . Parameter discovery module may forward these parameters to HVS feedback module which again may employ these parameters when configuring or reconfiguring transformation module . One of parameters may comprise one or more input file formats supported by external display device to accept 2D and or 3D video content. Parameter discovery module may interface with transformation module and more specifically display format module to configure display format module to format transformed 3D video data in accordance with the determine input file format . Alternatively HVS feedback module may interface with display format module to configure display format module in the manner described above with respect to parameter discovery module . HVS feedback module may also interface with other modules to initially configure these modules of transformation module based on parameters and user preferences so as to tailor generation of 3D video content for presentation by external display device in accordance with user preferences .

Once configured transformation module interfaces with source device via communication channel to retrieve 2D and or 3D video content from source device . Upon receiving for example 2D video content transformation module converts the video data of 2D video content to 3D video content which is formatted in accordance with the determined input file format . Transformation module then outputs 3D video data formatted in accordance with the determined input file format as 3D video content which mobile device forwards to display device .

Referring to after receiving at least a portion of 3D video content display device decapsulates received 3D video content by removing the input file formatting headers to generate coded 3D video data . Display device then decodes coded 3D video data to generate 3D video data which display device presents via display to one or more viewers for their consumption. While presenting this video data and while transformation module converts 2D video content to 3D video content HVS feedback module interfaces with display device to retrieve video data .

As described above HVS feedback module and more specifically qualitative evaluation module of HVS feedback module analyze decoded video data to determine qualitative metrics as described above where these qualitative metrics describe a quality of visualization of decoded video data . HVS feedback module may also employ quantitative evaluation module to determine quantitative metrics which described in quantitative terms the quality of decoded video data . Based on these metrics HVS feedback module reconfigures modules of transformation module to refine the quality of visualization of decoded video data .

HVS feedback module may also base this reconfiguration of modules on a combination of two or more of metrics parameters and user preferences . For example given a particular one of user preferences related to a user s preferred contrast level HVS feedback module may analyze metrics e.g. a perceived contrast level in view of this preferred contrast level and generate configuration data that refines subsequent generation of 3D video content such that video data exhibits a contrast level that is near if not equal to the preferred contrast level. HVS feedback module may also analyze metrics in view of parameters so as to refine generation of 3D video content such that metrics analyzes from subsequent video data are improved with respect to the particular display capabilities defined by parameters . In some instances all three of metrics parameters and user preferences may be employed in the above manner to generate subsequent generation of 3D video content that is refined with respect to metrics given parameters and user preferences .

After being reconfigured in the manner described above transformation module continues to convert 2D video content to 3D video content . Transformation module then outputs 3D video content which comprises 3D video data formatted in accordance with the determined input format . Mobile device forwards 3D video content to external display device . The above process may continue in this manner in what may be characterized as an iterative process such that for each of the iterations video content is refined with respect to metrics that define a perceived quality of visualization of video data .

While described above with respect to HVS feedback module analyzing decoded 3D video data HVS feedback module may alternatively analyse coded 3D video data. Moreover display device may include its own HVS feedback module similar to HVS feedback module that analyzes either coded or decoded 3D video data. In this way display device may analyze the 3D video data itself and forward the one or more metrics determined with its own HVS feedback module to HVS feedback module which may use these metrics to adjust the transformation of video data to the 3D video data. The techniques therefore should not be limited in this respect.

Similar to mobile device shown in the example of mobile device includes wireless interfaces A wired interfaces B video capture module local storage module and user interface module . Each of wireless interfaces A wired interfaces B video capture module and local storage module may source that is provide video data and or content and for this reason may be referred to generally as sources. These sources may provide video data which may comprise 2D video data or 3D video data which may include 2D video data plus additional depth information or 2D video data plus additional depth and occluded areas information or 2D video data plus depth occluded areas and global effects information .

Mobile device also includes an image video processing unit an image video encoder unit an image video buffer unit a display processing unit an internal display a wireless display WD host unit and a wireless display interface . Image video processor unit represents a hardware unit or a combination hardware and software units that process video data to generate 3D video data . Image video encoder unit represents a hardware unit or a combination of hardware and software units that encode video data in accordance with one of one or more codecs supported by image video encoder unit to generate encoded video data . While shown as an encoder image video encoder unit may also perform decoding. Encoder unit outputs encoded video data which is stored to image video buffer unit which may comprise memory or a storage device such as the examples of both memory and storage devices described above.

In some instances mobile device provides a preview mode by which to preview 3D video data locally on internal display which may comprise a light emitting diode LED display an organic LED OLED display a cathode ray tube display a plasma display a liquid crystal display LCD or any other type of display. In this preview mode image video processing unit may generate preview video data A B preview video data . Preview video data A may comprise right eye video data that forms the right eye perspective of 3D video data . Preview video data B may comprise left eye video data that forms the left eye perspective of 3D video data . When video capture device comprises a 3D video capture device this 3D video capture device may produce both right eye video data A and left eye video data B which image video processing unit may forward to display processing unit .

Display processing unit represents a hardware unit or a combination of hardware and software units that processes buffered video data to format buffered video data in accordance with a input format supported by one or more of external display devices such as one or more of external display devices A N external display devices . Display processing unit may output this formatted buffered video data as 3D video content . Display processing unit may include a display format interface DFI which is not shown in for ease of illustration purposes by which to determine the input file format supported by one or more of external display devices .

Display processing unit may also receive one or more of preview video data and format this preview video data for presentation by internal display . Display processing unit may output this formatted preview video data A B as preview video content A B preview video content one or both of which internal display may present to a user of mobile device simultaneous to outputting 3D video content to one or more of external display devices .

In instances where internal display supports 3D video content playback preview video data may comprise 3D video data that display processing unit formats for display or presentation by internal display . In instances where internal display supports both 3D video content and 2D video content playback image video processing unit may determine whether to generate preview video data as either 2D or 3D video data based on user preference a type of application e.g. 2D video data is often preferred for email text while computer graphics CG is often rendered as 3D vide data operating power mode available battery power and other preferences metrics and parameters that typically influence decisions of this type.

Image video processing unit image video encoder unit and display processing unit may comprise one or more of a processor such as a general purpose processor which may be referred to as a computer processing unit or CPU that executes one or more instructions stored to a computer readable media a digital signal processor DSP a graphics processing unit GPU or any other type of processor. Alternatively image video processing unit may comprise dedicated hardware such as a field programmable gate array FPGA and an application specific integrated circuit ASIC . In some instances both dedicated hardware and one or more processors may be combined to provide the various operations described in this disclosure with respect to units and .

While described above a formatting buffered video data to generate 3D video content in accordance with a input file format supported by external display devices display processing unit ordinarily only supports file formats for wired interfaces such as HDMI DVI and other wired file formats. Particularly display processing unit may not support wireless file formats considering that there is not as of yet any formal or standardized wireless file format. WD host unit may however support wireless display file formats and provide an interface to such wireless displays so as to determine whether one or more of external display devices support wireless file formats. WD host unit may therefore represent a hardware unit or a combination of hardware and software units that provide an interface by which to determine wireless display formats supported by one or more of external display device and then format buffered 3D video data in accordance with the determined wireless display formats. Consequently WD host unit may facilitate cross platform playback by enabling mobile device to wireless transmit 3D video content in accordance with a wireless file format supported by one or more external display devices .

WD host unit may for example implement the WHDMI or WMDDI protocol described above to determine wireless display parameters including the supported wireless file format concerning the capabilities of those of external display devices that comprise wireless displays. WD host unit may receive buffered 3D video data from display processing unit format or reformat in the case where display processing unit initially formats this data buffered 3d video data in accordance with a determined one of the wireless input file formats and returns the 3D video content to display processing unit . Display processing unit forwards this 3D video content to wireless display interface which may comprise one of wireless interfaces B but is shown separate for purposes of illustration. Wireless display interface then forwards this 3D video content to one or more of external display device .

While shown as included within mobile device WD host unit may reside external from mobile device and interface with mobile device via one of wireless interfaces A or wired interfaces B. When external from mobile device WD host unit may comprise a wireless display interface similar to wireless display interface or another similar interface that facilitates communication with a WD client unit which is described in more detail below. In some instances WD host unit may comprise a software module that is executed by display processing unit so as to display processing unit to perform wireless file formatting. To the extent display processing unit includes hardware to execute WD host unit in the above instance WD host unit may be considered to comprise both a hardware and software unit.

In some instances WD host unit may identify that two of the selected destination devices e.g. external display devices support different wireless file formats. In this instance WD host unit generates a first 3D video content formatted in accordance with the first determined wireless file format and a second 3D video content formatted in accordance with the second determined wireless file format. Wireless display interface then forwards first and second 3D video content to the appropriate ones of external display devices . Thus although shown in the example of as only sending a single 3D video content that 3D video content may comprise first and second version or more generally a plurality of versions of 3D video content where each version is formatted in accordance with a different wireless file format.

One wireless format may comprise a format that leverages a transport protocol such as a real time transport protocol RTP to encapsulate a video data segment an audio data segment and a depth data segment of 3D video content in a different one of a plurality of packets. Moreover the format may leverage this transport protocol to encapsulate additional segments of information corresponding to occlusion information and or global effects information as shown below with respect to the example of . WD host unit includes a transport unit that implements this transport protocol to encapsulate the various segments of the 3D video content in a different one of a plurality of packets. After encapsulating the various segments to different packets transport unit add metadata for enhancing playback of 3D video content within an optional data field in a header of one of the plurality of packets. This metadata may define parameters that facilitate the rendering of additional views and that promote playback on particular displays. The metadata may also define user preferences such as a desired contrast sharpness color temperature 2D or 3D screen resolution and 2D or 3D playback format pillared box stretch original etc. .

After formatting 3D video data in accordance with this file format that involves the transport protocol WD host unit may forward the resulting 3D video content to wireless display interface which transmits the packets to one or more of external display devices . When wireless display interface transmits this content to two or more of external display devices the transmission may be referred to as WD multicasting of 3D video content . In any event this transmission may occur simultaneous to display of preview video data via internal display . Often presentation of preview video data and 3D video content is synchronized. Alternatively presentation of preview video data may occur before presentation of 3D video content by one or more of external display devices .

WD host unit then interfaces with the selected ones of external wireless display devices via a wireless display interface such as wireless display interface to determine parameters that define one or more capabilities of the selected ones of external wireless display devices . These parameters may be similar to parameters discovered by parameter discovery module . One example parameter may include a wireless input file format supported by the selected ones of external wireless display devices .

Meanwhile image video processing unit may receive video data from one or more of the selected sources A B and . In terms of source which is shown in as video capture device video capture device may comprise a stereo camera with two image sensors that simultaneously capture images incident on the two sensors at the same time to provide two viewpoints for a given image or series of images in the case of video capture .

Video capture device may capture video data in a synchronous manner in a number of ways. In a first way data originating from the two sensors is received over the same memory bus using either of fast serial capture or parallel capture with buffers. In the case of serial capture image video processing unit takes into account a time offset between the serial capture of two different views during depth extraction. In a second way data originating from the two sensors is received over different memory buses to avoid complications arising when a single bus is shared between the two sensors. In a third way data originating from the two sensors is typically in a streaming format and is written to display buffers to be previewed on embedded or external displays.

In this preview mode typically only one of the two 2D viewpoints or in other words data streams from one of the two sensors may be previewed to conserve memory bandwidth and or power. User preferences input via user interface module may indicate whether 2D or 3D e.g. one or both viewpoints are sent to the display processor . User input may be entered in real time or near real time for a given video recording or image capture session or may be specified as a general preference. User preference may also indicate whether to present data in 2D or 3D based on a given application such that as described above 2D is used for text email and web browsing and 3D is used for camcorder and media player. Image video processing unit may determine whether to transform video data into 2D or 3D video data based on an available battery power or power management setting with or without user input intervention which may be referred to as occurring automatically .

In the case of 3D preview where internal display presents preview video data in 3D display processing unit may receive raw uncompressed 3D or image data for both viewpoints as two separate streams e.g. preview video data A and B. Display processing unit may also receive 2D video data directly from the sensor and 3D depth information from image video processing unit image video encoder unit or from the 3D coded image video file. Display processing unit may also receive compressed 3D data from the 3D encoded image video file e.g. stored to local storage module which may be streamed over a wired or wireless interface A B to one of external display devices . In this case if display mirroring is enabled internal display may obtain uncompressed but processed video streams from the sensors signal processor. Optionally internal display may only render 2D image e.g. one of preview video data . Display processing unit may also receive 2D data from a single sensor of video capture device where image video processing unit may comprise 2D to 3D processing module similar to 2D to 3D transformation module of transformation module shown in that converts the 2D data into 3D data.

In any event image video processing unit may transform the received video data into 3D video data or otherwise refine 3D video data to generate 3D video data and potentially preview video data . Image video processing unit then forwards this data to 3D image video encoder which encodes 3D video data to output encoded 3D video data . Image video buffer unit buffers or otherwise stores encoded 3D video data and display processing unit retrieves buffered video data from image video buffer unit for formatting in accordance with an input file format. Display processing unit as noted above may also receive preview video data and format this video data for display on internal display simultaneous to the formatting of buffered 3D video data .

However given that display processing unit may not support wireless 3D file formats for delivering 3D video data to the selected ones of external wireless displays display processing unit forwards data to WD host unit . WD host unit then prepares 3D video data in accordance with the determined parameters in the manner described above . In particular WD host unit may format 3D video data in accordance with a wireless 3D file format supported by one or more of the selected ones of external wireless display devices .

WD host unit then forwards prepared 3D video data back to display processing unit which forwards this prepared 3D video data which may also be referred to as 3D video content to wireless display interface . Wireless display interface interfaces with the selected one or more of external wireless display devices to wirelessly transmit this prepared 3D video data to these one or more of external wireless display devices .

Alternatively as noted above WD host unit may be integrated with wireless display interface whereupon display processing unit forwards without performing any formatting buffered 3D video data to wireless display interface . WD host unit in this instance formats buffered 3D video data in accordance with the determined wireless file formats supported by the selected ones of externals wireless display devices . Wireless display interface then forwards this prepared 3D video data to the selected ones of external wireless display devices .

Each of portions may comprise various segments of buffered video data including both a video segment A N video segments and depth segments A N depth segments and segments A N of buffered audio data audio segments . Transport unit assigns each of these portions a corresponding time stamp A N time stamps which are shown in the example of as TS through TS N. Transport unit then encapsulates each of segments and with a corresponding one of time stamps that was assigned to the corresponding one of portions in which each of segments resides.

For example transport unit assigns a time stamp A of TS to portion A in which segments A A reside. Transport unit then encapsulates each of segments A A and A with a time stamp A of TS resulting in the encapsulated segments shown in the example of . This encapsulation may conform to RTP. Transport unit may also encapsulate each of segments with other header information in accordance with RTP to form packets containing a different one of segments as well as generate and embed the above described metadata in the header of each of these packets which is described in more detail below with respect to .

More information regarding RTP and the forming of packets in accordance with RTP can be found in request for comments RFC 2250 entitled RTP Payload Format for MPEG1 MPEG2 Video dated January 1998 RFC 2736 entitled Guidelines for Writers of RTP Payload Format Specification dated December 1999 RFC 3016 entitled RTP Payload Format for MPEG 4 Audio Visual Streams dated November 2000 RFC 3550 entitled RTP A transport Protocol for Real Time Applications dated July 2003 RFC 3640 entitled RTP Payload Format for Transport of MPEG 4 Elementary Streams dated November 2003 RFC 3984 entitled RTP Payload Format for H.264 Video dated February 2005 and RFC 5691 entitled RTP Payload Format for Elementary Streams with MPEG Surround Multi Channel Audio dated October 2009 each of which are hereby incorporated by reference in their entirety.

Leveraging RTP in this manner to produce 3D video content formatted as an RTP stream may facilitate backwards compatibility in the sense that a 2D only display device may receive this 3D video content and present only video segments without regard for depth segments . That is this 2D only display device may drop or otherwise not recognize depth segments and only present the 2D video data sent as video segments . In this respect the foregoing file format that leverages the extensibility of RTP to deliver depth segments that are backwards compatible with respect to 2D only display devices may promote cross platform 3D video playback by virtue of this backward compatibility.

Payload type field A stores the type of data which in this example indicates depth data stored to the segment which may also be referred to as a payload to distinguish this data from header data encapsulating the payload. Depth range field B stores data directed to a width and size of a depth plane for a given 3D video image or video. The depth plane can be the same size or smaller than the current image. Camera model parameters field D stores data related to extrinsic and intrinsic camera model parameters. Optimal viewing parameters field E stores data directed to a viewing angle which is defined by a size of the viewing screen and a viewing distance as well as a convergence plane the above noted depth range brightness and a number of other related parameters. Compression type field F stores data describing a type of compression used in the payloads of the video or audio segments which may indicate JPEG H.264 or a proprietary compression algorithm.

Transport unit then determines time stamps for each of portions of video data and audio data as described above . Transport unit encapsulates segments of the same one of portions with the corresponding one of time stamps to form packets also as described above . WD host unit may determine metadata in the manner described above whereupon transport unit embeds the determined metadata in headers of the packets . The resulting packets with the embedded metadata may resemble the packets shown in the example of . After embedding the metadata transport unit forwards the packets to wireless device interface which proceeds to forward the packets to the selected ones of external wireless display devices .

External WD client unit likewise comprises a device that interfaces with a display platform of system A. External WD client similar to external WD host unit may comprise a so called dongle that interfaces with display platform via a wired interface such as a USB interface a composite audio video A V interface an HDMI interface or any other wired interface.

Display platform may represent a device that interfaces with a 3D display device of system A such as a digital video disc DVD player an audio video receiver a Bluray disc player a multimedia player a digital video recorder DVR or any other device that provides a platform for interfacing with a 3D display device . In some instances display platform may comprise a personal computer such as a laptop or desktop computer or a device dedicated to performing 2D to 3D conversion. 3D display device may be similar to external display devices shown in the example of . While shown as separate from 3D display device display platform may be integrated into 3D display device .

As noted above there as of yet has not been any standardized wireless display protocol by which to forward video data wirelessly to display devices. Given this lack of standardized wireless display protocol most display platforms such as display platforms do not support wireless video data receipt or transmission. Use of external WD host unit and external WD client unit may overcome this limitation.

To illustrate consider that mobile device may source 2D video data which external WD host unit may receive through the wired connection between mobile device and external WD host unit noted above. External WD host unit may establish a wireless link with external WD client unit and then establish a session over wireless link . This session may comprise an RTP session. As external WD host unit receives 2D video data WD host unit formats 2D video data in accordance with a wireless display protocol supported by external WD client unit . For example WD host unit may prepare 2D video data by formatting 2D video data in accordance with the file format shown in the example of except that 2D video data would not include any depth segments and therefore the resulting formatted data would not include any depth packets leaving each portion to only have video and audio segments.

External WD host unit may also interface with external WD client unit to determine parameters defining the capabilities of display platform and or 3D display device . External WD host unit may interface with external WD client unit to request these parameters. In response to this request external WD client unit may interface with display platform via the wired interface to discover capabilities of display platform in the manner described above e.g. using WHDMI . Display platform may also have discovered parameters of 3D display device when interfacing with 3D display device using for example HDMI. Display platform may then return parameters describing the capabilities of one or both of display platform and 3D display device which external WD client unit forwards to external WD host unit . External WD host unit may then generate metadata based on these parameter and embed this metadata into the headers of the various packets as described above.

In any event WD host unit transmits formatted 2D video data to external WD client unit via wireless link . Upon receiving this formatted 2D video data external WD client unit may decapsulate the various segments to reform 2D video data as well as extract the metadata embedded in the packet headers. External WD client unit then forwards reformed 2D video data to display platform along with the metadata.

As shown in the example of display platform includes a 2D to 3D processing module and a display format module which may be similar to 2D to 3D processing module and display format module of . While similar 2D to 3D processing module and display format module may be of a more limited nature in that 2D to 3D processing module may only support generation of certain 3D video data e.g. depth as opposed to another view and display format module may only support a specific device specific 3D input file format e.g. 2D plus Z as opposed to a multi view stream . Despite the more limited nature of both of 2D to 3D processing module and display format module both of modules and may be configurable to a certain extent and may utilize the forwarded metadata to improve generation of 3D video content from the received 2D video data . 3D display device receives and presents 3D video content .

2D to 3D processing module may comprise as one example a plug in or other software module for either a hardware or software based media player. Display format module may also comprise in one example a plug in or other software module for either a hardware or software based media player. Display format module may perform display format interleaving which may be necessary for multi view displays.

As described above 2D to 3D processing module converts 2D video data into the 3D video data of 3D video content by extracting depth information. This depth extraction may involve identifying and segmenting large and small scale features from one or more frames of 2D video data . The depth extraction also involves classifying 2D video data into regions such as a background region occluded regions and foreground regions. Once classified the depth extraction identifies a position and location of moving objects in a 3D model based on structure and motion in 2D video data . The result of the depth extraction is a depth value for each pixel in the 2D images or frames of 2D video data which is stored as an N bit bitmap image for each frame. Alternatively a depth map may be generated for each region and the scene is composed on the fly during rendering.

In the case of stereoscopic displays that take as input 2D plus depth formatted 3D video content the information in the depth map and 2D video data are formatted to the 2D plus z file format to be input to 3D display device . If the display input file format for 3D display device is a multiple streams file format where multiple streams or views of video are encapsulated in a single container 2D to 3D processing module may generate one or more additional secondary viewpoints corresponding to the 2D video data based on 2D video frames and associated depth maps. Display format module then interleaves the original 2D video data or 2D video data with the secondary views depending on the required viewing angle number of views and other parameters defined for the display or by the user e.g. as metadata .

Mobile device may receive 2D video data from any of the sources described above with and perform 2D to 3D processing either in non real time or real time or near real time. Alternatively mobile device may receive 3D video content and transform 3D video content into the 2D plus z format as specified in MPEG C Part 3 . Mobile device may encapsulate this 3D video data using existing file formats such as MP4 that have been modified to carry an additional packet per frame for the depth map or z information. The encapsulation header for the depth packet may be that specified for user data in the file format specification as an informative element. Using this informative element may enable backwards compatibility with 2D video displays. The depth packet in this MP4 wireless display protocol may be associated with the corresponding video packet through timing sync information or alternatively through frame unique identification information for the video packets such as a sequence numbers may be used for the association as well.

In system C external WD host unit receives 3D video data and formats this data in accordance with a wireless display protocol resulting in formatted 3D video data similar to that shown in . WD host unit may intercept 3D video data at a parser for the MP4 file and retrieve video audio and depth packets. WD host unit then re encapsulates these packets with RTP headers to form RTP streams which WD host unit streams to external WD client device over a real time streaming protocol RTSP . These streams may be formatted in accordance with the formatting described above with respect to the example of . In effect WD host unit modifies a transport protocol e.g. RTP to carry 2D plus z 3D video data .

External WD client unit receives these streams via wireless link decapsulates the streams to reform 3D video data encoded in accordance with the 2D plus z encoding and forwards this 3D video data to Z to MV processing module . Z to MV processing module converts the 3D video data from the 2D plus z encoding format to the multiview stream encoding format by rendering multiviews from 3D video data . Display format module then interleaves these multiviews in the manner described above to generate 3D video content which 3D display device receives and presents for consumption by one or more viewers.

Mobile multimedia device may include a mobile multimedia processor not shown in that includes internal WD client unit and a Z to graphics processing unit GPU processing module Z to GPU processing module . The multimedia processor may include a GPU. In this sense WD client unit and Z to GPU processing module may be considered to be hosted on the multimedia processor with the GPU. Z to GPU processing module performs depth to multiview conversion processes so as to convert 3D video data encoded in accordance with the 2D plus z format to a multiview encoding format. Internal WD client unit and external WD host unit may communicate with one another in the same manner as that described with respect to these same units in system B of only that the communication involves 3D video data as described with respect to system C of .

In any event internal WD client unit receives encapsulated 3D video data formatted in accordance with the wireless display protocol and decapsulates this encapsulated 3D video data to reform 3D video data . Internal WD client unit forwards this data to Z to GPU processing module which utilizes the GPU as a general purpose computing engine by which to perform the conversion processes to generate 3D video data formatted in accordance with a multiview encoding format. Use of the GPU as a general purpose computing engine may involve using OpenCL which comprises a framework for writing programs that execute across heterogeneous platforms consisting of CPUs and GPUs. OpenCL includes a language for writing functions that execute on OpenCL devices plus application programmer interfaces APIs that are used to define and then control the platform.

Z to GPU processing module may perform real time or near real time depth to multiview conversion using the GPU and thereby may enable interfacing with heterogeneous displays that accept 3D content in different formats such as 2D plus z formats and multistream video formats For example Z to GPU processing module may generate 3D video data formatted in accordance with the 2D plus z format which 3D display A may directly accept without intervening processing by display platform . Z to GPU processing module may also generate 3D video data formatted in accordance with the multiview format and forward this data to display platform . Display format module of display platform may interleave the streams from multiview formatted 3D video data to generate 3D video data which 3D display device B may receive and present.

In any event mobile device may forward either 2D video content or 3D video content to external WD host unit which encapsulates this content according to the wireless display protocol in the manner described above and forwards formatted content via wireless link to internal WD client unit . If the content is 2D video content internal WD client unit may then forward this data to 2D to 3D processing unit to perform real time or near real time 2D to 3D conversion e.g. depth extraction . If required by one or both of displays A B 2D to 3D processing module may forward the 3D video data to Z to GPU processing module to generate multiview formatted 3D video data as described above. Alternatively if internal WD client unit receives 3D video content and one or both of displays A B require a multiview file format internal WD client unit may forward 3D video content to Z to GPU processing module which generates 3D video data formatted in accordance with the multiview file format.

Mobile multimedia device may then forward content to 3D display device A as described above. Alternatively mobile multimedia device may forward 3D video data to display platform as described above. Display platform again as described above may interleave the multiple views using display format modules and forward resulting 3D video content to 3D display device B.

In system E mobile device may chose to drive 2D or 3D video content over the wireless display protocol implemented by external and internal WD host client units . As noted above WD host unit and WD client unit may exchange parameters describing displays one or more through mirroring to which the client is connected where these parameters include a 3D content format supported by these displays. Depending on the available bandwidth on wireless link WD client unit may perform the 2D to 3D conversion process rather than mobile device . Performing the conversion within mobile multimedia device may also promote a better user experience and visual quality. Moreover depending on the parameters of the 3D display the depth to multiview conversion process is executed. Additionally mobile multimedia device may perform format translation from the wireless display protocol to the display format.

In all of system A E the various components may utilize WD host and client units to enable traditionally wired displays to become wireless displays or at least to become accessible via a wireless connection. Consequently the various internal and external WD host and client units may promote cross platform 3D video playback by converting wired displays onto wireless displays. Moreover the various WD host and client units do not mandate a particular 3D content format but only provide for a wireless display transport protocol by which to efficiently transmit 3D video data wirelessly while still enabling backwards compatibility with 2D only display devices. In this respect the various internal and external WD host and client units may further promote cross platform 3D video playback.

The techniques described herein may be implemented in hardware firmware or any combination thereof. The hardware may in some instances also execute software. Any features described as modules units or components may be implemented together in an integrated logic device or separately as discrete but interoperable logic devices. In some cases various features may be implemented as an integrated circuit device such as an integrated circuit chip or chipset. If implemented in software the techniques may be realized at least in part by a computer readable medium comprising instructions that when executed cause a processor to perform one or more of the methods described above.

A computer readable medium may form part of a computer program product which may include packaging materials. A computer readable medium may comprise a computer data storage medium such as random access memory RAM synchronous dynamic random access memory SDRAM read only memory ROM non volatile random access memory NVRAM electrically erasable programmable read only memory EEPROM FLASH memory magnetic or optical data storage media and the like. The techniques additionally or alternatively may be realized at least in part by a computer readable communication medium that carries or communicates code in the form of instructions or data structures and that can be accessed read and or executed by a computer.

The code or instructions may be executed by one or more processors such as one or more DSPs general purpose microprocessors ASICs field programmable logic arrays FPGAs or other equivalent integrated or discrete logic circuitry. Accordingly the term processor as used herein may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described herein. In addition in some aspects the functionality described herein may be provided within dedicated software modules or hardware modules. The disclosure also contemplates any of a variety of integrated circuit devices that include circuitry to implement one or more of the techniques described in this disclosure. Such circuitry may be provided in a single integrated circuit chip or in multiple interoperable integrated circuit chips in a so called chipset. Such integrated circuit devices may be used in a variety of applications some of which may include use in wireless communication devices such as mobile telephone handsets.

Various examples of the disclosure have been described. These and other examples are within the scope of the following claims.

