---

title: Estimating a computing job complexity
abstract: In a method for estimating a complexity of a computing job, selected data objects relevant to a data repository are retrieved. In addition, points are assigned to multiple elements of the selected data objects according to a predefined schedule and scores for the selected data objects are calculated by applying a mathematical function to the multiple elements and complexities of the data objects are estimated based upon the calculated scores and the predefined schedule. In addition, a complexity of the computing job is estimated based upon the estimated complexities of the data objects and the estimated complexity of the computing job is stored.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09563866&OS=09563866&RS=09563866
owner: ACCENTURE GLOBAL SERVICES LIMITED
number: 09563866
owner_city: Dublin
owner_country: IE
publication_date: 20100405
---
The present application shares some common subject matter with co pending and commonly assigned U.S. patent application Ser. No. 11 874 542 entitled Complexity Estimation of Data Objects filed on Oct. 18 2007 by Christopher Killian et al. the disclosure of which is hereby incorporated by reference in its entirety.

Organizations typically receive enormous amounts of data from external sources and or create large amounts of data internally on a daily basis. The data is often stored in a data warehouse which is an electronic database typically residing in servers. In essence such a data warehouse constitutes the institutional memory of an organization that may be accessed as desired to gain insight into the operation culture and performance of the organization.

There exists a number of software tools or suites typically implemented using a database management system DBMS that supports queries into the data warehouses to provide business intelligence to the organizations. As referred herein business intelligence includes applications and technologies that work together to collect provide access to and analyze data and information about operations of an organization. Thus an organization often implements a business intelligence software application in its operations to obtain a more comprehensive knowledge of the factors affecting their business such as metrics that is data measurements on sales production human resources and other internal operations to enable the organization to make better business decisions. Examples of commercially available business intelligence software applications include but are not limited to BusinessObjects of Business Objects S.A. Paris France MicroStrategy 8 of MicroStrategy Inc. McLean Va. and Hyperion Intelligence of Hyperion Solutions Corp. Santa Clara Calif.

Business intelligence software or applications hereinafter BI applications typically include multiple components for providing report creation data viewing and data distribution in one or more databases of interest. For example the BI applications allow so called reports in which various components of the stored data may be organized and presented in a desired format to be developed for instance in a single document. The BI applications also allow so called universes that describe the structure content organization internal relationships etc. of various databases to be developed. The BI applications may generate the universes as metadata layers which interface with databases to map everyday business terms to the data stored in the databases. In addition the universes simplify the processes of creating reports viewing data and distributing data by providing an easy way to see and understand the data contained in the databases. The reports and universes are specific examples of data objects used to interact with an organization s data warehouse.

The data objects rely upon the architectures of the underlying applications used to create and execute them. If these applications are changed substantially an organization s investment made in its data objects may be lost or substantially increased to the extent that the organization must either abandon or re author the desired data objects. This may be a substantial loss because organizations often develop hundreds if not thousands of data objects used to interact with the data warehouse. However rather than re authoring the data objects entirely it is possible to transform pre existing data objects to be compatible with the revised underlying applications. In addition it is possible to determine the complexities of the data objects through use of conventional techniques.

It would however be beneficial to have improved techniques for determining the complexities of the data objects.

The present disclosure describes a method for estimating a complexity of a computing job that improves upon conventional complexity estimation techniques. In the method selected data objects relevant to a data repository are retrieved where each of the selected data objects is formed of multiple elements. Points are assigned to the multiple elements according to a predefined schedule. The predefined schedule may include point value assignments for the multiple elements that are based upon a user s historical knowledge of the relevance each of the multiple elements has over the selected data objects. In addition scores for the selected data objects are calculated by applying a mathematical function to the multiple elements. In certain instances the mathematical function is applied to both the points and the multiple elements in calculating the scores. Moreover complexities of the data objects are estimated based upon the calculated scores and the predefined schedule and a complexity of the computing job is estimated based upon the estimated complexities of the data objects. The estimated complexity of the computing job is further stored for instance in a computer accessible device.

For simplicity and illustrative purposes the principles of the embodiments are described by referring mainly to examples thereof. In the following description numerous specific details are set forth in order to provide a thorough understanding of the embodiments. It will be apparent however to one of ordinary skill in the art that the embodiments may be practiced without limitation to these specific details. In other instances well known methods and structures have not been described in detail so as not to unnecessarily obscure the embodiments.

The system is depicted as including an analyzer tool in communication with a storage device storing data objects and data repositories . The analyzer tool is also depicted as being in communication with a user interface . Each of the data objects is formed of a plurality of elements and is relevant to one or more of the data repositories . For instance each of the data objects may be considered as being relevant to a particular data repository if the data object is used to access manipulate organize operate upon or present etc. data stored in the particular data repository or particular ones of the data repositories . By way of particular example a data object such as a report or a universe pertaining to data contained in a particular data repository may be considered as being relevant to that data repository or particular ones of the data repositories and vice versa.

The analyzer tool may be implemented to analyze the data objects stored in the storage device to for instance estimate a complexity of a computing job involving the data objects . Generally speaking the computing job may comprise a computer implemented operation involving a manipulation or a transformation of the data objects . Thus in one particular example the analyzer tool may be operated to estimate the complexity of a computing job in which selected data objects relevant to one or more data repositories are transformed to be compatible with a revised set of applications.

The analyzer tool may receive input from a user through the user interface . The input may comprise various sets of instructions for the analyzer tool to implement or execute. In addition the input may comprise values that the analyzer tool may assign to the elements in estimating the complexities of the data objects . Various manners in which the analyzer tool may implement the values received from a user through the user interface are described in detail herein below.

According to an example the storage device which may comprise any type of machine readable memory or the like may be local to a processing apparatus implementing the analyzer tool . In another example the storage device is remote from the analyzer tool as in the case of a remote server computer or the like. Generally speaking a data object as used throughout the present disclosure may be defined to include virtually any item that may be used to access manipulate organize operate upon or present data stored in a data warehouse including but not limited to reports and universes created using Business Objects authoring tools. For example so called reports may be developed using Business Objects Desktop Intelligence application which allows users among other things to define a preferred layout of various data stored within the data warehouse . In a similar vein a so called universe may be defined as a metadata or semantic layer that may be developed using Business Objects Universe Designer application to describe the location of one or more databases the types of data stored within such databases the internal relationships between data elements within such databases etc. thereby allowing relevant queries to be established for the databases.

As described in greater detail below the analyzer tool may be implemented using stored instructions that may be executed by a suitable processing apparatus such as a computing device. The user interface may include a graphical user interface presented to a user via a suitable display device. The analyzer tool may obtain values for various elements forming the data objects via the user interface . In one regard the use of the user input values for the elements generally enables the overall complexity ratings for each analyzed data object to be estimated which may be employed to estimate efforts required to transform individual or groups of analyzed data objects .

Referring now to a system in accordance with an embodiment is illustrated. It should be understood that the system may include additional elements and that some of the elements described herein may be removed and or modified without departing from a scope of the system .

As shown the system includes one or more user terminals in communication with a communication network . Also shown in the system are an interface server and one or more data server s in communication with the communication network . As described in greater detail below each of the terminal s may include a suitable processing apparatus capable of implementing the processing described below. Additionally each of the terminal s includes the necessary hardware and or software needed to communicate with the network via a wired and or a wireless connection. Thus the terminal s may be embodied by desktop laptop handheld computers wireless communication devices personal digital assistants or any other similar devices having the necessary processing and communication capabilities. In an embodiment the network may comprise a public communication network such as the Internet or World Wide Web and or a private communication network such as a local area network LAN wide area network WAN etc. as known by those having ordinary skill in the art.

The interface server and the data server s which may comprise suitable server computers as known in the art collectively implement the functionality of the at least one storage device illustrated in . For example the interface server may implement a suitable DBMS that is compatible with Business Objects reports and universes. Such software is provided to serve as an interface to the various data servers that in turn implement a data warehouse. As described below each of the terminals may comprise client software applications that allow the terminals to communicate with the one or more servers via the intervening communication networks . The data objects operated upon as described herein are preferably stored on either the interface server or more typically locally on the terminals .

In an alternative embodiment at least some portion of the functionality described below relative to the terminals may be implemented by one or more web servers connected to the communication network . The web servers may provide a suitable interface to the terminal via the communication network while performing the necessary processing needed to implement the analyzer tool described herein. In this manner a more centralized implementation is possible. In a further alternative embodiment another terminal similar in construction and functionality as the network based terminals may be provided. In this embodiment the other terminal s communicates directly with the interface server bypassing the network . In addition although the data server s are described above as implementing a data warehouse they may be equally employed as storage for operational data that is data not yet stored in a data warehouse.

Referring now to there is shown a block diagram of a system containing a processing apparatus suitable for implementing the terminals illustrated in according to an embodiment. It should be understood that the system may include additional elements and that some of the elements described herein may be removed and or modified without departing from a scope of the system .

As shown the processing apparatus includes one or more processors in communication with one or more storage devices . The processor s may comprise a microprocessor microcontroller digital signal processor co processor or other similar devices known to those having ordinary skill in the art. In addition the storage device s may comprise a combination of volatile or nonvolatile memory such as random access memory RAM or read only memory ROM . Such storage devices may be embodied using any currently known media such as magnetic or optical storage media including removable media such as floppy disks compact discs etc. In any regard the storage device has stored thereon instructions that may be executed by the one or more processors such that the processor s implement the functionality described herein. In addition or alternatively some or all of the software implemented functionality of the processor s may be implemented using firmware and or hardware devices such as application specific integrated circuits ASICs programmable logic arrays state machines etc.

The processor s may access one or more databases of interest stored on the storage device s via a universe not shown . The universe is a metadata layer or database interface that is logically situated above the storage device s to provide an interface to the database. The universe maps common terms to the data in the storage device s through the creation of data classes and objects in the universe for the data structures such as data tables in the storage device s . Objects represent fields in a database table or other data structure classes are logical groupings of objects and the universe is a collection of classes. Thus the universe simplifies the process of querying accessing and retrieving the structured collection of records or data in the storage device s by providing automatic joins between the database tables based on predetermined key values or parameters.

As further shown in the processor s may be in communication with various user interface devices via a user input output interface that allows a user of the processing apparatus to interact therewith. For example the user interface devices may comprise one or more display screens keyboards user selection devices for instance a mouse and cursor combination a touch screen voice recognition interfaces etc. speakers microphones etc. The processor s may also be in communication with other interfaces that provide one or more communication interfaces between the processing apparatus and external devices such as the network and or the servers illustrated in . The nature of the other interfaces necessarily depends on the external devices with which the processing apparatus is configured to communicate. For example where the processing apparatus is coupled to a network such as network via a wired connection the interfaces may comprise the software firmware and or hardware necessary to terminate the relevant network protocols. Similarly where the processing apparatus communicates with a wireless network the interfaces may comprise the components needed to terminate the wireless protocol.

The storage devices comprise executable instructions that may be used to implement an analysis spreadsheet as well as one or more data access interface programs . According to an embodiment the analysis spreadsheet may be implemented using a known spreadsheet application such as the Microsoft Excel spreadsheet program. In a similar vein the data access interfaces may be implemented using Business Objects Desktop Intelligence client or Business Objects Universe Designer client. The data access interface programs may also comprise customized data mining software capable using known programming techniques of directly analyzing the data objects . Further still such customized data mining software may cooperate with application programming interfaces APIs of the commercial clients for instance Desktop Intelligence or Universe Designer for the same purpose. In combination the analysis spreadsheet and the data access interface s implement that functionality of the analyzer tool used to estimate the complexities of one or more data objects based upon the particular elements forming the data objects . In particular the data access interface s may access the targeted data objects and obtain the elements forming the targeted data objects and the values assigned to the elements . In turn the analysis spreadsheet operates upon values obtained by the data access interface from the data objects .

To the extent that the data objects are stored remotely relative to the processing apparatus the data access interface s accesses the data objects via one of the other interfaces implemented within the processing apparatus . In an alternative embodiment the desired data objects may be stored in the storage device s thereby allowing direct analysis by either the analysis spreadsheet or the data access interface .

With reference now to there is shown a flow diagram of a method for estimating a complexity of a computing job according to an embodiment. It should be apparent to those of ordinary skill in the art that the method represents generalized illustrations and that other steps may be added or existing steps may be removed modified or rearranged without departing from the scopes of the method .

The description of the method is made with reference to the system illustrated in and thus make reference to the elements cited therein. In this regard the processing apparatus and more particularly the processor s is configured to implement the method unless otherwise noted.

At step the processing apparatus initiates the method through receipt of a command through the user interface to estimate a complexity of a particular computing job. According to an example the particular computing job comprises for instance a computer implemented operation involving a manipulation or a transformation of the data objects such that the data objects may be implemented with a revised set of applications.

At step the processor s identifies which of the data objects stored on the storage device s or data repositories are relevant to the computing job. According to an example the processor s may identify all of the data objects contained in one or more storage devices to be relevant to the computing job. In another example the processor s may identify all of the data objects having predetermined file extensions such as reports .rep or universes .unv as being relevant to the computing job.

In a further example a correlation between the data objects and the computing job may have previously been made by a user and the processor s may receive the identification of the particular data objects from the user through the user interface . In this example the user may rely upon historical interactions with the data objects and various computing jobs to determine which of the data objects are relevant to the particular computing job. In addition the user may store information pertaining to the data objects considered to be relevant to a particular computing job in the storage device s which the processor s may access at step to identify the data objects that are relevant to the computing job.

At step the processor s retrieves the identified data objects from the one or more storage devices . According to an example the processor s physically moves the data objects from one location in the storage device s to another location in the storage device s for processing. In another example the processor s is configured to analyze the relevant data objects without moving them from their respective locations in the storage device s in which case the retrieval performed at step is merely a designation of which of the data objects the processor s is configured to analyze.

At step the processor s selects elements of each of the data objects retrieved at step to be analyzed. Each of the data objects may be formed by a plurality of elements . By way of particular example in which the data objects comprise mappings the elements are transformations of the mappings.

According to a first example the processor s selects all of the elements forming each of the retrieved data objects at step . In another example the processor s selects a particular subset of the elements for each of the retrieved data objects . In this example the processor s may be programmed to determine which of the elements are more relevant to the data objects than other elements . For instance the processor s may select those elements that affect the data objects to a predefined level or require a predefined level of resources at step .

As a further example a user may identify the subsets of all of the elements forming the retrieved data objects to be selected at step . In this example the user may rely upon historical interactions with the various elements of the data objects to identify which of the elements should be selected at step . As such the elements may be selected based upon a user s subjective criteria which may also be based upon the user s experience with the data objects.

At step the processor s assigns points to the multiple elements of the data objects retrieved at step according to a predefined schedule. According to an example the predefined schedule contains an indication that each of the elements has the same point values in which the processor s assigns those same point values to each of the elements . In another example the predefined schedule includes one of multiple values that may be assigned to the elements . In this example a user may define the values that are to be assigned to the elements in the predefined schedule for instance based upon knowledge of the relative importance levels of the elements to the data objects which may be attained through historical interactions with the elements and the data objects .

At step the processor s calculates scores for each of the retrieved data objects by applying a mathematical function to the selected elements . In some embodiments the processor s also calculates the scores based upon the points assigned to the selected elements . The mathematical function may include one or more of an addition function a subtraction function a multiplication function a division function etc. In one embodiment the mathematical function may be applied to the selected elements for each of the data objects to thus enable some of the selected elements to be weighted more heavily as compared with other ones of the selected elements of each of the data objects . Particular examples in which the mathematical function may be applied to calculate the scores at step will be described in greater detail herein below.

At step the complexities of the retrieved data objects are estimated based upon the calculated scores of the data objects . In one embodiment the complexities of the data objects are estimated based upon a predefined schedule that assigns a complexity level to the data objects based upon their respective scores. Thus for instance a first range of scores may be defined to be equivalent to a low complexity data object transformation a second range of scores may be defined to be equivalent to a medium complexity data object transformation a third range of scores may be defined to be equivalent to a complex data object transformation a fourth range of scores may be defined to be equivalent to a very complex data object transformation etc.

At step a complexity of the computing job is estimated based upon the estimated complexities of the data objects . According to an embodiment the computing job is estimated to have a complexity level that is equivalent to a mathematical function of the estimated data object complexity levels. The mathematical function may be the median mean average the highest estimated complexity level the lowest estimated complexity level etc. of the estimated complexities of the data objects .

At step the processors s may employ the estimated complexity of the computing job in estimating the amount of resources including for instance computing resources personnel etc. required to perform the computing job. By way of particular example the data objects may have been developed for applications used by a client and the processing apparatus may be implemented by a user who may be a prospective contractor wishing to use manipulated or transformed versions of the data objects for applications used by the contractor in performing services for the client. In this example the client may have requested that the user submit a bid for the services to be performed by the user. In developing the bid the user may use the processing apparatus to estimate the complexity and or the amount of resources required to perform the computing job and to base the bid amount on the estimated complexity and or the estimated amount of resources.

At step the processor s stores the estimated complexity of the computing job and or the estimated resources required to perform the computing job in a storage location such as the storage device an external storage device one or more of the servers etc.

Turning now to there is shown a flow diagram of a method of documenting existing intelligence reports into a standard spreadsheet format according to an embodiment. It should be apparent to those of ordinary skill in the art that the method represents generalized illustrations and that other steps may be added or existing steps may be removed modified or rearranged without departing from the scopes of the method .

Generally speaking the processor s may implement the method prior to performing the method to for instance create spreadsheet s containing information pertinent to the data objects that the processor s may access in implementing the method . More particularly for instance the processor s may access the spreadsheet s created through implementation of the method in identifying the data objects and the elements and in certain instances determining the counts of each of the elements contained in the data objects . In one respect the method generally provides the processor s with a relatively quick and easy location from which to retrieve information during a process for estimating a complexity of a computing job.

At step the processor s accesses one or more intelligence reports. The one or more intelligence reports may comprise either or both of web intelligence webi reports and desk intelligence deski reports. At step the processor s identifies relevant data from the intelligence reports. The relevant data may include for instance all of the elements of one or more particular data objects . By way of example the relevant data may include the transformations and SQL overrides of a mapping contained in an intelligence report. In this example the relevant data may also include a summary count of the transformations and the SQL override lengths.

At step the processor s creates a spreadsheet with information pertaining to the relevant data identified at step . By way of example the processor s creates the spreadsheet to contain a listing of the elements of one or more data objects contained in the intelligence report s along with a summary count of each of the elements . As another example the processor s creates the spreadsheet to contain a listing of universe components and report components of BO universes and BO reports along with a summary count of the universe components and the report components.

Some or all of the operations set forth in the methods and may be contained as utilities programs or subprograms in any desired computer accessible medium. In addition some or all of operations may be embodied by computer programs which may exist in a variety of forms both active and inactive. For example they may exist as software program s comprised of program instructions in source code object code executable code or other formats. Any of the above may be embodied on a computer readable storage device.

Exemplary computer readable storage devices include conventional computer system RAM ROM EPROM EEPROM and magnetic or optical disks or tapes. Concrete examples of the foregoing include distribution of the programs on a CD ROM or via Internet download. It is therefore to be understood that any electronic device capable of executing the above described functions may perform those functions enumerated above.

The methods and may be applied to various types of environments. An example of a suitable environment includes an environment containing databases structured with data integration products available from the Informatica Corporation of Redwood City Calif. Another example of a suitable environment includes an environment containing universes and reports. A further example of a suitable environment includes an environment containing databases structured with the DataStage product available from the IBM Corporation of Armonk N.Y. A still further example of a suitable environment includes an environment operable with database management products available from Ab Initio of Lexington Mass.

With reference to the environment containing databases structured with data integration products available from the Informatica Corporation of Redwood City Calif. the storage device comprises an Informatica repository and the data objects comprise mappings. Likewise with reference to the environment containing a database structured as a DataStage repository the data objects comprise mappings. In addition the elements of the mappings in both types of environments comprise transformations and SQL overrides. With reference to the environment containing an Ab Initio managed database the data objects comprise graphs and the elements of the graphs comprise transformations. Although the Informatica the DataStage and Ab Initio environment examples are described together it should be understood that the processor s may employ the examples separately depending upon the type of data repository from which the complexity information is obtained.

An example of a predefined schedule of transformations and point values for an Informatica repository is depicted in and an example of a predefined schedule for a DataStage repository is depicted in . The predefined schedule of transformations and point values for an Ab Initio repository may be similar to the predefined schedules and depicted in . Either or both of the predefined schedules and may be stored for instance as a lookup table in the storage device and the processor s may access the predefined schedule in assigning points to the selected elements at step in the method . The listing of transformations and point values in the predefined schedule of and the listing of transformations and point values in are intended to provide particular examples of suitable predefined schedules and and are thus not intended to limit a scope of the invention to the transformations and points depicted therein. It should thus be understood that the predefined schedules and may include additional transformations or existing transformations may be removed or replaced and the points may also vary from that shown in without departing from a scope of the invention.

According to an embodiment the transformations contained in the predefined schedule comprise all of the transformations forming the mappings data objects for a particular database. In another embodiment the predefined schedule includes a selected set of transformations for instance those transformations that a user has identified as being the most relevant to a particular database as discussed above. In addition a user may define the points assigned to each of the transformations based upon knowledge gained from previous interactions with the mappings graphs or data objects of a particular database.

With reference back to the method the processor s accesses the predefined schedule at step to assign scores for the transformations and calculates scores for the retrieved mappings graphs by applying a mathematical function to the selected transformations and the assigned points at step . By way of particular example the processor s determines the transformations and the numbers of each of the transformations contained in each of the mappings or graphs data objects for instance from a mapping detail sheet not shown that identifies each of the mapping names of a computing job and a SQL override summary sheet not shown that identifies which of the mappings have used SQL overrides. The mapping detail sheet provides detailed information about each of the mappings such as folder name workflow name session name mapping name transformation associated with the mappings a count of the transformation type used in the mapping etc. The SQL override summary sheet provides information such as the folder name relating to the computing job on which the complexity estimation is being performed the mapping names where SQL overrides exists the types of transformations used for overriding purposes the SQL override statement used in the transformation etc. According to an embodiment the mapping detail sheet and the SQL override summary sheet are created through implementation of the method .

In addition for each of the mappings or graphs data objects the processor s multiplies the number of each of transformations with their assigned points and sums the products. Thus for instance with respect to the predefined schedule if a mapping graph has 2 sources 2 source qualifiers 1 update strategy 3 aggregators 1 joiner 1 expression and 1 target at step the score N for that mapping will be calculated as 2 0 2 0 1 2 3 2 1 2 1 1 1 0 11 Equation 1 

The processor s may perform a similar process to determine the respective scores for the transformations depicted in the predefined schedule .

In addition at step the processor s assigns a score for the SQL overrides data objects according to a predefined schedule depicted in . Similarly to the predefined schedules and depicted in the predefined schedule for the SQL overrides may be stored for instance as a lookup table in the storage device and the processor s may access the predefined schedule in assigning points to the selected SQL lengths elements at step in the method . The listing of SQL override lengths and associated point values in the predefined schedule is intended to provide one particular example of a suitable predefined schedule and is thus not intended to limit a scope of the invention to the actual relationships depicted therein. It should thus be understood that the predefined schedule may include additional SQL override length ranges or existing SQL override length ranges may be removed or replaced and the points may also vary from that shown in without departing from a scope of the invention.

According to an embodiment the point values assigned to the SQL overrides are based upon the lengths of the SQL overrides . The lengths of the SQL overrides are calculated from the from clause to the end of the select statement as it is specified in the SQL override option of a source qualifier and look up transformation. In addition a user may define the points assigned to each of the SQL override lengths based upon knowledge gained from previous interactions with the SQL overrides data objects of a particular database.

With reference back to the method the processor s accesses the predefined schedule at step and calculates scores for the retrieved SQL override by applying a mathematical function to the SQL override and the assigned points at step . By way of particular example the processor s determines the length of the SQL override and based upon a comparison function of where the length falls in the SQL length ranges the processor s assigns the corresponding point value to the SLQ override.

The scores N for each of the mappings graphs retrieved at step may be calculated in manners similar to those described above. In addition scores for each of the mappings graphs may comprise a combination of the scores calculated for the transformations and for the SQL overrides associated with each of the mappings graphs . For instance the scores of the transformations may be added to the scores of the SQL overrides to obtain a total score for the mappings or graphs data objects .

The processor s estimates the respective complexities of the mappings or graphs data objects as indicated at step . More particularly for instance the processor s identifies the points assigned to each of the mappings or graphs which includes both the transformations and the SQL overrides and estimates a complexity based upon the points. The processor s may estimate the complexities of the mappings or graphs based upon a complexity schedule. An example of a suitable complexity schedule is depicted in . Similarly to the predefined schedules and depicted in the complexity schedule may be stored for instance as a lookup table in the storage device and the processor s may access the complexity schedule in estimating complexities of the mappings or graphs data objects at step in the method . The listing of points ranges and associated complexities in the complexity schedule is intended to provide one particular example of a suitable complexity schedule and is thus not intended to limit a scope of the invention to the actual relationships depicted therein. It should thus be understood that the complexity schedule may include additional ranges and that the existing ranges may be removed replaced and or modified and that the assignment of the complexities may also vary from that shown in without departing from a scope of the invention.

According to an embodiment the processor s is configured to accumulate the complexities of the mappings graphs and to provide a summary of the complexities of each of the mappings graphs . For instance the processor s may create a spreadsheet containing a summary of the mappings graphs and their associated complexities. In addition the processor s may cause the spreadsheet to be displayed on a display to enable a user to identify how each of the mappings graphs has been categorized.

An example of a suitable summary spreadsheet is depicted in . As shown therein the summary spreadsheet identifies the folder name from which the mappings graphs are obtained the names of the individual mappings graphs for instance for a particular computing job the transformation points and the SQL override points associated with each of the mappings graphs the total points associated with each of the mappings graphs and the complexity assigned for each of the mappings graphs .

In addition the processor s estimates the complexity of the computing job based upon the respective complexities of the mappings graphs and the SQL overrides data objects as indicated at step . The processor s may estimate the computing job complexity by applying a mathematical function to the complexity levels of the mappings graphs . The mathematical function may be the median mean average the highest estimated complexity level the lowest estimated complexity level etc. of the estimated complexities of the mappings or graphs data objects .

With reference now to a business objects environment containing universes and reports the storage device comprises a business objects BO repository. In addition the data objects comprise BO universes and BO reports. The elements of the BO universes comprise universe components and the elements of the BO reports comprise report components.

Generally speaking the processor s is configured to employ the method in the BO environment to analyze the complexity of a computing job in this case a BO application comprised of BO universes and BO reports. In one embodiment the complexity of the BO application may be used to calculate the full time employees and resources required in a knowledge transfer of an application or an application outsourcing project. In one respect the complexity of the existing BO universes and BO reports may be useful while estimating resources and the size effort of change problem requests in a support project.

The processor s is configured to identify and retrieve selected BO universes and BO reports data objects at steps and . The BO reports may comprise Deski and Webi reports for instance and may have been created through implementation of the method . The selection of the BO universes and BO reports at step may be based upon a historical knowledge of the relevance of the BO universes and BO reports to a particular BO repository as discussed above with respect to step in . In addition at step the processor s is configured to select universe components and report components elements . As described above again with respect to the universe components and report components selected at step may be based upon a historical knowledge of the relevance of each of the universe components and report components to their respective BO universes and BO reports.

An example of a predefined schedule of universe components points percentages and complexity levels is depicted in . In addition an example of a predefined schedule of report components points percentages and complexity levels is depicted in . The predefined schedules and may be stored for instance as lookup tables in the storage device and the processor s may access the predefined schedules and in assigning points to the selected universe components elements and the selected report components at step in the method . The listing of universe components and points in the predefined schedule and report components and points in the predefined schedule is intended to provide particular examples of suitable predefined schedules and and is thus not intended to limit a scope of the invention to the universe components and points and report components and points depicted therein. It should thus be understood that the predefined schedules and may include additional universe components report components or existing universe components report components may be removed or replaced and the points points may also vary from that shown in without departing from a scope of the invention. The predefined schedules and may also include a greater or a lesser number of complexity designations that what is shown therein without departing from a scope of the invention.

The predefined schedule includes a column containing the number of respective universe components contained in a BO universe data object and a column containing the points assigned to each of the universe components . The predefined schedule also includes three columns that identify when each of the number of respective universe components is considered to have a simple medium or complex complexity.

As shown in the columns that define the respective complexity levels the complexities of the universe components are defined based upon differing thresholds. For instance with respect to the first universe component the universe component pertaining to tables views is considered to have a simple complexity when the number of tables views is between 0 20 a medium complexity when the number of tables views is between 21 30 and a complex complexity when the number of tables views is greater than 30. In addition the remaining universe components are considered to have a simple medium and complex complexity levels under differing thresholds of numbers as compared with the first universe component .

The threshold values for determining the complexity levels of each of the universe components and the points assigned to the universe components may be based upon a historical knowledge of the relevance of each of the universe components to the BO universe data object . As such the threshold values for the complexity levels and the point values may be user defined and may also vary for different BO universes and applications.

The predefined schedule includes a column containing the number of respective report components contained in a BO universe data object and a column containing the points assigned to each of the report components . The predefined schedule also includes three columns that identify when each of the number of respective report components is considered to have a simple medium or complex complexity.

As shown in the columns that define the respective complexity levels the complexities of the report components are defined based upon differing thresholds. For instance with respect to the first report component the report component pertaining to tables views is considered to have a simple complexity when the number of tables views is between 0 2 a medium complexity when the number of tables views is between 3 6 and a complex complexity when the number of tables views is greater than 6. In addition the remaining report components are considered to have simple medium and complex complexity level designations under differing thresholds of numbers as compared with the first report component .

The threshold values for determining the complexity levels of each of the report components and the points assigned to the report components may be based upon a historical knowledge of the relevance of each of the report components to the BO reports data object . As such the threshold values for the complexity levels and the point values may be user defined and may also vary for different BO reports and applications.

With respect to the method at step the processor s assigns respective points to each of the universe components based upon the predefined schedule and the respective points to each of the report components based upon the predefined schedule . In addition at step the processor s calculates scores for each of the universe components contained in the BO universe by determining the numbers of each of the identified universe components in the BO universe and for each of the report components contained in the BO report by determining the numbers of each of the identified report components in the BO report. As such the processor s may perform an initial step of identifying the universe components and counting each instance of the universe components and identifying the report components and counting each instance of the report components.

The processor s also estimates complexities of the BO universes data objects based upon the complexity designations of the respective universe components and their assigned points percentages and the BO reports data objects based upon the complexity designations of the respective report components and their assigned points percentages as indicated at step . As such for example the first universe component tables views may be identified as having a medium complexity and the second universe component classes may be identified as having a simple complexity. In this example the medium complexity designation is weighted at 25 and the simple designation is weighted at 3 of the total complexity level. In addition the processor s may use the weighted complexity designations of each of the universe components and the report components to identify the complexity of the BO universe containing the universe components and the BO report containing the report components . For instance the processor s may estimate the complexity of the BO universe to be equivalent to a weighted average of the complexity designations of the universe components and the complexity of the BO report to be equivalent to a weighted average of the complexity designations of the report components .

The processor s may further estimate a complexity of the computing job based upon the complexities of one or more of the BO universes and BO reports data objects at step . For instance the processor s may determine a complexity designation of the computing job based upon a weighted average of the complexities of the BO universes and BO reports. The complexity designation of the computing job is based upon a weighted average of the BO universes because the BO universes may have different levels of relevance with respect to the computing job and may thus be weighted differently from each other. Likewise the complexity designation of the computing job is based upon a weighted average of the BO reports because the BO reports may have different levels of relevance with respect to the computing job and may thus be weighted differently from each other. The weighting of the BO universes and the BO reports may also be user defined and may thus be based upon a historical knowledge of the various relevance levels of the BO universes and BO reports.

The processor s may further estimate resources required to perform the computing job based upon the complexity of the computing job at step . Thus by way of a particular example the processor s may employ the computing job complexity in determining the number of full time employees required to perform knowledge transfer of an application for an application outsourcing project.

According to an embodiment the processor s is configured to accumulate the complexities of the BO universes and the BO reports and to provide a summary of the complexities of each of the BO universes and BO reports. For instance the processor s may create a spreadsheet containing a summary of the universe components and the report components and their associated complexities. In addition the processor s may cause the spreadsheet to be displayed on a display to enable a user to identify how each of the BO universes and the BO reports have been categorized.

The computing apparatus includes one or more processors such as the processor s . The processor s may be used to execute some or all of the steps described in the methods and . Commands and data from the processor s are communicated over a communication bus . The computing apparatus also includes a main memory such as a random access memory RAM where the program code for the processor s may be executed during runtime and a secondary memory . The secondary memory includes for example one or more hard disk drives and or a removable storage drive representing a floppy diskette drive a magnetic tape drive a compact disk drive etc. where a copy of the program code for the methods and may be stored.

The removable storage drive reads from and or writes to a removable storage unit in a well known manner. User input and output devices may include a keyboard a mouse and a display . A display adaptor may interface with the communication bus and the display and may receive display data from the processor s and convert the display data into display commands for the display . In addition the processor s may communicate over a network for instance the Internet LAN etc. through a network adaptor .

What has been described and illustrated herein is an embodiment along with some of its variations. The terms descriptions and figures used herein are set forth by way of illustration only and are not meant as limitations. Those skilled in the art will recognize that many variations are possible within the spirit and scope of the subject matter which is intended to be defined by the following claims and their equivalents in which all terms are meant in their broadest reasonable sense unless otherwise indicated.

