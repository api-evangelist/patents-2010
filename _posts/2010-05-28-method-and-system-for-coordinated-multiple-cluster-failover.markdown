---

title: Method and system for coordinated multiple cluster failover
abstract: Hyperclusters are a cluster of clusters. Each cluster has associated with it one or more resource groups, and independent node failures within the clusters are handled by platform specific clustering software. The management of coordinated failovers across dependent or independent resources running on heterogeneous platforms is contemplated. A hypercluster manager running on all of the nodes in a cluster communicates with platform specific clustering software regarding any failure conditions, and utilizing a rule-based decision making system, determines actions to take on the node. A plug-in extends exit points definable in non-hypercluster clustering technologies. The failure notification is passed to other affected resource groups in the hypercluster.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08429450&OS=08429450&RS=08429450
owner: Vision Solutions, Inc.
number: 08429450
owner_city: Irvine
owner_country: US
publication_date: 20100528
---
This application is a continuation application U.S. patent application Ser. No. 11 732 670 entitled METHOD AND SYSTEM FOR COORDINATED MULTI CLUSTER FAILOVER filed Apr. 4 2007 now U.S. Pat. No. 7 757 116 the disclosure of which is wholly incorporated by reference herein.

This invention generally relates to the administration of distributed data processing systems. More particularly this invention relates to methods and systems for coordinating the use of data processing resources across multiple clusters of computers in the event of a disruption.

In order to be competitive businesses must maintain continuous operations with as little interruption as possible and maximize business uptime. Businesses rely heavily on enterprise wide back end computer systems to support its operations from handling electronic mail managing inventories providing critical data to widely dispersed employees to serving web pages and taking product orders from consumers in online sales. Oftentimes the components that these functions require are shared. For example a database maintains the inventory of a company s products for internal control purposes but it is also modified when a customer makes a purchase through a website. The same database may also be accessed by employees in the field through the company s extranet in order to retrieve sales information for strategic planning. Thus because the functions performed by the computer system are so tightly integrated with the operations of the business their availability is directly related to business uptime. It is therefore critical that businesses have computer systems with high availability.

The key element of high availability computer systems is not having a single point of failure. Prior enterprise level computing solutions often utilized a single mainframe computer running all of the services required by the entire business. Despite the improved reliability of individual hardware and software components if there were a failure in just one component the entire system would become inoperative. Additionally if maintenance were required the system would likewise be inoperative while such maintenance tasks were completed. In response clustering of computer systems was developed.

A cluster is a collection of logically grouped computers operating as a unit. One type of cluster configuration treats an entire group of computers as a single computer. This configuration is instituted for performance where numerous low cost consumer level computers having easily replaceable components are tied together to process tasks in parallel. Such a configuration enables high processing capabilities. Another configuration groups two or more similar computers for fault tolerance purposes. If one computer goes offline the other takes over operations. The user sees no noticeable difference because both the data and any running applications are replicated across the two systems.

With simpler overall information systems architecture running fewer applications and data exchanges past enterprise computing solutions segregate discrete operational units and strictly limits data and application sharing. Separate computer systems are in place for each division of an enterprise. For example the accounting department has a separate computer system completely isolated from the engineering department. To the extent that clustering is utilized in these enterprise computing solutions it is merely to provide failovers within these departmental systems.

Because of cheaper network components and the need for rapidly sharing large volumes of information it is necessary to consolidate computer systems across the enterprise. Companies often rely upon a single platform that is a single operating system and a single set of application and data services for the computing needs of the entire enterprise. In order to ease administration servers are organized according to their roles within the network. Since the roles are interdependent however there is also a need to manage the operation of these services from a single control point. Such a system is contemplated in U.S. Pat. No. 6 438 705 to Chao et. al. which discloses the transfer of operations from one node to another. There remains a need for coordination of actions between clusters. Even though a node of one platform may only fail over to another node of the same platform with mergers and growth of business organizations such resources across diverse platforms can no longer be viewed as completely independent resources. Accordingly failover switchover actions must be coordinated at the enterprise level. There may be dependencies between resources where some resources need to be available before others are initiated. In other words an ordering of activities is required. As multiple users may be managing these large and diverse networks a method is needed to ensure that conflicting actions are not performed.

Organizations are increasingly relying on different platforms to provide an overall enterprise computing system. A particular platform may have features required by the organization that another may not or different platforms may have been deployed to reduce single points of failure. In single platform there is an inherent danger that a common problem across the same operating system and applications will bring the entire computer system down. Additionally it is desirable to provide one service on one platform and a second service that is dependent on the first service on another platform. An example of such a configuration would be a database server operating on one platform and a web server operating on another platform with the web server retrieving data from the database server. In this regard the web server is dependent upon the database server. Although one server may be in a remote location with respect to another server in some instances the servers may be running on different partitions of a single hardware platform in a single location. For example System i previously known as AS 400 and developed by IBM Corporation of Armonk N.Y. can host a partition running the AIX operating system also developed by IBM a partition running the Linux partition an i5 OS partition as well as multiple Windows developed by Microsoft Corporation of Redmond Wash. partitions running on integrated xServer cards also developed by IBM .

In each of the above described system configuration scenarios there are dependencies between the servers that is the availability of one server is predicated on the availability of another server. In certain situations it may also be necessary to start up each such server groupings in a particular order. Continuing with the example above the Linux partition may utilize a DB2 database also developed by IBM from an i5 OS partition via the Open Database Connectivity ODBC or Java Database Connectivity JBDC Application Programming Interfaces API . This may be a local virtual connection that requires the application running on the Linux partition to be switched to a backup System i platform when the i5 OS partition is switched.

Accordingly there is a need in the art for coordinating the failover of such diverse operating systems with each dependency between the platforms being accounted for. More particularly there is a need for a system capable of providing a rules structure for handling the aforementioned dependencies. A system that coordinates the activation of applications and services in the proper order is needed in the art.

In accordance with one aspect of the present invention a method and system for managing multiple heterogeneous clustering platforms is disclosed. This overall network is referred to as a hypercluster. In further detail one embodiment of the present invention relates to a method for coordinating availability of data processing resources between first and second clusters of nodes. The method may include the step of receiving a disruption event associated with the first cluster. Thereafter the method may include deriving a local action code from a hypercluster rules list. The local action code may correspond to the disruption event and contain a cluster activation sequence. Furthermore the method may include transmitting the local action code to an active cluster manager for execution of the cluster activation sequence.

Another embodiment of the present invention relates to an apparatus for coordinating availability of data processing resources between a local node in a first cluster and a remote node. The apparatus may include a local event receiver for capturing local disruption events as well as an event translator for translating the local disruption event to a universal event code. Furthermore the apparatus may include a hypercluster event receiver for capturing remote disruption events from one of the nodes of the second cluster. The apparatus may further include a router for correlating the universal event code to a cluster activation sequence in accordance with a set of hypercluster rules.

The above mentioned hypercluster rules are contemplated as being simple declarative rules with no additional procedural programming being necessary for improving administrative ease. Where further control is necessary the hypercluster rules may be extended to utilize external programming.

The present invention will be best understood by reference to the following detailed description when read in conjunction with the accompanying drawings.

Common reference numerals are used throughout the drawings and the detailed description to indicate the same elements.

The detailed description set forth below in connection with the appended drawings is intended as a description of the presently preferred embodiment of the invention and is not intended to represent the only form in which the present invention may be constructed or utilized. The description sets forth the functions and the sequence of steps for developing and operating the invention in connection with the illustrated embodiment. It is to be understood however that the same or equivalent functions and sequences may be accomplished by different embodiments that are also intended to be encompassed within the spirit and scope of the invention. It is further understood that the use of relational terms such as first and second and the like are used solely to distinguish one from another entity without necessarily requiring or implying any actual such relationship or order between such entities.

With reference to at the most basic level a high availability computer cluster includes a primary node and a secondary node . One object of high availability clusters is to provide redundancy and eliminate single points of failure and so when there the primary node becomes unavailable for planned or unplanned reasons the secondary node is immediately able to take over operations. While both primary and secondary nodes and are concurrently running absent failure or user initiated planned switchovers only the primary node provides service to a recipient over an external network connection . As utilized herein the term recipient may refer to a client another server or any other apparatus operative to receive the services provided by the primary node . Such configurations are referred to as an Active Passive configuration. Alternatively the load upon the primary node may be shared with the secondary node in an Active Active configuration. While the computer cluster has been described in terms of the primary node and the secondary node one of ordinary skill in the art will recognize that additional backup nodes may be utilized. The high availability computer cluster is the building block for the network of computers upon which one embodiment of the present invention may be implemented.

In the exemplary embodiment of the computer cluster the primary node and the secondary node are both connected to an internal network over an interconnect device . The external network connection is also tied to the interconnect device . As those skilled in the art will appreciate the internal network can be any suitable local area network such as Ethernet I.E.E.E. 802.3 or Token Ring. Further interconnect device can be a hub a router or any suitable network interconnect device suitable for a selected local area network. In the above described active passive high configuration the computer cluster is assigned a single logical host address that the recipient can identify. Thus when the recipient requests a service from the computer cluster it need only specify the logical host address. From the perspective of the recipient the computer cluster appears as a single computing resource regardless of the node providing the service. In order to coordinate the operation of the primary node and the secondary node a separate heartbeat network connection is provided. The primary node monitors the status of the secondary node and the secondary node monitors the status of the primary node over the heartbeat network connection .

Data being processed by the computer cluster is stored in one or more data storage devices connected locally or via a storage area network SAN . Additional data storage devices may be added to increase redundancy and improve availability. The data storage device appears as a locally connected device to the primary node and the secondary node in that data is written to and read from the storage device at the block level. Typically the storage area network uses the Small Computer System Interface SCSI protocol over at least a 1 gigabit fiber channel. Instead of a SCSI connection over a fiber transmission line the storage area network may utilize the Internet SCSI iSCSI protocol over a conventional LAN networking technology. The data storage device is commonly a hard drive although tape drives and other similar devices may be utilized.

The primary node and the secondary node are both data processing devices that have the general configuration including various software components and hardware devices as illustrated in of a computer system . More particularly there is processor a random access memory a storage device and a network interface device . With respect to the processor it is understood that any general purpose microprocessor may be utilized and based on instructions being processed thereby controls the operation of the computer system . In this regard the functionality of the random access memory the storage device and the network interface device is controlled by the processor . Physically the processor is connected to the respective devices via a bus or other like channel. As indicated above the storage device may be a SAN device connected over a fiber channel and the network interface device may provide Ethernet or other like networking services. The hardware devices and may be interfaced to the processor with additional controllers and other like hardware devices. The operation of the hardware devices and are governed by an operating system that may include various modules of instructions executed by the processor to control the hardware devices and . In further detail the random access memory is controlled by a memory manager the storage device is controlled by a controller and the network interface device is controlled by a network driver . The modules provide an interface between a hardware level and a software level . One of ordinary skill in the art will recognize that the above hardware components and organization have been presented by way of example only and not of limitation and any other suitable components may be added as necessary.

As indicated above the operating system may be any one of the numerous computer operating systems known in the art. For example the operating system may be a UNIX variant such as AIX previously known as Advanced IBM Unix Linux FreeBSD or a Microsoft Windows or other operating systems such as i5 OS previously known as OS 400 . Various applications interact with the operating system such as a web server application a database server application and so forth. With respect to the database server application however it is understood that in the i5 OS operating system it is integrated into the operating system .

There is also a cluster manager that is in communication with the server applications to determine the current state of availability with respect to such applications. Cluster manager is typically specific to the operating system . For instance the cluster manager where the operating system is Windows it may be the Microsoft Cluster Server MSCS where the operating system is i5 OS it is the ORION product from Vision Solutions Inc. of Irvine Calif. or System i clustering from IBM and where the operating system is Linux it may be the open source Linux HA High Availability or a proprietary clustering software such as the Orion Java based Linux Clustering software. It is understood that any suitable cluster manager may be substituted so long as it is compatible with and capable of interfacing with certain components that are in accordance with an aspect of the present invention. Generally the cluster manager coordinates failover and switchover actions between nodes that are concurrently running the same cluster manager . Specifically the cluster manager monitors the status of the other node s with the heartbeat functionality as described above fences off failed nodes from writing to data storage devices and receives configuration information from an administrator among other functions. It normally provides an exit program capability which may be utilized by users of the clustering service to provide further functionality such as managing applications. In one embodiment of the present invention it is used to provide an interface to a Hypercluster manager from the individual platform specific clusters.

With reference now to the block diagram of the logical relationships of the components making up a hypercluster is illustrated. As indicated above clustering is the concept of logically grouping individual computer nodes seen as a single unified computing resource by the clients of that resource and coordinating the transfer of operations within the cluster in the event of a planned or unplanned termination. Nodes or individual computer systems as described above are logically grouped into cluster resource groups also referred to herein as CRGs. Each cluster resource group provides some service or resource to a client as a single unit. Thus broadly defined the term hypercluster refers to one or more clusters. In further detail each cluster may be made up of one or more resource groups with heterogeneous clustering and operating system architectures. In accordance with one aspect of the present invention a Hypercluster manager handles various functions relating to the organization and management of the Hypercluster.

Generally recovery operations in the event of a node failure are performed within the resource group that is when a failure occurs in one node a second node takes over. The recovery may also be coordinated with other clusters in the hypercluster when dependencies are defined in hypercluster rules. According to this aspect of the present invention the failover switchover within one cluster resource group can be coordinated with failover switchover in another cluster resource group. The dependency types include location co locate and anti co locate sequence the order in which applications and services are activated and dependencies on foreign resources.

In the exemplary configuration illustrated in a variety of computer nodes running different operating systems are combined to form clusters of like operating systems. These clusters in turn are combined to form the hypercluster comprising heterogeneous operating systems. A given node is identified with the operating system running on that node and a given cluster or cluster group is likewise identified with the operating system running on the nodes of that cluster. By way of example only and not of limitation hypercluster is comprised of a Linux cluster a first i5 OS cluster and a second i5 OS cluster and a Windows cluster . The Linux cluster is a logical grouping of a first Linux node and a second Linux node . The resource provided by the first and second Linux nodes is identified as a Linux Cluster Resource Group CRG . As utilized herein a CRG is a group of associated resources that can be switched between various nodes in a given cluster. A single cluster may contain multiple resource groups comprising a set of services such as switched databases switched disks and the like.

The first i5 OS cluster is a logical grouping of a first i5 OS node and a second i5 OS node and the services provided thereby are identified as a first i5 OS CRG . The second i5 OS cluster is a logical grouping of a third i5 OS node and a fourth i5 OS node and the services provided thereby are identified as a second i5 OS CRG . Finally the Windows cluster is a logical grouping of a first Windows node and a second Windows node . The services provided by the first and second Windows nodes are identified as a Windows CRG .

At the most basic level if a single node fails in any of the clusters the other node in the cluster will take over its operations. For example if the first Windows node fails then the second Windows node assumes the role previously held by the first Windows node . It is to be understood that the specified arrangement is by example only and not of limitation as the two nodes can be members of multiple resource groups which are representative of different services running concurrently on the same nodes.

In accordance with another aspect of the present invention a method of coordinating failovers across multiple clusters is provided. Referring back to a hypercluster manager is in communication with the cluster manager and receives node status information therefrom as well as from other instances of hypercluster managers on other computer nodes. The hypercluster manager stores information relating to the dependencies amongst the resource groups in the hypercluster . In order to ensure portability of the hypercluster manager code it is preferably written in Java and runs on a Java Virtual Machine JVM . However it is to be understood that any programming environment that abstract the details of the specific platform on which it runs may be substituted for example by way of a virtual machine. Further details relating to the structure and modalities required for coordinating actions between clusters across diverse clustering architectures and operating systems will be described below.

In accordance with one aspect of the present invention a tokenized three phase commit is utilized at the cluster level for Linux AIX and UNIX. As understood other cluster protocols such as Windows Cluster Manager i5 OS cluster manager and others do not use a tokenized three phase commit. The foregoing protocols all provide the modality to trigger an exit program when an event occurs however. It is understood that the term event refers to an activity that can change or be part of a set of actions that change the state and or membership of a cluster and its associated resource groups. Events may include activities requested by a user or events such as power failures. Generally events affect the availability of various resources. The aforementioned exit programs may be leveraged as an exit plug in for various cluster managers so that the exit can be made to send an event to the Hypercluster manager. The Hypercluster manager may thus wait for the Hypercluster level tokenized three phase commit. Alternatively cluster rules can specify that the sending program need not be forced to wait and an immediate continue response is given to the exit program. The individual cluster can continue as predefined for the event while the Hypercluster events run asynchronously. It will be recognized by those having ordinary skill in the art that either of the foregoing may be implemented but depending on the types of relationships and the existence of timing or other restrictions in which view synchrony must be maintained at all times one may be preferred over the other. As utilized herein the term view refers to a perspective of all nodes in a cluster and their states including whether they are up operational or down inoperative and whether they are designated primary secondary etc. Since a Hypercluster contains a list of views of disjoint clusters the Hypercluster view is a view of all nodes in all clusters comprising the Hypercluster.

The finite state machine in accordance with one aspect of the present invention includes the ability for each State and Event Combination to perform three actions a rule search transmission of a response message and the performance of actions typically defined as exit programs or spawned procedures based on a match of the rules before performing a transition to the subsequent state. This state machine is used in designating the states of the various nodes in a cluster and or hypercluster under various conditions.

where Sigma input alphabet of symbols conditions S The set of states in the state machine and where S is not Empty sis the initial start state where s S to the state transition s S cond s S and F Set of Final or accepting states where F S.

where R a set of rules to be checked at State s A a set of actions to be performed and where sx cond x r R a A m M s S and M Resulting message on Rule Match as in A action above . This extended definition maintains the rules for finite state automata but permits the association of rule based actions to each transition in the state diagram.

The Orion Extended state machine manages the conditions and associated actions for these events. The Orion Extended Finite State Machine OEFSM looks at the events from the perspective of a cluster resource group but with an open interface to other resources groups. Therefore in order to consistently manage the hypercluster the protocol must achieve consistent views and consistent membership history. That is all of the components of the hypercluster must agree on the view in each Cluster Resource Group and on the sequence of changes to the state. If a node is removed from a particular cluster and associated Cluster Resource Groups all participants in the hypercluster must understand and share the changes to the view that this causes. This is due to the dependencies between operating systems.

The following is the definition used in consistent history view synchronicity protocols ConsistentHistory 0 

Where means for all refers to a union of two sets means equivalent to and refers to a Boolean OR . Additionally as utilized herein or sigma refers to the alphabet of conditions and means contained in and refers to a Boolean AND .

View History Consistency is a means of solving the case where a partitioned cluster allows some events to continue occurring asynchronously and where these events are then placed in a view so that all partitions in the cluster agree on the series of views in which events occur. According to another embodiment this restriction may be relaxed where the hypercluster can delay cluster events or the execution of generated events resulting in event serialization.

A view definition includes the members of a cluster and or hyperclusters and their roles. This relates to the need for consistent views and consistent membership. A view definition must be shared between members in the cluster even though the events are asynchronous. That is the messages must be passed between the clusters so that a new shared consistent view of the cluster and its resource groups is achieved. The token control ensures that conflicting actions are not taken by distinct users or external events so that consistent views can be achieved once all messages are processed in all clusters. The dependencies defined in the hypercluster rules ensure that the messages and state changes are processed in the proper order.

Generally node membership in a cluster is defined as Nodes Cluster if N C M C and If EmptySet. To prevent conflicting activities from occurring in disjoint clusters that is Cluster C contains node set and Cluster C contains node set M and where Disjoint 1 2 

the coordination must not be by states but rather by message passing as in a partitioned cluster. Token control must exist between the clusters of the hypercluster in order to properly serialize activities over all clusters.

Three phase commit refers to an algorithm with three steps to ensure that a process does not make a decision unless every process that has not yet failed is ready to make such decision. That is every non failed process must be guaranteed to achieve a Consistent View . As utilized herein the term process refers to a dynamic resource with a state. For example a process may be a database application such as a DB2 Database Management System running on a node with an active or an inactive state. It is understood that multiple processes can be hosted on a node in a cluster such as web servers communications processors and the like.

With reference to the states of the three phase commit include Waiting for Event WTEV Working for Token WTTK Waiting for Common View WTCV Have Common View HACV Have Token HVTK In Operation INOP Waiting for Commit WTCM Ready to Commit RDCM and Reject Commit RJCM . The events of the three phase commit include Received Token RCTK Received View RCCV and Received RollBack RCRB . The three phase commit actions include Receive Event RCEV Receive Token RQTK Receive Common View RCCV and Initiate Commit BGCM .

The three phase commit algorithm in accordance with one aspect of the present invention gets n processes to agree on a result which is either 1 or 0 for a commit decision. It requires 3n rounds as each process communicates with all other processes. The algorithm does not provide a solution to the view agreement problem or to the actions to be performed on the Orion Extended Finite State Machine and does not consider some economies that can be achieved by utilizing the Tokenized cluster approach. Thus there can only be one token owner and one token controller at any one point and so a single node proposes an action. As such it is possible to reduce the lower bound on the number of rounds as the cluster manager has incurred the overhead of performing heartbeat and token allocation. Therefore such tasks need not be repeated as part of the three phase commit. The following is a more formal description of this issue 

Token allocation View Agreement and Cluster state change transactions are each three phase commit cycles. A conventional termination protocol is utilized to detect cases where the requesting node fails at a point where the other processes are left in an undecided state. The tokenized three phase commit termination protocol is as follows if the token owner is not the token controller and the token is returned to the token controller after the commit phase the originating process did not fail so the termination protocol is not required. This is understood to be the vast majority of cases. Thus the number of rounds required in the normal case is shortened. After performing a commit action affected nodes other than initiator sends messages to the token controller to determine whether the token has been returned. If the token controller responds that it has possession of the token and that it is available no further termination required. If the token is lost after a commit action then all processes must assume zero. If the token owner is the token could token controller and if the token allocated state is set to not allocated then the requesting process is completed as long as the token controller has not changed. The view includes the token owner and the token controller and so this is available to all nodes.

The CRG is defined by a group of associated resources which can be switched between nodes in a cluster. A single cluster may contain multiple cluster resource groups comprising a set of switched databases switched disks etc. The CRG states include Waiting for Event State 0 or Start State WTEV Starting STRT AmPrimary ISPR Pending Bringing Applications UP PNPR and Bringing Application Down PNDN . The CRG events include Fail Over RCFO Switch Over RCSW App is Down RCDN and App is Up RCUP . CRG actions include Initiate Switch Over STSW and Initiate Fail Over SFTO .

According to another aspect of the present invention the world heartbeats required to monitor a large number of nodes in multiple clusters is reduced. As understood the number of heartbeats in an n to n cluster is Hearbeats n n 1 . For example a cluster would require six heartbeats. Further if there are two three node clusters performing the n to n heartbeating then the total number of heartbeats is 14. Specifically the first three node cluster requires six heartbeats the second three node cluster requires another six heartbeats and the hypercluster including two individual clusters requires another two heartbeats. If heartbeats are to be performed on each of the nodes in the above example there would be n n 1 or 6 5 or 30 heartbeats.

As indicated above the hypercluster is defined as a disjoint cluster where the hypercluster relationship is used to control autonomous events that happen outside the control of human actors conscious events that happened under the control or at the initiative of human actors and generated events that are generated based on input from an autonomous or a conscious event. Communications in the hypercluster include view changes from autonomous event request for view changes from initiated or generated events. More specifically 

It is understood that while a single cluster and a cluster resource group each have tokens a hypercluster may not. If serialized events are required a single HyperCluster token may be implemented although this may impact the speed with which actions are carried out in the individual clusters.

Cluster resource group relationships are within a cluster and between clusters in a hypercluster specifically between specific cluster resource groups in clusters. All cluster events including heartbeats are sent to all nodes in the cluster. With respect to the hypercluster the individual clusters do not share all the events with nodes in other clusters. Thus the hypercluster is responsible for informing other clusters of events having relevance thereto. All cluster resource group events where a hypercluster relationship exists are sent to the associated clusters along with the associated view of the cluster in which it occurred. It is understood that this maintains view consistency. Coordinated Failover defines dependencies during switchovers between multiple cluster resource groups in the hypercluster .

Generally the token use and allocation rules specify that there is one Cluster Token CT per cluster and or Hypercluster one Resource Token RT per cluster Resource Group one Token Controller TC per cluster and or HyperCluster and one Token Controller per Cluster Resource Group. If a Cluster Resource Group is partitioned it functions similarly to two cluster resource groups that is a one token exists for two partitions for a total of two. Accordingly there is provided state steps or phases at which the three phase commit will stop. The token use and allocation rules can be changed dynamically as the extended state machine can check state and sub state status and determine when the operation is complete. Sub states can define if the token is to be returned or not. The purpose of the FSA is to provide deterministic behavior for the state transitions by the Extended FSA allows behavior to be attached to the defined states. It is assumed that all view changes are sent to all related Cluster Resource Groups and that a state change is a view change.

In a preferred embodiment of the present invention there is provided meta rules or rules relating to the use of rules. Some sample meta rules include 

Specific definitions of meta definitions of exit program names and protocols are illustrated in the VISION CPML.DTD file of Appendix B. As will be appreciated by those of ordinary skill in the art such meta definitions are particular to the clustering application being managed in an installation.

In the exemplary illustration of the hypercluster is configured for redundancy in running a web application where the first i5 OS CRG provides a database service the Linux CRG provides web services. A web server may retrieve information from a database to generate web pages transmitted to remote user and so the availability of the web server on the Linux resource group depends on the availability of the database service on the first i5 OS CRG .

In an exemplary configuration as illustrated in suppose the first i5 OS CRG and the second i5 OS CRG provide database resources while the Windows CRG and the Linux CRG provide web server resources. A web server typically retrieves information from a database to generate web pages to transmit to a remote user and so the availability of the web service resource is said to be dependent on the availability of the database resources. In this exemplary configuration the web server Windows CRG dependent on the database second i5 OS CRG and the web server Linux CRG is dependent on the database first i5 OS CRG . Additionally as shown in the third and fourth i5 OS nodes and the first and second Windows nodes and are in a first location while the first and second i5 OS nodes and the first and second Linux nodes are in a second location . As indicated above the Windows cluster the first and second i5 OS clusters and the Linux cluster combine to form the hypercluster . In this configuration the resources are said to be co located and is a way to ensure availability in case all of the computers in one location go offline due to a catastrophe such as complete destruction power outage or other failure. Depending on the system administrator s configuration if either the database resource on the second i5 OS CRG or the web server resource on the Windows CRG fails the hypercluster transfers the databases resources to the first i5 OS CRG and the web server resources to the Linux CRG . Because one resource is dependent on the other the order of activating the resources may be crucial. In one instance the database resource on the first i5 OS CRG is brought online and then the web server resource on Linux CRG follows. This type of grouping is referred to as a regular dependency. On the other hand there may be situations where the order of activating the resources may be inconsequential. Such a configuration is referred to as a regular relationship.

Referring now to an example of the steps taking place after a resource group failure occurs will be explained from the perspective of the hypercluster . The first and second Windows nodes and form the Windows cluster the third and fourth i5 OS nodes defines the second i5 OS cluster the first and second Linux nodes forms the Linux cluster and the first and second i5 OS nodes form the first i5 OS cluster . As understood the groupings are discussed in terms of clusters and not CRGs.

The roles of the cluster managers and hypercluster managers on each of the nodes are substantially the same. Instances of cluster managers and coordinate failovers within the Windows cluster and cluster managers and coordinate failovers within the second i5 OS cluster . Further cluster managers and coordinate failovers within the Linux cluster and cluster managers and manage failovers within the first i5 OS cluster . Hypercluster managers and collectively manage failovers across clusters and reside on the first Windows node the second Windows node the third i5 OS node the fourth i5 OS node the first Linux node the second Linux node the first i5 OS node and the second i5 OS node respectively.

The nodes in the Windows cluster and the second i5 OS cluster are connected to each other via local area network connections and a first router and data originating from these nodes is transmitted from network interface devices . As indicated above these clusters are in the first location . The nodes in the Linux cluster and the first i5 OS cluster are connected to each other via local area network connections and a second router and data originating from these nodes is transmitted from network interfaces . These two clusters are in the second location and are physically separated from the first location . Both the first and second routers are connected to the Internet via Internet connections and

As can be seen each node in the Hypercluster is capable of communications with every other node in the same. The illustration of the Internet here is by way of example only and not of limitation and any suitable wide area network may be used. Further it should be understood that the organization of the particular hypercluster shown in is also by way of example only and that there are numerous configurations in which aspects of the present invention may be implemented.

a. A single node in any of the clusters fails but the system is operational and there are no dependencies defined between the application on the failed node and other applications in other clusters. For example a web server on the first Windows node crashes. The remaining nodes in the hypercluster is still operational and able to communicate with the other nodes and so the cluster manager is able to transmit a message to the cluster manager and the second Windows node in particular a web server takes over the web server functions. The cluster manager will transmit a message to hypercluster manager that this has occurred and takes action accordingly. Once the cluster manager receives notice of the failure in the first Windows node it also transmits a message to the hypercluster manager that a failure has occurred. In this arrangement failure of the first Windows node does not create availability problems because it is handled intra cluster. When the aforementioned notice is sent to the hypercluster manager there are no relationships found so there is no need for operations to be transferred to a separate cluster. No further action is required of either the hypercluster manager or and each issues a continue response to the cluster manager respectively. This decision is based on specified dependency rules.

b. A single node fails with dependencies to another node but that single node is completely offline and unable to communicate with any other node. For example a power outage forces Windows node offline. Further dependency rules dictate that the web server runs off of a database application and that the web server runs off of a database application . After the first Windows node is interrupted the cluster manager detects the event and accordingly transfers web serving functionality to the second windows node . A cluster exit program associated with the second windows node which was just been transferred operations calls the Hypercluster manager . Based upon the aforementioned dependency rules the Hypercluster manager determines that the database functionality must be transferred over to a database application from the database application and must be active before the web server on the second windows node is activated. The cluster manager transmits a message to the hypercluster manager where control is transferred from the database application to the database application . When this is available the exit program on the second Windows node is instructed to continue. As understood this decision making process is based on a specified rule.

c. A resource provided by a cluster is manually ended for maintenance with dependencies defined between the resource and the nodes in the cluster. For example the third and fourth i5 OS nodes may include a switched disk with the web servers dependent thereupon. Furthermore data for the database applications on the third and fourth i5 OS nodes may be hosted or stored on the switched disk . Upon manual termination of the third i5 OS node the exit programs associated with the cluster manager sends a message that a switchover has been requested. The exit program from the cluster manager of the third i5 OS node sends a message to the local Hypercluster manager that the third i5 OS node is being deactivated. The message is translated and the rules are checked for dependencies. In the present example a dependency may state that the first Windows node must be linked to the third i5 OS node and the second Windows node must be linked to the fourth i5 OS node . In this configuration the rule will direct that when the database applications on the second i5 OS cluster performs a switchover the web servers in the Windows cluster also performs a switchover. Because the web services are dependent on the database service it is necessary to start the switchover process on the second i5 OS cluster before starting the switchover process on the Windows cluster .

d. An entire cluster fails resulting in a separate cluster assuming responsibility. For example both the third and fourth i5 OS nodes fail resulting in the first and second i5 OS nodes taking over. More particularly all of the nodes in a cluster fail and is unable to communicate with any other nodes in the Hypercluster . Unlike the previous three scenarios there is no heartbeat failure and the failing node does not report an event to the local Hypercluster manager . According to one embodiment Hypercluster wide heartbeating is not performed. Still if any resources exist that are dependent on the resource the other cluster managers recognize this event as a failure of a needed resource. If rules are defined such that a switchover to the first i5 OS cluster or others is to be made then the event will be sent thereto via the Hypercluster manager . An example of the syntax and required fields in the Hypercluster rules is illustrated in the HC RULES.DTD XML data type definition file of Appendix A. By way of example if the Hypercluster rules Site List is Windows CRG and the event list is COMM to represent that it cannot communicate with a cluster a message is sent to ATTLIST ID and Site ID the second i5 OS CRG with the same group ID and Event Name START to start that cluster.

It is understood that the platform specific plugin for the Windows cluster defines a message to the exit program to modify the floating or takeover IP address to ensure that any connections are made to the first i5 OS cluster group instead of the first i5 OS cluster group . The local takeover IP address associated with the second i5 OS cluster group is insufficient to cause clients to move to the first i5 OS cluster group . If the first i5 OS cluster is defined as the backup cluster the third and fourth i5 OS nodes immediately transfer operations to itself.

Again it is at the option of the system administrator to transfer the operations of the web server and such a decision is determined by a rule. By way of example only and not of limitation this will typically occur as multiple nodes under the same operating system and within the same cluster. As will be appreciated by one of ordinary skill in the art the foregoing example will most likely not be encountered in actual configurations. One possible scenario is where a web server is generating local data and or using static data that can be used from a backup. Any replicated databases or switched disks would typically be in the same cluster so any data completely outside a cluster would most likely not have a current database. Nevertheless the foregoing example illustrates one aspect of the present invention in which automated switchover actions including any dependencies attendant therewith can be performed between independent clusters despite the fact that there is no hypercluster wide heartbeating amongst the constituent clusters.

e. Manual switchover initiated on one node where the node has a dependency with another cluster. By way of example a manual switchover may be initiated between the first Linux node and the second Linux node where both have dependencies to the i5 OS cluster . Thereafter the i5 OS cluster may make a switch from the first i5 OS node to the second i5 OS node. In this instance the cluster manager is instructed by a user action to switch over operations to the second Linux node possibly due to planned maintenance on the first Linux node .

Further details relating to the functionality of the Hypercluster managers in relation to the above contingencies will be considered more fully below.

In the foregoing examples references were made to rules. These rules are typically set by the system administrator but need not be human generated. In an exemplary configuration the rules can be generated by autonomic agents capable of adjusting the parameters of the hypercluster for optimal efficiency based on historical data and other criteria.

With reference now to the detailed operation of the hypercluster manager will be provided. In its most basic and generic form the hypercluster manager receives an input processes that input and generates a response. More specifically the hypercluster manager receives event notifications from the node on which it resides local node and other nodes in the hypercluster . Additionally it receives input from either a local node user or another node in the hypercluster in the form of updates to the rules utilized to make decisions on how to control the hypercluster. Based on these inputs hypercluster manager generates as outputs instructions for actions to be taken on the local node or dispatches events notifications to the other nodes. An event translator translates all platform specific cluster events to a global hypercluster event identifier.

The first type of input is an event notification from the local node. The cluster manager detects an event on the local node and the hypercluster manager receives notice of this event by capturing it from the cluster manager . The format and meaning of the local event code is only understood by the local cluster manager and is generally not recognized by the cluster managers on other platforms. The local event code may further include extensions to existing events and notification objects. The event translator receives the local event code and using an event translation table converts same to a format understood by a hypercluster rule engine and event action router referred to as the hypercluster router . The Event translation table is preferably an Extensible Markup Language XML file that maps the local event code to a corresponding hypercluster event code but can also be any object that describes the relationship between a local event code and hypercluster event code. An example data type definition of the event translation table is shown in the TRANSLATOR RULES.DTD file in Appendix C. Following its translation local event code is placed in a local event queue .

The hypercluster router retrieves the hypercluster event code as well as a hypercluster rules list . In an exemplary configuration hypercluster rules list is an XML file structured to include the following elements representative of 1. The hypercluster event code 2. The node and resource group from which it was received and 3. The hypercluster action code the action to take in response to the event code . Appendices A B and C contain examples of XML that provides the Hypercluster and event translation rules. The hypercluster event code can be representative of a node that has already started or stopped or that has begun starting or stopping.

Additionally the hypercluster router searches for a rule where the first and second elements of the XML file matches the retrieved hypercluster event code and node resource group. After finding such a rule hypercluster router extracts the third element hypercluster action code and places that information in a hypercluster actions queue . A hypercluster actions handler subsequently retrieves the hypercluster action code from the queue and converts it to a local action code which is an action code in a format understood by local cluster manager . The local action code is captured by local cluster manager and implements that action. This action can either be starting the node or stopping the node.

Simultaneously the hypercluster event code is sent to all of the hypercluster managers in the other nodes that hypercluster rules list requires. The hypercluster router and the Hypercluster Actions handler determine if the event impacts the receiving cluster. The hypercluster event code is first placed in an outgoing events queue and retrieved therefrom by a hypercluster events dispatcher agent . The hypercluster events dispatcher agent then creates a packet of information conforming to a hypercluster protocol containing the destination node the origin node and the retrieved hypercluster event code. The packet is then sent to the destination node.

A second type of input is an event notification from a remote node. A remote hypercluster manager transmits a message to the local node in the same fashion as described above. The remote hypercluster manager places an event code into a hypercluster protocol packet and transmits it to specified destination nodes. When the local hypercluster manager receives such a packet the hypercluster events dispatcher agent extracts the hypercluster event code and determines whether its destination is the local node. Because the destination information is encapsulated into the packet the extracted destination information is compared with the local node identification. If the two values match that is the local node and the extracted destination node are equivalent the hypercluster event code is placed into an incoming events queue . Otherwise the event is discarded as having no impact on the node.

Hypercluster router then retrieves the event code from the incoming events queue and initiates a processing procedure similar to that described above for local event codes. The hypercluster router retrieves the hypercluster rules list and searches for a rule where the event and source node resource group elements of the XML file matches the retrieved hypercluster event code and node resource group. After finding such a rule the hypercluster router extracts the hypercluster action code if any and places that information in the hypercluster actions queue . The hypercluster actions handler subsequently retrieves the hypercluster action code from the queue and converts it to a local action code which is an action code in a format understood by local cluster manager . The local action code is captured by local cluster manager and implements that action. This action can likewise be either starting the node or stopping the node. There may be a platform specific plugin which facilitates this capture of the local action code.

A third type of input is an update to the hyperclusters rules list . Such updates may originate from a local node where a system administrator makes changes to the list locally. It may also originate from a remote node where a change made from that remote node is being propagated to the local node. The details of these update procedures will not be discussed in turn. It should be noted here that in each hypercluster manager there is a different version of the hypercluster rules list . Only rules affecting the node on which it resides are in the list.

A system administrator may locally modify the hypercluster rules list . A Java user interface is a client application running on a separate computer but is on the same physical network as the local node. The application receives configuration and rules changes from the system administrator and transmits it to a Java listener server . A rules propagator verifier receives the updates confirms that the update maintains the integrity and syntactical correctness of the hypercluster rules list and writes those updates. Additionally if the updates affect the rules on any other node the new or changed rule is transmitted to those nodes via the hypercluster protocol . As an example the system administrator creates a rule that starts a remote resource when the local resource is stopped. The change is made to local version of the hypercluster rules list and the rules propagator and verifier places the change in a hypercluster protocol message destined for the nodes that are part of the remote resource. The tokenized view synchronicity protocol is used to ensure that multiple conflicting changes to the rules are propagated.

A remote node can also make changes to a local rule. A message in accordance with the hypercluster protocol is received by the Java listener server component and examines the message to determine that it is a rule. The change is then sent to the rules propagator verifier which in turn modifies the hypercluster rules list to reflect the same.

Further details pertaining to the processing of various data by the Hypercluster manager in the aforementioned use cases a e will now be described.

a. A single node in any of the clusters fails but the system is operational and there are no dependencies defined between the application on the failed node and other applications in other clusters. Upon a failure of the first Windows node the cluster manager on the second Windows node recognizes the node failure event. The aforementioned exit program transmits to the hypercluster manager the local cluster event to the event translator . The event translator converts the local cluster event to a Hypercluster event identifier with the event translation table . The translated Hypercluster event identifier is stored in the local events queue and is further processed by the Hypercluster router . The Hypercluster router verifies the Hypercluster rules list and determines that no dependencies exist on the failover of the Windows cluster . An appropriate response as dictated by the Hypercluster rules list is transmitted to the hypercluster actions queue . The response is processed by the Hypercluster actions handler which sends a message back to the local cluster manager that it may proceed without dependencies. As indicated above the exit program called to communicate with the local cluster manager is defined in the VISION CPML.DTD file of Appendix B. Additional parameters needed to send responses to the particular cluster manager is also defined therein. Thereafter the cluster manager of the second Windows node proceeds with the switchover.

b. A single node fails with dependencies to another node but that single node is completely offline and unable to communicate with any other node. As explained above the first Windows node may fail and dependency rules dictate that the web server runs off of a database application and that the web server runs off of a database application . Upon detecting the failure in the first Windows node the cluster manager in the second Windows node forwards that local cluster event to the event translator . The event translator references the event translator table to determine the Hypercluster event identifier which is stored in the local events queue . Thereafter the Hypercluster router receives the hypercluster event identifier and based upon the Hypercluster rules list it is determined that a dependency exists between the second i5 OS cluster .

The appropriate local node response of transferring the web server to the web server on the second Windows node is also determined from the processing of the Hypercluster rules list and placed in the hypercluster actions queue . Because it is specified that the database application must be available prior to initiating the second Windows node the local node response in the hypercluster actions queue is suspended momentarily until the availability of the database application on the fourth i5 OS node is confirmed.

The appropriate response in the third and fourth i5 OS nodes is stored in the outgoing hypercluster events queue and transmitted to the hypercluster managers and of the third and fourth i5 OS nodes respectively. The switchover within the second i5 OS cluster is performed. Upon completion of these actions a token is transmitted back to the Hypercluster manager of the second Windows node serving as the confirmation of availability of database application . More specifically the token is retrieved by the Hypercluster events dispatcher agent queued in the incoming hypercluster events queue and processed by the Hypercluster router . Recognizing that the previous operation was suspended pending receipt of the token the Hypercluster router instructs the hypercluster actions handler to continue with the above described local node response. Such instructions are processed by the Hypercluster platform specific plugin and performed by the cluster manager 

c. A resource provided by a cluster is manually ended for maintenance with dependencies defined between the resource and the nodes in the cluster. For example the third and fourth i5 OS nodes may include a switched disk with the web servers dependent upon the same. When the third i5 OS node is ended for maintenance the cluster manager initiates an exit program thereby generating the local cluster event in the Hypercluster manager . The local cluster event is forwarded to the event translator where it is translated to a Hypercluster event identifier by referencing the event translation table . The Hypercluster event identifier is temporarily stored in the local events queue and is processed by the Hypercluster router . The Hypercluster router processes the Hypercluster rules list which includes a rule that Windows cluster is dependent on the availably of the second i5 OS cluster . Additionally the rule states that the database application must be available before the Windows cluster is switched over. An instruction directing the first Windows node to shut down is stored in the outgoing Hypercluster events queue and transmitted to the Hypercluster manager via the Hypercluster events dispatcher agent .

The first Windows node is shut down and a confirmation is transmitted back to the Hypercluster manager of the third i5 OS node . Upon receiving this confirmation the Hypercluster router generates instructions to the cluster manager to shut down the third i5 OS node and also instructs the Hypercluster manager to switch over. After the takeover is complete the Hypercluster manager transmits a message to the hypercluster manager that the fourth i5 OS node specifically the database application thereon is active. The cluster manager on the second Windows node is then activated.

d. An entire cluster fails resulting in a separate cluster assuming responsibility. By way of the example above the first i5 OS cluster may take over upon failure of the second i5 OS cluster that is when both the third i5 OS node and the fourth i5 OS node fail. No event is triggered at this point because each cluster is responsible for heartbeating and sending its status to the Hypercluster . Next a dependent resource for example the web server of Windows CRG fires an application failure event because it cannot communicate with the downed second i5 OS CRG . This event is transmitted to each of the remaining Hypercluster managers and while being of particular interest to the Hypercluster managers and . Turning to the event is received by the Hypercluster event dispatcher agent and queued in the incoming Hypercluster events queue for processing by the Hypercluster router . The Hypercluster router consults the Hypercluster rules list to determine that the Windows CRG is dependent upon the second i5 OS CRG and that the dependency may be switched over to the first i5 OS CRG .

An instruction to activate the first i5 OS CRG is stored in the Hypercluster actions queue which is processed by the Hypercluster actions handler . The particular instructions to activate the first i5 OS node and the second i5 OS node as both comprising the first i5 OS CRG are relayed to the cluster manager via the respective instance of the platform specific plugin . Where an immediate start is desired no further action is necessary. Where a delayed start is desired however the Hypercluster router is directed to wait for a confirmation from the local cluster manager that the first and second i5 OS nodes are indeed activated. Generally the cluster manager provides this upon activation and is stored in the local cluster events . The event translator consults the event translation table and the activation is detected by the Hypercluster router . At this point the Hypercluster manager is aware of the fact that the first i5 OS CRG is active. It is contemplated that the in addition to a notification that the first i5 OS node is active an IP address thereof may also be communicated to the Windows CRG via the Hypercluster events dispatcher agent . Upon receipt by the Windows CRG the dependent database application has been fully switched over to the first i5 OS CRG .

e. Manual switchover initiated on one node where the node has a dependency with another cluster. As described above a manual switchover may be initiated between the first Linux node and the second Linux node where both have dependencies to the i5 OS cluster . The first node to receive the action will receive a Hypercluster token. When the second Linux node receives the Hypercluster token it will initiate a switchover via the exit program associated with the cluster manager . This exit program is contemplated to forward the action to the event translator to translate the switchover action. This will then be sent to the Hypercluster router which will determine that the first and second i5 OS nodes and have a co residency requirement defined with respect to the Linux nodes and . That is if Linux is running on the first Linux node the node is set as primary. The same is true for the second Linux node and the second i5 OS node . In addition the relationship specifies that the first and second i5 OS nodes must be available before the first and second Linux nodes so that the evoked switchover on the i5 OS CRG is complete before the Linux CRG is completed.

The particulars shown herein are by way of example and for purposes of illustrative discussion of the embodiments of the present invention only and are presented in the cause of providing what is believed to be the most useful and readily understood description of the principles and conceptual aspects of the present invention. In this regard no attempt is made to show structural details of the present invention in more detail than is necessary for the fundamental understanding of the present invention the description taken with the drawings making apparent to those skilled in the art how the several forms of the present invention may be embodied in practice.

