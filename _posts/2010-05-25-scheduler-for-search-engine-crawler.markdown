---

title: Scheduler for search engine crawler
abstract: A scheduler for a search engine crawler includes a history log containing document identifiers (e.g., URLs) corresponding to documents (e.g., web pages) on a network (e.g., Internet). The scheduler is configured to process each document identifier in a set of the document identifiers by determining a content change frequency of the document corresponding to the document identifier, determining a first score for the document identifier that is a function of the determined content change frequency of the corresponding document, comparing the first score against a threshold value, and scheduling the corresponding document for indexing based on the results of the comparison.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08161033&OS=08161033&RS=08161033
owner: Google Inc.
number: 08161033
owner_city: Mountain View
owner_country: US
publication_date: 20100525
---
This application is a continuation of U.S. patent application Ser. No. 10 853 627 filed May 20 2004 now U.S. Pat. No. 7 725 452 which is a continuation in part of U.S. patent application Ser. No. 10 614 113 filed Jul. 3 2003 now U.S. Pat. No. 7 308 643 which are incorporated by reference herein in their entirety.

The present invention relates generally to search engine crawlers for use in computer network systems and in particular to a scheduler for a search engine crawler.

A search engine is a software program designed to help a user access files stored on a computer for example on the World Wide Web WWW by allowing the user to ask for documents meeting certain criteria e.g. those containing a given word a set of words or a phrase and retrieving files that match those criteria. Web search engines work by storing information about a large number of web pages hereinafter also referred to as pages or documents which they retrieve from the WWW. These documents are retrieved by a web crawler or spider which is an automated web browser which follows every link it encounters in a crawled document. The contents of each document are indexed thereby adding data concerning the words or terms in the document to an index database for use in responding to queries. Some search engines also store all or part of the document itself in addition to the index entries. When a user makes a search query having one or more terms the search engine searches the index for documents that satisfy the query and provides a listing of matching documents typically including for each listed document the URL the title of the document and in some search engines a portion of document s text deemed relevant to the query.

While web pages can be manually selected for crawling such manual assignment becomes impracticable as the number of web pages grows. Moreover to keep within the capacity limits of the crawler web pages should be added or removed from crawl cycles to ensure acceptable crawler performance. For instance as of the end of 2003 the WWW is believed to include well in excess of 10 billion distinct documents or web pages while a search engine may have a crawling capacity that is less than half as many documents.

Therefore what is needed is a system and method of automatically selecting and scheduling documents for crawling based on one or more selection criteria. Such a system and method should be able to assess the stature e.g. page rank of a web page and schedule the web page for crawling as appropriate based on its stature.

A scheduler for a search engine crawler includes a history log containing document identifiers e.g. URLs corresponding to documents e.g. web pages on a network e.g. Internet . The scheduler is configured to process each document identifier in a set of the document identifiers by determining a content change frequency of the document corresponding to the document identifier determining a first score for the document identifier that is a function of the determined content change frequency of the corresponding document comparing the first score against a threshold value and scheduling the corresponding document for indexing based on the results of the comparison. The threshold value can be computed from an initial sampling of document identifiers. One or more factors can be used to compute a score including page rank crawl history and the like.

A method of scheduling documents to be downloaded by a search engine crawler includes retrieving a number of document identifiers each document identifier identifying a corresponding document on a network. For each retrieved document identifier the method determines a content change frequency of the corresponding document and determines a first score for the document identifier that is a function of the determined content change frequency of the corresponding document. It then compares the first score against a threshold value and schedules the document for indexing based on the result of the comparison.

A computer readable medium has stored thereon instructions which when executed by a processor cause the processor to perform the operations of the method described above.

The daily crawl layer comprises URLs to be crawled more frequently than the URLs in segments . In addition daily crawl layer includes high priority URLs that are discovered by the web crawler system during a current epoch. An epoch is a predetermined time period e.g. a day .

The real time layer includes URLs to be crawled multiple times during a given epoch e.g. multiple times per day . For example the URLs in the real time layer are crawled every few minutes or N times per hour where N is a value greater than or equal to 1 . The real time layer can also include newly discovered URLs that have not been crawled but should be crawled as soon as possible.

The URL scheduler determines which URLs will be crawled in each epoch and stores that information in the data structure . The controller selects a segment from the base layer for crawling. The selected segment is referred to hereinafter as the active segment. Typically at the start of each epoch the controller selects a different segment from the base layer as the active segment so that over the course of several epochs all the segments . . . n are selected for crawling in a round robin manner. The URL scheduler revises the daily crawl layer and or the real time layer by moving URLs to the layers and or from the base layer or vice versa. Alternately in some embodiments URLs are scheduled to the daily and real time layers without regard to their absence or inclusion in the base layer . When a document appears in document indexes generated by both the daily and base crawl layers for instance the front end of the search engine provides a mechanism for using the most recent version of the document when responding to queries.

A query independent score also called a document score is computed for each URL by URL page rankers . The page rankers compute a page rank for a given URL by considering not only the number of URLs that reference a given URL but also the page rank of such referencing URLs. Page rank data is provided to URL managers which pass a page rank value for each URL to URL server robots content filters and other servers in the system . An explanation of the computation of page rank is found in U.S. Pat. No. 6 285 999 which is incorporated by reference herein in its entirety.

From time to time the URL server requests URLs from the URL managers . In response the URL managers provide the URL server with URLs obtained from data structure . The URL server then distributes URLs from the URL managers to crawlers hereinafter also called robots or bots to be crawled. A robot is a server that retrieves documents at the URLs provided by the URL server . The robots use various known protocols to download pages associated with URLs e.g. HTTP HTTPS gopher FTP etc. .

In embodiments where the robots use a calling process that requires domain name system DNS resolution a dedicated local DNS database can be used to store IP addresses for URLs that have been crawled in the past. This feature allows previously crawled URLs to be pre resolved with respect to DNS resolution thus enabling a high percentage of the system s DNS resolution operations to be handled locally at high speed.

To address the handling of URLs that use or are regulated by cookies a cookie database can be included in system for providing stable storage for cookies sent to robots by cookie servers not shown on the Internet. The cookie database is structured so that cookie servers can update the status of cookies upon request. The ability to access cookies acquired by robots on previous crawls provides a number of possible advantages to subsequent robot queries such as speeding up the login process to the URL on the second crawl gaining access to preferred web content and possibly regulating which content is accessed from the URL. Further the use of the cookie database enables robots to crawl content that is regulated by cookie servers.

Pages obtained from URLs that have been crawled by robots are delivered to the content filters . In typical embodiments there is more than one content filter in system because of the computational demands of the content filter . Alternatively the content filter can be implemented as part of each robot . Each content filter sends the retrieved web pages to Dupserver to determine if they are duplicates of other web pages using for example techniques described in co pending U.S. patent application Ser. No. 10 614 111 filed Jul. 3 2003 which is hereby incorporated by reference as background information.

In some embodiments the content filters write out four or more types of log files including link logs RTlogs and history logs and status logs . The link log contains one link record per URL document. A URL document is a document obtained from a URL by a robot and passed to a content filter . Each link log record comprises all the links e.g. URLs also called outbound links that are found in the URL document associated with the record and the text that surrounds the link. The log records in an RTlog include the full content of the documents obtained by robots . Each document is coupled with a score e.g. page rank that was assigned to the source URL of the document by the page rankers .

Indexers and obtain documents from the RTlogs and on a high throughput basis and make these documents searchable by a front end querying system not shown . Global state manager reads link logs and uses the information in the link logs to create link maps . The records in the link map are similar to records in the link log with the exception that text is stripped and the records are keyed by a fingerprint of the normalized value of the source URL. In some embodiments a URL fingerprint is a 64 bit integer determined by applying a hash function or other one way function to a URL. The bit length of the URL fingerprint may be longer or shorter than 64 bits in other embodiments. The records in each link map may optionally be sorted or keyed by a fingerprint. The link maps are used by the page rankers to adjust the page rank of URLs within data structure . Preferably such page rankings persist between epochs.

In addition to creating the link maps the global state manager creates anchor maps . In contrast to records in a link map records in an anchor map are keyed i.e. indexed by the URL fingerprints of outbound URLs present in the link log . The records in each anchor map may optionally be sorted by outbound URL fingerprints as well as being keyed by outbound URL fingerprints. Thus each record in an anchor map comprises a fingerprint of an outbound URL and text that corresponds to the URL in the link log . The anchor maps are used by indexers and to facilitate the indexing of anchor text as well as to facilitate the indexing of URLs that do not contain words. The indexing of anchor text is described more fully in U.S. patent application Ser. No. 10 614 113 filed Jul. 3 2003.

In some embodiments the URL scheduler determines whether to add or remove URLs from the daily layer and the real time layer based on information stored in records in the history logs . The history log records include information indicating how frequently the content associated with the URLs is changing hereinafter also referred to as URL change frequency or content change frequency and individual URL page ranks set by the page rankers . Note that the history logs also contain log records for URLs that are not found in data structure . For instance the history log can contain log records for URLs that no longer exist and or log records for URLs that exist but are no longer scheduled for crawling e.g. due to a request by the website owner that the URL not be crawled due to objectionable content or for any other reasons .

The URL fingerprint is for example an N bit number where N is a value or a bit length that is generated from the corresponding URL by first normalizing the URL text e.g. converting host names to lower case and then passing the normalized URL through a fingerprinting function that is similar to a hash function except the fingerprint function guarantees that the fingerprints are well distributed across the entire space of possible numbers. In some embodiments the fingerprint modulus S where S is the number of segments in base layer e.g. fingerprint modulus in the case where there are 12 segments in base layer is used to select the segment in which to place a given URL. In some embodiments additional rules are used to partition URLs into a segment of base layer the daily crawl layer and or the real time layer .

The TimeStamp indicates the time the record was recorded. The Crawl Status indicates whether the corresponding URL was successfully crawled i.e. whether the particular download attempt documented by this history log record was successful . The Content Checksum also called the content fingerprint is a numerical value corresponding to the content of the downloaded document if the download was successful. In some embodiments this checksum value is generated by computing a predefined checksum on the contents of the downloaded document. The Content Checksum can be used to determine whether the content of a web page has changed. When web pages have identical content they will also have the same Content Checksum . The URL scheduler can compare these content fingerprints with previous content fingerprints obtained for the corresponding URL e.g. identified by URL FP in the history log record on a previous crawl to ascertain whether the web page has changed since the last crawl.

Similarly the Link Checksum is a numerical value corresponding to the values of all the outbound links on the web page associated the URL . In some embodiments the Link Checksum is generated by computing a predefined checksum on the output links of the downloaded document. In some embodiments the URL scheduler is configured to use the Link Checksum to determine whether any of the outbound links on the web page associated with the corresponding URL have changed since the last crawl. For example the URL schedule may be configured to compare the Link Checksum of the downloaded document with the Link Checksum for the most recent prior download of the same URL to see if they are equal. If they are not equal a change has occurred in the set of outbound links in the document e.g. at least one outbound link has been added removed or changed in value .

The Source ID provides an indication of whether the robot accessed the URL using the Internet which can be considered to be a first database of documents or an internal repository of documents which can be considered to be a second database of documents .

The Download Time provides an indication of how long it took a robot to download the web page associated with the corresponding URL FP .

The Error Condition records any errors that were encountered by a robot when attempting to download the web page associated with the URL FP . An example of an error is HTTP4 which indicates that the web page does not exist. Other distinct error types may be used to indicate if an existing web page is unavailable or unreachable.

The Segment ID identifies the particular crawl segment . . . associated with the URL FP at the time that the document download operation represented by this record was performed or attempted.

Page Rank includes the page rank assigned to the URL FP at the time that the document download operation represented by this record was performed or attempted. The page rank of a URL may change over time as the set of pages having links to the page corresponding to URL FP changes and as the page ranks of these referring pages change. The Page Rank included in any particular record for a URL FP represents a snapshot of the corresponding URL s page rank at the time represented by the timestamp .

In some embodiments the determination as to what URLs are placed in daily crawl layer and or real time layer as opposed to base layer is determined by computing a Daily Score which is a composite score of the form Daily Score 1 page rank change frequency age Eq. 1A where F is a function of a specified document s page rank change frequency and age or a subset of those parameters. For instance in one embodiment Daily Score page rank URL change frequency Eq. 1B 

The mechanism by which URL scheduler obtains URL change frequency data is best understood by reviewing . When a URL is accessed by a robot the information is passed through content filters . Content filters among other things determine whether a URL has changed e.g. by checking Content Checksum and when a URL was last accessed by a robot . This information is placed in the history logs which are passed back to the URL scheduler . By reviewing the log records for a particular URL each of which indicates whether the content of a URL changed since the immediately previous time the URL was crawled the URL schedule or other module can compute a URL change frequency. This technique is particularly useful for identifying URL s having content i.e. the content of the page at the URL that changes infrequently or perhaps not at all. Further the computation of a URL change frequency can include supplemental information about the URL. For instance the URL scheduler can maintain or access information about web sites i.e. URLs whose content is known to change quickly.

In cases where the URL scheduler determines that a URL should be placed in a segment of base layer the placement of the URL into a given segment . . . of base layer is random or pseudo random so that the URLs to be crawled are evenly distributed or approximately evenly distributed over the segments . . . . In some embodiments a mathematical function e.g. a modulo function is applied to the URL FP to achieve the random selection of a segment . . . in which to place the URL.

In some embodiments it is not possible to crawl all the URLs in an active segment daily crawl layer and or real time layer during a given epoch. In some embodiments this problem is addressed using two different approaches. In a first approach a Crawl Score is computed for each URL in an active segment the daily layer and or the real time layer . Only those URLs that receive a high Crawl Score e.g. above a threshold value are passed on to the next stage URL managers for downloading. In a second approach URL scheduler determines an optimum crawl frequency for each such URL and passes the crawl frequency information on to the URL managers . The crawl frequency information is then ultimately used by URL managers to decide which URLs to crawl. These two approaches are not mutually exclusive and a combined methodology for prioritizing the URLs to crawl based on both the Crawl Score and the optimum crawl frequency may be used.

In embodiments where a Crawl Score is computed the URL scheduler determines which URLs will be crawled downloaded from the Internet during the epoch by computing a Crawl Score or referencing a previously computed Crawl Score for each URL. Those URLs that receive a high Crawl Score e.g. above a predefined threshold are passed on to the next stage URL managers whereas those URLs that receive a low Crawl Score e.g. below the predefined threshold are not passed on to the next stage during the given epoch. There are many different factors that can be used to compute a Crawl Score including the current location of the URL active segment daily crawl segment or real time segment page rank and crawl history. The crawl history can be obtained from the history logs .

Although many possible Crawl Scores are possible in some embodiments the Crawl Score is a composite score computed as follows Crawl Score 2 page rank change frequency age Eq. 2A where F is a function of a specified document s page rank change frequency and age or a subset of those parameters. In some embodiments a document s age is defined as the time since the last download of the document by a web crawler. In other embodiments the age of a document u is defined as Age Now last crawl expected shelf life Eq. 2B where the expected shelf life u of a document u is based on an expiration time provided by the document s source or based on other information e.g. rates of change known about the document or other documents from the same source or based on such information known about other documents considered to be similar to document u . Such information may be statistical information about the rates of change of a set of documents and such information maybe distilled using various statistical or mathematical techniques to produce an expected shelf life value for a particular document. In one embodiment Crawl Score page rank URL change frequency time since last crawl of URL . Eq. 2C In another embodiment Crawl Score page rank URL change frequency time since last crawl of URL . Eq. 2D In yet another embodiment Crawl Score page rank age Eq. 2E where the age of document u may be defined or computed using any of a variety of techniques as mentioned above. In this last embodiment information about a document s content change frequency may be incorporated into or otherwise taken into account in the age parameter of the Crawl Score function.

Additionally many modifications to the Crawl Score including modifications using cutoffs and weights are possible. For example the Crawl Score of URLs that have not been crawled in a relatively long period of time can be weighted so that the minimum refresh time for a URL is a predetermined period of time e.g. two months . In some embodiments the URL change frequency is computed using the Content Checksum stored in the history log . In some embodiments the Content Checksum is generated by applying the 32 bit Ethernet CRC to the content of the document at the URL while in other embodiments other checksum functions are used. If the document at a URL is altered the Content Checksum will have a different value. The time since last crawl variable can be computed from the TimeStamp and the current system time derived from a master system clock or the like.

In embodiments where crawl frequency is used the URL scheduler sets and refines a URL crawl frequency for each URL in the data structure . The URL crawl frequency for a given URL represents the optimum crawl frequency or more generally a selected or computed crawl frequency for a URL. The crawl frequency for URLs in the daily crawl layer and the real time layer will tend to be higher than the crawl frequency for URLs in the base layer . The crawl frequency for any given URL can range from high values e.g. representing crawl repeat rates of multiple times per hour to low values e.g. representing crawl repeat rates of less than once per month . In some embodiments the optimal crawl frequency for a URL is computed based on the historical change frequency of the URL and the page rank of the URL.

In addition to other responsibilities the URL scheduler determines which URLs are deleted from the data structure and therefore dropped from the system . The URLs are removed from the data structure to make room for new URLs to be added to the data structure . In some embodiments a Keep Score is computed for each URL in data structure . The URLs are then sorted by the Keep Score and the URLs that receive a low Keep Score are eliminated as newly discovered URLs are added to the data structure . In some embodiments the Keep Score for a document u is set equal Keep Score 3 page rank change frequency age Eq. 3A where F is a function of a specified document s page rank change frequency and age or a subset of those parameters. In one embodiment the Keep Score for a document u is set equal to the page rank of the document as determined by the page rankers . URL Scheduler Computer System

The memory stores an operating system e.g. Linux or Unix a network communication module a system initialization module and a URL scheduler module . The operating system generally includes procedures for handling various basic system services and for performing hardware dependent tasks. The network communication module is used for connecting the system to the servers hosting the content filters and possibly to other servers or computers via one or more communication networks wired or wireless such as the Internet other wide area networks local area networks metropolitan area networks and the like. The system initialization module initializes other modules and data structures stored in memory required for the appropriate operation of the system .

The URL scheduler module is used to implement various aspects of the present invention as described below with respect to . The memory also includes scoring functions and data structures e.g. data structure used by the URL scheduler . In some embodiments the data structures include a history log a schedule output file and thresholds . In some embodiments the URL scheduler computer system is a runtime system integrated into a search engine crawler system e.g. URL scheduler in web crawler system and the scoring functions and thresholds are computed in one or more context filters . In other embodiments the URL scheduler computer system is a stand alone system that performs background processing independent of the web crawling system .

After the scores are computed the sample set of URLs is sorted in descending or ascending order into three sorted lists based on the computed Keep Crawl and Daily Scores. For the sorted list associated with the Keep Score a cutoff score hereinafter also referred to as a Keep Score threshold is selected based on a target size of a set of URLs to be included in base layer . For the sorted list associated with the Crawl Score a cut off score hereinafter also referred to as a Crawl Score Threshold is selected based on a target size of a set of URLs from the base layer to be re crawled as opposed to being fetched from a repository . For the sorted list associated with the Daily Score a cut off score hereinafter also referred to as a Daily Score Threshold is selected based on a target size of a set of URLs to be moved from the base layer into the daily crawl layer . Any new URLs discovered during a crawl can be placed in the smallest segment in base layer . Each of these URLs will have a record in the history log after being crawled for the first time and will thereafter become part of the normal scheduling process as described with respect to .

To better illustrate the URL scheduler initialization process let us assume that we have collected a database of URLs each URL having an associated page rank change frequency and a time value indicating a period of time that has transpired since the URL was last crawled. The URLs and associated information can be represented as shown in Table I below. Note that this example has been simplified by using integers to represent the URL FP and page rank. In practice the crawling system would process several billion URLs and the URL FPs and page ranks could be any N bit integer or other value.

Table I includes a randomly selected sample set of URLs resulting from the performance of step in . In some embodiments the number of URLs in the sample set is at least one million e.g. in one embodiment the number of URLs in the sample set is about ten million . In some other embodiments the number of URLs in the sample set is at least 50 000. A Keep Score Crawl Score and Daily Score are computed from the sample set of URLs then sorted by Score into three sorted lists of URLs as shown in Table II below. The sorted lists include a Keep List a Crawl List and a Daily List. Note that in this example the Keep Score is set equal to the Page Rank and the Daily and Crawl Scores are computed using Equations 1B and 2B .

After computing the sorted lists for each Score cut off scores i.e. threshold values are selected as a function of URL target size. In this example the target sizes are arbitrarily selected based on the respective URL capacities of the base layer and daily layer or real time layer and the bandwidth of the crawler.

Thus if the URL capacity of the base layer is 70 of the entire set of known URLs the Keep Threshold is 3 since 70 of the URLs in the sample set have a Keep Score that exceeds 3 including URLs 5 6 1 7 10 9 and 4 see column 1 of Table II .

If the capacity of the crawler is 50 of known URLs the Crawl Threshold is 147 since there 50 of the URLs in the sample set have a Crawl Score that exceeds 147 including URLs 6 5 1 9 and 10 see column 2 of Table II .

If the URL capacity of the daily layer is 20 of known URLs the Daily Threshold may be set to 128 since 20 of the URLs in the sample set have a Daily Score that exceeds 128 including URLs 5 and 6 see column 3 of Table II . In practice where the sample set has thousands or millions of URLs the differences between Daily Score values between adjacent items in the sorted list will typically be very small. The threshold score may be selected as being equal to lowest Daily Score of the URLs to be included in the selected set of URLs or the next lowest Daily Score depending on how the threshold is applied to select URLs for the daily crawl. In particular if the selection criteria is URLs having a score above the threshold then the threshold score is the highest score of any URL that is not to be included in the selected set and if the selection criteria is URLs having a score equal to or above the threshold then the threshold score is the lowest score of any URL to be included in the selected set. Similar criteria are used for selecting the Keep Threshold and the Crawl Threshold.

After the cut off scores are selected they are stored e.g. in memory for use by the URL scheduler in performing a scheduling process on the entire data structure of system as described below with reference to . Note that the initialization process described above assumes that the sample set of URLs is a good statistical representation of the entire data structure of URLs and therefore the selected threshold values will result in an allocation of URLs to the various segments in data structure without exceeding the capacity constraints of those segments or the capacity of the crawler system to download web pages.

In some embodiments the Crawl Reuse Flag is a single bit which can be set to logic 1 to indicate that the URL should be crawled and to logic 0 to indicate that the URL should be fetched from a repository or vice versa. Similarly the Daily Flag can be a single bit that is set to logic 1 to indicate that the URL should be included in the daily layer and to logic 0 to indicate that the URL should not be included in the daily layer or vice versa. In some embodiments the Daily Flag can have more than two values. For example in one embodiment each Daily Flag has one of three values crawl i.e. download from Internet reuse i.e. use copy from document repository and crawl if new i.e. download if the document has been modified since a specified date and or time .

By example if the threshold values determined using Table II as discussed above are applied against the URLs in Table I the URLs would be allocated as shown in Table III below. Note that a logic 1 in the Crawl Flag or Daily Flag column indicates that the URL will be crawled and included in the Daily Crawl respectively.

Thus referring to Table III the base layer will include a total of 7 URLS 6 5 1 7 10 9 4 . URLs 5 6 1 10 and 9 will be crawled and URLs 7 and 4 will be retrieved from a repository i.e. reused . URLs 5 and 6 will be moved from the base layer to the daily crawl layer or real time layer where they will be crawled more frequently e.g. once a day and URLs 1 7 10 9 and 4 will remain in the base layer where they will be crawled less frequently e.g. every ten days .

The foregoing description for purpose of explanation has been described with reference to specific embodiments. However the illustrative discussions above are not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifications and variations are possible in view of the above teachings. The embodiments were chosen and described in order to best explain the principles of the invention and its practical applications to thereby enable others skilled in the art to best utilize the invention and various embodiments with various modifications as are suited to the particular use contemplated.

