---

title: Network acceleration device having persistent in-memory cache
abstract: A network acceleration device includes a persistent, in-memory cache of network content. For example, the cache may store content in a manner that allows a software process to map virtual memory to specific, known regions of an underlying physical memory. Upon detecting a failure of a process executing within the network device, the network acceleration device may restart the software process and remap data structures of the cache to the known regions of the physical memory without necessarily requiring that the cache content be reloaded from a non-volatile memory, such as a hard drive. In this manner, the network acceleration device may accelerate download speeds by avoiding timely cache content restoration in the event of a software process failure.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08171099&OS=08171099&RS=08171099
owner: Juniper Networks, Inc.
number: 08171099
owner_city: Sunnyvale
owner_country: US
publication_date: 20100304
---
This application is a continuation of U.S. application Ser. No. 11 228 006 filed Sep. 15 2005 now U.S. Pat. No. 7 676 554 granted Mar. 9 2010 the entire contents of which are incorporated herein by reference.

The invention relates to computer networks and more particularly to network acceleration devices within a computer network that cache network content.

In a typical network environment client devices request and download content stored within network servers. Exemplary content includes web pages that may contain one or more of text graphics video and sound data. Other examples of content include files multimedia data streams e.g. audio or video data streams electronic messages and data tables. Upon receiving the content requests the network servers typically retrieve the requested content break the requested content into packets and transmit the packets to the requesting client device. Routers and other network infrastructure direct these packets through the network to the client devices which in turn reconstruct the content from the packets and present the content to users via applications residing on the client devices.

The network may experience a variety of issues that result in decreased download speeds at the client devices. These issues include a large volume of content requests to a single network server that overload or otherwise diminish the capacity of the network server to timely service the requested content network congestion and limited network bandwidth. To increase download speeds the network may employ one or more intermediate network acceleration devices located between the client devices and the servers to address the above listed issues or other issues that adversely effect download speeds. For example a network acceleration device may compress the requested content before transmitting requested content to the client devices. As another example a network acceleration device may cache content when a client device first requests the content. The network acceleration device may then intercept subsequent requests for that same content and provide the cached content to the requesting client devices thereby avoiding additional accesses to the servers. In this manner a network acceleration device may be employed to facilitate transactions between the client and network server to increase the overall efficiency of downloading content.

In one aspect the network acceleration device caches multiple encodings for the same network content and intelligently provides the encodings based on the capabilities of the requesting client device. For example during execution the network acceleration device may receive one or more content requests from client devices and the network acceleration device may service these requests. The content request may indicate various encodings of the content that the client device may accept and the cache may store one or more of these encodings of the content. If the encoding is not stored within the cache the network acceleration device may download the natural encoding of the content encode the content into another acceptable encoding and store this non natural encoding to the cache. The network acceleration device may receive other requests for this content and store other encodings of this content to the cache. Thus the cache may store multiple encodings of content.

In some instances each of the encodings may provide a smaller size than the natural encoding. Because the cache stores smaller file sizes the network acceleration device may more quickly access requested content in comparison to downloading the content from a server and provide smaller encodings to facilitate decreased download speeds. The term encodings is used herein to refer to different cached instances generated from the same network content i.e. the natural encoding . Examples of different encodings of the same network content include but are not limited to compressed and uncompressed instances of the content. Other examples include raw images and corresponding thumbnails and complete web content and corresponding derived content reduced for display limited devices having particular display requirements such as personal digital assistants PDAs network enabled cell phones digital televisions or other devices.

In another aspect the network acceleration device may simultaneously cache and intelligently serve different historical versions of network content. For example the network device may receive one or more requests for original content however subsequent updates to the content may create varying versions of the content e.g. a pre update version and a post update version. Client devices that requested the content prior to the update receive the pre update version from the network acceleration device which may have previously stored the pre update version to the cache. Client devices that requested content after the update receive the post update version from the network acceleration device which may have also stored the post update version to the cache. Moreover the network acceleration device facilitates the simultaneous delivery of the pre update version and the post update version without waiting for delivery of the pre update version to be complete. Thus the network acceleration device may facilitate decreased download times by seamlessly and transparently providing both versions of the content simultaneously.

In yet another aspect the network acceleration device may further facilitate download speeds by providing a cache that does not necessarily require content reloading after failure of a software process managing the cache. For example the cache may store content in a manner that allows the software process to map virtual memory to specific known regions of an underlying physical memory. Upon detecting a failure of a client session the network acceleration device may restart the software process and remap data structures of the cache to the known regions of the physical memory. In this manner the network device may facilitate decreased download speeds by avoiding timely cache content restoration from a static memory such as a hard drive in the event of a software process failure.

In another aspect the network acceleration device may employ a communication protocol and optionally an application programming interface API that facilitates the sharing of cache resources among multiple network acceleration devices thereby allowing multiple acceleration devices to be clustered and share a common cache. For example a first network acceleration device may receive a content request and forward the content request to a second network acceleration device which maintains the shared cache via the API. The second network acceleration device receives the content request via the API and accesses the cache to determine whether the requested content resides within the cache. Upon determining that the content resides in the cache the second network acceleration device may transmit the requested content from the cache to the first network acceleration device via the network or may directly provide the content to the requesting client. In this manner this clustered approach may advantageously allow clients serviced by one network acceleration device to benefit from previous network access requests issued by clients serviced by a different network acceleration device. Moreover the techniques may reduce the cost associated with these other network acceleration devices in comparison to the network acceleration device incorporating and maintaining the shared cache.

In one embodiment a method comprises storing content of a cache within a named region of a physical memory provided by the network device and mapping one or more data structures within the memory space associated with a software process executing on a network device to the named region of the physical memory. The method further comprises remapping the data structures to the named memory region of the physical memory in response to a failure of the software process and accessing the content stored in the cache after the failure without restoring the content to the cache.

In another embodiment a network device comprises an in memory cache that stores content within a named region of a physical memory provided by the network device. The network device further comprises a cache server executing on the network device having a respective memory space. The cache server remaps one or more data structures of the memory space to the named memory region of the physical memory in response to a failure to allow access of the content stored in the cache without restoring the content to the cache.

In another embodiment a computer readable medium comprises instructions that cause a programmable processor to store content to a cache within a network device in accordance with a named memory region of the cache. The instructions further cause the programmable processor to detect a failure of a process executing within the network device and remap the named memory region of the cache after detecting the failure to allow access of the content stored in the cache without restoring the content to the cache.

The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features objects and advantages of the invention will be apparent from the description and drawings and from the claims.

Network servers may comprise web servers mail servers database servers and other such common server types that store content such as web pages electronic messages and data tables respectively. Client devices may comprise any number of personal computers laptops workstations wireless devices personal digital assistants PDAs cellular phones network ready appliances or other devices.

In the example of network acceleration device couples to client devices via respective links A N links . Although not shown one or more networks may be located between client devices and network acceleration device and the client devices need not directly couple to the network acceleration device. These intermediate networks and network may also include other network devices such as routers switches hubs gateways session border controllers VPN devices and other network servers and network acceleration devices. While described herein in reference to network acceleration device the principles of the invention may be applied to other devices such as any of the network devices listed above.

Network typically comprises a packet based network capable of transmitting content from network servers to client devices via structured packets and network acceleration device may facilitate this transfer by caching content requested by the client devices. Thus network acceleration device comprises a cache not shown in that locally stores content received from network servers . The cache may comprise an in memory cache or in other words a cache that functions without relying exclusively on static memory such as a hard drive but within a dynamic memory such as random access memory RAM dynamic RAM DRAM and synchronous DRAM SDRAM .

The cache may store one or more encodings or transforms of content and network acceleration device intelligently provides the encodings based on the capabilities of the requesting client device. The term encodings is used herein to refer to different cached instances generated from the same network content i.e. the natural encoding and some transformed version of the natural encoding . Examples of different encodings of the same network content include uncompressed natural HTML and compressed instances of the HTML content. Exemplary compression formats include PZIP PZIP2 GZIP and Deflate encodings as well as those other encodings described above. Other examples include raw images i.e. the natural encoding in this case and corresponding thumbnails and regular web content and corresponding derived content for devices having particular display requirements such as personal digital assistants PDAs network enabled cell phones digital televisions or other devices.

In general client devices transmit requests for content via links to network acceleration device and in response to these content requests network acceleration device accesses its cache to determine whether the requested content resides locally within the cache i.e. performs a cache lookup. The content requests issued by client devices may comprise a HTTP GET request and may indicate the types of encodings client devices may accept.

Upon determining acceptable encodings network acceleration device performs the cache lookup to determine if any of the acceptable encodings of the requested content are currently stored in the cache. If the cache determines that it is currently storing an acceptable encoding i.e. a cache hit the cache transmits the acceptable encodings to the requesting client devices via links .

In addition network acceleration device may simultaneously cache and intelligently serve different historical versions of network content. For example network acceleration device may receive one or more requests for original content however subsequent updates to the content may create varying versions of the content e.g. a pre update version and a post update version. Client devices that requested the content prior to the update receive the pre update version from the network acceleration device which may have previously stored the pre update version to the cache. Client devices that requested content after the update receive the post update version from the network acceleration device which may have also stored the post update version to the cache. Moreover network acceleration device facilitates the simultaneous deliver of the pre update version and the post update version without waiting for delivery of the pre update version to be complete. Thus network acceleration device may facilitate decreased download times by seamlessly and transparently providing both versions of the content simultaneously.

In some embodiments network acceleration device maintains the integrity of this stored content despite failures of one or more internal software processes that manage the cache. For example during the course of servicing content requests network acceleration device may detect a failure of one or more internal software processes that manage the in memory cache. However as further described below network acceleration device may store the cached content in memory in a manner that allows the software process to map virtual memory of its process space to specific known regions of an underlying physical memory. Upon detecting a failure of a software process which typically results in corruption of the virtual memory space the network acceleration device restarts the software process and remaps data structures of the cache to the known regions of the physical memory. Thus network acceleration device is able to continue to access its cache after detecting the failure without reloading the contents stored in the cache prior to the failure even though the cache is an in memory cache. Thus the in memory cache may maintain a quasi persistent nature in that the cache remains functional even after a software process failure that would normally cause conventional devices to reload the contents of the cache from a hard drive or some other static memory source.

In this manner network acceleration device provides a quasi persistent in memory cache capable of storing multiple encodings of content as well as multiple encodings and historical versions of the content. The cache may enable network acceleration device to more quickly provide content to client devices . For example some of the encodings are frequently smaller in size than the natural content thereby decreasing the time necessary for client devices to download the requested content. Further because network acceleration device may store multiple historical versions of content network acceleration device may continue to transmit a first version of the content to one of client devices and need not interrupt this transmission to replace the first version with the second version of the content. Thus network acceleration device may more quickly service simultaneous content requests concerning different versions of the same content. Finally network acceleration device may more quickly service content requests during a failure of a software process within network acceleration device because network acceleration device need not reload or repopulate the cache after the failure as described above.

Client devices may comprise substantially similar client devices and for ease of illustration only the components of client devices A are discussed herein however each of client devices may comprise similar components. Client device A comprises one or more software applications A that request content from servers. Software applications A typically include one or more of a web browser such as Internet Explorer Netscape Navigator and Mozilla Firefox an email application such as Microsoft Outlook Express or Microsoft Outlook or any other common network application.

Network acceleration device represents any intermediate network device capable of facilitating the download of content by client devices . Network acceleration device comprises a plurality of cache clients A N cache clients a cache daemon and a cache . Cache clients and a cache daemon may execute as separate software processes within an operating environment provided by an operating system and one or more microprocessors not shown in of network acceleration device . In some embodiments cache clients may execute as separate process threads within another process such as cache daemon server engine or in other embodiments cache clients may comprise data structures within another process. In yet other embodiments cache clients may comprise data structures that execute within the operating system itself. In the embodiments where cache clients comprise data structures these data structures may store state information regarding connections with client devices and request information so as to attempt to fulfill any received client requests. For ease of illustration cache clients are discussed herein in reference to separate processes however the principles of the invention should not be limited as such. Cache clients and cache daemon may comprise executable instructions that cause a processor to perform actions attributable to the principals of the invention as described herein and may be stored in any suitable computer readable media.

Cache clients intercept content requests from clients and interact with cache daemon in accordance with a request and response model. Cache daemon serves as a broker for cache or in other words cache daemon regulates access to cache by selectively servicing cache access requests from cache clients .

In one embodiment cache daemon includes an application programming interface cache storage API and a cache daemon server engine . Cache storage API enables cache daemon server engine to access cache on behalf of requesting cache clients . Cache daemon server engine may operate in accordance with one or more state machines not shown in to service requests from cache clients and may further operate in such a manner as to minimize locking requirements during concurrent operations within cache . Cache storage API may provide a set of functions that allow for cache manipulations which include accessing an object in the cache inserting an object and retrieving cache statistics. As described in further details below cache daemon server engine may accept cache messages via a cache message protocol which may utilize a transport control protocol TCP thereby allowing cache clients to execute on other network acceleration devices within a cluster and interact with cache daemon over a network medium to share access to cache .

In one embodiment cache comprises a named memory map region that contains objects as represented by object cache in that reference the physical memory of cache . For purposes of this invention objects represent general purpose data structures capable of storing references e.g. pointers to other objects and data buffers as well as data in the form of variables and other such common data storage forms. Object cache may also store object metadata which may include information about object sizes values corresponding to how many times particular objects have been referenced and other information pertinent to caching algorithms.

Named memory map region may represent a form of cache storage that allows cache daemon to map virtual memory of its process space to specific known regions of an underlying physical memory. In other embodiments named memory map region may represent a form of cache storage that allows cache daemon to map physical memory as opposed to virtual memory reserved for cache operation to specific known regions of the underlying physical memory. In this manner named memory region allows cache to maintain a quasi persistent nature as described in more detail below in the event cache daemon or other software process within network acceleration device fails. Cache may comprise an in memory cache or in other words a cache contained at least partially within dynamic memory such as RAM DRAM SDRAM and partially independent of static memory such as hard drives and flash memory.

Generally software applications of client device A issue content requests which are intercepted by network acceleration device . Upon intercepting the requests network acceleration device spawns a new cache client to service the requests. Alternatively network acceleration device may utilize an existing cache client to service the request. In any event the cache client servicing the requests interacts with cache daemon via cache daemon server engine which in turn passes any necessary information for determining whether the requested content is stored within cache via cache storage API . The information may include the name of the content requested such as an HTTP address universal resource locator URL or universal resource identifier URI as well as encodings software applications accepts and other such information necessary to access cache .

Cache daemon server engine may first decide to accept or deny the content request but upon acceptance cache daemon server engine performs a cache lookup via cache storage API by formulating a query based on the information received from the requesting cache client . For example cache daemon server engine may formulate a query requesting a specific encoding of the content by including a cache key based on the HTTP address and the particular encoding.

Cache receives the query and performs the cache lookup to determine whether the requested content resides within cache as described below in more detail. Upon a determination that the requested content resides within cache i.e. a cache hit cache returns a token to cache daemon server engine via cache storage API . Cache daemon server engine begins reading the requested content from cache based on the token which may comprise a pointer that references the requested content. Cache daemon server engine next transmits this requested content to the requesting cache client which forwards the requested content to software applications .

However if cache the requested content does not reside within cache i.e. a cache miss cache daemon server engine informs the requesting cache client of the cache miss. The requesting cache client may next request this content from an appropriate network server such as one of network servers . Typically the requesting cache client requests only the natural encoding e.g. HTTP encoding of the content from the appropriate network server and network acceleration device may employ a compression module not shown in that automatically transforms the natural content into one of the encodings software applications may accept. Upon receiving the content from the network server and encoding the content the appropriate cache client may simultaneously maintain three sessions. For example the cache client may maintain a first server session with the appropriate network server to download the requested content a second client session with software applications to forward the encoded requested content to software applications and a third cache session with cache daemon to insert the encoded requested content into cache .

Moreover although not illustrated in network acceleration device may include a multiplex demultiplex process by which two or more cache clients may share one or more communications sessions e.g. HTTP sessions with a server thereby reducing the number of open sessions maintained by the server.

Throughout the above described process named memory map region of cache provides a caching mechanism to ensure its quasi persistent nature. In particular named memory map region resides in the virtual memory associated with the process space of cache daemon and is utilized to maintain object cache within specific physical memory regions as discussed below in more detail. In the event that cache daemon fails while accessing cache via cache daemon network acceleration device may simply restart cache daemon and remap the data structures associated with cache to named memory map region . This allows cache daemon to continue to utilize cache without necessarily repopulating cache even though the original process space associated with cache daemon may have been lost. Thus because named memory map region provides cache with references via object cache to the physical memory of cache cache may maintain a quasi persistent nature by remapping objects within object cache to the physical memory upon failure unlike conventional caches that require repopulation of the cache after detecting a failure of a client session.

Named memory map region may represent a form of cache storage that allows a cache daemon such as cache daemon to map virtual memory of its process space to specific known regions of an underlying physical memory as described above. While named memory map region comprises objects that reference the underlying physical memory of cache named memory map region may also comprise other forms of data to enable efficient operation of cache . As shown in named memory map region comprises cache information and cache statistics to facilitate efficient operation of cache . Cache information may store low level cache information such as information relating to the size remaining available space and other such information relevant to a cache such as cache of . Cache statistics may store statistical information relevant to the cache such as the number and frequency of cache misses the number and frequency of cache hits and other such statistical information relevant to cache .

In this example named memory map region also comprises cache table and object cache that includes cache entry pages input output vector pages cache data pages and page descriptor pages . Cache table may comprise a data structure such as a linked list a tree or hash table to store header objects representing the rows for each line of the cache and referencing various objects stored within object cache shown as the dashed box in . Cache entry pages may store pages of entry objects that enable the cache e.g. cache to access data objects. Input output vector pages may store data objects that further enable the cache to access data buffers. Cache data pages may store pages of data buffers that hold the stored content. Finally page descriptor pages may store information describing the pages of cache data pages .

While most of these components of named memory map region are discussed in more detail below generally cache table maintains the structure of the cache by utilizing the header objects to indicate each line of the cache. The header objects comprises references e.g. pointers to the entry objects stored in cache entry pages and the cache entry objects may reference the data objects of input output vector pages . The data reference objects may reference the data buffers of cache data pages that store the requested content including multiple encodings and versions of the identical requested content. Named memory map region utilizes page descriptor pages to properly manage cache data pages . In order to maintain consistent reference of the underlying physical memory as referenced by named memory map region cache data pages may comprise a buffer allocator as described below to properly allocate data buffers i.e. the underlying physical memory within cache . In other words the buffer allocator ensures that named memory map region consistently references the same underlying physical memory of cache in the event of remapping named memory map region upon failure of a process such as cache daemon of network acceleration device .

In the event that cache daemon for example fails network acceleration device restarts cache daemon which determines whether to reload the contents of cache upon restarting. Typically contents within cache are not corrupted and thus cache daemon does not reload the contents but only remaps the objects within named memory map region . Cache daemon consistently remaps the objects to the same underlying physical memory as before the failure. In doing so cache daemon maintains the quasi persistent nature of cache because after remapping the objects cache daemon may resume servicing cache access requests without reloading the contents of cache . Thus named memory map region as described in more detail below comprises these components to facilitate decreased download speeds and provide for the quasi persistent nature of cache in accordance with the principles of the invention.

Typically upon a cache miss cache daemon server engine informs the requesting cache client of the cache miss and the requesting cache client requests the requested content from a network server such as one of network servers of . After receiving the content cache client communicates the content within cache messages to cache daemon server engine via a cache message protocol. Upon receiving these cache messages cache daemon server engine writes the content to cache via cache storage API . Upon initiating the write cache allocates the necessary space within data buffer of cache data pages to accommodate the content via interaction with buffer allocator . Buffer allocator allocates a data buffer by consulting free block table and communicates with page manager via page manager API to retrieve the page upon which the selected free buffer resides. Page manager determines the page upon which the free buffer resides and returns a reference to this page to buffer allocator . Buffer allocator updates page descriptor table to reflect the allocation of the selected data buffer from within the determined page and returns a reference e.g. pointer to the selected data buffer to cache daemon server engine via data buffer API and cache storage API . Cache daemon server engine utilizes this reference to write the content received via the cache messages to the data buffer i.e. the underlying physical memory of cache referenced by the pointer.

In the event of failure of one of the process such as cache daemon executing within network acceleration device cache daemon upon restart may remap named memory map region so as to reference the same underlying physical memory as before the failure and as described above. Buffer allocator may facilitate the remap of named memory map region by allocating data buffers in a consistent and orderly manner such that upon remapping named memory map region cache daemon may perform the remap without relying upon buffer allocator . In this manner buffer allocator does not allow for inconsistent mapping of named memory map region thereby facilitating the quasi persistent nature of cache .

Named memory map region further comprises data objects A O data objects that each reference respective data buffers A O data buffers i.e. the underlying physical memory of cache . Input output vector pages may comprise data objects and cache data pages may comprise data buffers . Data objects may also form a linked list structure as represented by the dotted line between data object B and data object M which indicates that data object B comprises a pointer to data object C and continuing in this fashion until reaching the last data object i.e. data object M thereby forming a data object chain. While discussed in reference to chains or linked lists named memory map region may employ other data structures such as trees hash tables and queues to implement the organization of entry objects A N data objects A M and the invention should not be limited as such.

In general upon receiving a content request from one of client devices and spawning a respective one of cache clients to service the request the newly spawned cache client may cause cache daemon server engine to perform a cache lookup by issuing a query based on information contained within the content request to cache . Upon receiving the query cache performs the cache lookup by traversing header objects within named memory map region until selecting one of header objects that matches the query. For example named memory map region may be accessible via a name e.g. HTTP address and an encoding type all of which the query may indicate by way of providing cache with a cache key. Cache may traverse header objects until finding an appropriate row that possibly contains content having the same HTTP address and encoding specified by the query via the cache key. Next cache follows reference of the selected one of header objects to a respective one of entry objects however in the event the selected one of header objects does not point to one of entry objects cache may respond to cache daemon server engine that the content is not stored within named memory map region i.e. a cache miss has occurred.

However if the selected one of header objects does point to one of entry objects cache begins to traverse the entry object chain until finding the appropriate one of entry objects as determined again by the query. For example cache may traverse entry objects and select one of entry objects upon finding one of entry objects that contain information matching the HTTP address and encoding as that specified by the query. If cache does not find an appropriate one of entry objects as determined based on the query cache responds to cache daemon server engine that a cache miss has occurred. However if cache finds an appropriate one of entry objects cache follows the reference to an associated one of data objects .

Barring a cache miss cache traverses data objects until selecting one or more of data objects that match the query as discussed above. If cache reaches the end of a data object chain such as data object M without selecting one of data objects cache informs cache daemon server engine that a cache miss has occurred. However if cache selects one of data objects i.e. a cache hit cache follows the reference associated with the selected one or more of data objects to a respective one or more of data buffers . Cache returns a token e.g. pointer to the appropriate entry object begins to reads the contents from the respective one of data buffers and returns the contents to cache daemon server engine which in turn passes the contents to the requesting cache client . Upon receiving the content the requesting cache client formats the contents into packets and transmits the packets to the requesting application such as one of software applications .

Throughout this process a cache miss may occur as indicated above and in the event of a cache miss cache daemon server engine informs the requesting cache client of the cache miss. The requesting cache client may request the content from an appropriate network server download the content from the network server and forward the content to the requesting one of software applications . The requesting cache client may simultaneously insert the content into cache by requesting access to cache via cache messages transmitted in accordance with a cache message protocol to cache daemon server engine . Cache daemon server engine typically grants access receives the content from the requesting cache clients and formulates a query to determine an appropriate location within cache to store the content. After receiving the query cache may create a new entry object within named memory map region and return a token i.e. pointer to the new entry object. Cache may further allocate a new data buffer similar to one of data buffers by way of a buffer allocator such as buffer allocator of and create the necessary data object to reference this newly allocated data buffer. Cache may map the entry object to the data buffer thereby completing the mapping and maintaining named memory map region in a consistent manner.

Upon receiving the token cache daemon server engine may use the token to reference the underlying physical memory of cache i.e. the newly allocated data buffer and write the content received from the requesting cache clients to this newly allocated data buffer via cache storage API . In some embodiments the requesting cache client may encode the content to a non natural encoding prior to writing the contents to the newly allocated data buffer. In some instances a cache miss may occur because named memory map region does not contain a particular encoding of the content but contained another encoding of the content that the application could not accept. Thus the requesting cache client may write this encoded content to cache even though cache stores another encoding of the same content.

In instances where a previous encoding of the content already exists within cache cache may traverse to the appropriate one of entry objects but does not necessarily traverse any of data objects associated with the appropriate one of entry objects . Instead cache creates a new data object that comprises a pointer to the newly allocated data buffer and writes the requested content to an associated data buffer. Thus data objects A M point to respective data buffers A M where one or more of data buffers A M store a different encoding of the same content. In this manner named memory map region may store multiple encodings of the same content to facilitate the download of content.

Moreover because named memory map region comprises the various objects discussed above cache may maintain a quasi persistent nature because of the consistent allocation of data buffers by buffer allocator . In particular upon failure of a process such as cache daemon within network acceleration device cache daemon upon restart may remap header objects entry objects and data objects to reference the same data buffers as before the failure without reloading the contents to data buffers . Thus in employing named memory map region cache may remain persistent despite failure of a process executing within network acceleration device because of the consistent named allocation of data buffers .

Write token list object comprises a first pointer to token A and a second pointer to data object A. Token A serve as a reference to write token list object and may enable a cache daemon server engine such as cache daemon server engine to write a first version to a data buffer A on the behalf of requesting client devices such as client devices as described below. Data object A may be similar to data objects of and similarly comprise a pointer to data buffer A which may store content of a first version.

Read token list objects each comprise pointers to a respective token list in which the first token list comprises tokens B C and the second token list comprises tokens D E. Cache may organize read token list objects such that read token list object A references a newer version of the same content than read token list object B. Read token list objects also each comprise pointers to data object lists in which the first data object list comprises data objects B C and the second data object list comprises data object D E. Each of data objects B E comprises pointers to respective data buffers B E similar to data objects . Similar to token A tokens B E serve as a reference to respective read token objects A and may enable requesting cache clients to read content stored to respective data buffers B E.

Initially only one version of the same content may reside within cache however upon receiving a request for this content the requesting cache client may determine whether a newer version of the content resides on an appropriate network server. For example the requesting cache client may request information pertaining to the content stored on the network server such as the size of the content file and may further request information pertaining to the content stored in cache such as the size of the file of the content stored to the cache via a cache access request directed to cache daemon server engine . The requesting cache client may compare these two forms of information to determine whether a new version exists.

In the event that the requesting cache client determines that the appropriate network server does not store a new version the requesting cache client requests access to cache via a cache storage API such as cache storage API so as to read the content from cache . Cache daemon server engine receives the cache access request and issues a corresponding query directing cache to return a reference to the appropriate object that references the newest version of the requested content. Cache complies with this query by creating a new token such as token C that references read token list object A and returning token C to cache daemon server engine . Cache daemon server engine utilizes token C to read the newest version of content stored in data buffers B C and forwards this content to the requesting cache clients . Upon receiving this content the requesting cache clients forward the content to software applications residing on respective requesting client devices .

As illustrated in other of cache clients may have previously requested content prior to an update to the content and cache may store this first version of content in data buffers D E. During these previous requests cache may create and return the contents of the data buffers D E associated with tokens D E to these other cache clients . Simultaneous to these first version reads cache provides for the storing of a second newer version of the content and may create and return tokens B C as described above to facilitate the simultaneous reading of this second or newer version of the content stored in data buffers B C.

In the event that the requesting cache client determines that the network server stores a third and yet newer version of the requested content the requesting cache client may download this third version possibly encode the third version into an encoding acceptable by the requesting application such as one of applications and forward the possibly encoded content to the requesting application . Cache client may simultaneously request access to cache via cache messages to cache daemon server engine which may write the third version to cache via a cache storage API such as cache storage API on behalf of the requesting cache clients . Cache daemon server engine receives the request and issues a query to cache which as described above may create new objects and allocate the necessary data buffers based on the query. Cache may instantiate token A to reference write token list object and return token A to the cache daemon server engine .

Upon receiving token A the requesting cache clients may transmit the content to cache daemon server engine which simultaneously writes the third version of the content to data buffer A on behalf of the requesting cache clients while reading the first version from data buffers D E and the second version from data buffers B C. Upon completing the write of the third version cache may via cache daemon server engine remove token A and make a determination as to whether to reorganize data object A and its respective data buffer A based on the number of tokens currently referencing write token list object . In this example after removing token A no other tokens currently reference write token list object and therefore cache may reorganize data object A and its respective data buffer A by creating another read token list object similar to read token list objects that references data object A and removing write token list object . Cache may organize this newly created read token list object so that it comes before read token list object A and therefore represents that the newly created read token list references a newer version of the content than that referenced by read token list object A.

Similarly cache may perform a similar reorganization procedure with respect to read token list objects . After the other of cache clients complete their reads of the first or second version of the content stored to respective data buffers B C and data buffers D E cache may remove their respective tokens that reference read token list objects respectively. If no tokens reference the respective read token list objects cache may remove all the data structures directly and indirectly referenced by read token list objects and return the respective data buffers to buffer allocator for reuse. However cache may not remove read token list object A and all data structures directly and indirectly referenced if cache determines that the removal of one of read token list objects would remove the newest version of the content currently readable from the cache.

For example cache daemon server engine may finish reading the second version of the content stored to data buffers B C on behalf of two of cache clients and cache may remove both of tokens B C via interactions with cache daemon server engine in accordance with cache storage API that was previously assigned to these two of cache clients . Upon removal of tokens B C cache may determine that no tokens currently reference read token list object list however cache daemon server engine may not remove read token list object A because it references the newest version of the content currently readable in the cache. As illustrated in cache has not yet reorganized the third version such that it is readily readable as described above. Thus cache may not cause cache daemon server engine to remove read token list object A. However should cache daemon server engine remove tokens D E cache may determine by way of the organization of the read token list object chain that read token list object B references a first and older version of the content than that referenced by read token list object A. Thus cache may return data buffers D E to buffer allocator for reuse and remove data objects D E and read token list object B. In this manner cache stores multiple versions of the same content and seamlessly services content requests to transparently provide these multiple version of the same content to the requesting cache clients .

Initially an application such as software applications of client device A transmits a content request to a network acceleration device such as network acceleration device . Network acceleration device spawns a new cache client or in some embodiments utilizes an existing cache client such as cache client A to service the content request . Cache client A determines acceptable encodings that the requesting applications may accept and issues cache access request to cache daemon server engine which in turn formulates queries in accordance with cache storage API in order to determine whether the requested content resides within cache as described above . The query in effect causes cache to perform cache lookup and the query may specify the particular content and acceptable encodings as described in the cache access request . The query may identify the content by including a key to the content based on information contained within the content request such as an HTTP address and an encoding. Cache may respond to the cache lookup query in one of two ways.

If the requested content does not reside within cache i.e. a cache miss YES branch cache daemon server engine may inform cache client A of the cache miss and cache client A may request the content from an appropriate network server such as one of network servers of . In response to the server request for the content cache client A receives the requested content form the appropriate network server and encodes the requested content to one of the predetermined acceptable encodings such as PZIP GZIP or Deflate as described above . Next cache client A again issues a cache access request to cache daemon server engine in order to store the encoded content to cache as described above . Simultaneously cache client A may transmit the encoded content to software applications of client A thereby completing the transaction .

If the requested content resides within cache i.e. a cache hit NO branch 110 cache may begin transmitting the requested content to cache daemon server engine which in turn transmits the requested content to cache client A . Cache client A upon receiving the content transmits the content to the requesting software applications .

Upon servicing this content request various applications may continue to transmit content requests and network acceleration device may continue to service these requests as represented by the arrow from to . While servicing these content requests cache daemon server engine may store additional encodings of content previously stored to cache resulting in storage of multiple encodings of identical content to cache . In this manner cache may store the multiple encodings to facilitate a decrease in download speeds at the client device because typically the encodings are of a smaller size than the natural encoding. Thus client devices such as client devices may more quickly download the encoded content than the natural encoding of the content.

Initially as described above network acceleration device receives a content request from an application such as software applications and spawns or utilizes an existing cache client such as cache client A . Cache client A issues a cache access request via cache messages formulated in accordance with a cache message protocol to cache daemon server engine of cache daemon whereupon receiving the request cache daemon server engine performs a cache lookup as discussed above . If cache determines that the requested content does not reside within cache cache responds to the lookup by indicating a cache miss YES branch . However if cache determines that the requested content resides within cache cache responds to the cache lookup by indicating a cache hit NO branch .

In response to a cache miss cache daemon server engine informs cache client A of the cache miss and in response cache client A requests and receives the content from an appropriate network server as described above . Upon receiving the requested content from the network server cache client A begins to transmit the received content to the requesting client device . Cache client A also simultaneously stores the requested content to cache by issuing a cache access request via the cache daemon server engine which in turn writes the downloaded content to cache via cache storage API as described above . Cache may continue to store information until either all the information has been stored YES branch or until a process executing within network acceleration device such as cache daemon fails YES branch .

In response to a cache hit cache allows cache client A to read the requested content from cache and upon reading a portion of the content cache client A transmits this portion of the requested content to software applications of client device A . If cache successfully finishes reading the requested content network acceleration device may continue to receive content requests YES branch . If cache client A is not finished reading the requested content and no processes executing within network acceleration device fail during this read cache client A may continue to read the content from cache NO branch NO branch .

In the event that cache daemon for example fails either while storing content to cache YES branch or reading content from cache YES branch network acceleration device as described above releases cache client A restarts the failed process e.g. cache daemon and remaps object cache so as to reference the same underlying physical memory as before the crash .

In some embodiments network acceleration device does not release cache client A when the failed process prevented a cache write but upon restart of the failed process allows cache client A to continue to download the content and store the requested content to cache via cache daemon server engine . In these embodiments cache client A may continue to store the requested content upon determining that a process repeatedly fails during repeat attempts to cache this content thereby preventing the requesting client devices from receiving the content. Thus to prevent reoccurring failure while storing this content cache client A may continue to store this content via cache daemon server engine so that the content is available the next time the requesting client device requests the content which may enable download of the content despite repeated failures within network acceleration device .

Initially network acceleration device receives a content request from an application such as software applications of client device and spawns a cache client or in some embodiments utilizes an existent cache client such as cache client A that services the content request . Cache client A determines whether a new version of the requested content resides on an appropriate network server as discussed above . In determining whether a new version exists cache client A may request access to cache thereby causing cache daemon server engine perform a cache lookup via queries formulated in accordance with cache storage API to determine whether cache stores an existing version of the requested content. If cache stores an existing version cache client A may compare for example the file sizes of the content stored to cache to the file size of the content existing on the network server.

If in the above example the file sizes differ cache client A may determine that a new version exists on the appropriate network server and download the new version from the appropriate network server YES branch . Next cache client A issues a cache access request to cache daemon server engine in order to write the downloaded content to the cache . Cache daemon server engine receives the cache access request and causes cache to return a token that cache daemon server engine uses to insert content received from cache client A . The token may be substantially similar to token A of and may reference a write token list object such as write token list object . As in other cache sessions may read previous versions of the content as represented by tokens B E.

Upon receiving the token cache daemon server engine on behalf of cache client A writes the requested content to a data buffer such as data buffer A by referencing first the write token list object via token A second data object A and third data buffer A . As content is received from client A cache daemon server engine may continue to write the new version of the content until finished NO branch .

If in the above example the file sizes match cache client A determines that the server contains the same version as that stored in cache and issues a cache request to cache daemon server engine in order to read the content from cache as discussed above NO branch . Cache daemon server engine causes cache to return a token in response to receiving the cache access request . The token may be substantially similar to token B and likewise references a read token list object such as read token list object A. Cache client A next reads the content from cache via the cache daemon server engine which utilizes token B to reference data buffers B C as described above . Cache daemon A may continue to receive the requested content until cache daemon server engine finished reading the requested content from cache NO branch .

Upon finishing either the write of the new version of content YES branch or the read of requested content from cache YES branch cache may inform cache daemon server engine that it is finished and cache daemon server engine may remove the token from either write token list object or read token list object A respectively . In either event cache determines whether to reorganize objects previously referenced by the token . If the token was used for writing to cache cache determines that reorganization is necessary and reorganizes the referenced data object and associated data buffer by moving them to the front of the read token list object chain YES branch . If the token was used for reading content from cache cache may determine to reorganize the objects based on the number of tokens assigned to each of the read token list objects .

If no tokens are associated with a particular read token list object cache may reorganize the objects by removing the data objects and data buffers associated with the particular read token list object but only if the read token list object references an older version and another read token list object references a newer version YES branch . For example if read token list object B was not referenced by any tokens cache may remove read token list object B because it references an older version of content than read token list object A. If no reorganization is necessary or upon finishing reorganization of the objects network acceleration device may continue to receive content requests . In this manner cache may store multiple versions of content and seamlessly and transparently transmit the multiple versions to client devices such as client devices .

As well as comprising cache network acceleration device A includes a cache daemon that provides a cache daemon server engine and a cache storage API similar to cache daemon . Each of network acceleration devices B N comprises respective cache clients that their respective network acceleration devices B N have spawned in response to receiving content requests from client devices not shown in . Each of cache clients may communicate with cache daemon via cache daemon server engine . Cache daemon server engine may utilize a cache message protocol that comprises a network based communication protocol e.g. the transport control protocol TCP or user datagram protocol UDP so as to allow network acceleration devices B N to treat network acceleration device A as a target session. Moreover a TCP based or UDP based cache message protocol may enable cache to act as a sharable data store because cache daemon server engine need not realize that cache clients are remote to network acceleration device A. In other words cache daemon server engine may allow cache to transparently service requests from both local and remote cache clients . Although not illustrated in each of network acceleration devices A N may couple to client devices such as client devices of and may receive content requests from their respectively coupled or shared client devices.

Upon receiving content requests network acceleration devices B N spawn or in some embodiments utilize existing cache clients to service these content requests. Cache clients may issue cache access requests i.e. a cache message to cache daemon via cache daemon server engine using the TCP based cache message protocol. Cache daemon server engine receives the cache access request and performs a cache lookup via cache storage API as described above. In this respect network acceleration devices B N represent clients of network acceleration device A and network acceleration device A represents a server of cached content inserted and maintained by the cluster of network acceleration devices.

Cache determines whether the requested content resides within cache and may perform the process described above to return the requested content to the requesting cache clients via a response cache message which cache daemon server engine formulates in accordance with the TCP based cache message protocol. Cache daemon returns the requested content via cache daemon server engine to the requesting cache clients of their respective network acceleration devices B N. In this manner network acceleration device A may share cache among a cluster of network acceleration devices B N thereby reducing the cost associated with network acceleration devices B N because network acceleration devices B N need not include a cache but may still provide cache functionality. Moreover any individual one of network acceleration devices benefit from content previously downloaded and stored within cache by another network acceleration device.

Initially a client network acceleration device such as one of network acceleration devices B N receives a content request from a client device such as client device A of and spawns or in some embodiments utilizes an existing cache client such as one of cache clients to service the received content request . One of cache clients of for example network acceleration device B upon receiving the content request issues a cache access request to cache daemon of network acceleration device A via a network communication protocol to cache daemon server engine . Cache daemon server engine receives the cache access request via the network communication protocol e.g. the TCP or UDP based cache message protocol discussed above and performs a cache lookup via cache storage API based on cache access request as described above . As further described above cache may respond to the cache lookup by informing cache daemon whether a cache hit or a cache miss has occurred.

If a cache miss occurs YES branch cache daemon informs the requesting cache clients of the cache miss and upon receiving this notification the requesting cache clients download the content from an appropriate network server as described above . Once downloaded the requesting cache client forwards the content to the requesting client device and in some embodiments cache client may first encode the content prior to forwarding it to the requesting client device . Simultaneous to forwarding the content the requesting cache clients may request cache access via cache daemon server engine to write the contents to cache and upon receiving cache access the requesting cache client transfers the downloaded content to cache via cache daemon server engine . Upon receiving the downloaded content cache daemon stores the content in the manner discussed above via the cache storage API into cache .

In response to a cache hit NO branch cache daemon server engine informs the requesting cache clients of the cache hit whereupon the requesting cache clients receive the content via cache messages from cache daemon server engine which read the content from cache via cache storage API . As the content is received from cache the requesting cache clients may forward the requested content to the requesting client device . In this manner cache daemon server engine may facilitate the ability of network acceleration device A to share of cache among a cluster of network acceleration devices B N so as to allow network acceleration device B for example to benefit from the previous content requests issued by another network acceleration device such as network acceleration device N. Moreover cache daemon server engine may reduce the cost associated with the cluster of network acceleration devices B N because they do not incur the additional cost of including a cache but may continue to provide cache functionality.

While shareable cache functionality is described above cache may include the other principles of the invention discussed above such as the ability to store multiple encodings and multiple versions of content. Moreover cache may maintain a quasi persistent nature by use of a named memory map region such as named memory map region . Thus while cache daemon server engine may facilitate sharing of cache functionality among multiple network devices cache may further facilitate decreased download speeds by storing multiple encodings and versions of content as well as maintaining a quasi persistent nature as described above. Various embodiments of the invention have been described. These and other embodiments are within the scope of the following claims.

