---

title: Encapsulated accelerator
abstract: A data processing system comprising a host computer system and a network interface device for connection to a network, the host computer system and network interface device being coupled together by means of a data bus, and: the network interface device comprising: a controller unit having a first data port for connection to a network, a second data port, and a data bus interface connected to said data bus, the controller unit being operable to perform, in dependence on the network endpoints to which data packets received at the network interface device are directed, switching of data packets between the first and second data ports and the data bus interface; and an accelerator module having a first medium access controller coupled to said second data port of the controller unit and a processor operable to perform one or more functions in hardware on data packets received at the accelerator module, the said first medium access controller being operable to support one or more first network endpoints; the host computer system supporting: a plurality of guest software domains including a first guest software domain having an application; and a privileged software domain configured to present a virtual operating platform to said plurality of guest domains, the privileged software entity including a first software driver for the controller unit but not including a second software driver for the accelerator module; wherein the application is configured to access the accelerator module by means of one or more first data packets formed in accordance with a predetermined network protocol and directed to one or more of the first network endpoints.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08996644&OS=08996644&RS=08996644
owner: Solarflare Communications, Inc.
number: 08996644
owner_city: Irvine
owner_country: US
publication_date: 20101209
---
This invention relates to a network interface device including one or more accelerator units and a data processing system comprising such a network interface device.

Achieving the right balance between the functionality performance of a network interface device and power cost considerations has long been the subject of debate particularly in terms of the choice as to which aspects of the communication and other protocols that might operate over the device should be accelerated in hardware at the network interface device. Such acceleration functions are referred to as offloads because they offload processing that would otherwise be performed at the CPU of the host system onto the network interface device.

Usually the offload is chosen to be a specific function of the network protocol stack that is amenable to hardware acceleration. Typically this includes the data integrity aspects of a protocol such as TCP IP checksums iSCSI CRC digests or hashing or lookup operations such as the parsing of data flows onto virtual interface endpoints. Whether or not a particular function of a network protocol is amenable to hardware acceleration depends on several factors which will now be discussed.

A. Whether or not a function may be performed based solely on the contents of an individual network packet. This property is termed stateless when applied to an offload. A stateless offload requires little local storage at the network interface for example TCP IP checksum insertion on transmission requires buffering of a single Ethernet frame. In contrast a statefull operation may require the interface to store state relative to a large number of network flows over a large number of network packets. For example an Ethernet device that performs reassembly of TCP IP flows into units which are larger than the MSS Maximum Segmentation Size would be required to track many thousands of packet headers. Statefull protocol offloads can therefore require the network interface to have significant amounts of fast memory which is both expensive and power hungry.

B. Whether or not a function may be directly implemented in parallel logic operating over a single or small number of passes of the data contained within the network packet. This property is termed tractable. For example the AES GCM cryptographic algorithm has been designed such that the internal feedback loop may be unrolled when implemented. This enables a hardware designer to scale an AES GCM engine s performance bandwidth by simply adding more gates in silicon which by Moore s Law can be readily accommodated as higher speeds are required. In contrast the Triple DES cryptographic algorithm may not be unrolled into parallel hardware. This requires an implementation to iterate repeatedly over the data. In order to improve the performance of an iterative algorithm the implementation must scale in clock frequency which is becoming increasingly difficult on silicon based processes. Being untractable iterative algorithms are more difficult to implement as hardware offloads.

C. Whether or not a protocol function has been designed for hardware execution. Generally the specification of a hardware protocol will be unambiguous and strictly versioned. For example Ethernet line encodings are negotiated at link bring up time and once settled upon are strictly adhered to. Changing encoding requires a re negotiation. By contrast the TCP protocol that has not been specifically designed for execution at hardware is specified by many 10s of RFCs Request For Comments . These specifications often present alternative behaviours and are sometimes conflicting but together define the behaviour of a TCP endpoint. A very basic TCP implementation could be made through adherence to a small number of the RFCs but such a basic implementation would not be expected to perform well under challenging network conditions. More advanced implementations of the TCP protocol require adherence to a much larger number of the RFCs some of which specify complex responses or algorithms that are to operate on the same wire protocol and that would be difficult to implement in hardware. Software oriented specifications are also often in a state of continued development which is sometimes achieved without strict versioning. As such software oriented specifications are usually best expressed in high level programming languages such as C which cannot be easily parallelized and converted to hardware logic representation.

D. Whether or not a function is well known and commonly used enough for it to be considered for implementation in a commercial network interface device. Often application specific functions such as normalisation of stock exchange data feeds are only known to practitioners of their field and are not widely used outside of a few companies or institutions. Since the cost of implementing a function in silicon is tremendously expensive it might not be commercially viable to implement in hardware those functions whose use is limited to a small field.

In summary features that are typically chosen to be implemented as offloads in hardware are those which are stateless tractable hardware oriented well known and commonly used.

Unfortunately there are number of functions which do not meet these criteria and yet being performance sensitive greatly benefit from being accelerated in hardware offloads. For example in the Financial Services sector it is often the case that large numbers of data feeds must be aggregated together and normalized into a unified data model. This normalisation process would typically unify the feed data into a database by for example time representation or stock symbol representation which would require hundreds of megabytes of data storage to implement in hardware. Other niche application spaces that greatly benefit from being accelerated in hardware offloads include event monitoring equipment in high energy particle colliders digital audio video processing applications and in line cryptographic applications.

Often the hardware suitable for accelerating protocol functions in such niche application spaces does not exist because it is simply not commercially viable to develop. In other cases bespoke network interface hardware has been developed which implement the application specific offloads required but at significant cost such as with the Netronome Network Flow Engine NFE 3240. Additionally many bespoke hardware platforms lag significantly behind the performance of commodity silicon. For instance 40 Gb s Ethernet NICs are now available and the shift to 100 Gb s commodity products is quickly approaching yet most bespoke NICs based upon an FPGA are only capable of 1 Gb s.

To give an example the hardware offloads for a normalisation process in the Financial Services sector would typically be implemented at a NIC based upon an FPGA Field Programmable Gate Array controller that includes the features of a regular network interface as well as the custom offloads. This requires the FPGA controller to define for instance the Ethernet MACs and PCIe core as well as the custom offload engines and would typically be provided with a set of bespoke drivers that provide a host system with access to the hardware offloads of the FPGA. This implementation strategy is problematic because the speed and quality of FPGA chips for NICs is not keeping pace with the innovation of commodity NICs that use application specific integrated circuits ASICs . In fact the design and implementation of the PCIe core is often the rate determining factor in bringing a custom controller to market and FPGA vendors typically lag the commodity silicon designs by a year.

Furthermore the problem is becoming more acute as systems become more integrated and demand that NICs offer more commodity features such as receive side scaling RSS support for multiple operating systems network boot functions sideband management and virtualisation acceleration such as the hardware virtualisation support offered by the PCI SIG I O Virtualisation standards . This is being driven by the increasing use of virtualisation in server environments and data centres and in particular the increasing use of highly modular blade servers.

A data processing system is shown in of the type that might be used in the Financial Services sector to provide hardware accelerated normalisation of certain data feds. The data processing system includes a bespoke network interface device NIC coupled to a host system over communications bus . NIC has two physical Ethernet ports and connected to networks and respectively networks and could be the same network . The bespoke NIC is based around an FPGA controller that provides offloads and in hardware. The offloads could for example perform normalisation of data feeds received at one or both of ports and . Typically the NIC will also include a large amount of high speed memory in which the data processed by the hardware offloads can be stored for querying by software entities running at host system .

Generally host system will have an operating system that includes a kernel mode driver for the bespoke NIC and a plurality of driver libraries by means of which other software at user level is configured to communicate with the NIC . The driver libraries could be in the kernel or at user level . In the case of a host system in the Financial Services sector software might be bank software that includes a set of proprietary trading algorithms that trade on the basis of data generated by the offloads and and stored at memory . For example memory could include a database of normalised stock values the normalisation having been performed by the offloads and in accordance with known database normalisation methods. Typically host system will also include management software by means of which the NIC can be managed.

Since NIC provides a customised function set the vendor of the NIC will provide the driver and driver libraries so as to allow the software to make use of the custom functions of the NIC. Any software running at user level on the host system must therefore trust the vendor and the integrity of the driver and driver libraries it provides. This can be a major risk if the software includes proprietary algorithms or data models that are valuable to the owner of the data processing system. For example the data processing system could be a server of a bank at which high frequency trading software is running that includes very valuable trading algorithms the trades being performed at an exchange remotely accessible to the software over network or by means of NIC . Since all data transmitted to and from the host system over the NIC traverses the kernel mode vendor driver and vendor libraries the software including its trading algorithms are accessible to malicious or buggy code provided by the NIC vendor. It would be an onerous job for the bank to check all the code provided by the NIC vendor particularly since the drivers are likely to be regularly updated as bugs are found and updates to the functionality of the NIC are implemented. Furthermore a NIC vendor may require that a network flow is established between the management software of the NIC to the NIC vendor s own data centres. For example this can be the case if the NIC is a specialised market data delivery accelerator and the market data is being aggregated from multiple exchanges at the vendor s data centers. With the structure shown in the bank would not be able to prevent or detect the NIC vendor receiving proprietary information associated with software .

Financial institutions and other users of bespoke NICs that need to make use of hardware offloads are therefore currently left with no choice but to operate NICs that offer a level of performance behind that available in a commodity NIC and to trust any privileged code provided by the NIC vendor that is required for operation of the NIC.

There have been efforts to arrange network interface devices to utilise the processing power of a GPGPU General Purpose GPU provided at a peripheral card of a data processing system. For example an Infiniband NIC can be configured to make peer to peer transfers with a GPGPU as announced in the press release found at 

http colon slash slash gpgpu dot org 2009 11 25 nvidia tesla mellanox infiniband and the Nvidia GPUDirect technology is described at 

However despite offering acceleration for particular kinds of operations such as floating point calculations GPGPUs are not adapted for many kinds of operations for which hardware acceleration would be advantageous. For example a GPGPU would not be efficient at performing the normalisation operations described in the above example. Furthermore in order for a NIC to make use of a GPGPU the NIC typically requires an appropriately configured kernel mode driver and such an arrangement therefore suffers from the security problems identified above.

Other publications that relate to memory mapped data transfer between peripheral cards include Remoting Peripherals using Memory Mapped Networks by S. J. Hodges et al. of the Olivetti and Oracle Research Laboratory Cambridge University Engineering Department a copy of the paper is available at

http colon slash slash www dot cl dot cam dot ac dot uk research dtg www publications public files tr.98.6.pdt 

and Enhancing Distributed Systems with Low Latency Networking by S. L. Pope et al. of the Olivetti and Oracle Research Laboratory Cambridge University Engineering Department a copy of the paper is available at

http colon slash slash www dot cl dot cam dot ac dot uk research dtg www publications public files tr.98.7.pdf .

There is therefore a need for an improved network interface device that provides a high performance architecture for custom hardware offloads and an secure arrangement for a data processing system having a network interface device that includes custom hardware offloads.

According to a first aspect of the present invention there is provided a data processing system comprising a host computer system and a network interface device for connection to a network the host computer system and network interface device being coupled together by means of a data bus and the network interface device comprising a controller unit having a first data port for connection to a network a second data port and a data bus interface connected to said data bus the controller unit being operable to perform in dependence on the network endpoints to which data packets received at the network interface device are directed switching of data packets between the first and second data ports and communication queues at the host computer system and an accelerator module having a first medium access controller coupled to said second data port of the controller unit and a processor operable to perform one or more functions in hardware on data packets received at the accelerator module the said first medium access controller being operable to support one or more first network endpoints the host computer system supporting a plurality of guest software domains including a first guest software domain having an application and a privileged software domain configured to present a virtual operating platform to said plurality of guest domains the privileged software entity including a first software driver for the controller unit but not including a second software driver for the accelerator module wherein the application is configured to access the accelerator module by means of one or more first data packets formed in accordance with a predetermined network protocol and directed to one or more of the first network endpoints.

Preferably the plurality of guest software domains includes a second guest software domain having a driver library for said accelerator module the driver library supporting a second network endpoint and the privileged software domain being configured so as to allow the application to access the driver library by means of one or more second data packets directed to said second network endpoint the second data packets being formed in accordance with the predetermined network protocol. Preferably the driver library is configured to manage the accelerator module by means of driver commands encapsulated within data packets of the predetermined network protocol and directed to one or more of the first network endpoints.

Preferably the first guest software domain includes a communications library configured to translate send and receive requests by the application into the transmission and reception of data packets formed in accordance with the predetermined network protocol.

Suitably the accelerator module further comprises a memory configured for storing data generated by the said one or more functions performed by the processor and the application is configured to access said memory by means of one or more read requests encapsulated in one or more first data packets formed in accordance with the predetermined network protocol.

The first software driver could be integral with the kernel level code of the hypervisor or virtual machine monitor.

Suitably the predetermined network protocol is UDP IP and the network endpoints can be identified by IP addresses. Suitably the network interface device is an Ethernet network interface device and the first network endpoint can be identified by an Ethernet address.

Suitably the application is a trading platform configured to perform trades at one or more financial exchanges accessible by means of the network interface device.

Preferably the controller unit is configured to forward data packets received at the second data port to the accelerator module over the first data port only if those data packets are directed to one of the one or more first network endpoints.

According to a second aspect of the present invention there is provided a network interface device comprising a controller unit having a first data port for connection to a network a second data port and a data bus interface for connection to a host computer system the controller unit being operable to perform in dependence on the network endpoints to which data packets received at the network interface device are directed switching of data packets between the first and second data ports and the data bus interface and an accelerator module having a first medium access controller coupled to said second data port of the controller unit and a processor operable to perform one or more functions in hardware on data packets received at the accelerator module wherein said first medium access controller is operable to support one or more first network endpoints such that in use data packets identified by the controller unit as being directed to said first network endpoints are sent over the second data port to the accelerator module.

The said one or more functions performed by the processor could be non communications functions. Suitably the said one or more functions performed by the processor do not include functions relating to the performance of network communications protocols in use at the network interface device. The one or more functions performed by the processor could include one or more of normalisation of financial information prior to storage at a memory of the accelerator module or transmission to a host computer system accessible over the data bus interface serialisation of trades directed to a financial exchange analysis of scientific data digital audio and or video processing and in line cryptographic functions.

Preferably the first data port of the network interface device includes a medium access controller coupled to a physical layer transceiver.

Preferably the second data port of the network interface device includes a second medium access controller coupled to a second serial interface device and the first medium access controller of the accelerator module is coupled to the second data port by means of a first serial interface device said first and second serial interface devices being connected so as to allow the communication of data between the second data port and the accelerator module. Alternatively the second data port of the network interface device includes a second medium access controller coupled to a second physical layer transceiver and the first medium access controller of the accelerator module is coupled to the second data port by means of a first physical layer transceiver said first and second physical layer transceivers being connected so as to allow the communication of data between the second data port and the accelerator module.

The controller unit and the accelerator module could be provided at separate peripheral devices and the controller unit is coupled to the accelerator module by a connection between the first serial interface device or first physical layer transceiver and the second serial interface device or second physical layer transceiver.

The network interface device could further comprise a second media access controller and at the accelerator module a DMA controller and a data bus interface wherein the controller unit is configured to provide a virtual interface of the network interface device that includes said second media access controller but not a physical layer transceiver and the DMA interface is configurable so as to allow the accelerator module to exchange data with said virtual interface of the network interface device over a DMA channel.

The network interface device could further comprise a data bus bridge arranged for connecting the data bus interface of the hardware accelerator and the data bus interface of the controller unit to a data bus.

The controller unit and the accelerator module could be provided at separate peripheral devices and their respective data bus interfaces are configured for connection to a data bus.

Suitably the data bus is a PCIe data bus and the virtual interface supports one or more SR IOV or MR IOV virtual configurations.

Suitably the accelerator module further comprises a memory configured for storing data generated by the said one or more functions performed by the processor. Suitably the accelerator module is operable to receive data packets from a host computer system accessible over the data bus interface and the accelerator module is configured to in response to receiving a data packet including a read request directed to the memory respond to the read request by encapsulating the requested data in one or more data packets for transmission to a network endpoint identified in the read request. Suitably the accelerator module is configured to respond only to read requests received from a predetermined set of one or more network endpoints or over a predetermined set of one or more DMA channels.

The first medium access controller could be an Ethernet MAC and the first network endpoint identified by an Ethernet address.

According to a third aspect of the present invention there is provided a network interface device comprising a hardware interface for receiving an accelerator module a DMA interface a controller unit having a data port for connection to a network by means of a first medium access controller and a physical layer transceiver and being configured to provide a virtual interface of the network interface device that includes a second media access controller but not a second physical layer transceiver and is associated with a DMA channel established over the DMA interface and a data bus bridge connected to the hardware interface and the controller unit and configured for connection to a host computer system 

wherein the controller unit is operable to perform in dependence on the network endpoints to which data packets received at the network interface device are directed switching of data packets between the data port the virtual interface and the data bus bridge such that in use with an accelerator module at the hardware interface data packets identified as being directed to a network endpoint associated with the accelerator module are sent over a DMA channel associated with the virtual interface and established between the virtual interface and the accelerator module.

The following description is presented to enable any person skilled in the art to make and use the invention and is provided in the context of a particular application. Various modifications to the disclosed embodiments will be readily apparent to those skilled in the art.

The general principles defined herein may be applied to other embodiments and applications without departing from the spirit and scope of the present invention. Thus the present invention is not intended to be limited to the embodiments shown but is to be accorded the widest scope consistent with the principles and features disclosed herein.

The present invention provides solutions to the problems identified in the prior art by offering a novel network interface device and data processing system architecture. A network interface device NIC configured in accordance with the present invention is not limited to providing an interface to a particular network fabric having a particular kind of interface to a host system or to supporting a particular set of network protocols. For example such a NIC could be configured for operation with an Ethernet network IEEE 802.11 network or a FibreChannel network interface to a host system over a PCIe PCI X or HTX bus support communications over UDP TCP IP or IPsec. A host system could be any kind of computer system at which a network interface device can be supported such as a server. A host system comprising a network interface device will be referred to herein as a data processing system. Note that a network interface device configured in accordance with the present invention need not be provided as a device for connection to an expansion slot e.g. PCIe or communications port e.g. eSATA of a host system and could form part of the host system. For example the network interface device could be located at the motherboard of a host system. The network interface device is connected to the host system by means of an internal or external communications bus.

A network interface device and host system configured in accordance with the present invention is shown in . The NIC is a 4 port NIC comprising a controller having four ports and . Ports and provide access to physical networks and but ports and are not directed to external network fabrics and are instead directed to a hardware accelerator . Importantly all the custom hardware offloads of the NIC are provided at the one or more accelerator integrated circuits so as to allow controller integrated circuit to remain uncustomised. The accelerator integrated circuits could be for example one or more FPGAs or other programmable integrated circuits. Optionally hardware accelerator includes a memory for the storage of data relating to the offloads performed at accelerator IC .

A standard commodity network interface controller can be used as controller which brings with it all the performance advantages of using commodity silicon. For example in the case of an Ethernet NIC controller could be a 40 Gb s part configured to support four ports at 10 Gb s. At the present time it is not possible to support a throughput of 40 Gb s at FPGAs or other programmable integrated circuits. Aside from the raw speed improvements gained by using a commodity ASIC controller ASIC controllers and their software drivers are generally more highly optimised and ASICs are cheaper smaller and consume less power for a given performance level than FPGAs or other programmable ICs. Furthermore the expensive custom parts can also be smaller and more straightforward because the accelerator ICs do not need to provide the functions of a regular NIC controller such as host interfaces support for parts of a network stack etc. .

Each of the ports and includes a Medium Access Controller MAC e.g. a data link layer device . MACs and of the NIC are provided with a PHY implementing the physical layer communication protocol in use over the NIC and coupling the MACs to the physical medium of networks and . These NIC MACs could be implemented at controller could be provided at a separate integrated circuit or could be part of a multi chip module MCM with the controller IC.

In the present invention the network interface device and hardware accelerator are configured such that the hardware accelerator can be addressed as a network endpoint. Hardware accelerator is configured to present itself as one or more network endpoints to which data packets can be addressed by providing a MAC for each port of the hardware accelerator. The MACs of the hardware accelerator could be implemented at accelerator IC could be provided at a separate integrated circuit of the accelerator or could be part of a multi chip module MCM with the accelerator IC . Hardware accelerator therefore differs from other forms of custom accelerator for example a GPGPU that terminate data flows and that require a NIC configured to support a proprietary interface to the accelerator and or an interface that requires additional driver layers at the host computer system.

The hardware accelerator could be configured to process data packets arriving in data flows at its one or more endpoints and forward the processed data packets or data generated in dependence on the received data packets onto one or more receive queues at the host computer system. Thus the hardware accelerator could process a stream of data packets arriving at one or more of its endpoints on the fly. The routing table of the NIC would be configured to appropriately switch data packets between the endpoint s of the accelerator receive queues of the host computer system and the network endpoints of remote hosts accessible over the network. Alternatively or additionally the hardware accelerator could process data packets arriving in data flows at its one or more endpoints and store the processed data in its memory the accelerator being configured to allow appropriately formed data packets to query the data stored in the memory.

A data processing system and network interface device configured in accordance with the present invention is shown in . MACs and of the NIC that are directed to the hardware accelerator are coupled to the MACs and of the accelerator by means of links which could comprise PHYs or serial interface devices such as a KX4 serial device. The use of serial interface devices has the advantages that they are low power and can be implemented using standard SERDES libraries. Preferably the links each comprise a PHY or serial interface device located at the NIC and a corresponding PHY or serial interface device located at the hardware accelerator so as to provide the physical interface between the MAC of the NIC and the corresponding MAC of the hardware interface e.g. between MACs and .

The PHYs or serial interface devices of the NIC could be implemented at NIC controller could be provided at a separate integrated circuit of the NIC or could be part of a multi chip module MCM with the controller IC . The PHYs or serial interface devices of the hardware accelerator could be implemented at accelerator IC could be provided at a separate integrated circuit of the accelerator or could be part of a multi chip module MCM with the accelerator IC .

A network interface device configured in accordance with a second embodiment of the present invention is shown in the network interface device being for use in the data processing system of . Hardware accelerator includes a DMA interface configured so as to allow one or more DMA channels to be established between the hardware accelerator and the NIC . NIC includes a data bus interface bridge so as to provide a connection between the hardware accelerator and the data bus and the NIC controller and the data bus. In the example shown in the data bus is a PCIe data bus and the bridge is a PCIe bridge. In this embodiment each data port of the hardware accelerator is a dedicated DMA channel over which DMA data transfers can be performed between the hardware accelerator and NIC controller. The hardware accelerator is connected to the PCIe bridge by means of an interface appropriate to the bridge circuitry being used. Suitable interfaces might be one of a simple UART a SERDES device such as a KX4 serial interface device and a local bus interface.

The hardware accelerator and NIC are configured so as to establish a dedicated DMA channel between themselves and allow the low latency exchange of data between the hardware accelerator and NIC. The hardware accelerator therefore includes DMA interface which is preferably part of accelerator IC . The DMA interface implements the memory model so that the NIC controller can act as a bus master DMA device to memory locations supported at the hardware accelerator. Such memory locations could be virtual memory locations.

The NIC includes a MAC for each port or dedicated DMA channel of the hardware accelerator in there are two ports with each MAC representing a virtual interface device of the NIC. The NIC does not need to provide a PHY for MACs and and neither does the hardware accelerator for its MACs and . By arranging for each dedicated DMA channel of the hardware accelerator to address a corresponding virtual interface of the NIC the virtual interfaces of the NIC can represent the data ports or DMA channels of the hardware accelerator and are virtual ports of the NIC .

In both embodiments NIC controller includes switch functionality so as to allow the switching of data packets between its data ports and data bus . The controller is therefore operable to send data packets to the hardware accelerator that are received at the NIC and identified as being directed to the hardware accelerator in the same way as it might direct data packets destined for a remote endpoint on network to port . This can be achieved by programming the switch of controller to route data packets to particular data ports in dependence on the network endpoint i.e. network address to which each data packet is directed. Preferably the switch of controller can also be programmed such the particular network endpoint at the host system to which a data packet is directed determines the DMA channel into which it is delivered.

More generally a NIC configured in accordance with the present invention could have any number of ports provided that it has at least one port directed to a network and at least one port directed to a hardware accelerator as described herein.

Note that the accelerator integrated circuits need not be programmable and could be bespoke ASICs. This is unusual because of the high cost of designing and manufacturing an ASIC. However it will be apparent that many of the advantages of the present invention remain a network interface controller ASIC is generally more highly optimised than a bespoke controller ASIC that is designed to support one or more hardware offloads and because many of the complex functions present in a network interface controller need not be designed and manufactured at great expense as part of the custom ASIC. Alternatively the accelerator IC could be a microprocessor or a dedicated hardware unit such as a time stamp or cryptographic module .

It is advantageous if NIC is provided in two parts hardware accelerator and a reference NIC that includes all the parts of the NIC shown in except for the hardware accelerator or equally an accelerator IC and a reference NIC that includes all the parts of the NIC shown in except for the accelerator IC. By providing at the reference NIC an interface configured to receive a hardware accelerator or accelerator IC a single reference NIC design can be used with a variety of different hardware accelerators. This allows the custom offloads provided at the NIC to be readily upgraded or modified by simply replacing the hardware accelerator or accelerator IC at the NIC and installing new versions of the driver libraries for the hardware accelerator accelerator IC at the host system.

The controller is configured to interface with host system over data bus which could be for example a PCIe data bus. The data bus could alternatively be the backplane of a blade server and could itself operate in accordance with one or more network protocols for example the data bus could be a high speed Ethernet backplane.

In accordance with preferred embodiments of the present invention host system is a virtualised system comprising a privileged software entity such as a hypervisor or virtual machine monitor that presents a virtual operating platform to a plurality of guest operating systems and . The privileged software entity operates at a higher level of privilege e.g. kernel mode than the guest operating systems which operate at a lower level of privilege e.g. user level mode .

Privileged software entity includes a network interface device driver that is configured to provide a software interface to NIC controller . Importantly because controller is not customised driver can be a standard driver for the controller whose code has been certified by a trusted party such as the vendor of the privileged software entity e.g through the VMWare IOVP or Microsoft WHQL programs . The driver could also be digitally signed so as to authenticate the origin of the code. For example if the NIC is an Ethernet NIC and the privileged software entity a Hyper V Hypervisor of Microsoft Windows Server 2008 then driver could be provided by the NIC vendor and certified by Microsoft for operation in the hypervisor. Since any software installed at the host system must necessarily trust the platform on which it was installed software executing at guest OS can trust the driver over which it communicates. Furthermore since driver does not provide any custom functionality and need not be updated when any offload functions implemented at the NIC are modified it would be possible for the operator of software running at guest domain to check the driver for any malicious or buggy code and trust that the driver is certified and remains unmodified throughout the production life of the machine.

Privileged software entity also includes a soft switch configured to route data packets between the guest operating systems and the network endpoints served by the NIC i.e. on networks or or at the hardware accelerator and between network endpoints at the guest operating systems themselves. Network endpoints are for example Ethernet or internet protocol IP network addresses. Typically the soft switch operates only on the standard set of network protocols supported by driver .

One of the guest operating systems is configured to include driver libraries for the hardware accelerator. Importantly driver libraries are configured to communicate with the hardware accelerator by means of data e.g. commands responses state information encapsulated within network packets directed to an endpoint of the hardware accelerator. Such data packets are routed at soft switch onto data bus for the NIC and at the switch functions of NIC controller the data packets are routed onwards to port or and hence the hardware accelerator. Similarly hardware accelerator is configured to communicate with driver libraries by means of data e.g. commands responses state information encapsulated within regular network packets directed to an endpoint of guest operating system e.g. a receive queue of the driver libraries . In this manner communications between the driver libraries of the hardware accelerator and the hardware accelerator itself can be achieved using regular network packets that can be handled as such at the switches of the system. The benefits of this are twofold firstly it allows the hardware accelerator to be implemented at a high speed port of a commodity NIC as though the hardware accelerator is a network entity addressable over a particular port and secondly it allows the driver libraries for the hardware accelerator to be located outside of the kernel at a guest operating system having a low privilege level.

The architecture of the host system is therefore arranged such that none of the code relating to the functions of the hardware accelerator is at a higher privilege level than any sensitive or secret software executing in another guest operating system . Software could be for example a bank s high frequency trading software comprising a set of highly valuable proprietary trading algorithms. By isolating driver libraries from software in this manner the owners of software can be confident that any malicious or buggy code provided by the vendor of the hardware accelerator cannot cause the activities of software to be revealed. Accelerator vendor domain could also include any management software for the hardware accelerator.

Accelerator vendor libraries and accelerator management software are arranged to configure the offload functions performed by the hardware accelerator. This can be by for example defining the normalisation parameters to be applied to each type of stock and managing the use of memory by the offloads of the accelerator IC.

Software is configured to communicate with accelerator driver libraries by addressing the driver libraries as a network endpoint. In other words software transmits network data packets to a network endpoint represented by a receive queue of the driver libraries as though the driver libraries were a remote network entity. Similarly driver libraries are configured to communicate with software by addressing the software as a network endpoint. The data packets sent between the software and driver libraries encapsulate commands responses and other data in an analogous way to the system calls and responses exchanged between software and kernel drivers in conventional host systems.

Since all data to and from the hardware accelerator is encapsulated as network data packets software can communicate with vendor libraries and hardware accelerator by means of a generic application programming interface API at the software domain . The API maps network send and receive requests by software into the transmission and reception of network data packets. Preferably the protocol in use over connections between software and the hardware accelerator or vendor libraries is a light low latency protocol such as UDP User Datagram Protocol . The API could be a POSIX API or other generic API suitable for use at domain . No proprietary accelerator vendor code is therefore required at domain .

As is well known in the art some aspects of the formation of data packets in accordance with the network protocol could be performed at the NIC such as checksum formation. However it is preferable that connections between software and hardware accelerator or vendor libraries are configured such that checksums are not required in data packets exchanged between those entities.

Using a standard network encapsulation and a commodity NIC controller for all messages exchanged with the hardware accelerator has a number of advantages 

1. Non accelerated data flows that do not need or benefit from hardware acceleration can be delivered to the host system in a conventional manner without passing through the hardware accelerator. This allow such data flows to be delivered with the lowest possible latency which for example is very important for high frequency trading applications.

2. Data flows can be delivered using receive side scaling RSS interrupt moderation and other techniques that improve performance at a host system having a multi core CPU architecture.

3. Data flows can be delivered using direct guest access to the guest domains of the virtualised host system with the hardware virtual switch of controller being configured to select the appropriate DMA delivery channel.

4. A PCIe controller can be selected that implements the SR IOV or MR IOV virtualisation standards that allow multiple DMA channels to be mapped directly into virtual guest address spaces.

None of these advantages depends upon additional functionality being provided at the hardware accelerator. It can be particularly advantageous to use one or more of these three techniques together at a data processing system.

Note that the advantages described above of a NIC configured in accordance with the present invention do not rely on the NIC being supported at a host system having a virtualised architecture as shown in other host system architectures could be used with NIC in which the offload functions of the hardware accelerator can be accessed as network endpoints. However a data processing system comprising the combination of NIC and host system of is particularly advantageous since it provides all the performance cost and flexibility benefits of a NIC as described herein with all the security and stability benefits of a host system having the architecture shown in the figure.

The data processing system and network interface card described herein benefits from the fact that all the kernel mode components of the system can be provided by the commodity vendor and so can be more easily made robust over a large number of operating systems. For example commodity NIC software is implemented in the mass market and hence benefits from a commensurate level of engineering and investment. The use of such commodity code reduces the likelihood that the NIC driver would cause instabilities at the data processing system.

The operation of NIC with host system will now be described by way of example. Suppose the data processing system is a high frequency trading server owned by a bank and the accelerator IC of the hardware accelerator at the NIC provides a set of database normalisation offloads that can be performed on stock data received from an exchange accessible over network . Such offloads would be performed by the accelerator IC prior to storing the normalised data at a database in memory . By appropriately configuring the routing tables of the switch of NIC controller stock data feeds arriving at port of the NIC from the exchange would be directed to port by the controller for normalisation by the appropriate hardware offloads defined at the accelerator IC. The routing tables of the switch of NIC controller can be configured by means of driver as is well known in the art typically in response to a routing table update request from management software supported at the host system. Preferably the bank s trading software would be configured to cause the routing table of the controller switch to be maintained such that stock feeds received from the remote exchange are directed to endpoints accessible over port . The hardware accelerator may represent a plurality of endpoints with each endpoint relating to a different feed for example.

As stock feeds stream in over port and are routed for normalisation at the accelerator IC a normalised database of stock data is built up at memory . This is the data that is valuable to the bank s trading algorithms embodied in trading software and that must be accessed in order to allow the software to make trading decisions. Access to the hardware accelerator is mediated by accelerator vendor libraries . Thus if trading software requires access to the hardware accelerator the vendor libraries are configured to establish connection s between one or more endpoints of the hardware accelerator and one or more endpoints of the trading software.

Once a connection between the trading software and hardware accelerator has been established e.g. a connection between an endpoint of the hardware and an endpoint at guest domain has been set up trading software can read and write to hardware accelerator by means of generic API and the protocol stack. In this example data is exchanged between the trading software and hardware accelerator in accordance with the UDP protocol. Thus in response to appropriate read requests from software data from the stocks database at memory is encapsulated at the hardware accelerator in data packets for transmission over the network to the receive queues of software at guest domain . To ensure low latency delivery of data to the trading software the NIC controller is configured to deliver data packets directed to guest domain over DMA channels established between the NIC and the receive queues of the guest domain. In this manner the trading software can access the stocks database generated by the normalisation offloads of the accelerator IC in order to allow the proprietary trading algorithms embodied in the software to determine the optimum trading decisions.

Note that the term database is used to refer to an organised cache of data at memory and does not imply any particular general purpose database architecture. Database queries sent by the trading software in network data packets are preferably formatted in accordance with an API defined by the vendor of the hardware accelerator.

As trading software determines the trades it wishes to make it transmits its stock trade requests over network to the appropriate exchange in this example port of the NIC is dedicated to receiving stock feed data and port of the NIC is dedicated to handling the trades performed by the software at one or more remote exchanges accessible over network . Thus the NIC handles both accelerated flows to from the hardware accelerator and conventional non accelerated flows. Typically the financial exchange at which the trades are requested is the same exchange from which the data feeds are being received. Trade requests could alternatively be sent over a separate network interface device.

The hardware accelerator need not be located at network interface device or and could be provided at another unit of the data processing system and connected directly to the network interface device. For example the hardware accelerator could be provided at a PCIe card connected to the NIC by a serial ribbon cable. If the NIC is configured in accordance with the second embodiment described above ports and of the NIC could be provided with PHYs and the hardware accelerator could be connected to the NIC by a network link for example the hardware accelerator could be another peripheral card of the host system and a short loop through network link could be provided between the accelerator and NIC. In fact if the PCIe root complex of a PCIe card supporting the hardware accelerator were to support DMA data transfers then the hardware accelerator could be provided at a PCIe card and connected to a PCIe NIC by DMA channels established between the hardware accelerator and NIC over the PCIe bus without any additional connection being provided between the two PCIe cards.

The network interface device itself need not be provided at a discrete peripheral card of the system and could be located at the mainboard of the system i.e. as a LOM device .

The data packets exchanged between the receive queues of the host system and the network endpoints of the hardware accelerator could be for example UDP data packets directed to network endpoints identified by IP addresses.

A media access controller or MAC configured in accordance with the present invention could include multiple protocol layers and is not restricted to handling a MAC layer communications protocol. Which protocol layers are supported at a MAC depends on the particular network protocols in use over the data port for which the MAC is provided. For example if the data port is a port of an Ethernet network interface device directed to a hardware accelerator the MAC would preferably perform only the Ethernet MAC layer but could also perform the Ethernet LLC layer. With such an arrangement the network endpoint supported at the MAC of a hardware accelerator would be an Ethernet network address and data communicated with the hardware accelerator would be encapsulated in Ethernet frames at the NIC.

The applicant hereby discloses in isolation each individual feature described herein and any combination of two or more such features to the extent that such features or combinations are capable of being carried out based on the present specification as a whole in the light of the common general knowledge of a person skilled in the art irrespective of whether such features or combinations of features solve any problems disclosed herein and without limitation to the scope of the claims. The applicant indicates that aspects of the present invention may consist of any such individual feature or combination of features. In view of the foregoing description it will be evident to a person skilled in the art that various modifications may be made within the scope of the invention.

