---

title: Cache synchronization for solid state disks
abstract: Described embodiments provide a media controller that synchronizes data cached in a buffer and corresponding data stored in one or more sectors of a storage device. A buffer layer module of the media controller caches data transferred between the buffer and the storage device. One or more contiguous sectors are associated with one or more chunks. The buffer layer module updates a status corresponding to each chunk of the cached data and scans the status corresponding to a first chunk of cached data. If, based on the status, the first chunk of cached data is more recent than the corresponding data stored on the storage device, a media layer module synchronizes the data on the storage device with the cached data. The status corresponding to the group of one or more sectors is updated. The media layer module scans a next chunk of cached data, if present.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08352690&OS=08352690&RS=08352690
owner: LSI Corporation
number: 08352690
owner_city: Milpitas
owner_country: US
publication_date: 20100324
---
This application claims the benefit of the filing date of U.S. provisional application Nos. 61 245 112 filed Sep. 23 2009 and 61 245 973 filed Sep. 25 2009 the teachings of which are incorporated herein in their entireties by reference.

The subject matter of this application is related to U.S. patent application Nos. 12 436 227 filed May 6 2009 12 475 710 filed Jun. 1 2009 12 475 716 filed Jun. 1 2009 12 477 996 filed Jun. 4 2009 12 478 013 filed Jun. 4 2009 12 508 879 filed Jul. 24 2009 12 508 915 filed Jul. 24 2009 12 643 471 filed Dec. 21 2009 12 649 490 filed Dec. 30 2009 and 12 722 828 filed Mar. 12 2010 the teachings of all of which are incorporated herein in their entireties by reference. The subject matter of this application is also related to U.S. patent application Nos. 12 731 631 filed Mar. 25 2010 12 767 985 filed Apr. 27 2010 12 768 058 filed Apr. 27 2010 12 769 882 filed Apr. 29 2010 and 12 769 910 filed Apr. 29 2010.

The present invention relates to flash memory storage devices and in particular to storing logical to physical translation data for solid state disks SSDs .

Flash memory is a type of non volatile memory that is electrically erasable and re programmable. Flash memory is primarily used in memory cards and USB flash drives for general storage and transfer of data between computers and other digital products. Flash memory is a specific type of electrically erasable programmable read only memory EEPROM that is programmed and erased in large blocks. One commonly employed type of flash memory technology is NAND flash memory. NAND flash memory forms the core of the flash memory available today especially for removable universal serial bus USB storage devices known as USB flash drives as well as most memory cards. NAND flash memory exhibits fast erase and write times requires small chip area per cell and has high endurance. However the I O interface of NAND flash memory does not provide full address and data bus capability and thus generally does not allow random access to memory locations.

There are three basic operations for NAND devices read write and erase. The read and write operations are performed on a page by page basis. Page sizes are generally 2bytes where N is an integer with typical page sizes of for example 2 048 bytes 2 kb 4 096 bytes 4 kb 8 192 bytes 8 kb or more per page. Pages are typically arranged in blocks and an erase operation is performed on a block by block basis. Typical block sizes are for example 64 or 128 pages per block. Pages must be written sequentially usually from a low address to a high address. Lower addresses cannot be rewritten until the block is erased.

A hard disk is addressed linearly by logical block address LBA . A hard disk write operation provides new data to be written to a given LBA. Old data is over written by new data at the same physical LBA. NAND flash memories are accessed analogously to block devices such as hard disks. NAND devices address memory linearly by page number. However each page might generally be written only once since a NAND device requires that a block of data be erased before new data is written to the block. Thus for a NAND device to write new data to a given LBA the new data is written to an erased page that is a different physical page than the page previously used for that LBA. Therefore NAND devices require device driver software or a separate controller chip with firmware to maintain a record of mappings of each LBA to the current page number where its data is stored. This record mapping is typically managed by a flash translation layer FTL in software that might generate a logical to physical translation table. The flash translation layer corresponds to the media layer of software and or firmware controlling an HDD.

Associated with each page is a spare area typically 100 500 bytes generally used for storage of error correction code ECC information and for storage of metadata used for memory management. The ECC is generally needed for detecting and correcting errors in the user data stored in the page and the metadata is used for mapping logical addresses to and from physical addresses. As such the additional bytes of memory are hidden from the user and are not available for storing user data. The first block block 0 of a flash die is generally provided from the manufacturer error free and is commonly used by designers to include program code and associated metadata for block management.

For consumer applications HDDs generally have data sectors that are sized in powers of two e.g. 512 2 bytes per sector . Flash memories structured with page sizes that are a multiple of the HDD sector size might efficiently work with the HDD system by storing multiple entire sectors in a page e.g. a 4096 byte page can store eight 512 byte sectors . However enterprise based HDD systems generally do not use sectors sized by powers of two but use larger sectors generally either 520 or 528 bytes per sector instead of 512 bytes. Thus typical flash memories perform inefficiently for enterprise applications since there are unused bytes in each page.

Typically for high capacity solid state disks SSDs several design tradeoffs might be considered when implementing a method to maintain a logical to physical translation table. These tradeoffs typically include efficient random access memory RAM usage efficient flash usage fast address lookup for both read operations and write operations fast write performance and fast reconstruction of the translation table on device startup.

Several techniques are known in the art for maintaining the logical to physical translation table. One such approach is known as direct page mapping an example of which is described in the paper by Andrew Birrell Michael Isard et al. A DH PFD Vol. 41 Issue 2 pp. 88 93 April 2007 which is incorporated herein by reference in its entirety hereinafter Birrell . Direct page mapping maintains a lookup table in RAM having an entry for each flash page and a summary page for metadata at the end of each block from which the logical to physical translation table may be reconstructed at startup. For example a direct page mapped translation table might contain for every LBA a logical sector number corresponding to a physical block number and a physical page number. Thus direct page mapping comprises a single level logical to physical translation. The summary page for each block might contain the LBA and valid bits for each page in the block so that the translation table can be reconstructed at startup. Thus the direct page mapping scheme requires a large amount of RAM on the order of 1 2 MB per GB of user storage to store the translation table which can become burdensome for higher capacity SSDs.

Another approach is known as block mapping. Block mapping generally classifies blocks as either data blocks D blocks or update blocks U blocks . The total size of the D blocks is the effective storage space for user data while U blocks are invisible to users. Generally when a write command cannot be accommodated in the D block corresponding to the LBA a U block is allocated to receive the new data and the old data in the D block is invalidated. Subsequent writes to that D block will be received by the allocated U block. When the U block becomes full another U block might be allocated or the U block might be merged with the original D block. Thus block mapping maintains a lookup table in RAM that maps a logical block to a physical block. Block mapping lacks a page level map instead relying on the typical case that data is stored in sequential order within the block. For example a block mapped translation table might contain a logical sector number corresponding to a logical block number and a logical page number. The logical block number can be translated into a physical block number and the logical page number might correspond to a physical offset within the physical block. Thus block mapping comprises a two level logical to physical translation. The size of the translation table is proportional to the number of blocks in the flash memory thus requiring less RAM than a page mapped translation table.

However because block mapping does not have a page level map the flash media may be inefficiently utilized when the data access workload is non sequential. For non sequential data access workloads block mapping might require data to be copied and re written numerous times to maintain the correct mapping. An example of block mapping is described in the paper by Jeong Uk Kang Heeseung Jo et al. A S FTLNAND FM6 pp. 161 170 Oct. 22 25 2006 which is incorporated herein by reference in its entirety hereinafter Kang .

A third approach for maintaining the logical to physical translation table is known as a superblock mapping scheme. Superblock mapping groups together a set number of adjacent logical blocks into a Superblock. Superblock mapping maintains a page global directory PGD in RAM for each Superblock. Page middle directories PMDs and page tables PTs are maintained in the spare areas of the flash pages. Each LBA can be divided into a logical block number and a logical page number with the logical block number comprising a superblock number and a PGD index offset. The logical page number comprises a PMD index offset and a PT index offset. Each entry of the PGD points to a corresponding PMD. Each entry of the PMD points to a corresponding PT. The PT contains the physical block number and the physical page number of the data. To translate a logical address to a physical address in Superblock mapping a module must access RAM to read the PGD access flash to read the PMD access flash to read the PT and access flash to access the requested data address. Super block mapping thus comprises a four level logical to physical translation and provides page mapping.

The PMD s and PT s are stored in the spare areas of the flash pages to provide page mapping without using an excessive amount of RAM. However because the spare area is used to store page level mapping information less memory is available for error correction codes ECC . Further the limited amount of memory available in the spare area precludes storing complicating mapping information. Finally reconstruction of the translation table at startup can be time intensive. An example of a superblock mapping scheme is described in Kang.

As described previously for write operations NAND devices store the new data for the LBA on a new page unlike hard disk drives HDDs that can rewrite individual physical sectors. Thus a NAND device generally requires that a block be erased before new data can be written to the block. Further as described above often a NAND device will write new data for a given LBA to an erased page that is a different physical page from the page previously used for that LBA. Thus NAND devices also generally require the device driver software or the separate controller chip periodically initiate a process to erase data that is stale or out of date. As would be apparent to one of skill in the art without periodically erasing out of date data the flash memory would fill up with data that is mostly out of date. This inefficiency would reduce the realized flash memory capacity because less current data could be stored. Therefore device driver software or controller chips generally periodically run a garbage collection routine adapted to provide efficient flash memory utilization by erasing out of date blocks. An example of a garbage collection routine is described in Kang. Garbage collection routines impact performance of the flash memory system by utilizing processor resources and potentially delaying write operations to the flash media.

However NAND device blocks can be erased relatively few times before device failure typically on the order of 100 000 erasures . Therefore over the operational life of an SSD blocks of flash memory will fail and become unusable. Thus the device driver software or the separate controller chip should minimize the number of erasures and must also maintain a record of bad blocks. For example device driver software or controller chips might implement wear leveling to spread the erasing and writing of blocks over the entire flash memory to avoid repeatedly erasing and writing a given subset of blocks.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used to limit the scope of the claimed subject matter.

Described embodiments provide a media controller that synchronizes data cached in a buffer and corresponding data stored in one or more sectors of a storage device. A buffer layer module of the media controller caches data transferred between the buffer and the storage device. One or more contiguous sectors are associated with one or more chunks. The buffer layer module updates a status corresponding to each chunk of the cached data and scans the status corresponding to a first chunk of cached data. If based on the status the first chunk of cached data is more recent than the corresponding data stored on the storage device a media layer module synchronizes the data on the storage device with the cached data. The status corresponding to the group of one or more sectors is updated. The media layer module scans a next chunk of cached data if present.

In accordance with embodiments of the present invention a flash controller is provided that temporarily stores data being read from or written to a flash media in a cache buffer. Since some data might be cached in RAM it is possible that data stored in the cache is dirty meaning that data stored in the cache is more recent than corresponding data stored in the flash media. As described herein data transfers might be segmented into smaller internal data transfers chunks where each chunk corresponds to a predefined LBA range sectors . In embodiments of the present invention a buffer layer of the flash controller might synchronize dirty sectors at a coarse or fuzzy level rather than synchronizing each individual sector. For example the buffer layer might check whether a dirty sector mask bitmask is nonzero for a given chunk which includes all cases where one or more sectors are dirty. The buffer layer might send entire chunks of data possibly including some combination of dirty valid and empty sectors to a media layer of the media controller to be written to the flash media.

Flash controller controls transfer of data between flash media and an external device coupled to communication link . Flash controller might be implemented as a system on chip SoC . Flash controller might include internal RAM buffer and might also be coupled to additional external memory shown as external RAM buffer . In an exemplary embodiment internal RAM buffer comprises 128 kB of static RAM SRAM and external RAM buffer comprises 512 MB of double data rate version 2 dynamic RAM DDR2 DRAM . RAM buffer might act as a cache for processor while RAM buffer might act as a read write buffer between flash media and communication link . Processor includes software and or firmware as needed for operation including for logical to physical translation in accordance with exemplary embodiments of the present invention as described subsequently. Although shown in as a single processor processor might be implemented with multiple processors. For embodiments having multiple processors inter processor communication might be employed such as described in related U.S. patent application Ser. No. 12 436 227.

For example flash controller receives one or more requests for flash media access such as read or write operations from one or more external devices via communication link . Such requests for access to flash media generally include at least one logical block address LBA where data should be read or written. For example the requests might be to read from or write to a i single flash address ii a group of contiguous flash addresses or iii a group of non contiguous flash addresses. Received requests are processed by host layer . Host layer i controls host interface specific commands e.g. SATA commands ii coordinates host side data transfers and command execution and iii processes any other host commands e.g. status updates . Host layer is in communication with buffer layer . FTL translates the LBA into a physical address of the desired data. FTL also interfaces with buffer layer . Since data transfers between communication link and flash media are temporally stored in buffer memory buffer layer generally directs the data traffic between host layer and FTL . For example if an external host not shown provides via communication link data to be written to flash media buffer layer might coordinate temporary storage of the data in buffer until FTL coordinates writing the data to flash media . Similarly if the external host requests to read data from flash media buffer layer might temporarily store the data in buffer until host layer coordinates sending the data to the host via communication link .

Embodiments of the present invention include groups of Superblocks called wear level units. Host requests might be striped across multiple wear level units to provide parallel execution. Striping might be performed on a per page basis meaning that each page is striped across multiple wear level units. In exemplary embodiments of the present invention a wear level unit might correspond to one flash die as shown in . For example flash dies through N might be configured such that data is striped across two or more dies analogously to hard drives in a redundant array of independent disks RAID with each die through N corresponding to a wear level unit. Alternatively embodiments of the present invention might configure each flash die through N as a separate stand alone flash memory device without data striping.

LSN corresponds to a logical block number LBN and a logical page number LPN . FTL derives LBN by dividing LSN by a number of sectors per block of flash media . FTL derives LPN by dividing LSN by a number of sectors per page of flash media . LBN in turn corresponds to Superblock number and block index while LPN corresponds to page index . As described a Superblock generally is a logical collection of blocks representing a fixed range of LBAs. FTL derives Superblock number and block index from LBN by dividing LBN by a number of blocks per Superblock where Superblock number corresponds to the quotient and block index corresponds to the remainder. Page index is derived from LPN by dividing LPN by a number of pages per block and page index represents the physical page offset within the block. For example if a flash page size is 4096 bytes and the sector size is 512 bytes each flash page can store up to 8 sectors. An exemplary block might contain 128 pages. In this example LPN is equal to LSN divided by 8 and page index is equal to LPN divided by 128.

As described herein each page includes a small spare area generally used to store error correcting code ECC data. The ECC fields are written to the spare area by flash controller . In addition to storing ECC data embodiments of the present invention might use the spare area of each page to store additional information for logical to physical address translation. For example FTL of might store the wear level unit number corresponding to the physical block in the spare area of one or more pages of the block. FTL might store a sequence number SN in the spare area of one or more pages of a physical block. The SN might represent the order in which FTL assigned the block to the Superblock. Each time a block is assigned for data storage the SN is incremented. Thus blocks having a higher SN were assigned more recently than blocks having a lower SN. The SN might also represent the order in which FTL wrote the pages of the block where every time a page is written the SN is incremented such that more recently written pages have a higher SN. FTL might also store the LSN corresponding to the page in the spare area or store a bad block indicator BBI in the spare area of one or more pages of a block that has failed in whole or in part . Embodiments of the present invention might further utilize the spare area to support enterprise system sector sizes e.g. 520 or 528 bytes per sector instead of 512 bytes such as described in related U.S. patent applications Ser. Nos. 12 477 996 and 12 478 013.

Each Superblock has a summary page shown in as summary page . Summary page contains the summary mapping data for the associated Superblock. For example summary page contains the block index and page index shown as block indices X and page indices Y for all X blocks and all Y pages in Superblock associated with the summary page. The summary page block indices include all physical blocks both data blocks and update blocks within the Superblock. Together block indices X and page indices Y are stored as data pointer which points to the physical address of each page of the Superblock. Summary page might also include a pointer to the location of the Active Block active block pointer and next free page free page pointer within the Superblock as well as the location of the next page of a partially written block as a result of garbage collection partial block page pointer . The summary page generally includes all necessary information to convert a logical address to a physical address of flash media . Embodiments of the present invention might perform garbage collection to erase pages containing out of date data such as described in related U.S. patent application Ser. No. 12 508 879. As will be described subsequently with regard to the summary page is updated periodically by FTL to include more up to date mapping data that might be stored in ABT or PGD for each Superblock for example the block index of the active block and the page index to the next free page.

As shown in PGD is a data structure that includes a series of entries for each Superblock shown as Superblocks Q in a wear level unit. PGD might include the pointer block index and page index to the summary page associated with each Superblock for example summary page pointer corresponding to Superblock . PGD might include ABT pointer that points to the location of the active block table e.g. ABT for the Superblock. PGD might be stored in a reserved area of flash media with other mapping data such as summary pages.

Each Superblock has an Active Block Table ABT shown in as ABT . ABT tracks the currently active block in each Superblock in a wear level unit. ABT contains for each Superblock Q in a wear level unit a list of page offsets indicating the written pages of the active block in the Superblock shown as page offsets N . ABT might be written in top down order such that page offset corresponds to the first page written in the active block and page offset N corresponds to the most recently written page in the active block. As will be described herein ABT might represent mapping data for the active block of a Superblock for write operations that have been completed to flash but the corresponding summary pages have not yet been updated. ABT might be stored in RAM e.g. at least one of buffer and buffer and reconstructed at startup of the storage device from summary pages stored in media . The interaction between PGD ABT and updating of summary pages e.g. summary page of will be described in greater detail with regard to . As shown in each wear level unit might also contain a list of failed blocks bad block list that includes pointers bad block pointer to failed blocks within the wear level unit.

In exemplary embodiments of the present invention summary pages e.g. summary page of for all the Superblocks of a wear level unit are stored out of line in a separate Superblock the summary page Superblock . Thus no pages in data Superblocks are used to store mapping data keeping the number of available pages per block to a power of two. In exemplary embodiments of the present invention one or more Superblocks of each wear level unit the map Superblocks might be reserved to store mapping data. The summary page of the map Superblock the map page and is saved in line as the first page of the map Superblock. FTL is configured to skip reserved Superblocks making them inaccessible by host requests thus reserving the Superblocks for mapping data.

At step buffer layer allocates buffer space for one or more chunks in the current segment of the read operation for which data is to be read. Buffer layer might allocate buffer space for the entire read and transfers all of the chunks from flash media . At step buffer layer requests data from FTL corresponding to at least a portion of the data requested by the read request received at step to be transferred from flash media . At step FTL provides the chunk data to buffer layer and at step buffer layer temporarily stores the data in buffer . At step buffer layer requests that host layer retrieve the chunk data stored in buffer at step . At step host layer transfers the chunk data to communication link . At step buffer layer deallocates the space in buffer that was allocated in step for the current group of one or more chunks. At step if there are more chunks to transfer processing returns to step for buffer layer to allocate buffer space for the next group of one or more chunks to be processed. If there are no more chunks to be transferred processing continues to step where the read operation ends.

As will be described in greater detail with regard to and embodiments of the present invention might perform host side operations for example steps and of for a first group of one or more chunks and media side operations for example steps of for a subsequent group of one or more chunks in parallel. For example by segmenting the read operation into chunks a first set of chunks might be transferred between FTL and buffer layer step and host layer might then transfer the first set of chunks to communication link step . Concurrently with one or more of the host side operations for the first set of chunks a second set of chunks for the same read operation might be transferred from FTL to buffer layer step and so on until all chunks for the read operation are transferred. Thus embodiments of the present invention provide the ability to perform host side and media side transfers in parallel.

If at step no summary page exists for the requested Superblock an error occurred and an error code or other predetermined data might be returned at step . If at step the summary page for the requested Superblock exists at step the summary page is read. The summary page can either be read from flash or as described herein from a cached copy of the summary page stored in RAM. At step the pointer for the requested page is read from the summary page based on the Block Index and Page Index. At step if the requested page pointer does not exist in the summary page an error code or other predetermined data might be returned at step . At step if the requested page pointer exists in the summary page at step FTL reads the requested page from flash media as described with regard to . As indicated by dashed lines and respectively and as will be described in greater detail with regard to if additional pages remain to be read from flash the process returns to step otherwise the read operation ends at step .

Thus as shown in and for a read operation at most two flash media read operations occur per each requested address i if the summary page data is not cached in RAM the summary page is read from flash e.g. step and ii the actual data location is read from flash e.g. step . For a sequential read operation this process is simplified. For the first page of a sequential read at most two flash media read operations occur per address i if the summary page is not cached in RAM the summary page is read from flash e.g. step and ii the actual data location is read from flash e.g. step . For subsequent pages of the sequential read operation the page address might simply be incremented e.g. step to read the next page from flash e.g. step .

At step buffer layer provides data for the one or more chunks to FTL . At step FTL writes one or more pages of the chunk data to flash media . At step buffer layer deallocates the space in buffer allocated at step for the current chunks. At step if there are additional chunks having data to be written processing returns to step . If there are no additional chunks to be written at step the write operation is ended. As described above with regard to the read operation of and as will be described in greater detail with regard to and embodiments of the present invention might perform host side operations for example steps of for a first group of one or more chunks and media side operations for example steps and of for a subsequent group of one or more chunks in parallel.

At step if the Active Block is not full or if the number of written pages in the Active Block is below a threshold then at step the active page index is updated to point to the next free page in the active block and is stored to the PGD. If at step the Active Block is full or if the number of written pages in the Active Block is above a threshold a new active block might be allocated and the process advances to step . At step the summary page for the Superblock containing the active block is read. The summary page can either be read from flash or as described herein from a cached copy of the summary page stored in RAM. If a summary page for the Superblock containing a newly allocated active block does not exist a new summary page is allocated. At step the data from the summary page and the active block table is merged to create an updated summary page. FTL allocates a new active block for the Superblock at step and writes a new summary page for the Superblock to flash at step . At step FTL updates the Page Global Directory e.g. PGD of to point to the new summary page and the new active block for the Superblock. Then at step the written page offset is stored in ABT and the current active page and the active block are stored in PGD . The next active page is the current page incremented by one or page 0 of the next sequential block.

As indicated by dashed line steps through could be repeated if the Active Block of the summary page superblock also happened to become full at the same time. For example a write operation occurs and the active block is full as described previously. Upon updating the summary page at step the active block of the summary page superblock could become full. In that instance steps through would be repeated to allocate a new active block for the summary page superblock. Otherwise as indicated by dashed line the media write operation ends at step .

If at step the write operation is for multiple sequential pages then at step host layer requests that FTL initiate media write operation shown in . At step if there are additional pages of the write request remaining to be written the page offset is incremented at step and the process returns to step to write the next page and so on until the last page has been written. At step if the last page was written at step the write operation ends.

As shown in and in general write operations require only a single flash operation writing to the active block . The active block data might be recovered from ABT which is stored in RAM. If the active block being written becomes full additional steps for updating the mapping data e.g. step or step might require flash media accesses for example to i read the summary page from flash e.g. step ii write a new summary page to flash e.g. step iii update the page global data for the superblock e.g. step .

In embodiments of the present invention the summary pages for each Superblock might be periodically updated for example during idle time of flash memory storage system . As described with regard to ABT might be employed to buffer mapping data for flash media write operations that are completed between updates of the summary pages. shows a flow diagram of exemplary summary page update routine executed by FTL . Summary page update routine might be performed when ABT becomes full or is filled more than a predetermined threshold . At step update summary page routine is initiated by FTL . At step FTL reads the Page Global Data e.g. PGD of and the Active Block Table e.g. ABT of . ABT might be written in one order e.g. top down and read in the opposite order e.g. bottom up generally forming a last in first out LIFO buffer. By reading ABT in the opposite order it is written in the event that a certain block is written multiple times before the summary page is updated FTL only updates the summary page once for every full block avoiding multiple updates of the same summary page for stale data. FTL might scan the entries of ABT for example sequentially from the highest indexed Superblock to the lowest indexed Superblock to determine if ABT contains data for one or more Superblocks that are more recent than summary page s for the Superblock s . PGD is read by Superblock number.

At step FTL merges the entries of ABT PGD and summary page for any Superblocks determined to have summary pages that are out of date. By merging the ABT entries PGD entries and summary page entries FTL creates a new up to date summary page for the Superblock s . At step a new active block is allocated and the active block pointer in the summary page e.g. pointer is updated. At step the new summary page is written. At step the map page i.e. the summary page for the map Superblock is updated to include the new page addresses for the summary page and the summary page pointer and active block pointer are updated in PGD . At step all mapping data has been updated and summary page update routine is ended.

The frequency with which FTL performs periodic summary page update routine is generally a tradeoff between the number of write operations to flash media and the amount of RAM e.g. buffer needed to store ABT . The more frequently the summary pages are updated the more write operations are performed and the less RAM is needed to store ABT . The less frequently the summary pages are updated the fewer write operations are performed and the more RAM is required to store ABT . The fewer write operations are performed the fewer erase operations are performed potentially extending the operating life flash media but requiring more RAM. Embodiments of the present invention provide that the summary page update frequency might be a user selectable setting of flash memory controller . Alternatively at system startup flash memory controller might automatically detect the amount of RAM available for example the size of buffer and configure ABT to a default size.

Although an HDD controller might generally access a single HDD serially an SSD controller such as flash controller of might access one or more flash devices in parallel shown in as flash dies N . In some instances large data transfers might span multiple of the flash dies N . Embodiments of the present invention divide data transfers internally into smaller segments chunks and employ one or more virtual circular buffers to facilitate parallel processing of host side and media side data transfers.

In some embodiments of the present invention a physical buffer e.g. buffer and buffer is reused within virtual circular buffer as soon as the buffered data is transferred to its destination for example flash media in the example of . This minimizes the effect of large data transfers on the buffer space available in buffers and for other operations of flash controller . Alternatively flash controller might be configured to replace the physical buffers of virtual circular buffer with alternate physical buffers in between handling of chunks for a large data transfer. This might allow buffer layer flexibility in configuring and allocating buffer space such as for example selectably increasing or decreasing the number of physical buffers for a virtual circular buffer as described with regard to

Embodiments of the present invention provide multiple virtual circular buffers e.g. virtual circular buffer of operating simultaneously to support parallel processing of multiple large data transfers. For example referring back to buffer layer employing N virtual circular buffers allows processing of multiple large data transfers in parallel because data is transferred in parallel between the N virtual circular buffers and the N flash dies N . Further the number of virtual circular buffers in operation might be selectable by buffer layer . For example if flash controller is under a heavy workload for large data transfers buffer layer might allocate an additional virtual circular buffer to provide parallel processing of the large data transfers. Virtual circular buffers are useful for skip read and skip write operations such as described in related U.S. patent application Ser. No. 12 508 915. Virtual circular buffers are also useful in performing data transfers across logical partition boundaries e.g. Superblock boundaries .

On startup of flash memory storage system mapping data stored in volatile memory e.g. RAM buffers and requires reconstruction. The reconstruction process is desirably completed quickly to allow access of flash media . For example ABT is stored in RAM and is reconstructed on startup to allow access of flash media . shows a flow diagram of map data reconstruction . At step FTL initiates reconstruction for example on startup of flash memory storage system . At step FTL requests that buffer layer allocate space in RAM e.g. at least one of buffer and buffer of for ABT which is initialized to predetermined default values. At step FTL scans the blocks within each Superblock of each wear level unit and groups the blocks based on block type. Step will be described in greater detail with regard to . At step FTL processes the grouped blocks and updates the corresponding mapping data structures e.g. the data structures of . Step will be described in greater detail with regard to . At step if additional blocks remain to be scanned and processed processing continues to step where the block index is incremented and the next block is scanned at step . This continues until at step FTL determines that all the blocks of flash media have been scanned and processed in which case processing advances to step . At step FTL determines if any blocks did not appear the ABT the summary pages or the bad block list. For the described embodiment blocks that did not appear in these data structures are presumed to have been in queue for erasure at the last power down. Thus at step these blocks are again placed in the queue for erasure. If at step no blocks need to be queued for erasure or after step when blocks are placed in the queue processing continues to step where the reconstruction operation is complete.

At step FTL erases the block. At step if the erase of the block was successful the block is then added to the free block list at step . As described herein blocks in the free block list might be allocated by FTL as Update Blocks to a Superblock when additional data blocks are required to support write operations. If the erase of the block was unsuccessful the block cannot be erased and has failed. In general with flash memory devices after a successful erase operation all the bits of the block are set to logic 1. A failed erase operation might be detected if the block is read and one or more bits within the block are not set to logic 1. At step if the erase of the block was unsuccessful the block address is added to the bad block list e.g. bad block list of corresponding to the wear level unit containing the Superblock. Additionally FTL might attempt to write a bad block indicator flag in the spare area of one or more pages of the failed block. After the bad block list is updated processing continues to step where the scan and group blocks sub routine is ended and processing returns to step of .

At step if the read of metadata at step is successful processing continues to step . At step the host LBA and the media LSN of the block are determined for example from the metadata read at step . At step if the host LBA of the block is greater than or equal to 0 then the process continues to step where the block is determined to be a data block. If at step the host LBA is not greater than or equal to 0 then the block might be an anchor block a summary block or the map block.

For example as shown in flash media might be divided into one or more separate physical spaces shown as anchor space and data space . Anchor space contains data that must be stored in particular physical blocks anchor blocks thus the blocks are anchored in a particular physical position in flash media . Anchor blocks might store at least a portion of the software or firmware for flash controller or might store configuration files or other data required by flash controller at power up. As described herein the first block block 0 of a flash die is generally provided from the manufacturer error free and might generally be used as an anchor block. Data space holds all other data including user data data blocks and mapping data. Mapping data such as the map block and summary blocks might be stored in reserved space which is one or more segments of data space that are reserved for storing mapping data. Reserved space is not accessible by host requests e.g. host read and write requests . In exemplary embodiments of the present invention reserved space is placed immediately after anchor space or at the end of data space . Since they are not accessible by the host blocks in anchor space and reserved space generally might not have corresponding host LBAs.

Referring back to if at step the host LBA was not greater than or equal to 0 then at step if the LSN is equal to 0 the block is determined to be a map block at step . The map block is the location of the map page i.e. the block reserved for storing the summary page of the summary page Superblock as described with regard to . As described herein in some embodiments of the present invention the map page might be stored in the first block after the anchor blocks such that the map page can always be located by FTL even if mapping data has been corrupted. If the LSN is greater than 0 processing continues to step where if the LSN is equal to an LSN in the reserved Superblock s then at step the block is determined to be a summary block i.e. a block reserved for storing summary pages of data Superblocks .

If at step the LSN was not equal to a reserved LSN at step the LSN of the block is checked against the LSNs of anchor space . If at step the LSN of the block is equal to an LSN in the anchor space at step the block is determined to be an anchor block. If at step the LSN was not recognized by FTL at step an error code might be generated and flash controller might perform subsequent processing. Once the block type is determined for example by one of steps bad block free block anchor block map block summary block and data block processing continues to step where scan and group blocks sub routine is ended and processing returns to step of .

Alternative embodiments of the present invention might simplify the block type determination. As described herein metadata might be stored in the spare area of the first page of each block. Exemplary embodiments of the present invention might store a block type field in the metadata. This block type field might include a code to indicate that the corresponding block is one of i a bad block ii an anchor block iii a reserved block iv a summary block v a map block vi a data block and vii an erased block. This block type metadata field might be stored each time a block is written. For example flash memory storage system might be initially programmed with firmware during a manufacturing process. During this initial programming as blocks used to store elements of the firmware are written the corresponding block type field might be written to indicate that these blocks are anchor blocks. During initial programming one or more reserved areas of media might be determined and the block type field for these blocks might be set to indicate that the blocks are reserved. Similarly during initial programming if any bad blocks are detected the corresponding block type field might be set to indicate that the block is bad. After initial programming during manufacturing the block type field for all other blocks might be set to indicate that the blocks are erased. These erased blocks are available for subsequent use by flash memory storage system and as each block is written as summary blocks or data blocks or as each block is subsequently erased the corresponding block type field might be updated accordingly.

If at step the block is not a summary block processing continues to step . At step if the block being processed by FTL in step of is a data block the process continues to step . At step if the current block is not the active block of the Superblock processing might continue to step and the PGD might be updated similarly as described for a summary block. At step if the current block is the active block of the Superblock the process continues to step . At step the active block is scanned sequentially to find the last written page of the active block. At step the page offsets stored in ABT are updated to reflect the order of writes to the active block at step the page offsets stored in ABT are up to date and the process continues to step .

At step FTL checks to see if the active block is full or if the amount free pages left in the active block has reached a minimum threshold. At step if the Active Block is full or if the number of written pages in the Active Block is above a threshold a new active block is allocated at step similarly as described with regard to . At step FTL updates the summary page e.g. summary page of for the Superblock associated with the active block similarly as described with regard to and . At step FTL updates the active block table e.g. ABT of to point to the new active block allocated at step and also updates PGD such that ABT pointer points to the new active block allocated at step . Then at step sub routine ends and the process returns to step of . At step if the active block is not full the process continues to step where sub routine ends and the process returns to step of .

At step if the block being processed by FTL in step of is the map block the process continues to step . At step FTL locates the last written page of the map block to locate the most recent map page in order to locate the most recent location of the summary pages for each Superblock. Once the last written page of the map block is located at step PGD is updated to point to the current location of the map page at step . If at step the block is either a free block or a bad block the sub routine of step ends at step where the process returns to step of .

Embodiments of the present invention provide for at least one of RAM buffer and RAM buffer to act as a read write data cache for data being transferred between media and communication link . In general an efficient hash table might be configured such that the memory allocated to it is approximately double the number entries expected to be stored in the hash table because there are diminishing returns for maintaining a larger hash table with a limited number of entries. Further as described herein embodiments of the present invention might configure RAM buffers and to store for example PGD or to cache recently accessed summary pages for faster access. However hash tables generally are set to a fixed size at the compile time of the software firmware operating on flash controller . Embodiments of the present invention provide dynamic sizing of hash tables for example a hash table used to track the contents of the data cache during operation of flash memory controller .

Hash table size update operation might beneficially be performed during operation of flash memory controller since the size of external RAM buffer might not be a known fixed value at the compile time of software running on flash memory controller . For example the size of RAM buffer and thus the amount of RAM available to store the cache and the cache hash table might differ depending on the needs of end users of flash memory controller . Additionally as described herein embodiments of the present invention might employ different sector sizes of flash media depending on the desired use of flash memory controller . Thus the sector size of flash media might not be a known fixed value at the compile time of software running on flash memory controller since the sector size might be changed if flash media is re formatted. Changes in sector size formatting of flash media correspond to changes in the number of data chunks allocated to the cache and tracked by the cache hash table.

At step buffer layer determines the number of items being managed in the cache hash table for example by scanning the cache or alternatively FTL might communicate to buffer layer a desired number of items to be cached e.g. the number of chunks allocated to the cache . As described herein the cache hash table might track a number of data chunks allocated to the cache and the LBA ranges of data chunks stored in the cache. At step if the number of actual or desired number of cache items has reached a threshold at step buffer layer sets the size of the cache hash table to a corresponding higher value as will be described with regard to and the process continues to step . If at step the number of actual or desired number of cache hash table items has not reached a maximum threshold at step buffer layer sets the size of the cache hash table to a corresponding lower value as will be described with regard to and the process continues to step . Once the size of the cache hash table is set for example by one of steps higher value or lower value cache hash table size update operation is complete at step . Thus in comparison to a fixed size cache hash table at software firmware compile time the cache hash table resize threshold might be a fixed value at compile time and the cache hash table itself might be resized as needed during the operation of flash memory controller .

For embodiments of the present invention to perform this calculation buffer layer might double the most significant bit MSB of the number of items stored in the cache hash table 25 11001 . Doubling just the MSB of the number of items stored 10000 16 16 2 32 100000 . However as will be determined at the threshold test of step 64 not 32 is the nearest power of 2 to 50 which is double the number chunks in the cache. The test of step are performed by checking the second most significant bit MSB of the number of items stored in the cache hash table. When the second MSB is one the test at step is true and the cache hash table size is set to a corresponding size e.g. rounded to the next higher power of 2 at step . Alternatively when the second MSB is zero the test at step is false and the cache hash table size is decreased e.g. rounded to the next lower power of 2 at step . In the above example the second MSB is one 25 11001 thus at step the cache hash table size is set to the next higher power of 2 which is 64 1000000.

Further the operations employed for the computations are relatively simple for example a logical AND operation is performed on the number of items stored and a bit mask and the resulting number is left shifted by one bit resulting in twice the MSB. If the second MSB is one the resulting number is left shifted again to equal the higher power of 2 otherwise the resulting number is the nearest power of 2 and is used as the hash table size. Although embodiments of the present invention test the MSB and the second MSB of the number of items stored in the cache hash table to determine whether the threshold has been reached other tests are possible. For example the size of the cache hash table might be updated when a threshold number of entries is crossed between a lower power of 2 and a higher power of 2. For example a hash table having a size N where N is a power of 2 might be doubled in size when the number of chunks allocated to the cache exceeds N 2 or any other fixed value.

As described with regard to embodiments of the present invention might store portions of mapping data in a cache in RAM for example to provide efficient performance of flash memory controller in i sequential ii sequential streaming and iii limited range random data transfers while operating with a relatively limited amount of RAM. Embodiments of the present invention might store in a RAM cache e.g. at least one of buffers and one or more recently accessed summary pages per each wear level unit. The summary page cache might employ dynamic hash table sizing as described with regard to . Described embodiments of the present invention might maintain the summary page cache in order of most recently accessed summary page to least recently accessed summary page although other alternative structures are possible.

As shown in allocations of summary pages to cache of by FTL begin by selecting the last cache entry. As shown the last cache entry is allocated and its status is set to pending shown as pending cache entry . Although the exemplary case shown in shows that the cache entries are empty the allocation of new cache entries is substantially the same when the cache entries are full the last cache entry is allocated and obtains pending status. A valid page near tail end is a less recently accessed entry and can be replaced with a new cache entry. As shown in head end is unlinked from cache entry and is linked to pending cache entry . Pending cache entry has been unlinked from tail end and tail end has been linked to the next closest cache entry . Thus pending cache entry moves to the head end of cache since it is the most recently accessed summary page in cache and the next cache entry from the tail end moves to the tail end of cache since it is the least recently accessed summary page in cache . A cache entry will maintain pending status until either i the summary page is read from flash and loaded into the cache or ii an abort condition occurs. When the summary page is read from flash and loaded into the cache the cache entry s status is updated from pending to valid as described below with respect to . When an abort condition occurs the cache entry s status is updated from pending to empty as described below with respect to . In the event that all cache entries have pending status any subsequent cache allocation requests are denied until one or more of the pending cache entries have been processed.

As described herein a summary page might be updated as a result of a write operation to flash media . When the summary page is updated the new summary page is given an entry at the head end of cache shown as pending cache entry . The previously cached version of the summary page shown as cache entry is stale and FTL invalidates the entry which returns to empty status and is moved to the tail end of cache .

As described herein flash memory controller might temporarily store data in RAM buffers. For example some mapping data might be cached in RAM e.g. at least one of buffers and for example summary pages e.g. summary page might be cached as described with regard to . Further data being read from or written to flash media might be cached in a buffer in RAM e.g. at least one of buffers and as described with regard to and respectively. Since some data might be cached in RAM it is possible that data stored in the cache is dirty meaning that data stored in the cache is more recent than corresponding data stored in flash media . As described herein data transfers might be segmented into smaller internal data transfers chunks where each chunk corresponds to a predefined LBA range sectors .

If the host operation is a write operation e.g. write operation of once the requested sector data is provided from host layer to the cached sector the cached sector obtains dirty status as indicated by state transition . The cached sector is dirty because it contains more recent data than the sector stored in flash media . A dirty cached sector obtains valid state when sector stored in flash media is synchronized with the cached sector as indicated by state transition . The cache media synchronization operation will be described in greater detail with regard to . A dirty sector might re obtain locked status as indicated by state transition if a subsequent write operation is requested that includes the same sector. That sector would then re obtain dirty state as indicated by state transition . As indicated by state transitions and a sector might transition between locked state and dirty state multiple times before a cache media synchronization occurs and the dirty cached sector obtains valid state as indicated by state transition . A cached sector having valid state or a cached sector having dirty state might obtain empty status as indicated by state transitions and respectively. A dirty or valid sector might become empty if for example the sector is included in a range of data invalidated by buffer layer . A range of data might be invalidated by buffer layer for example when the read or write operation is complete and the buffer is deallocated e.g. step of or step of .

Referring back to at step buffer layer scans dirty bitmasks Z to determine if any chunks stored in the cache are dirty . For example embodiments of the present invention might check if each of bitmasks Z is nonzero to determine if a chunk contains dirty sectors. If the current chunk is not dirty at step buffer layer scans the next chunk and the process returns to the test of step . If the current chunk is dirty at step buffer layer adds the chunk to a list of dirty chunks to be sent to FTL to be written to flash media . At step if there are cached chunks remaining to be scanned at step buffer layer scans the next chunk and the process returns back to the test of step . Chunks are scanned until a maximum transfer length is reached. For example in embodiments of the present invention the maximum transfer length within flash controller is kB. If the last chunk Z within a span of LBAs equal to the maximum transfer length has been scanned the process advances to step where buffer layer provides the data of the dirty chunks to FTL to be written to flash media e.g. the write operation of . FTL might optionally confirm that the data was written by reading back the written sectors. At step buffer layer clears the dirty sector bitmasks Z of and cache media synchronization is complete at step .

Thus in embodiments of the present invention buffer layer might synchronize dirty sectors at a coarse or fuzzy level rather than synchronizing each individual sector. For example buffer layer merely checks whether dirty sector mask bitmasks Z are nonzero which includes all cases where one or more sectors are dirty. However buffer layer does not track each individual sector or track how many sectors within a chunk must be updated. Thus buffer layer might reduce its overhead in controlling cache media synchronization by only performing synchronization of dirty cache data at a chunk level rather than at a sector level. Buffer layer might send entire chunks of data possibly including some combination of dirty valid and empty sectors to FTL to be written to flash media .

Reference herein to one embodiment or an embodiment means that a particular feature structure or characteristic described in connection with the embodiment can be included in at least one embodiment of the invention. The appearances of the phrase in one embodiment in various places in the specification are not necessarily all referring to the same embodiment nor are separate or alternative embodiments necessarily mutually exclusive of other embodiments. The same applies to the term implementation. 

While the exemplary embodiments of the present invention have been described with respect to processing blocks in a software program including possible implementation as a digital signal processor micro controller or general purpose computer the present invention is not so limited. As would be apparent to one skilled in the art various functions of software may also be implemented as processes of circuits. Such circuits may be employed in for example a single integrated circuit a multi chip module a single card or a multi card circuit pack.

The present invention can be embodied in the form of methods and apparatuses for practicing those methods. The present invention can also be embodied in the form of program code embodied in tangible media such as magnetic recording media optical recording media solid state memory floppy diskettes CD ROMs hard drives or any other non transitory machine readable storage medium wherein when the program code is loaded into and executed by a machine such as a computer the machine becomes an apparatus for practicing the invention. The present invention can also be embodied in the form of program code for example whether stored in a non transitory machine readable storage medium loaded into and or executed by a machine or transmitted over some transmission medium or carrier such as over electrical wiring or cabling through fiber optics or via electromagnetic radiation wherein when the program code is loaded into and executed by a machine such as a computer the machine becomes an apparatus for practicing the invention. When implemented on a general purpose processor the program code segments combine with the processor to provide a unique device that operates analogously to specific logic circuits. The present invention can also be embodied in the form of a bitstream or other sequence of signal values electrically or optically transmitted through a medium stored magnetic field variations in a magnetic recording medium etc. generated using a method and or an apparatus of the present invention.

It should be understood that the steps of the exemplary methods set forth herein are not necessarily required to be performed in the order described and the order of the steps of such methods should be understood to be merely exemplary. Likewise additional steps may be included in such methods and certain steps may be omitted or combined in methods consistent with various embodiments of the present invention.

As used herein in reference to an element and a standard the term compatible means that the element communicates with other elements in a manner wholly or partially specified by the standard and would be recognized by other elements as sufficiently capable of communicating with the other elements in the manner specified by the standard. The compatible element does not need to operate internally in a manner specified by the standard.

Also for purposes of this description the terms couple coupling coupled connect connecting or connected refer to any manner known in the art or later developed in which energy is allowed to be transferred between two or more elements and the interposition of one or more additional elements is contemplated although not required. Conversely the terms directly coupled directly connected etc. imply the absence of such additional elements. Signals and corresponding nodes or ports may be referred to by the same name and are interchangeable for purposes here.

It will be further understood that various changes in the details materials and arrangements of the parts which have been described and illustrated in order to explain the nature of this invention may be made by those skilled in the art without departing from the scope of the invention as expressed in the following claims.

