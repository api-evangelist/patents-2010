---

title: Out of order delivery for data and metadata mirroring in a cluster storage system
abstract: Described herein are a system and method for remote mirroring of data and metadata from a local node to a remote node using out-of-order delivery (OOD), while also providing data integrity at the remote node. OOD may utilize increased throughput of multiple connection paths between nodes. A mirroring layer/engine executing on the local node may receive related groups of data and metadata for storing to the remote node, each related group comprising one or more data sets and one metadata set that describes and is associated with each of the one or more data sets in the related group. The mirroring layer provides data integrity at the remote node by ensuring that the metadata set of a related group is stored to the remote node only after all the data sets in the related group are stored to the remote node, thus ensuring data consistency at the remote node.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08386425&OS=08386425&RS=08386425
owner: NetApp, Inc.
number: 08386425
owner_city: Sunnyvale
owner_country: US
publication_date: 20100219
---
The present invention relates to storage systems and particularly to out of order delivery for data and metadata mirroring in a cluster storage system.

A storage system typically comprises one or more storage devices into which information may be entered and from which information may be obtained as desired. The storage system includes a storage operating system that functionally organizes the system by inter alia invoking storage operations in support of a storage service implemented by the system. The storage system may be implemented in accordance with a variety of storage architectures including but not limited to a network attached storage environment a storage area network and a disk assembly directly attached to a client or host computer. The storage devices are typically disk drives organized as a disk array wherein the term disk commonly describes a self contained rotating magnetic media storage device. The term disk in this context is synonymous with hard disk drive HDD or direct access storage device DASD .

The storage operating system of the storage system may implement a high level module such as a file system to logically organize the information stored on volumes as a hierarchical structure of data containers such as files and logical units LUs . For example each on disk file may be implemented as set of data structures i.e. disk blocks configured to store information such as the actual data for the file. These data blocks are organized within a volume block number vbn space that is maintained by the file system. The file system may also assign each data block in the file a corresponding file offset or file block number fbn . The file system typically assigns sequences of fbns on a per file basis whereas vbns are assigned over a larger volume address space. The file system organizes the data blocks within the vbn space as a logical volume each logical volume may be although is not necessarily associated with its own file system.

A known type of file system is a write anywhere file system that does not overwrite data on disks. If a data block is retrieved read from disk into a memory of the storage system and dirtied i.e. updated or modified with new data the data block is thereafter stored written to a new location on disk to optimize write performance. A write anywhere file system may initially assume an optimal layout such that the data is substantially contiguously arranged on disks. The optimal disk layout results in efficient access operations particularly for sequential read operations directed to the disks. An example of a write anywhere file system that is configured to operate on a storage system is the Write Anywhere File Layout WAFL file system available from NetApp Inc. Sunnyvale Calif.

The storage system may be further configured to operate according to a client server model of information delivery to thereby allow many clients to access data containers stored on the system. In this model the client may comprise an application such as a database application executing on a computer that connects to the storage system over a computer network such as a point to point link shared local area network LAN wide area network WAN or virtual private network VPN implemented over a public network such as the Internet. Each client may request the services of the storage system by issuing access requests read write requests as file based and block based protocol messages in the form of packets to the system over the network.

A plurality of storage systems may be interconnected to provide a storage system architecture configured to service many clients. In some embodiments the storage system architecture provides one or more aggregates and one or more volumes distributed across a plurality of nodes interconnected as a cluster. The aggregates may be configured to contain one or more volumes. The volumes may be configured to store content of data containers such as files and logical units served by the cluster in response to multi protocol data access requests issued by clients.

Each node of the cluster may include i a storage server referred to as a D blade adapted to service a particular aggregate or volume and ii a multi protocol engine referred to as an N blade adapted to redirect the data access requests to any storage server of the cluster. In the illustrative embodiment the storage server of each node is embodied as a disk element D blade and the multi protocol engine is embodied as a network element N blade . The N blade receives a multi protocol data access request from a client converts that access request into a cluster fabric CF message and redirects the message to an appropriate D blade of the cluster.

The nodes of the cluster may be configured to communicate with one another to act collectively to increase performance or to offset any single node failure within the cluster. Each node in the cluster may have a predetermined failover partner node that may take over resume storage functions of the node upon failure of the node. When a node failure occurs where the failed node is no longer capable of processing access requests for clients the access requests sent to the failed node may be re directed to the partner node for processing. As such the cluster may be configured such that a partner node may take over the work load of a failed node. As used herein a local source node may have data and metadata that is mirrored copied to a remote destination node in the cluster storage system as discussed below . The remote node may comprise a predetermined failover partner node of the local node. As used herein various components residing on the local node may likewise be referred to as a local component e.g. local memory local de staging layer etc. and various components residing on a remote node may likewise be referred to as a remote component e.g. remote memory remote de staging layer etc. .

A cluster provides data access service to clients by providing access to shared storage comprising a set of storage devices . Typically clients will connect with a node of the cluster for data access sessions with the node. During a data access session with a node a client may submit access requests read write requests that are received and performed by the node. For the received write requests the node may produce write logs that represent the write requests and locally store the write logs to a volatile storage device from which the node may at a later time perform the write logs on the storage devices .

To ensure data consistency and provide high data availability the write logs may also be stored to two non volatile storage devices. Typically the write logs of the node may be locally stored to a non volatile storage device and also be stored remotely to a non volatile storage device at a partner node sometimes referred to herein as mirroring data to a remote node . As such if the local node fails the remote partner node will have a copy of the write logs and will still be able to perform the write logs on the storage devices. Also if the write logs stored at the partner node is corrupted or lost the write logs stored locally in the non volatile storage device at the local node can be extracted retrieved and used to perform the write logs on the storage devices.

As such data in a local non volatile storage device at a local node may be mirrored to a remote non volatile storage device of a remote node to provide failover protection e.g. in case the local node crashes and high availability of data in the cluster storage system. The mirrored data may comprise write logs or any other data that is to be stored to the non volatile storage devices.

Currently remote mirroring of data implements an in order delivery IOD requirement whereby mirroring applications and connections between the nodes typically support in order delivery of data between the nodes. For in order delivery of data the data is expected to be received at the remote node in the same time order as it was sent at the local node. For example if data sets are sent at the local node in a time order comprising data sets W X and then Y the IOD requirement requires that the remote node receives the data sets in the same time order i.e. receive in order W X and then Y . IOD of data results when there is a single connection path between the local and remote nodes.

In contrast out of order delivery OOD of data results when there are multiple connection paths between the local and remote nodes. Multiple connection paths may be implemented to increase data throughput and bandwidth between nodes. For OOD of data the data is not expected to be received at the remote node in the same time order as it was sent at the local node and may arrive in any order. As such in the above example data set Y may arrive at the remote node prior to data sets W and X in OOD.

OOD of data from the local node to the remote node may compromise data integrity at the remote node. Typically for a group of related data sets e.g. data sets W X Y there may also be a metadata set e.g. metadata set Z that describes each of the related data sets e.g. metadata set Z describes data sets W X Y the metadata set to also be stored to the local and remote non volatile storage devices. As used herein a related group of data and metadata sets may comprise one or more data sets and one metadata set that describes and is associated with each of the one or more data sets. As used herein data integrity exists when the metadata set of a related group is written to the remote non volatile storage device only after each of the data sets within the related group is written to the remote non volatile storage device. If the metadata set of a related group is written before each of the data sets within the same related group is written data corruption and inconsistency in the remote non volatile storage device may result.

For example the data sets of a related group may comprise data sets W X Y and metadata set Z where metadata set Z specifies that there are 3 valid data sets and the time order of transmitting to the remote node is W X Y and then Z. A valid data set may comprise client data that is pending to be stored to the local and remote non volatile storage devices. In IOD of data data integrity is intact since the time order of receiving and writing to the remote node is also W X Y and then Z where metadata set Z is written to the remote non volatile storage device only after data sets W X and Y are written . When the metadata set Z is written to the remote non volatile storage device this indicates that 3 valid data sets have already been successfully written to the remote non volatile storage device. As such in IOD of data the data and metadata stored at the remote node would be consistent as metadata set Z written to the remote non volatile storage device would accurately reflect that 3 valid data sets W X and Y have been written to the remote non volatile storage device.

However in OOD of data data integrity may not exist if for example metadata set Z is received and written to the remote node prior to data sets X and Y. In this example the data and metadata stored at the remote node would not be consistent as metadata set Z being written to the remote non volatile storage device would indicate that the 3 valid data sets W X and Y have already been written to the remote non volatile storage device when this in fact is not true. If a crash were to occur at the remote node before data sets X and Y were written to the remote non volatile storage device data corruption at the remote non volatile storage device would result. As such use of OOD of data typically does not provide data integrity at the remote non volatile storage device at each point in time.

Thus IOD is typically used for remote mirroring as it provides data integrity at the remote node at any point in time. However use of IOD for remote mirroring has significant drawbacks. For example multiple connection paths between the nodes may be used to increase data throughput and connection bandwidth between nodes. However multiple connection paths between nodes may cause OOD of data. As such IOD of data for remote mirroring may not take advantage of the increased data throughput and connection bandwidth provided by multiple connection paths between the nodes and OOD of data. As such there is a need for an improved method for remote mirroring of data and metadata between nodes of a cluster storage system.

Described herein are a system and method for remote mirroring copying data and metadata from a local node to a remote node using OOD. In some embodiments OOD is used for remote mirroring of data to the remote node while also providing data integrity at the remote node at any given point of time. In these embodiments the OOD of data may utilize the increased data throughput of multiple connection paths between the local and remote nodes.

In some embodiments the local source node executes software layers or applications referred to as mirroring clients that may require data and metadata to be stored to a local non volatile storage device and mirrored stored to a remote non volatile storage device on the remote destination node. In some embodiments a mirroring client comprises a software layer e.g. file system layer of a storage operating system executing on the local node. For storing data and metadata to the local non volatile storage device a mirroring client may send the data and metadata to software layers of the storage operating system that store the data and metadata using methods known in the art. For storing data and metadata to the remote non volatile storage device each mirroring client may also send a stream of data and metadata to a mirroring layer engine sometimes referred to as an interconnect IC layer engine of the storage operating system that stores the data and metadata using methods described herein.

In some embodiments the mirroring layer engine may perform embodiments described herein. The mirroring layer may receive the stream of data and metadata from each mirroring client and store the received data and metadata to a remote node using OOD of data while also providing data integrity at the remote node. Each mirroring client may send related groups of data and metadata sets a related group comprising one or more data sets and one metadata set that describes and is associated with each of the one or more data sets in the related group. In these embodiments the mirroring layer provides data integrity at the remote node by ensuring that the metadata set of a related group is stored to the remote node only after all the data sets in the related group are stored to the remote node thus ensuring data consistency at the remote node.

In some embodiments the mirroring layer does so by producing a data and metadata request DMR data structure for each mirroring client sending data and metadata sets to the mirroring layer. The mirroring layer may treat each received data and metadata set as a request having a unique request identifier from the mirroring client to mirror store the data or metadata set to the remote node. In some embodiments herein the terms data or metadata set may be used interchangeably with the terms data or metadata request. The mirroring layer may queue store each received data and metadata set request to the DMR data structure for the mirroring client. In some embodiments the mirroring layer then transmits data sets requests from the DMR data structure to the remote node for storage to the remote non volatile storage device. In these embodiments the mirroring layer may delay transmitting the metadata sets requests from the DMR data structure to the remote node until each related data set request in the related group of the metadata set request has been completed i.e. successfully stored to the remote non volatile storage device . By doing so data integrity may be provided at the remote node in OOD of data and metadata.

Typically each mirroring client will continually query the mirroring layer to determine if its data and metadata sets requests sent to the mirroring layer have been completed. The mirroring layer may also produce a request field variable for indicating the request identifiers of sets requests that have been currently completed thus far. In some embodiments the mirroring layer may update the value of the request field variable in a manner that makes the OOD of data and metadata transparent to the mirroring client.

In some embodiments the mirroring layer may perform remote mirroring using OOD while maintaining data integrity at the remote node without use of a processor executing on the remote node. In these embodiments the mirroring layer may perform the remote mirroring using remote direct memory access RDMA methods without requiring use or involvement of a processor of the remote node. While other methods using OOD for remote mirroring may require the processor of the remote node to re assemble data and metadata received out of order at the remote node to provide data integrity embodiments described herein may not require such re assembly of data and metadata and or the use of the processor of the remote node.

OOD may also be used for compliance with an existing network infrastructure. A particular network infrastructure may support OOD whereas current remote mirroring does not. To use OOD in compliance with the existing network infrastructure may avoid the cost of a dedicated network infrastructure for remote mirroring and utilize the existing network infrastructure.

In the following description numerous details are set forth for purpose of explanation. However one of ordinary skill in the art will realize that the embodiments described herein may be practiced without the use of these specific details. In other instances well known structures and devices are shown in block diagram form in order not to obscure the description with unnecessary detail.

The description that follows is divided into three sections. Section I describes a cluster environment in which some embodiments operate. Section II describes a storage operating system having a mirroring layer for remote mirroring of data and metadata to a remote node. Section III describes a method and apparatus for providing data integrity in remote mirroring of data and metadata to a remote node using OOD.

As shown in each node may be organized as a network element N blade and a disk element D blade . The N blade includes functionality that enables the node to connect to clients over a computer network while each D blade connects to one or more storage devices such as disks of a disk array . The nodes are interconnected by a cluster switching fabric discussed below .

It should be noted that although disks are used in some embodiments described below any other type of storage device may be used as well. For example a solid state storage device may be used instead the solid state device having no mechanical moving parts for reading and writing data. Some examples of solid state devices include flash memory non volatile storage device NVRAM Magnetic Random Access Memory MRAM Phase Change RAM PRAM etc. In other embodiments other storage devices other than those mentioned here may also be used.

Also it should be noted that while there is shown an equal number of N and D blades in the illustrative cluster there may be differing numbers of N and or D blades and or different types of blades implemented in the cluster in accordance with various embodiments. For example there may be a plurality of N blades and or D blades interconnected in a cluster configuration that does not reflect a one to one correspondence between the N and D blades. As such the description of a node comprising one N blade and one D blade should be taken as illustrative only. For example a node may also have one N blade and a plurality of D blades a plurality of N blades and one D blade or a plurality of N blades and a plurality of D blades.

The clients may be general purpose computers configured to interact with the node in accordance with a client server model of information delivery. That is each client may request the services of the node e.g. by submitting read write requests and the node may return the results of the services requested by the client by exchanging packets over the network . The client may submit access requests by issuing packets using file based access protocols such as the Common Internet File System CIFS protocol or Network File System NFS protocol over the Transmission Control Protocol Internet Protocol TCP IP when accessing information in the form of files and directories. Alternatively the client may submit access requests by issuing packets using block based access protocols such as the Small Computer Systems Interface SCSI protocol encapsulated over TCP iSCSI and SCSI encapsulated over Fibre Channel FCP when accessing information in the form of blocks.

In some embodiments a client connects to a node for a data access session with the node . During a data access session the client may submit access requests that are received and performed by the node . Such access requests may include storage state requests a storage state request comprising a request that alters the data state of a storage device . Examples of storage state requests include requests for storing new data to a file deleting a file changing attributes of a file etc. For illustrative purposes storage state requests may be generically referred to herein as write requests.

In some embodiments the totality of storage space provided by the disks and disk arrays of the cluster comprise a total shared storage space referred to as shared storage of the cluster . In other embodiments the shared storage comprises the totality of storage space provided by other types of storage devices such as solid state storage devices . The shared storage is accessible by each D blade of each node in the cluster . In some embodiments the cluster may provide high availability of service to clients in accessing the shared storage . For example the nodes may be configured to communicate with one another e.g. via cluster switching fabric to act collectively to offset any single node failure within the cluster .

To ensure data consistency and provide high data availability a local source node e.g. local node A may have data and metadata stored to a local non volatile storage device that is mirrored copied to a remote non volatile storage device at a remote destination node e.g. remote node B in the cluster . Likewise remote node B may have data and metadata stored to the remote non volatile storage device that is mirrored copied to a local non volatile storage device at the local node A. The remote node B may comprise a predetermined failover partner node of the local node A. Likewise the local node A may comprise a predetermined failover partner node of the remote node B. As used herein various software and hardware components residing on the local node may be referred to as a local component e.g. local non volatile storage device local de staging layer etc. and various components residing on a remote node may be referred to as a remote component e.g. remote non volatile storage device remote de staging layer etc. .

The data and metadata mirrored from the local node A to remote node B may comprise for example write logs. As such if the local node A fails the remote partner node B will have a copy of the write logs and will still be able to perform the write logs on the storage devices. In other embodiments the data and metadata mirrored from the local node A to remote node B may comprise any other type of data and metadata. As such data in a local non volatile storage device at a local node may be mirrored to a remote non volatile storage device of a remote node to provide failover protection e.g. in case the local node crashes and high availability of data in the cluster storage system.

The cluster access adapter comprises a plurality of ports adapted to couple the node to other nodes of the cluster through the cluster switching fabric . In the illustrative embodiment Ethernet is used as the clustering protocol and interconnect media although it will be apparent to those skilled in the art that other types of protocols and interconnects may be utilized within the cluster architecture described herein. In alternate embodiments where the N blades and D blades are implemented on separate storage systems or computers the cluster access adapter is utilized by the N D blade for communicating with other N D blades in the cluster .

Each node is illustratively embodied as a dual processor storage system executing a storage operating system that preferably implements a high level module such as a file system to logically organize the information as a hierarchical structure of named data containers such as directories files and special types of files called virtual disks hereinafter generally blocks on the disks. However it will be apparent to those of ordinary skill in the art that the node may alternatively comprise a single or more than two processor system. Illustratively one processor executes the functions of the N blade on the node while the other processor executes the functions of the D blade .

The network adapter comprises a plurality of ports adapted to couple the node to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus may comprise the mechanical electrical and signaling circuitry needed to connect the node to the network. Illustratively the computer network may be embodied as an Ethernet network or a Fibre Channel FC network. Each client may communicate with the node over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system executing on the node to access information requested by the clients. The information may be stored on any type of attached array of writable storage device media such as video tape optical DVD magnetic tape bubble memory electronic random access memory micro electro mechanical and any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is preferably stored on the disks of array . The storage adapter comprises a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance FC link topology.

Storage of information on each array is preferably implemented as one or more storage volumes that comprise a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number vbn space on the volume s . Each logical volume is generally although not necessarily associated with its own file system. The disks within a logical volume file system are typically organized as one or more groups wherein each group may be operated as a Redundant Array of Independent or Inexpensive Disks RAID . Most RAID implementations such as a RAID 4 level implementation enhance the reliability integrity of data storage through the redundant writing of data stripes across a given number of physical disks in the RAID group and the appropriate storing of parity information with respect to the striped data. An illustrative example of a RAID implementation is a RAID 4 level implementation although it should be understood that other types and levels of RAID implementations may be used in accordance with the inventive principles described herein.

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data used in some embodiments. The processors and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data stored in the memory . In some embodiments the memory may comprise a form of random access memory RAM comprising volatile memory that is generally cleared by a power cycle or other reboot operation.

The storage operating system portions of which is typically resident in memory and executed by the processing elements functionally organizes the node by inter alia invoking storage operations in support of the storage services implemented by the node. It will be apparent to those skilled in the art that other processing and memory means including various computer readable media may be used for storing and executing program instructions pertaining to the invention described herein. In some embodiments the storage operating system comprises a plurality of software layers including a mirroring layer engine that are executed by the processors.

The local non volatile storage device may comprise one or more storage devices utilized by the node to locally store data. The local non volatile storage device may be employed as a backup memory that ensures that the storage system does not lose received information e.g. CIFS and NFS requests in the event of a system shutdown or other unforeseen problem. In some embodiments the non volatile storage device may comprise a rewritable computer memory for storing data that does not require power to maintain data information stored in the computer memory and may be electrically erased and reprogrammed. Some examples of non volatile storage devices include disks flash memory non volatile storage device NVRAM Magnetic Random Access Memory MRAM Phase Change RAM PRAM etc. In other embodiments other non volatile storage devices are used other than those listed here.

In some embodiments the local non volatile storage device may locally store various data and metadata from software layers or applications referred to as mirroring clients executing on the node. For example a mirroring client may comprise a software layer e.g. file system layer or RAID layer of a storage operating system executing on the node. In other embodiments the mirroring client may comprise any other software layer or application that requests data and metadata to be stored to the local non volatile storage device and mirrored stored to a remote non volatile storage device on a remote node. For storing data and metadata to the local non volatile storage device a mirroring client may send the data and metadata to software layers of the storage operating system that store the data and metadata using methods known in the art. For storing data and metadata to the remote non volatile storage device each mirroring client may also send a stream of data and metadata to the mirroring layer engine sometimes referred to as an interconnect IC layer engine that mirrors stores the data and metadata to the remote node using methods described herein.

To facilitate access to the disks the storage operating system implements a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by disks . The file system logically organizes the information as a hierarchical structure of named directories and files on the disks. Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the file system to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers luns .

In the illustrative embodiment the storage operating system is preferably the Data ONTAP software operating system available from NetApp Inc. Sunnyvale Calif. that implements a Write Anywhere File Layout WAFL file system. However it is expressly contemplated that any appropriate storage operating system may be enhanced for use in accordance with the inventive principles described herein. As such where the term WAFL is employed it should be taken broadly to refer to any storage operating system that is otherwise adaptable to the teachings of this invention.

A file system protocol layer provides multi protocol file access and to that end includes support for the Direct Access File System DAFS protocol the NFS protocol the CIFS protocol and the Hypertext Transfer Protocol HTTP protocol . A VI layer implements the VI architecture to provide direct access transport DAT capabilities such as RDMA as required by the DAFS protocol . An iSCSI driver layer provides block protocol access over the TCP IP network protocol layers while a FC driver layer receives and transmits block access requests and responses to and from the node. The FC and iSCSI drivers provide FC specific and iSCSI specific access control to the blocks and thus manage exports of luns to either iSCSI or FCP or alternatively to both iSCSI and FCP when accessing the blocks on the node .

In addition the storage operating system includes a series of software layers organized to form a storage server D blade that provides data paths for accessing information stored on the disks of the node . To that end the storage server includes a file system module a de staging layer a storage RAID system layer and a disk driver system module . The RAID system layer manages the storage and retrieval of information to and from the volumes disks in accordance with I O operations while the disk driver system implements a disk access protocol such as e.g. the SCSI protocol.

The file system implements a virtualization system of the storage operating system through the interaction with one or more virtualization modules illustratively embodied as e.g. a virtual disk vdisk module not shown and a SCSI target module . The SCSI target module is generally disposed between the FC and iSCSI drivers and the file system to provide a translation layer of the virtualization system between the block lun space and the file system space where luns are represented as blocks.

The file system is illustratively a message based system that allocates storage space for itself in the disk array and controls the layout of information on the array. The file system further provides logical volume management capabilities for use in access to the information stored on the storage devices such as disks. That is in addition to providing file system semantics the file system provides functions normally associated with a volume manager. These functions include i aggregation of the disks ii aggregation of storage bandwidth of the disks and iii reliability guarantees such as mirroring and or parity RAID . The file system illustratively implements the WAFL file system hereinafter generally the write anywhere file system having an on disk format representation that is block based using e.g. 4 kilobyte kB blocks and using index nodes inodes to identify files and file attributes such as creation time access permissions size and block location . The file system uses files to store metadata describing the layout of its file system these metadata files include among others an inode file. A file data container handle i.e. an identifier that includes an inode number is used to retrieve an inode from disk.

All inodes of the write anywhere file system may be organized into the inode file. A file system fs info block specifies the layout of information in the file system and includes an inode of a data container e.g. file that includes all other inodes of the file system. Each logical volume file system has an fsinfo block that may be stored at a fixed or variable location within e.g. a RAID group. The inode of the inode file may directly reference point to data blocks of the inode file or may reference indirect blocks of the inode file that in turn reference data blocks of the inode file. Within each data block of the inode file are embedded inodes each of which may reference indirect blocks that in turn reference data blocks of a file.

Operationally an access request read write request from the client is forwarded as a packet over the computer network and onto the node where it is received at the network adapter . A network driver of layer or layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the write anywhere file system . Here the file system produces operations to load retrieve the requested data from disk if it is not resident in core i.e. in memory . If the information is not in memory the file system indexes into the inode file using the inode number to access an appropriate entry and retrieve a logical vbn. The file system then passes a message structure including the logical vbn to the RAID system the logical vbn is mapped to a disk identifier and disk block number disk dbn and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the dbn from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the access request the node and storage operating system returns a reply to the client over the network .

It should be noted that the software path through the storage operating system layers described above needed to perform data storage access for the client request received at the node may alternatively be implemented in hardware. That is in an alternate embodiment of the invention a storage access request data path may be implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC . This type of hardware implementation increases the performance of the storage service provided by node in response to a request issued by client . Moreover in another alternate embodiment of the invention the processing elements of adapters may be configured to offload some or all of the packet processing and storage access operations respectively from processor to thereby increase the performance of the storage service provided by the node. It is expressly contemplated that the various processes architectures and procedures described herein can be implemented in hardware firmware or software.

As used herein the term storage operating system generally refers to the computer executable code operable on a computer to perform a storage function that manages data access and may in the case of a node implement data access semantics of a general purpose operating system. The storage operating system can also be implemented as a microkernel an application program operating over a general purpose operating system such as UNIX or Windows NT or as a general purpose operating system with configurable functionality which is configured for storage applications as described herein.

In addition it will be understood to those skilled in the art that the invention described herein may apply to any type of special purpose e.g. file server filer or storage serving appliance or general purpose computer including a standalone computer or portion thereof embodied as or including a storage system. Moreover the teachings of this invention can be adapted to a variety of storage system architectures including but not limited to a network attached storage environment a storage area network and disk assembly directly attached to a client or host computer. The term storage system should therefore be taken broadly to include such arrangements in addition to any subsystems configured to perform a storage function and associated with other equipment or systems. It should be noted that while this description is written in terms of a write any where file system the teachings of the present invention may be utilized with any suitable file system including a write in place file system.

In some embodiments the storage server is embodied as D blade of the storage operating system to service one or more volumes of array . In addition the multi protocol engine is embodied as N blade to i perform protocol termination with respect to a client issuing incoming data access request packets over the network as well as ii redirect those data access requests to any storage server of the cluster . Moreover the N blade and D blade cooperate to provide a highly scalable distributed storage system architecture of the cluster . To that end each blade includes a cluster fabric CF interface module adapted to implement intra cluster communication among the blades e.g. communication between blades of the same node or communication between blades of different nodes using CF protocol messages.

For example the protocol layers e.g. the NFS CIFS layers and the iSCSI FC layers of the N blade may function as protocol servers that translate file based and block based access requests from clients into CF protocol messages used for communication with the D blade . In some embodiments the N blade servers convert the incoming client access requests into file system primitive operations commands that are embedded within CF protocol messages by the CF interface module for transmission to the D blades of the cluster . Notably the CF interface modules cooperate to provide a single file system image across all D blades in the cluster . Thus any network port of an N blade that receives a client request can access any data container within the single file system image located on any D blade of the cluster.

In some embodiments the N blade and D blade are implemented as separately scheduled processes of storage operating system . In other embodiments the N blade and D blade may be implemented as separate software components code within a single operating system process. Communication between an N blade and D blade in the same node is thus illustratively effected through the use of CF messages passing between the blades. In the case of remote communication between an N blade and D blade of different nodes such CF message passing occurs over the cluster switching fabric .

A known message passing mechanism provided by the storage operating system to transfer information between blades processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF application programming interface API . Examples of such an agnostic protocol are the SpinFS and SpinNP protocols available from NetApp Inc.

The CF interface module implements the CF protocol for communicating file system commands messages among the blades of cluster . Communication is illustratively effected by the D blade exposing the CF API to which an N blade or another D blade issues calls. To that end the CF interface module is organized as a CF encoder and CF decoder. The CF encoder of e.g. CF interface on N blade encapsulates a CF message as i a local procedure call LPC when communicating a file system command to a D blade residing on the same node or ii a remote procedure call RPC when communicating the command to a D blade residing on a remote node of the cluster . In either case the CF decoder of CF interface on D blade de encapsulates the CF message and processes the file system command. As used herein the term CF message may be used generally to refer to LPC and RPC communication between blades of the cluster.

In some embodiments the storage operating system also comprises a de staging layer that operates in conjunction with the file system and other software layers of the storage operating system to produce and store write logs to the local non volatile storage device . In general the de staging layer may receive write requests for files and perform the received write requests in two stages. In a first stage write requests received by the file system layer are sent to the de staging layer . The de staging layer produces a write log for each received write request a write log representing the write request. The write logs may be stored to the local non volatile storage device . In a second stage upon occurrence of a predetermined initiating event referred to as a consistency point accumulated local write logs stored in the local volatile storage device may be performed on the storage devices. To do so the accumulated local write logs may be sent to the RAID system layer that then performs the write logs. The consistency point may be initiated by various predetermined initiating events such as the occurrence of a predetermined time interval etc.

In some embodiments the storage operating system also comprises a mirroring layer that may reside underneath the storage RAID system layer and be a peer layer of the disk driver system layer as shown in . In other embodiments the mirroring layer may reside near other layers of the storage operating system . In some embodiments the mirroring layer may be pre included in storage operating system software. In other embodiments the mirroring layer may comprise an external auxiliary plug in type software module that works with the storage operating system to enhance its functions.

To ensure data consistency and provide failover protection the write logs may be stored to the local non volatile storage device as described above and also be mirrored stored to a remote non volatile storage device at a remote partner node. The file system and or the de staging layer may comprise mirroring clients that utilize and request data mirroring services of the mirroring layer . In some embodiments described below the file system may comprise a mirroring client that produces the write logs the write logs comprising the data and metadata that are to be mirrored stored to the remote non volatile storage device for illustrative purposes only. In other embodiments other mirroring clients may use the mirroring layer and or produce other types of data and metadata other than write logs that are to be mirrored stored to the remote non volatile storage device by the mirroring layer .

As used herein a mirroring client executing on a local node e.g. local node A may produce local data and metadata stored to the local non volatile storage device . The mirroring client may also send its local data and metadata to the mirroring layer engine for storing to a remote node e.g. remote node B using methods described herein. Likewise a mirroring client on the remote node may send its data and metadata shown as remote data and metadata to its mirroring layer engine for storing to the local non volatile storage device of the local node using methods described herein.

The mirroring layer engine may transmit the data and metadata of the local node to the remote node using the cluster switching fabric . The cluster switching fabric may utilize any type of network connection switches and or protocols known in the art. For example the cluster switching fabric may comprise a Fibre Channel interconnect using Fibre Channel fabric switches an Ethernet interconnect using Gigabit Ethernet switches and Ethernet clustering protocol etc. In some embodiments the cluster switching fabric may provide multiple network connection paths between two nodes in the cluster . The multiple connection paths may be provided through multiple hardware connections between the nodes. Multiple connection paths may be implemented to increase data throughput and bandwidth between the nodes. However use of multiple connection paths between nodes may result in OOD of data transfer between the nodes.

Currently remote mirroring of data implements an in order delivery IOD requirement whereby the data is expected to be received at the remote node in the same time order as it was sent at the local node. show conceptual diagrams illustrating conventional IOD of data and metadata between a local node and remote node. As shown in a local node A comprises a local non volatile storage device storing various data sets and metadata sets .

In the examples of the local non volatile storage device stores a first related group comprising data sets Data Data and Data and metadata set Count 3 and a second related group comprising data sets Data and Data and metadata set Count 5. The data sets and metadata sets may be stored to the local non volatile storage device by a mirroring client e.g. software layer or application executing on the local node A. A data set may comprise user client data sent from a user client such as an application to the mirroring client. As such a data set is typically not produced by the mirroring client. A metadata set may comprise metadata that describes one or more data sets . Metadata sets may be produced by the mirroring client to assist in processing the data sets .

Note that the metadata sets are typically stored to the same predetermined storage location in the local and remote non volatile storage devices . As such any new received metadata set typically overwrites updates the prior received metadata set at the same predetermined storage location in the local and remote non volatile storage devices . This is shown in where the new metadata set Count 5 has overwritten the previous metadata set Count 3 in the same storage location in the local and remote non volatile storage devices .

The mirroring client may also send the data sets and metadata sets to a conventional mirroring layer executing on the local node A for storing to the remote non volatile storage device on the remote node B. The mirroring layer may typically transmit the data sets and metadata sets to the remote node B through a single connection path between the local and remote nodes using IOD. As such the data and metadata sets are expected to be received at the remote node B in the same time order as it was sent at the local node A.

In the example of the data and metadata sets are transmitted from the local node A in the following time order Data is sent first Data is sent second Data is sent third and Count 3 is sent fourth. As such the data and metadata sets are received and stored to the remote storage at the remote node B in the following time order Data is received and stored first Data is received and stored second Data is received and stored third and Count 3 is received and stored fourth. Similarly in the example of the data and metadata sets are transmitted from the local node A in the following time order Data Data and then Count 5 then the data and metadata sets are received and stored at the remote node B in the same time order Data Data and then Count 5.

As such IOD of data provides data integrity at the remote node B at any given point in time. As used herein data integrity at the remote node B may exist when the metadata set of a related group is written to the remote non volatile storage device only after each of the data sets within the related group is written to the remote non volatile storage device. If the metadata set of a related group is written before each of the data sets within the same related group is written data corruption and inconsistency in the remote non volatile storage device may result.

For example the mirroring client may comprise the file system that writes data and metadata to the local non volatile storage device and also sends the data and metadata to the mirroring layer for mirroring storing to the remote non volatile storage device . In this example file system may periodically produce metadata sets to assist in tracking the data sets it produces. The metadata Count may specify the number of valid client data sets that have been produced so far by the file system for storage to the local and remote non volatile storage devices . As such the metadata set Count 3 specifies that 3 valid client data sets Data Data and Data have been produced so far for storage to the local and remote non volatile storage devices . When 2 more valid client data sets Data and Data are later produced by the file system the file system may then produce an updated metadata set Count 5 to indicate that a total of 5 valid client data sets Data Data have been produced so far.

Typically the metadata sets may also be used to track the total number of valid data sets that have been successfully stored thus far to the local and remote non volatile storage devices . As such the current metadata set stored at the predetermined metadata storage location in the local and remote non volatile storage devices should accurately reflect the total number of valid data sets successfully stored thus far. For example if the metadata set Count 5 is overwritten updated to the predetermined metadata storage location in the remote non volatile storage device there should be a total of 5 valid data sets successfully stored thus far to the remote non volatile storage device already. This may be referred to as data integrity at the remote node where the data and metadata is consistent at the remote node.

As described above data integrity at the remote node may also be defined as existing when the metadata set e.g. Count 3 of a related group is written to the remote non volatile storage device only after each of the data sets e.g. Data Data within the related group is written to the remote non volatile storage device. A related group of data and metadata sets may comprise one or more data sets e.g. Data Data and one metadata set e.g. Count 3 that describes and is associated with each of the one or more data sets.

In some embodiments the metadata set of a related group may be caused to be produced by the mirroring client by the production of the data sets of the related group. For example the production of metadata set Count 5 by the mirroring client is caused by the production of data sets Data and Data by the mirroring client whereby data sets Data and Data and metadata set Count 5 comprise a related group.

In further embodiments a related group may include one or more data sets that are received by the mirroring layer from the mirroring client next after a previous metadata set that is not in the related group and also include a following metadata set that is in the related group received by the mirroring layer next after the one or more data sets. For example the second related group comprises data sets Data and Data that are received by the mirroring layer next just after the previous metadata set Count 3 that is not in the related group and also includes the following metadata set Count 5 that is in the related group received next just after data sets Data and Data.

In some examples described herein the mirroring client comprises the file system and the data and metadata pertains to write logs. In other embodiments the mirroring client may comprise another software layer or application and the data and metadata may not pertain to write logs. For example the mirroring client may comprise the RAID system layer and the data may comprise client data and the metadata may comprise parity data. For example a first related group may comprise data sets Data Data and Data and a metadata set comprising a checksum of Data Data and Data a second related group may comprise data sets Data and Data and a metadata set comprising a checksum of Data and Data.

Each mirroring client may produce and send its own separate stream of data and metadata to the mirroring layer for processing the stream comprising related groups of data and metadata. As such related groups of data and metadata will be produced and received from the same mirroring client. The storage size for the data and metadata sets typically vary depending on how the mirroring client produces the data and metadata sets although there is typically a maximum storage size to a single data or metadata set e.g. 64 KB .

As described above data integrity at the remote node is kept intact using IOD. When multiple connection paths between the local and remote nodes are used to increase data throughput and bandwidth however OOD of data may result which may compromise data integrity at the remote node. For use of OOD is remote mirroring of data the data is not expected to be received at the remote node in the same time order as it was sent at the local node and may arrive in any order. For example in metadata set Counter 3 may be received at the remote node B prior to data sets Data and Data. As such metadata set Counter 3 will be stored to the remote non volatile storage device prior to data sets Data and Data. As a result data corruption and inconsistency at the remote non volatile storage device occurs.

In some embodiments methods and apparatus are described that provide remote mirroring storing of data and metadata using OOD over multiple connection paths while also maintaining data integrity at the remote node at any given point of time. In some embodiments a mirroring layer of a storage operating system executing on the local node may be configured to perform embodiments described herein.

In some embodiments the mirroring layer does so by producing a data and metadata request DMR data structure for each mirroring client sending data and metadata sets to the mirroring layer. The mirroring layer may treat each received data and metadata set as a request having a unique request identifier XID from the mirroring client to mirror store the data or metadata set to the remote node. The mirroring layer may queue store each received data and metadata set request to the DMR data structure for the mirroring client. In some embodiments the mirroring layer then transmits data sets requests from the DMR data structure to the remote node for storage to the remote non volatile storage device. In these embodiments the mirroring layer may delay transmitting the metadata sets requests from the DMR data structure to the remote node until each related data set request in the related group of the metadata set request has been completed i.e. successfully stored to the remote non volatile storage device . By doing so data integrity may be provided at the remote node in OOD of data and metadata.

Typically each mirroring client will continually query the mirroring layer to determine if its data and metadata sets requests sent to the mirroring layer have been completed. The mirroring layer may also produce a request field variable last complt request for indicating the request identifiers XIDs of sets requests that have been currently completed thus far. In some embodiments the mirroring layer may update the value of the request field variable in a manner that makes the OOD of data and metadata transparent to the mirroring client.

In some embodiments the mirroring layer may perform remote mirroring using OOD while maintaining data integrity at the remote node without use of a processor executing on the remote node. In these embodiments the mirroring layer may perform the remote mirroring using remote direct memory access RDMA methods without requiring use or involvement of a processor of the remote node. As known in the art RDMA comprises a communications protocol that provides transmission of data from the memory e.g. local non volatile storage device of one computer e.g. local node A to the memory e.g. remote non volatile storage device of another computer e.g. remote node B without involving the processor of the other computer. While other methods using OOD for remote mirroring may require the processor of the remote node to re assemble data and metadata received out of order at the remote node to provide data integrity embodiments described herein may not require such re assembly of data and metadata and or the use of the processor of the remote node.

The mirroring layer engine may concurrently perform the method in parallel for each mirroring client that sends data and metadata sets to the mirroring layer engine for mirroring storing the data and metadata sets to the remote node. The mirroring client may comprise for example a software layer of the storage operating system or any application executing on the local node. Some mirroring clients may send data and metadata sets that may or may not require IOD of the data and metadata sets to the remote node. For example the file system and RAID system mirroring clients may require IOD of the data and metadata sets. Other mirroring clients however may not require IOD and OOD of the data and metadata sets is allowable. The mirroring layer may process data and metadata sets from mirroring clients requiring or not requiring IOD in accordance with embodiments described herein.

For each mirroring client that sends data and metadata sets to the mirroring layer engine the method produces and maintains at a DMR data structure and a request field variable last cmplt request . In general the DMR data structure may be used to queue store data and metadata sets requests received from the mirroring client. The request field may be used to indicate currently completed requests of the mirroring client at the remote node and be used to respond to completion queries from the mirroring client discussed further below .

The method receives at a plurality of data and metadata sets requests from the mirroring client. The received data and metadata sets may comprise one or more related groups each related group comprising one or more data sets and one metadata set that describes and is associated with each of the one or more data sets in the related group. Each received data set or metadata set may comprise a data request or metadata request respectively from the mirroring client for the mirroring layer to mirror store the data or metadata set to the remote node. The method also generates and assigns at a unique request identifier XID for each received data and metadata set request. In some embodiments the request identifiers may comprise increasing sequential numbers e.g. 1 2 3 . . . that are assigned to the received data and metadata sets requests in the time order they are received. In some embodiments a request having a lower request identifier XID is received before another request having a higher request identifier XID . As such the request identifiers may indicate the time ordering of when requests were received by the method relative to each other.

The method stores at each received data and metadata set request along with the assigned request identifier XID to the DMR data structure . Each data and metadata set request and assigned request identifier may comprise an entry in the DMR data structure . In some embodiments the method may store the data and metadata sets requests to the DMR data structure based on the time order they are received where an earlier received set request is stored to a higher entry in the data structure than a later received set request. For example the method may fill the DMR data structure beginning from a first top entry to a last bottom entry . In some embodiments a higher entry in the DMR data structure may comprise a request received before another request in a lower entry in the DMR data structure . As such the entry positions of the data and metadata requests in the DMR data structure may indicate the time ordering of when the requests were received by the method relative to each other.

The method then transmits at all data sets requests currently stored in the DMR data structure that has not already been transmitted to the remote node B for storage to the remote non volatile storage device . Note that transmitting a data set request in the DMR data structure does not remove delete the data set request from the DMR data structure . In some embodiments the method may delay transmitting the metadata sets requests currently stored in the DMR data structure to the remote node until each related data set request in the related group of the metadata set request has been completed. By doing so data integrity may be provided at the remote node in OOD of data and metadata.

In some embodiments the method may transmit at the data sets requests in a time order based on the entry position of the data sets requests in the DMR data structure . For example the method may transmit the data sets requests beginning from the top entry to the lower entries of the DMR data structure . As such in some embodiments the method may transmit the requests in the DMR data structure in a First In First Out FIFO manner where the request that comes in first is handled first etc. In some embodiments the method may transmit at data sets requests of different related groups in parallel. For example the method may transmit Data XID Data XID and Data XID in parallel with Data XID and Data XID. In these embodiments beginning the transmitting of data sets requests does not depend on the completion of other data sets whether in the same or different related groups.

The method then determines at whether a data or metadata set request has been completed i.e. successfully stored to the remote non volatile storage device on the remote node B . The method may determine such by determining whether a request completion acknowledgement has been received from the remote node B. Typically the remote node will receive the data and metadata sets requests from the local node and upon completing the storing of a particular data or metadata set request to the remote non volatile storage device will transmit a request completion acknowledgement to the local node indicating that the particular data or metadata set request is completed. For example the transport layer low level driver of the remote node may transmit the request completion acknowledgements to the local node. The request completion acknowledgement may include the request identifier XID of the data or metadata set request that has just been completed referred to as the just completed set request .

If the method determines at No that a data or metadata set request has not been completed a request completion acknowledgement has not been received the method proceeds to step where it continues to receive data and metadata sets from the mirroring client. If the method determines at Yes that a data or metadata set request has been completed a request completion acknowledgement has been received the method then determines at whether all sets requests received by the method prior to the just completed request i.e. all sets requests having request identifiers XIDs that are lower than the request identifier XID of the just completed request are already completed. In some embodiments the method may do so by determining whether the just completed request is the first top request in the DMR data structure by examining the request identifier XID of the first top request in the DMR data structure . If so this indicates that all sets requests having lower request identifiers received prior to the just completed request are already completed.

If the method determines at Yes that all prior sets requests are completed the method then removes deletes at the just completed data or metadata set request from the DMR data structure e.g. by locating and deleting the entry having the same request identifier as the just completed set request . The method also updates overwrites at the request field last cmplt request based on the request identifier XID of the first top data or metadata set request in the DMR data structure . In some embodiments the request field last cmplt request is updated using the following equation last cmplt request XID of first set request 1. The method then proceeds to step .

If the method determines at No that all prior sets requests are not completed the method removes deletes at the just completed data or metadata set request from the DMR data structure but does not update the request field last cmplt request . The method then proceeds to step .

The method determines at whether the first top request in the DMR data structure is a metadata set request. If not the method proceeds to step where it continues to receive data and metadata sets from the mirroring client. If so the method transmits at the metadata set request that comprises the first request in the DMR data structure to the remote node B for storage to the remote non volatile storage device . Note that transmitting a metadata set request in the DMR data structure does not remove delete the metadata set request from the DMR data structure . By delaying transmission of the metadata set request to the remote node B until the metadata set request is the first top request in the DMR data structure ensures that all related data sets requests in the same related group have already been completed thus providing data integrity at the remote node. The method then proceeds to step .

A fourth time event occurs where request completion acknowledgements have been received for data sets requests and and metadata set request and the metadata set request is then transmitted to the remote node. At this time point sets requests may be removed from the DMR data structure and the request field may be updated to equal 6 since metadata set request is currently the first request in the DMR data structure . The value of the request field may be used to indicate currently completed requests of the mirroring client at the remote node and be used to respond to completion queries from the mirroring client discussed further below .

Typically each mirroring client will continually submit completion queries to the mirroring layer to determine if its data and metadata sets requests sent to the mirroring layer have been completed i.e. stored to the remote node . The mirroring layer may produce the request field for indicating the request identifiers of sets requests that have been currently completed thus far. Each completion query may contain a request identifier for a prior submitted set request and the mirroring layer may respond to the completion query based on the request identifier value stored in the request field. For example if the request identifier in the completion query is less than or equal to the request identifier value stored in the request field the mirroring layer may send a response to the mirroring client indicating that the set request having the request identifier has been completed.

In some embodiments the mirroring layer may update the value of the request field in a manner that makes the OOD of data and metadata transparent to the mirroring client. As described above the value of the request field is only updated after a set request is just completed at step of method and it is determined at step that all prior sets requests i.e. all sets requests having request identifiers XIDs that are lower than the request identifier XID of the just completed request are already completed. Updating the value of the request field in this manner conceals the OOD of data and metadata from the mirroring client in respect to responding to their completion queries. Thus the OOD of data and metadata will be transparent to the mirroring client.

Some embodiments may be conveniently implemented using a conventional general purpose or a specialized digital computer or microprocessor programmed according to the teachings herein as will be apparent to those skilled in the computer art. Some embodiments may be implemented by a general purpose computer programmed to perform method or process steps described herein. Such programming may produce a new machine or special purpose computer for performing particular method or process steps and functions described herein pursuant to instructions from program software. Appropriate software coding may be prepared by programmers based on the teachings herein as will be apparent to those skilled in the software art. Some embodiments may also be implemented by the preparation of application specific integrated circuits or by interconnecting an appropriate network of conventional component circuits as will be readily apparent to those skilled in the art. Those of skill in the art would understand that information may be represented using any of a variety of different technologies and techniques.

Some embodiments include a computer program product comprising a computer readable medium media having instructions stored thereon in and when executed e.g. by a processor perform methods techniques or embodiments described herein the computer readable medium comprising sets of instructions for performing various steps of the methods techniques or embodiments described herein. The computer readable medium may comprise a storage medium having instructions stored thereon in which may be used to control or cause a computer to perform any of the processes of an embodiment. The storage medium may include without limitation any type of disk including floppy disks mini disks MDs optical disks DVDs CD ROMs micro drives and magneto optical disks ROMs RAMs EPROMs EEPROMs DRAMs VRAMs flash memory devices including flash cards magnetic or optical cards nanosystems including molecular memory ICs RAID devices remote data storage archive warehousing or any other type of media or device suitable for storing instructions and or data thereon in.

Stored on any one of the computer readable medium media some embodiments include software instructions for controlling both the hardware of the general purpose or specialized computer or microprocessor and for enabling the computer or microprocessor to interact with a human user and or other mechanism using the results of an embodiment. Such software may include without limitation device drivers operating systems and user applications. Ultimately such computer readable media further includes software instructions for performing embodiments described herein. Included in the programming software of the general purpose specialized computer or microprocessor are software modules for implementing some embodiments.

Those of skill would further appreciate that the various illustrative logical blocks modules circuits techniques or method steps of embodiments described herein may be implemented as electronic hardware computer software or combinations of both. To illustrate this interchangeability of hardware and software various illustrative components blocks modules circuits and steps have been described herein generally in terms of their functionality. Whether such functionality is implemented as hardware or software depends upon the particular application and design constraints imposed on the overall system. Skilled artisans may implement the described functionality in varying ways for each particular application but such implementation decisions should not be interpreted as causing a departure from the embodiments described herein.

The various illustrative logical blocks modules and circuits described in connection with the embodiments disclosed herein may be implemented or performed with a general purpose processor a digital signal processor DSP an application specific integrated circuit ASIC a field programmable gate array FPGA or other programmable logic device discrete gate or transistor logic discrete hardware components or any combination thereof designed to perform the functions described herein. A general purpose processor may be a microprocessor but in the alternative the processor may be any conventional processor controller microcontroller or state machine. A processor may also be implemented as a combination of computing devices e.g. a combination of a DSP and a microprocessor a plurality of microprocessors one or more microprocessors in conjunction with a DSP core or any other such configuration.

The algorithm techniques processes or methods described in connection with embodiments disclosed herein may be embodied directly in hardware in software executed by a processor or in a combination of the two. In some embodiments any software application program tool module or layer described herein may comprise an engine comprising hardware and or software configured to perform embodiments described herein. In general functions of a software application program tool module or layer described herein may be embodied directly in hardware or embodied as software executed by a processor or embodied as a combination of the two. A software application layer or module may reside in RAM memory flash memory ROM memory EPROM memory EEPROM memory registers hard disk a removable disk a CD ROM or any other form of storage medium known in the art. An exemplary storage medium is coupled to the processor such that the processor can read data from and write data to the storage medium. In the alternative the storage medium may be integral to the processor. The processor and the storage medium may reside in an ASIC. The ASIC may reside in a user device. In the alternative the processor and the storage medium may reside as discrete components in a user device.

While the embodiments described herein have been described with reference to numerous specific details one of ordinary skill in the art will recognize that the embodiments can be embodied in other specific forms without departing from the spirit of the embodiments. Thus one of ordinary skill in the art would understand that the embodiments described herein are not to be limited by the foregoing illustrative details but rather are to be defined by the appended claims.

