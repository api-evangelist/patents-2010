---

title: Updating nodes considering service model constraints
abstract: Methods, systems, and computer-readable media for deploying an update to nodes propagated throughout a data center are provided. Launching new upgrade to hosting environment residing on the nodes typically invokes a mechanism (e.g., fabric controller) to form a group of nodes that are independent of one another with respect to upgrade domains, which are assigned to tenants (e.g., program components of service applications running within the data center) presently hosted by the nodes. The constraints of the update domains are articulated by service level agreements established for the service applications, respectively. Forming the group involves identifying independent nodes for membership, where no two members of the group host analogous tenants (belonging to a common service application) that are assigned to distinct update domains. However, it is acceptable to join to the group those nodes hosting analogous tenants that are each assigned to the same update domain.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08407689&OS=08407689&RS=08407689
owner: Microsoft Corporation
number: 08407689
owner_city: Redmond
owner_country: US
publication_date: 20100625
---
Large scale networked systems are commonplace systems employed in a variety of settings for running applications and maintaining data for business and operational functions. For instance a data center may support operation of a variety of service applications e.g. web applications email services search engine services etc. . These large scale networked systems typically include a large number of nodes distributed throughout the data center in which each node resembles a physical machine or a virtual machine running on a physical host. Due partly to the large number of the nodes that may be included within such large scale systems deployment of software both operating systems OSs and applications to the various nodes and maintenance of the software e.g. performing updates on each node can be a time consuming and costly process.

Similar to other articles of software these distributed service applications are susceptible to software failures or bugs affecting the software installed on the nodes of the data center. Therefore it is necessary to roll out new versions of the software to fix errors e.g. security vulnerabilities within or improve the functionality offered by the nodes. In both cases it is often necessary to stop and then restart potentially each and every one of the nodes as well as component programs e.g. tenants of a customer s service application residing on the nodes in order to properly perform an update.

At the present time data center administrators are limited to an individualized process that employs mechanisms or manual efforts directed toward installing and updating software individually on each node in a piecemeal fashion. Otherwise an administrator of the data center risks interruption and unavailability of the service applications running on top of the nodes comprising the data center. For instance performing a comprehensive upgrade of a current piece of software e.g. root operating system OS or hypervisor installed throughout the data center typically involves shutting down one node at a time applying the appropriate upgrade and then rebooting the node prior to accessing another node.

Accordingly the current process for performing a comprehensive update to a multitude of nodes which often relies on the data center administrators to manually perform the updates individually are ad hoc solutions are labor intensive and are error prone. Further these current solutions do not guarantee a reliable result that is consistent across the data center. These shortcomings of individualized upgrades are exaggerated when the data center is expansive in size comprising a multitude of interconnected hardware components e.g. nodes that support the operation of a multitude of service applications.

As such providing a reliable mechanism for understanding a distribution of the component programs of the service applications across the data center nodes and applying this understanding to update two or more nodes at once while honoring service level agreements SLAs established for the service applications would potentially ameliorate the problematic results of the piecemeal update processes currently in place.

This Summary is provided to introduce concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

Embodiments of the present invention relate to computer systems computerized methods and computer readable media for deploying an update concurrently on a plurality of nodes propagated throughout a distributed computing platform. Initially the nodes represent physical hosts e.g. computing devices capable of running tenants of a customer s service application within the distributed computing platform. As used herein the term tenants broadly refers to instances of one or more roles of a service application. These instances typically represent copies or replications of at least one role which resembles a component program for supporting particular functional aspects of the service application.

The deployment of an update may be invoked upon receiving an indication to install a patch to a root operating system OS or hypervisor residing on the nodes of the data center. At this point a fabric controller i.e. mechanism provisioned to manage and schedule installations and updates within the data center is triggered to form a group of nodes that are independent of one another with respect to upgrade domains which are assigned to tenants presently hosted by the nodes. As more fully discussed herein the phrase upgrade domain generally represents a maximum portion of a service application or class of tenant thereof that can be made unavailable without substantially degrading the performance of the service application. Typically the constraints e.g. particular of role instances or percentage of total service application of the update domains are articulated by a service model e.g. service level agreement SLA established for the service application.

In an exemplary embodiment forming a group of independent nodes involves selecting an initial group of nodes where no two members of the group of nodes host analogous tenants respectively of a common service application and where the analogous tenants are assigned to differing update domains. That is it is acceptable to join analogous tenants belonging to a common service application that are each assigned to the same update domain however constraints of the fabric controller prohibit joining analogous tenants assigned to distinct update domains to the group of independent nodes at the same time.

When attempting to increase the size of the group of independent nodes in order to maximize concurrent application of the update a subject node may be identified as a candidate for membership. The subject node may remain a candidate if the following criteria are met determining that a present state of the subject node s hosting environment e.g. root OS or hypervisor differs from a goal hosting environment state having the update applied and determining that a present number of members of the group of independent nodes shall remain beneath a specified threshold amount of nodes upon ostensibly joining the subject node to the group of independent nodes.

Upon satisfying at least the criteria above the fabric controller may determine whether subject node qualifies as an independent node with respect to members of the group of independent nodes. This step of determining the subject node s independence may be carried out by performing one or more of the following procedures a determining whether tenant s hosted by the subject node correspond to tenants hosted by the members of the group of independent nodes b when the tenant s hosted by the subject node correspond to any of the tenants within the group of independent nodes comparing update domains assigned to the corresponding tenants against update domains assigned to the hosted tenant s c when the update domains of the corresponding tenants match the update domains assigned to the hosted tenant s allowing the subject node to join the group of nodes and d when the update domains of the corresponding tenants conflict with the update domains assigned to the hosted tenant s temporarily refraining from joining the subject node to the group of nodes.

In the embodiments above new nodes are iteratively added to the group of independent nodes as a function of at least properties of the processed nodes i.e. nodes that have recently received the update being removed from the group. However in other embodiments an initial and complete group of independent nodes may be selected from the data center for concurrent updating prior to any nodes of the initial group receiving the update. In one instance a graph coloring algorithm may be employed to model the nodes within the data center as a graph. Next the graph coloring algorithm may apply a coloring scheme onto the graph as a function of the update domains assigned to tenants residing on the nodes. Upon application of the coloring scheme those nodes bearing one common color are aggregated as a first group of independent nodes while those nodes bearing another color are aggregated as a second group of independent nodes and so on. The members of the first group of independent nodes are subsequently staged for receiving the update in tandem. Upon the entirety of the first group of independent nodes receiving the update the members of the second group of independent nodes are staged for receiving the update in tandem. As such the nodes may be concurrently updated in waves of individual groups or concurrently via an evolving group that periodically accepts and releases members as discussed above.

The subject matter of embodiments of the present invention is described with specificity herein to meet statutory requirements. However the description itself is not intended to limit the scope of this patent. Rather the inventors have contemplated that the claimed subject matter might also be embodied in other ways to include different steps or combinations of steps similar to the ones described in this document in conjunction with other present or future technologies. Moreover although the terms step and or block may be used herein to connote different elements of methods employed the terms should not be interpreted as implying any particular order among or between various steps herein disclosed unless and except when the order of individual steps is explicitly described.

Embodiments of the present invention relate to methods systems and computer storage media having computer executable instructions embodied thereon that when executed perform methods in accordance with embodiments hereof for updating a root operating system OS or hypervisor of a plurality of nodes e.g. computing devices within the context of a distributed computing environment. Generally the update is carried out expediently by applying a software upgrade to a plurality of nodes at once while at the same time honoring the guarantees of the service models established for the service applications running on top of the nodes. In an exemplary embodiment honoring the service model guarantees involves allowing only independent nodes to be concurrently updated where nodes are deemed independent with respect to each other based on the update domains assigned thereto.

Accordingly in one aspect embodiments of the present invention relate to one or more computer readable media that has computer executable instructions embodied thereon that when executed perform a method for updating one or more nodes of a data center in compliance with service models established for service applications running within the data center. Initially the method involves the step of providing a group of independent nodes hereinafter group that are preselected for receiving an update. At some point a subject node is identified as a candidate for joining the group. The subject node may remain a candidate if the following criteria are met determining that a present state of the subject node s root OS or hypervisor differs from a goal root OS state having the update applied and determining that a present number of members of the group shall remain beneath a specified threshold amount of nodes upon ostensibly joining the subject node to the group.

Upon satisfying at least the criteria above the fabric controller may determine whether subject node qualifies as an independent node with respect to members of the group. This step of determining the subject node s independence may be carried out by performing one or more of the following comparison process a determining whether tenant s hosted by the subject node correspond to tenants hosted by the members of the group b when the tenant s hosted by the subject node correspond to any of the tenants within the group comparing update domains assigned to the corresponding tenants against update domains assigned to the hosted tenant s c when the update domains of the corresponding tenants match the update domains assigned to the hosted tenant s allowing the subject node to join the group and d when the update domains of the corresponding tenants conflict with the update domains assigned to the hosted tenant s temporarily refraining from joining the subject node to the group. Further upon determining that the subject node qualifies as an independent node the method involves writing to a storage location e.g. listing enumerating a current membership of the group an indicia that the subject node is joined as a member of the group.

In another aspect embodiments of the present invention relate to a computerized method for allocating nodes to role instances of service applications with consideration of update domains assigned to the role instances. One step of the method may include distributing on nodes of a data center various role instances that support the service applications presently running within the data center. An association between the nodes and the service applications running thereon may be written to a list. Further the nodes allocated to each of the role instances may be mapped against update domains assigned to each of the role instances.

In an exemplary embodiment the mechanism e.g. fabric controller carries out the method by employing the list and the mapping when allocating node s to an instance of a subject role. The step of allocating the node s to an instance of a subject role may include reading the list to identify role instances presently residing on a particular node and refraining from placing the subject role instance on the particular node when the subject role instance is analogous to the identified role instances.

The method may further involve the step of selecting an appropriate update domain to assign to the subject role that is placed on the allocated node s . Typically selection facilitates maximizing a number of nodes which include the allocated node that are updateable in tandem. In embodiments selecting the appropriate update domain to assign to the subject role includes one or more of the following steps reading the list to identify service applications having role instances presently residing on the allocated node reading the mapping to identify the update domains that are assigned to the role instances of the identified service applications ascertaining an update domain of the identified update domains that most frequently appears on nodes of the data center that host the role instances of the identified service applications and assigning the most frequently appearing update domain to the subject role.

In a third aspect an exemplary computer system is provided for performing a method that comprehensively updates nodes of a data center with consideration of both service models established for customers of the data center and pending localized comprehensive updates of service applications owned by the customers. In embodiments the computer system includes a processing unit coupled to a computer storage medium that stores a plurality of computer software components executable by the processing unit. Initially the computer software components include a fabric controller and a synchronization mechanism. The fabric controller initiates the comprehensive update to the nodes of the data center. Typically the comprehensive update concurrently affects a group of nodes that are determined to be independent of one another. In an exemplary embodiment the determination of independence involves a comparison process that includes but is not limited to one or more of the following steps maintaining a listing of tenants e.g. instances of roles of service applications hosted by members of the group and mapping the update domains assigned to the tenants respectively within the listing. Typically each of the update domains represents a percentage of instances of the particular role of the particular service application that are allowed to be concurrently offline. This percentage may be dictated by a service model e.g. service level agreement SLA established for the particular service application. The comparison process may additionally involve the step of disallowing a subject node to join the group upon determining a that the subject node hosts one or more tenants that correspond to at least one of the tenants enumerated by the listing and b that the subject node tenants are assigned an update domain that differs from the update domain mapped to the corresponding tenant of the listing.

The synchronization mechanism is provided to ensure that the service model is not violated. Generally this is achieved by performing a synchronization process that includes the following steps identifying that a localized update is pending for the particular service application and appending as entries to the listing indications of update domains assigned to tenants of the particular service application thereby abstaining from joining nodes hosting the tenants of the particular service application to the group designated to receive the comprehensive update.

Embodiments of the present invention relate to deploying an upgrade to operating systems accommodated by nodes that are distributed throughout a distributed computing environment or data center. In one instance the nodes represent computing devices capable of running role instances i.e. tenants of the service application within a distributed computing platform. As used herein the term roles or role instances is not meant to be limiting but may include any replication of at least one role which generally resembles a component program that supports particular functional aspects of a service application.

As such roles provide a template description of a functional portion of the service application. Roles are described by indicating the computer code implementing the role the conditions within the hosting environment that are required by the role configuration settings to be applied to the role and the role s set of endpoints for communication with other roles elements etc. In one instance the role s configuration settings may include collective settings which are shared by all instances of the role or individual settings that are particular to each instance of the role. In an exemplary embodiment the roles each represent a particular class of component of the service application. Typically the service model delineates how many instances of each of the one or more roles to place within the data center where each of the instances is a replication of the particular class of component or role. In other words each role represents a collection of instances of each class of components where the service application may have any number of classes of components for carrying out functions thereof.

Having briefly described an overview of embodiments of the present invention an exemplary operating environment suitable for implementing embodiments of the present invention is described below.

Referring to the drawings in general and initially to in particular an exemplary operating environment for implementing embodiments of the present invention is shown and designated generally as computing device . Computing device is but one example of a suitable computing environment and is not intended to suggest any limitation as to the scope of use or functionality of embodiments of the present invention. Neither should the computing environment be interpreted as having any dependency or requirement relating to any one or combination of components illustrated.

Embodiments of the present invention may be described in the general context of computer code or machine useable instructions including computer executable instructions such as program components being executed by a computer or other machine such as a personal data assistant or other handheld device. Generally program components including routines programs objects components data structures and the like refer to code that performs particular tasks or implements particular abstract data types. Embodiments of the present invention may be practiced in a variety of system configurations including hand held devices consumer electronics general purpose computers specialty computing devices etc. Embodiments of the invention may also be practiced in distributed computing platforms where tasks are performed by remote processing devices that are linked through a communications network.

With continued reference to computing device includes a bus that directly or indirectly couples the following devices memory one or more processors one or more presentation components input output I O ports I O components and an illustrative power supply . Bus represents what may be one or more busses such as an address bus data bus or combination thereof . Although the various blocks of are shown with lines for the sake of clarity in reality delineating various components is not so clear and metaphorically the lines would more accurately be grey and fuzzy. For example one may consider a presentation component such as a display device to be an I O component. Also processors have memory. The inventors hereof recognize that such is the nature of the art and reiterate that the diagram of is merely illustrative of an exemplary computing device that can be used in connection with one or more embodiments of the present invention. Distinction is not made between such categories as workstation server laptop hand held device etc. as all are contemplated within the scope of and reference to computer or computing device. 

Computing device typically includes a variety of computer readable media. By way of example and not limitation computer readable media may comprise Random Access Memory RAM Read Only Memory ROM Electronically Erasable Programmable Read Only Memory EEPROM flash memory or other memory technologies CDROM digital versatile disks DVDs or other optical or holographic media magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium that can be used to encode desired information and be accessed by computing device .

Memory includes computer storage media in the form of volatile and or nonvolatile memory. The memory may be removable nonremovable or a combination thereof. Exemplary hardware devices include solid state memory hard drives optical disc drives etc. Computing device includes one or more processors that read data from various entities such as memory or I O components . Presentation component s present data indications to a user or other device. Exemplary presentation components include a display device speaker printing component vibrating component etc. I O ports allow computing device to be logically coupled to other devices including I O components some of which may be built in. Illustrative components include a microphone joystick game pad satellite dish scanner printer wireless device etc.

With reference to a first node and or second node device may be implemented by the exemplary computing device of . Further tenant and or tenant may be provided with access to portions of the memory of and or allocated an amount of processing capacity available at the processors of for conducting operations that support their respective service applications.

Turning now to a block diagram illustrating a distributed computing environment is shown which is suitable for use in implementing embodiments of the present invention. In an exemplary embodiment the distributed computing environment is configured to apply an update concurrently to members of a group of independent nodes. The distributed computing environment includes the data center configured to accommodate and support operation of component programs or instances of roles of a particular service application according to the fabric controller . It will be understood and appreciated that the data center shown in is merely an example of one suitable for accommodating one or more service applications and is not intended to suggest any limitation as to the scope of use or functionality of embodiments of the present invention. Neither should the data center be interpreted as having any dependency or requirement related to any single resource combination of resources combination of nodes e.g. nodes and or set of APIs to access the resources and or nodes.

Further it will be understood and appreciated that the cloud computing platform shown in is merely an example of one suitable computing system environment and is not intended to suggest any limitation as to the scope of use or functionality of embodiments of the present invention. For instance the cloud computing platform may be a public cloud a private cloud or a dedicated cloud. Neither should the cloud computing platform be interpreted as having any dependency or requirement related to any single component or combination of components illustrated therein. Further although the various blocks of are shown with lines for the sake of clarity in reality delineating various components is not so clear and metaphorically the lines would more accurately be grey and fuzzy. In addition any number of nodes virtual machines data centers tenants or combinations thereof may be employed to achieve the desired functionality within the scope of embodiments of the present invention.

The cloud computing platform includes the data center configured to host and support operation of tenants and of a particular service application. The phrase service application as used herein broadly refers to any software or portions of software that runs on top of or accesses storage locations within the data center . In one embodiment one or more of the tenants and may represent the portions of software component programs or instances of roles that participate in the service application. In another embodiment one or more of the tenants and may represent stored data that is accessible to the service application. It will be understood and appreciated that the tenants and shown in are merely an example of suitable parts to support the service application and are not intended to suggest any limitation as to the scope of use or functionality of embodiments of the present invention.

Generally virtual machines and are allocated to the tenants and of the service application based on demands e.g. amount of processing load placed on the service application. As used herein the phrase virtual machine is not meant to be limiting and may refer to any software application operating system or program that is executed by a processing unit to underlie the functionality of the tenants and . Further the virtual machines and may include processing capacity storage locations and other assets within the data center to properly support the tenants and .

In operation the virtual machines and are dynamically allocated within resources e.g. first node and second node of the data center and endpoints e.g. the tenants and are dynamically placed on the allocated virtual machines and to satisfy the current processing load. In one instance a fabric controller is responsible for automatically allocating the virtual machines and and for placing the tenants and within the data center . By way of example the fabric controller may rely on a service model e.g. designed by a customer that owns the service application to provide guidance on how where and when to allocate the virtual machines and and to place the tenants and thereon.

In addition the fabric controller may be provisioned with responsibility to manage installations of updates to the nodes and . By way of example the fabric controller may rely on a service model to provide data related to update domains of the service application in order to strategically place the tenants and on appropriate nodes based on a current topology of the data center . In another example the fabric controller honor guarantees of the service model by strategically selecting the tenants and for joining a group of independent nodes scheduled for receiving the update where strategic selection is accomplished by ensuring that only one update domain for each tenant of the service application is scheduled for receiving the update at a time.

As discussed above the virtual machines and may be dynamically allocated within the first node and second node . Per embodiments of the present invention the nodes and represent any form of computing devices such as for example a personal computer a desktop computer a laptop computer a mobile device a consumer electronic device server s the computing device of and the like. In one instance the nodes and host and support the operations of the virtual machines and while simultaneously hosting other virtual machines carved out for supporting other tenants of the data center . Often the tenants and may include endpoints of distinct service applications owned by different customers.

Typically each of the nodes and include or is linked to some form of a computing unit e.g. central processing unit microprocessor etc. to support operations of the component s running thereon. As utilized herein the phrase computing unit generally refers to a dedicated computing device with processing power and storage memory which supports operating software that underlies the execution of software applications and computer programs thereon. In one instance the computing unit is configured with tangible hardware elements or machines that are integral or operably coupled to the nodes and to enable each device to perform a variety of processes and operations. In another instance the computing unit may encompass a processor not shown coupled to the computer readable medium accommodated by each of the nodes and . Generally the computer readable medium stores at least temporarily a plurality of computer software components that are executable by the processor. As utilized herein the term processor is not meant to be limiting and may encompass any elements of the computing unit that act in a computational capacity. In such capacity the processor may be configured as a tangible article that processes instructions. In an exemplary embodiment processing may involve fetching decoding interpreting executing and writing back instructions.

Per embodiments of the present invention the nodes and execute root operating systems and at least one hypervisor which is discussed more fully with respect to . Further the role instance s e.g. tenants and that reside on the nodes and support operation of service applications and may be interconnected via application programming interfaces APIs . In one instance one or more of these interconnections may be established via a network cloud not shown . The network cloud serves to interconnect resources such as the tenants and which may be distributably placed across various physical hosts such as nodes and . In addition the network cloud facilitates communication over channels connecting the tenants and of the service applications running in the data center . By way of example the network cloud may include without limitation one or more local area networks LANs and or wide area networks WANs . Such networking environments are commonplace in offices enterprise wide computer networks intranets and the Internet. Accordingly the network is not further described herein.

In an exemplary embodiment the fabric controller includes or is operably coupled to a synchronization mechanism . The synchronization mechanism is generally configured to monitor ongoing updates of the root OS s and or the hypervisor and to enforce the guarantees of the service model. One exemplary guarantee articulates that only one update domain for a service application is offline at a time. However in cases where one service application is presently being updated by its owner while at the same time a root OS and or hypervisor update is in progress the synchronization mechanism is relied upon to take corrective actions. This is especially relevant when the service application does not recognize or detect that a root OS hypervisor update is simultaneously occurring. By way of example the service application may have taken a particular tenant offline having one update domain UD assigned to it while the fabric controller may be attempting to schedule an update for another node with a similar tenant hosted thereon. If the hosted tenant has the same update domain UD then the guarantees of the service model are upheld when the fabric controller pulls the node offline. In contrast if the hosted tenant has a different update domain UD then the guarantees of the service model are violated when the fabric controller pulls the node offline.

In this second instance or in both instances above the synchronization mechanism may perform the following steps detect the customer initiated localized update to the software of the service application and determine whether the customer s service application presently has an update domain offline as a result of the owner s self initiated update schedule. If so an indicia of the service application is added to a list of applications being updated even though the service application may not be presently participating in the ongoing comprehensive update conducted by the fabric controller . Adding the service application indicia to the list effectively prevents the fabric controller from taking another update domain offline. The conflict is resolved in favor of the customer initiated updates. 

In another instance upon the synchronization mechanism detecting that the customer s localized update of a service application is simultaneously occurring in tandem with a data center wide comprehensive update of node hosting environments e.g. root OS and or hypervisor the synchronization mechanism may take the corrective action of blocking the customer s service application from taking another update domain offline. This corrective action is typically enforced when the fabric controller has already taken another update domain offline for the service application for purposes of advancing the comprehensive update. The conflict is resolved in favor of the fabric controller updates. Accordingly the synchronization mechanism momentarily pauses the customer initiated update and eventually resumes the customer initiated update upon concluding the comprehensive update. Further the synchronization mechanism is able to transparently resolve interfering updates. For instance from the customers external perspective the updates appear to be occurring in concurrence even though one is being stopped.

The following discussion illustrates an exemplary embodiment of the interaction between the fabric controller and the synchronization mechanism . With reference to the discussion above this embodiment also comprehensively updates nodes of a data center with consideration of both service models established for customers of the data center and pending localized comprehensive updates of service applications owned by the customers. Initially the fabric controller may invoke a comprehensive update to the nodes of the data center. This update may be triggered by any number of events such as a new patch that is launched for installation. In general the comprehensive update concurrently affects a group of nodes that are determined to be independent of one another. The determination of independence involves maintaining a listing of tenants hosted by members of the group of nodes and mapping an indication of update domains assigned to the tenants respectively within the listing. By way of example the update domains represent a percentage of instances of the particular role of the particular service application that are allowed to be concurrently offline as dictated by a service model established for the particular service application.

Upon the fabric controller determining a that the subject node hosts one or more tenants that correspond to at least one of the tenants enumerated by the listing and b that the subject node tenants are assigned an update domain that differs from the update domain mapped to the corresponding tenant of the listing the fabric controller refrains from adding a subject node to the group of nodes at least temporarily. Meanwhile the synchronization mechanism ensures that the service model is not violated by performing a synchronization process. In an exemplary embodiment the synchronization process involves one or more of the following steps a identifying that a localized update is pending for a particular service application and b appending as entries to the listing that reference update domains assigned to tenants of the particular service application. Accordingly by adding a record of the customer initiated updates to the listing the fabric controller recognizes the particular service application as having an update domain down and will typically abstain from joining nodes hosting the tenants of the particular service application to the group of nodes designated to receive the comprehensive update. Thus the fabric controller and the synchronization mechanism when acting in concert to monitor and record customer initiated updates recognize overlapping updates and enact an appropriate corrective action to preserve the guarantees of the service model.

Turning now to a diagrammatic view of exemplary layers within a node is shown in the context of an embodiment of the present invention. By way of background the cloud computing environment see reference numeral of hosts a number of service applications within a physical layout of its one or more data centers. Each data center tends to include racks that contain an aggregation of nodes such as fifty physical machines or blades to a rack. Typically each rack also includes a power supply and a network switch connecting the nodes to a network the support operation of the physical machines.

Processing capacity of the node is subdivided into one or more virtual machines VMs . In this way resources running on top of the VMs can be carved into substantially exclusive elements that can run independently of each other. These VMs also serve to host tenants of service application s . Thus service applications of different customers remain isolated in operation even when running on the VMs instantiated on a common node . By way of example the internal isolation provided by the VMs prevents sharing data from customer A to customer B when customers A and B are both allocated resources on the node . Accordingly these service applications are distributed in nature because most service applications tend to rely on multiple VMs in disparate physical machines to properly function.

The fabric controller e.g. see reference numeral of acts as a virtual manager for the node and the virtual machines . For instance the fabric controller is responsible for installing software on the node to implement updates to the root OS and hypervisor running thereon. In general the root OS refers to a portion of a hosting environment of the node for supporting the VMs . The hypervisor typically represents software that interfaces with hardware of the node . Typically installing software involves powering off the hardware of the node and taking offline the virtual machines running on top of the root OS . Then upon completing installation the hardware of the node is powered on thereby rebooting and bringing online the virtual machines .

By way of example the functionality of a photo sharing application may be split into two specific roles or tenants and propagated to many nodes within the data center. One of the roles may act as a front end subservice that manages a user interface e.g. webpage running on a customer s website for accepting and retrieving digital images. Another role may act as a back end subservice that forwards the accepted digital images to the storage locations allocated to the photo sharing application and catalogs the storage locations relative to the forwarded digital images to facilitate retrieval.

The support provided by the data center to the customer that owns the photo sharing application typically complies with a service model established for the service model. For operation purposes the service model describes how the service application is split into roles and where instances of the roles can be placed. For update purposes the service model dictates a number of instances of roles that can be taken offline concurrently. In an exemplary embodiment the service model governs a number of fault domains and update domains by which each role is partitioned. As such the fabric controller may leverage the service model to limit the scope of the update in appreciation of the portion of the service application that the customer is comfortable in allowing to be unavailable. Further although the fabric controller is described herein as the mechanism that invokes and manages the updates while honoring the service model a data center administrator may retain the ultimate authority in determining when and how to update the nodes.

In one embodiment fault domains represent a number of hardware failure units e.g. racks physical hosts data centers etc. over which the service application should be distributed in order to promote resilience upon the cloud computing platform experiencing an internal crash or upon a data center experiencing a contained equipment failure. In another embodiment update domains represent software upgrade units that are controllably applied based on the service models such that the service application is consistently running during the update. In one instance the update domain is defined as the maximum percentage unit e.g. 10 representing 10 VMs of a total of 100 VMs on which instances of a role are running of the service application that may be taken down at once per the service model when an update of the software on nodes is in progress. Once the nodes are finished receiving the update the VMs of a particular update domain are brought back online and another update domain is targeted for being taken offline. Generally this process of iteratively addressing one update domain at a time is not visible to the customer of the data center.

As mentioned above tenants represent instances of the roles i.e. logical portion of a complete software package which are copies of a role that are running in many VMs within the node and across the data center. Some instances of a particular role supporting a particular service application are assigned to a first update domain while others are assigned to a second update domain and so on. Thus the fabric controller is enabled to honor the service model for each service application during an upgrade by only taking one of the first or second update domains offline at a time. By way of example when it comes time to upgrade the root OS or the hypervisor generally referred to in combination as the hosting environment the fabric controller takes the entire node offline for a certain period of time to execute the upgrade. Accordingly in light of the downtime inherent with an upgrade embodiments of the present invention introduce technology to minimize the time required to update the root OS and hypervisor for all machines while at the same time satisfying the service model imposed constraint of shutting down tenants assigned to just a single update domain for an service application at a time.

Turning now to a graphical representation of a descriptive reservoir and pump technique for managing membership of the group of independent nodes is illustrated in accordance with an embodiment of the present invention. Initially a plurality of nodes are provided within the data center . Nodes are depicted as being in a non updated state which are running a previous version of a hosting environment while nodes are depicted as being in a goal hosting environment state which have the update applied to their hosting environment. The reservoir includes nodes that are scheduled to receive the update. That is the nodes have been added to the group of independent nodes and are passively staged and waiting in an online mode for a time when they may be taken offline to receive the update. The pump includes nodes that are presently in an offline mode and actively receiving the update. The nodes are also considered to be members of the group of independent nodes along with the nodes .

Accordingly the combination of the nodes and form the group of independent nodes. When a threshold amount of nodes is specified for the group the combination of nodes and is regulated to remain below the threshold. However this threshold can dynamically vary and is typically controlled by the fabric controller. In one instance the fabric controller uses criteria to vary the threshold such as processing capacity at the pump. That is the size and nodes of the reservoir can be changed over time allowing the number of nodes and being updated at a time to be dynamically adjusted.

In operation node s are added to the nodes of the reservoir upon determining that the members nodes and of the group of nodes remain below the current threshold. The pump is then invoked and the nodes therein are processed. In embodiments processing involves shutting the nodes down such that they are offline applying the appropriate update restarting the nodes setting up the VMs on the nodes and booting up the tenants hosted by the VMs. Based on this substantial amount of processing that occurs within the pump the amount of the nodes being processed at once may be limited for purposes of risk management e.g. in order to reduce exposure of update failure . In one example each of the nodes within the pump as well as the nodes within the reservoir may be updated at the same time or may be updated incrementally. In another instance only one node of the nodes in the pump is taken down at a time.

Once completely processed node s are evacuated from the pump and join the nodes with the goal hosting environment state. Further the node s may be added to the nodes of the reservoir based on the now revised group of independent nodes and . In other words evacuating the node s removes members from the group of independent nodes and thus changes the attributes e.g. releases update domains that blocked admittance of nodes for joining the group. As such the fabric controller may reexamine the pending members of the group upon the node s being evacuated and periodically add node s which may have been previously precluded from joining the group based prior attributes.

In one embodiment a list is maintained that captures an indicia of the nodes in the reservoir and the nodes in the pump. The list may include an identity of the service application tenant and update domains mapped against the indicia of the nodes and . In another embodiment the list memorializes just those nodes and respective update domains that are actively affected by the update. In this embodiment every time a node is taken offline it is added the list if not already there . Further indicia of the node s that are evacuated from the pump upon coming online are removed from the list. Thus this list facilitates the determination of the node s that may enter the reservoir. In yet another embodiment the list is employed to enforce the constraint that the node s joining the group should mutually lack update domain dependencies in relation to the nodes and . As used herein an update domain dependency denotes a condition in which two or more nodes host analogous tenants of a common service application that are assigned to distinct update domains respectively. In one example of enforcing constraints a graph coloring algorithm is employed to select the node s of the data center that mutually lack update domain dependencies in relation to the nodes and .

Embodiments of the graph coloring algorithm will now be discussed with reference to . Generally illustrate schematic depictions of tenants hosted on nodes where the tenants are assigned to various update domains in accordance with an embodiment of the present invention. Initially graph coloring refers to a method for picking a maximum number of members of the group from a predefined set of candidates where the members have some attributes that preclude the inclusion of the other candidates. As such every time a member node is selected for joining the group the remaining available candidates nodes for inclusion in the group are restricted as there are additional attributes update domain dependencies involved with gaining admittance to the group.

By way of example the landscape of the data center can be thought of as vertices in a graph and edges may be created between vertices if two nodes of the data center are hosting analogous tenants from the same application but those two tenants are in different update domains. Upon establishing the graph the graph coloring algorithm may implement one or more variations of a coloring scheme. In one variation of the coloring scheme the appropriate vertices of the graph are colored the same color at the same time. That is a maximum number of vertices are colored and removed as nodes are added to the group and processed before another color is considered. In this variation a high number of nodes are added to the group at once and are typically processed at the same time while the balance of the nodes must wait for the reservoir to fully drain before moving to the next group of nodes or color. This approach blocks progress on coloring new nodes until all nodes of the previous color are completely updated.

In another variation of the coloring scheme the nodes joined to the group can be re examined periodically. Thus upon removing one or more nodes from the group the graph is dynamically reevaluated to determine which nodes may be added based on the changed dependencies. This approach allows for iteratively making forward progress even when it takes more time to update some of the nodes in the group.

With reference to a schematic depiction shows the nodes and hosting tenants and tenants respectively. The tenant is part of a service application owned by customer A and is assigned to update domain UD . The tenant on the node is part of a service application owned by customer B and is assigned to UD. The tenant on the node is part of a service application owned by customer B and is assigned to UD. And the tenant is part of a service application owned by customer C and is assigned to UD. Although the nodes and include differing tenants and respectively which would typically indicate they are independent nodes they both host the tenant . Because the tenant on the node is assigned to UD which is distinct from UD assigned to the tenant on the node the nodes and are not independent and would be colored differently by the graph coloring algorithm.

With reference to a schematic depiction shows the nodes and hosting tenants and tenants respectively. As above the tenant is part of a service application owned by customer A and is assigned to UD. The tenant on the node is part of a service application owned by customer B and is assigned to UD. The tenant on the node is part of a service application owned by customer B and is now assigned to UD. And the tenant is part of a service application owned by customer A and is assigned to UD. Upon evaluation the nodes and both include tenants which are both assigned to the same UD. Thus initially the nodes and are independent. However because the tenant on the node is assigned to UD which is distinct from UD assigned to the tenant on the node the nodes and are not independent and would be colored differently by the graph coloring algorithm.

With reference to a schematic depiction shows the nodes and hosting tenants and tenants respectively. As above the tenant is part of a service application owned by customer A and is assigned to UD. The tenant on the node is part of a service application owned by customer B and is assigned to UD. The tenant on the node is part of a service application owned by customer B and is assigned to UD. And the tenant is part of a service application owned by customer A and is now assigned to UD. Upon evaluation the nodes and both include tenants and which are all assigned to the same UD. Thus the nodes and are independent and would be colored similarly by the graph coloring algorithm. Further this schematic depiction shows an example of a style of strategic allocation for initially placing update domains where placing repeating pairs or sets of update domains increase the likelihood of more nodes being identified as independent nodes during the comparison process.

Turning to a flow diagram is illustrated that shows an overall method for updating one or more nodes of a data center in compliance with service models of service applications running within the data center in accordance with an embodiment of the present invention. The method includes providing a group of independent nodes that are preselected for receiving an update as depicted at block . At some point a subject node is identified as a candidate for joining the group as depicted at block . As depicted at block the subject node may remain a candidate if the following criteria are met determining that a present state of the subject node s hosting environment e.g. root OS or hypervisor differs from a goal hosting environment state having the update applied see block and determining that a present number of members of the group shall remain beneath a specified threshold amount of nodes upon ostensibly joining the subject node to the group see block .

Upon satisfying at least the criteria above the fabric controller may determine whether subject node qualifies as an independent node with respect to members of the group as depicted at block . This step of determining the subject node s independence may be carried out by performing one or more of the following comparison process determining whether tenant s hosted by the subject node correspond to tenants hosted by the members of the group see block when the tenant s hosted by the subject node correspond to any of the tenants within the group comparing update domains assigned to the corresponding tenants against update domains assigned to the hosted tenant s see block when the update domains of the corresponding tenants match the update domains assigned to the hosted tenant s allowing the subject node to join the group see block and when the update domains of the corresponding tenants conflict with the update domains assigned to the hosted tenant s temporarily refraining from joining the subject node to the group see block . Further upon determining that the subject node qualifies as an independent node the method involves writing to a storage location e.g. listing enumerating a current membership of the group an indicia that the subject node is joined as a member of the group as depicted at block . Otherwise as depicted at block upon determining that the subject node is disqualified as an independent node the fabric controller may revisit at some later time e.g. expiration of a predetermined amount of time the subject node.

Turning now to a flow diagram is illustrated that shows an overall method for allocating nodes to role instances of service applications with consideration of update domains assigned to the role instances in accordance with an embodiment of the present invention. As depicted at block one step of the method may include propagating on distributed nodes of a data center various role instances that support the service applications presently running within the data center. An association between the nodes and the service applications running thereon may be written to a list as depicted at block . Further the nodes allocated to each of the role instances may be mapped against update domains assigned to each of the role instances as depicted at block .

In an exemplary embodiment the mechanism e.g. fabric controller carries out the method by employing the list and the mapping when allocating node s to an instance of a subject role as depicted at block . The step of allocating the node s to an instance of a subject role may include reading the list to identify role instances presently residing on a particular node see block and refraining from placing the subject role instance on the particular node when the subject role instance is analogous to the identified role instances see block .

The method may further involve the step of selecting an appropriate update domain to assign to the subject role that is placed on the allocated node s as depicted at block . Typically selection facilitates maximizing a number of nodes which include the allocated node that are updateable in tandem. In embodiments selecting the appropriate update domain to assign to the subject role includes one or more of the following steps reading the list to identify service applications having role instances presently residing on the allocated node see block reading the mapping to identify the update domains that are assigned to the role instances of the identified service applications see block ascertaining an update domain of the identified update domains that most frequently appears on nodes of the data center that host the role instances of the identified service applications see block and assigning the most frequently appearing update domain to the subject role see block .

Embodiments of the present invention have been described in relation to particular embodiments which are intended in all respects to be illustrative rather than restrictive. Alternative embodiments will become apparent to those of ordinary skill in the art to which embodiments of the present invention pertain without departing from its scope.

From the foregoing it will be seen that this invention is one well adapted to attain all the ends and objects set forth above together with other advantages which are obvious and inherent to the system and method. It will be understood that certain features and sub combinations are of utility and may be employed without reference to other features and sub combinations. This is contemplated by and is within the scope of the claims.

