---

title: Policy based service management in a clustered network storage system
abstract: A clustered network storage system includes a policy based storage efficiency management framework to allow users to more efficiently manage storage efficiency attributes on a plurality of storage objects in the clustered network storage system. The storage efficiency management framework includes a policy database for storing a plurality of polices, each of which includes a plurality of attributes. Each storage object in the clustered network storage system is assigned a policy. Storage efficiency operations can then be initiated by a storage efficiency engine according to the policies assigned to the storage objects by sending a single trigger to each node in the clustered network storage system and the storage efficiency engine determining on which storage objects to perform the storage efficiency operations.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09479585&OS=09479585&RS=09479585
owner: NetApp, Inc.
number: 09479585
owner_city: Sunnyvale
owner_country: US
publication_date: 20100713
---
At least one embodiment of the present invention pertains to network storage systems and more particularly to policy based management of storage efficiency operations of a node of a clustered network storage system.

A storage controller is a physical processing device that is used to store and retrieve data on behalf of one or more hosts. A network storage controller can be configured e.g. by hardwiring software firmware or any combination thereof to operate as a storage server that serves one or more clients on a network to store and manage data in a set of mass storage devices such as magnetic or optical storage based disks tapes or flash memory. Some storage servers are designed to service file level requests from hosts as is commonly the case with file servers used in a network attached storage NAS environment. Other storage servers are designed to service block level requests from hosts as with storage servers used in a storage area network SAN environment. Still other storage servers are capable of servicing both file level requests and block level requests as is the case with certain storage servers made by NetApp Inc. of Sunnyvale Calif. employing the Data ONTAP storage operating system.

As storage systems become larger in order to accommodate the need for more capacity several problems arise. Particularly the efficient use of storage space becomes more difficult. One problem in particular is duplicate data. A typical data volume can contain thousands or even millions of duplicate data objects. As data is created distributed backed up and archived many duplicate data objects are commonly created as an incidental result of these operations. The end result is inefficient utilization of data storage resources. Deduplication operations identify and eliminate the undesired duplicate data objects. Commonly this is done by deleting all but one copy of a given data object and replacing all duplicates of that data object with a reference to the singe remaining data object. Compression operations reduce the amount of physical storage space used to store a particular data segment. Storage efficiency operations such as deduplication and compression provide a benefit in storage space efficiency. The result can be reduced operation cost due to longer intervals between storage capacity upgrades and more efficient management of stored data.

A network storage system can have a simple architecture for example an individual storage server can provide one or more clients on a network with access to data stored in a mass storage subsystem. Recently however with storage capacity demands increasing rapidly in almost every business sector there has been a trend towards the use of clustered network storage systems to improve scalability.

In a clustered storage system two or more storage server nodes are connected in a distributed architecture. The nodes are generally implemented by two or more storage controllers. Each storage server node is in fact a storage server although it is implemented with a distributed architecture. For example a storage server node can be designed to include a network module N module to provide network connectivity and a separate data module e.g. D module to provide data storage and data access functionality where the N module and D module communicate with each other over some type of physical interconnect. Two or more such storage server nodes are typically connected to form a storage cluster where each of the N modules in the cluster can communicate with each of the D modules in the cluster.

A clustered architecture allows convenient scaling through the addition of more N modules and D modules all capable of communicating with each other. Further a storage cluster may present a single system image of stored data to clients and administrators such that the actual location of data can be made transparent to clients and administrators. An example of a storage controller that is designed for use in a clustered system such as this is a storage controller employing NetApp s Data ONTAP GX storage operating system.

Efficient use of storage space is also a concern in a clustered storage system and in fact the problem can even be magnified due to the distributed architecture of the clustered storage system. A large cluster can have dozens or even hundreds of nodes containing tens of thousands of volumes. Because of the distributed architecture the storage that a client accesses may not all be controlled by the same D module. Further a single D module can control storage accessed by multiple clients and managed by administrators in multiple locations. Storage efficiency operations e.g. deduplication and compression can be performed by the D module to improve the way storage space is used. An administrator may request storage efficiency operations to be performed by a number of D modules which are responsible for maintaining the storage devices associated with a client.

Configuring storage efficiency operations for a volume an abstraction of physical storage devices typically involves manually assigning a large number of attributes to the volume. For example a deduplication option whether data on the volume should be deduplicated a compression option whether data on the volume should be compressed a compression and or deduplication schedule a duration of the compression and or deduplication operation an operation type background vs. foreground etc. can be set for each volume. These attributes can be set for a particular volume depending on various factors such as the expected type of workload performance requirements characteristics of the data set availability of CPU power backup schedules etc.

With conventional technology all of these attributes are assigned individually for each volume in the storage system. This is true even in cases where the same configuration is valid for multiple volumes. Thus scalability challenges arise in administering storage efficiency operations in a clustered network storage environment. A large cluster can include tens of thousands of volumes. Individually configuring storage efficiency attributes of storage efficiency operations for such a large number of volumes can be very time consuming and burdensome.

The techniques introduced here provide a way to encapsulate storage efficiency operation attributes that are relevant to multiple storage objects e.g. volumes in a single policy so as to facilitate a more efficient process for assigning storage efficiency operation attributes. A single instance of this policy can then be assigned to each volume in the cluster. Assigning the same policy to multiple volumes enables a consistent storage efficiency strategy to be carried out for each of the volumes without requiring separate configuration of each volume. Configuring a new volume with an existing policy enables the new volume to automatically inherit all of the storage efficiency operation attributes of the policy. If the storage efficiency requirements change only a single policy needs to be changed and the change is automatically propagated to every volume to which that policy is assigned.

Further policy based management can also be integrated with a role based access control RBAC model for example where an administrator of a virtual server has read only access to a predefined set of policies created by a cluster administrator. This allows a cluster administrator to delegate the task of managing storage efficiency operations to one or more virtual server administrators while maintaining the necessary constraints for efficient cluster management.

The techniques introduced here also provide an efficient method for initiating policy related actions. Communication efficiency is achieved by sending a single trigger message per node for communications between a management module and the volumes associated with a policy. The trigger message is sent to the data module of each node in the clustered network storage system where the data module determines based on the content of the trigger message which volumes to run storage efficiency operations on. This approach improves CPU and bandwidth utilization and is particularly advantageous when the communication takes place over a wide area network WAN .

Other aspects of the techniques summarized above will be apparent from the accompanying figures and from the detailed description which follows.

References in this specification to an embodiment one embodiment or the like mean that the particular feature structure or characteristic being described is included in at least one embodiment of the present invention. Occurrences of such phrases in this specification do not necessarily all refer to the same embodiment.

The storage server can be for example one of the FAS series of storage server products available from NetApp Inc. The client systems are connected to the storage server via the computer network which can be a packet switched network for example a local area network LAN or wide area network WAN . Further the storage server can be connected to the disks via a switching fabric not shown which can be a fiber distributed data interface FDDI network for example. It is noted that within the network data storage environment any other suitable number of storage servers and or mass storage devices and or any other suitable network technologies may be employed.

The storage server can make some or all of the storage space on the disk s available to the client systems in a conventional manner. For example each of the disks can be implemented as an individual disk multiple disks e.g. a RAID group or any other suitable mass storage device s . The storage server can communicate with the client systems according to well known protocols such as the Network File System NFS protocol or the Common Internet File System CIFS protocol to make data stored on the disks available to users and or application programs. The storage server can present or export data stored on the disks as volumes and or qtrees to each of the client systems . Various functions and configuration settings of the storage server can be controlled by a user e.g. a storage administrator from a management station coupled to the network .

The storage server can include a storage efficiency framework to manage the storage efficiency operations on the disks . This storage efficiency framework can include a storage efficiency engine not shown to perform for example deduplication and compression operations to more efficiently use storage space on the disks.

Each node essentially provides similar functionality to that of a storage server in . However unlike the storage servers in the nodes are not operated and managed as distinct independent entities. Rather they are operated and managed collectively as a single entity i.e. a cluster . The cluster presents to users and administrators a single system image of all data stored by the cluster regardless of where any particular data resides within the cluster.

Each of the nodes is configured to include several modules including an N module a D module and an M host each of which can be implemented by using a separate software module and an instance of a replicated database RDB . RDB can be implemented as a number of individual databases each of which has an instance located in each of the nodes . The N modules include functionality that enables their respective nodes to connect to one or more of the client systems over the network while the D modules provide access to the data stored on their respective disks . The M hosts provide management functions for the clustered storage server system including user interface functionality to enable an administrator to manage and control the cluster. Accordingly each of the server nodes in the clustered storage server arrangement provides the functionality of a storage server.

The RDB is a database that is replicated throughout the cluster i.e. each node includes an instance of the RDB . The various instances of the RDB are updated regularly to bring them into synchronization with each other. The RDB provides cluster wide storage of various information used by all of the nodes and includes a volume location database VLDB . The VLDB indicates the location within the cluster of each volume of data in the cluster i.e. the owning D module for each volume and is used by the N modules to identify the appropriate D module for any given volume to which access is requested. Each volume in the system is represented by a data set identifier DSID and a master data set identifier MSID each of which is stored in two places on disk in the volume itself and in the VLDB. The DSID is a system internal identifier of a volume. The MSID is an external identifier for a volume used in file handles e.g. NFS and the like. The VLDB stores the identity and mapping MSIDs to DSIDs of all volumes in the system.

The nodes are interconnected by a cluster switching fabric which can be embodied as a Gigabit Ethernet switch for example. The N modules and D modules cooperate to provide a highly scalable distributed storage system architecture of a clustered computing environment implementing exemplary embodiments of the present invention. Note that while there is shown an equal number of N modules and D modules in there may be differing numbers of N modules and or D modules in accordance with various embodiments of the technique described here. For example there need not be a one to one correspondence between the N modules and D modules. As such the description of a node comprising one N module and one D module should be understood to be illustrative only.

Various functions and configuration settings of the cluster can be controlled by a user e.g. a storage administrator from a management station coupled to the network . A plurality of virtual interfaces VIFs allow the disks associated with the nodes to be presented to the client systems as a single shared storage pool. depicts only the VIFs at the interfaces to the N modules for clarity of illustration.

The storage controller can be embodied as a single or multi processor storage system executing a storage operating system that preferably implements a high level module called a storage manager to logically organize the information as a hierarchical structure of named directories files and special types of files called virtual disks hereinafter generally blocks on the disks. Illustratively one processor can execute the functions of the N module on the node while another processor executes the functions of the D module .

The memory illustratively comprises storage locations that are addressable by the processors and adapters for storing software program code and data associated with the present invention. The processor and adapters may in turn comprise processing elements and or logic circuitry configured to execute the software code and manipulate the data structures. The storage operating system portions of which are typically resident in memory and executed by the processing elements functionally organizes the storage controller by among other things invoking storage operations in support of the storage service provided by the node . It will be apparent to those skilled in the art that other processing and memory implementations including various computer readable storage media may be used for storing and executing program instructions pertaining to the technique introduced here.

The network adapter includes a plurality of ports to couple the storage controller to one or more clients over point to point links wide area networks virtual private networks implemented over a public network Internet or a shared local area network. The network adapter thus can include the mechanical electrical and signaling circuitry needed to connect the storage controller to the network . Illustratively the network can be embodied as an Ethernet network or a Fibre Channel FC network. Each client can communicate with the node over the network by exchanging discrete frames or packets of data according to pre defined protocols such as TCP IP.

The storage adapter cooperates with the storage operating system to access information requested by the clients . The information may be stored on any type of attached array of writable storage media such as magnetic disk or tape optical disk e.g. CD ROM or DVD flash memory solid state drive SSD electronic random access memory RAM micro electro mechanical and or any other similar media adapted to store information including data and parity information. However as illustratively described herein the information is stored on disks . The storage adapter includes a plurality of ports having input output I O interface circuitry that couples to the disks over an I O interconnect arrangement such as a conventional high performance Fibre Channel FC link topology.

Storage of information on disks can be implemented as one or more storage volumes that include a collection of physical storage disks cooperating to define an overall logical arrangement of volume block number VBN space on the volume s . The disks can be organized as a RAID group. One or more RAID groups together form an aggregate. An aggregate can contain one or more volumes file systems.

The storage operating system facilitates clients access to data stored on the disks . In certain embodiments the storage operating system implements a write anywhere file system that cooperates with one or more virtualization modules to virtualize the storage space provided by disks . In certain embodiments a storage manager logically organizes the information as a hierarchical structure of named directories and files on the disks . Each on disk file may be implemented as set of disk blocks configured to store information such as data whereas the directory may be implemented as a specially formatted file in which names and links to other files and directories are stored. The virtualization module s allow the storage manager to further logically organize information as a hierarchical structure of blocks on the disks that are exported as named logical unit numbers LUNs .

The network protocol stack in the N module includes a network access layer which includes one or more network drivers that implement one or more lower level protocols to enable the processing system to communicate over the network such as Ethernet Internet Protocol IP Transport Control Protocol Internet Protocol TCP IP Fibre Channel Protocol FCP and or User Datagram Protocol Internet Protocol UDP IP . The network protocol stack also includes a multi protocol layer which implements various higher level network protocols such as Network File System NFS Common Internet File System CIFS Hypertext Transfer Protocol HTTP Internet small computer system interface iSCSI etc. Further the network protocol stack includes a cluster fabric CF interface module which implements intra cluster communication with D modules and with other N modules.

In addition the storage operating system includes a set of data access layers organized to provide data paths for accessing information stored on the disks of the node these layers in combination with underlying processing hardware forms the D module . To that end the data access layers include a storage manager module that manages any number of volumes a RAID system module and a storage driver system module .

The storage manager primarily manages a file system or multiple file systems and serves client initiated read and write requests. The RAID system manages the storage and retrieval of information to and from the volumes disks in accordance with a RAID redundancy protocol such as RAID 4 RAID 5 or RAID DP while the disk driver system implements a disk access protocol such as SCSI protocol or FCP.

The data access layers also include a cluster fabric CF interface module to implement intra cluster communication link with N modules and or other D modules. The nodes in the cluster can cooperate through their respective CF interface modules and to provide a single file system image across all D modules in the cluster . Stated another way the various D modules can implement multiple distinct file systems within a single global namespace. A namespace in this context is a set of names that map to named objects e.g. files directories logical units . Global means that each name is independent of the physical location of the corresponding object. A global namespace therefore applies at least across an entire cluster and potentially can be extended to apply to multiple clusters. Thus any N module that receives a client request can access any data container within the single file system image located on any D module of the cluster and the location of that data container can remain transparent to the client and user.

The CF interface modules implement the CF protocol to communicate file system commands among the modules of cluster over the cluster switching fabric . Such communication can be effected by a D module exposing a CF application programming interface API to which an N module or another D module issues calls. To that end a CF interface module can be organized as a CF encoder decoder. The CF encoder of e.g. CF interface on N module can encapsulate a CF message as i a local procedure call LPC when communicating a file system command to a D module residing on the same node or ii a remote procedure call RPC when communicating the command to a D module residing on a remote node of the cluster. In either case the CF decoder of CF interface on D module de encapsulates the CF message and processes the file system command.

In operation of a node a request from a client is forwarded as a packet over the network and onto the node where it is received at the network adapter . A network driver of layer processes the packet and if appropriate passes it on to a network protocol and file access layer for additional processing prior to forwarding to the storage manager . At that point the storage manager generates operations to load retrieve the requested data from disk if it is not resident in memory . If the information is not in memory the storage manager indexes into a metadata file to access an appropriate entry and retrieve a logical VBN. The storage manager then passes a message structure including the logical VBN to the RAID system the logical VBN is mapped to a disk identifier and disk block number DBN and sent to an appropriate driver e.g. SCSI of the disk driver system . The disk driver accesses the DBN from the specified disk and loads the requested data block s in memory for processing by the node. Upon completion of the request the node and operating system returns a reply to the client over the network .

The data request response path through the storage operating system as described above can be implemented in general purpose programmable hardware executing the storage operating system as software or firmware. Alternatively it can be implemented at least partially in specially designed hardware. That is in an alternate embodiment of the invention some or all of the storage operating system is implemented as logic circuitry embodied within a field programmable gate array FPGA or an application specific integrated circuit ASIC for example.

The N module and D module can be implemented as processing hardware configured by separately scheduled processes of storage operating system . In an alternate embodiment the modules may be implemented as processing hardware configured by code within a single operating system process. Communication between an N module and a D module is thus illustratively effected through the use of message passing between the modules although in the case of remote communication between an N module and D module of different nodes such message passing occurs over the cluster switching fabric . A known message passing mechanism provided by the storage operating system to transfer information between modules processes is the Inter Process Communication IPC mechanism. The protocol used with the IPC mechanism is illustratively a generic file and or block based agnostic CF protocol that comprises a collection of methods functions constituting a CF API.

The operating system also includes M host which provides management functions for the cluster including user interface functionality to enable an administrator to manage and control the cluster e.g. through management station . Because each node in the cluster has an M host the cluster can be managed via the M host in any node in the cluster . The functionality of an M host includes generating a user interface such as a graphical user interface GUI and or a command line interface CLI for a storage network administrator. The functionality of an M host can also include facilitating the provisioning of storage creating and destroying volumes installation of new software controlling access privileges scheduling and configuring data backups storage efficiency management scheduling data mirroring function and other functions. The M host communicates with its local D module or with any remote D module by using a set of APIs. The M host includes network interfaces not shown to communicate with D modules and to communicate with one or more external computers or input output terminals used by network administrators.

As described above with reference to a non clustered network storage system each D module of the clustered network storage system of includes a storage efficiency engine . Each storage efficiency engine can perform for example deduplication and or compression operations to more efficiently use storage space on the storage devices not shown owned by D module . The goal of deduplication is to ensure that there is no more than one instance of a particular data segment stored in a volume thereby reducing the used storage space. The goal of compression is to reduce the amount of physical storage space used to store a particular data segment. These tasks are significantly more difficult in a clustered network storage system as a volume may be spread over multiple storage devices owned by multiple D modules in the cluster. The distributed architecture of a clustered network storage system also makes it difficult and time consuming for a network administrator to configure storage efficiency attributes for each volume.

Configuring storage efficiency operations on a volume typically involves manually assigning a large number of storage efficiency operation attributes to the volume. Storage efficiency attributes include a deduplication option whether data should be deduplicated a compression option whether data should be compressed a compression and or deduplication schedule a duration or stop time of a compression and or deduplication operation an operation type e.g. background foreground quality of service requirements etc. Determining what attribute values should be assigned to a particular volume depends on various factors such as the type of workload performance requirements characteristics of the data set availability of CPU power backup schedules etc. With conventional technology all of these attributes need to be assigned individually per volume. This is true even in cases where the same configuration is valid for multiple volumes. This introduces scalability challenges in administering storage efficiency in a cluster environment.

The techniques introduced here provide a method to encapsulate or group together for ease of distribution storage efficiency attributes which are relevant to multiple storage objects in the cluster in a single policy. A single instance of this policy can then be assigned to each storage object in the cluster. While a policy can be assigned to any type of storage object in the cluster e.g. volume aggregate LUN file etc. for ease of explanation the storage object is assumed to be a volume. Assigning the same policy to multiple volumes ensures that a consistent storage efficiency scheme is applied to each volume. Configuring a new volume with an existing policy ensures that the new volume automatically inherits all the attributes of the existing policy. The new volume is thus configured with the same storage efficiency attributes as the other volumes to which the policy is assigned and therefore also consistently applies the storage efficiency scheme. If the storage efficiency requirements change only a single policy needs to be changed. The change is automatically propagated to every volume to which that policy has been assigned.

Assigning a single policy to each volume instead of having to assign a number of storage efficiency operation attributes to each volume to implement storage efficiency operations greatly simplifies a network administrator s job. For example if performing compression operations requires more CPU power a single policy can be created with the compression option turned on and assigned to all volumes belonging to nodes having a multi core CPU. Similarly all volumes that have the same backup schedule can be assigned a policy specifying a convenient deduplication and or compression schedule and duration. This approach allows a large number of volumes to be grouped and collectively managed for storage efficiency purposes freeing up valuable time of the network administrator.

As shown in an M host includes a job manager and a policy database . After a user has created a policy the policy database stores the policy. The policy database can be included in the replicated database and can thus be manageable from any node in the cluster. The job manager triggers a storage efficiency operation on the volumes of the clustered network storage system using a trigger message sent from a single M host to each node of the clustered network storage system.

The D module includes a data structure which includes storage efficiency related information for each of the volumes owned by the D module . The storage efficiency related information includes policy information e.g. a mapping of which policy is assigned to which volume s . The policy information can also include storage efficiency operation attribute values. The policy information can also be stored in the volume itself. Volumes in each node are maintained by the D module in that node. The interaction between the elements of the storage efficiency framework as depicted in will be described in connection with the following examples of creating and configuring policies and carrying out storage efficiency operations according to the policies assigned to each volume.

Initially at a number of policies are stored in a clustered network storage system. At multiple volumes in the clustered network storage system are assigned a policy. Finally at a job manager in response to a scheduled event initiates a storage efficiency operation by using a trigger message sent from a single M host to each node in the clustered network storage system. A more detailed description of the process follows.

Each of the policies stored in the clustered network storage system includes a number of storage efficiency attributes as described below. The policies are stored in a policy database which can be included in the replicated database of the M host . The policy database is available to all nodes of the cluster and allows management of the storage efficiency policies and attributes to be performed from any node in the cluster.

A user e.g. a network administrator can create a policy and cause it to be stored through several interfaces provided by the storage efficiency management framework. For example a user can use a command line interface CLI command an application programming interface API or a GUI e.g. a web interface to a node to create a policy. The specific details of such interfaces are not germane to the technique introduced here. The user through the interface can set the values for the attributes included in the policy. These attributes can include a policy identifier e.g. name ID number etc. what storage efficiency operations are to be carried out under the policy a policy schedule for such operations and a duration or stop time for the operation. The user as previously described can manage the storage efficiency policies from any node in the cluster via its M host .

An illustrative policy created by a user can include the following policy attribute values. The policy identifier is assumed herein to be a policy name for ease of explanation however any policy identifier can be used. The policy name attribute can be descriptive of the time at which the associated operations are to run for example nightly or the policy name can be an arbitrary character string decided by the user. In the example of the nightly policy the value for a deduplication operation attribute could be set to yes and the value for a compression operation attribute could be set to no . A combination of storage efficiency operations can thus be selected in a single policy. The value for the schedule attribute in the example nightly policy could be for example a specific day and or time or it could be a time interval such as daily nightly etc. Similarly a value for the duration attribute of the storage efficiency operation for example three hours or a stop time 3 00 A.M. can be included in the nightly policy. The nightly policy contains various attributes specified once by a user and can now be assigned to multiple volumes by the assignment of the policy without the user having to specify each attribute value again. A policy can include other attributes not specified in the example above.

After the user has input the policy attributes the M host of the node where the user creates the policy checks the policy database to determine whether a policy by the same name has already been created. If there is no policy by the same name the M host creates a new entry in the policy database . If there is already a policy in the policy database with the same name an error message can be displayed to the user prompting the user to change the policy name and then the M host can create a new entry in the policy database with the new name.

The job manager maintains a schedule of jobs associated with various policies. This schedule can be part of the replicated database which is available to all nodes in the cluster. The M host checks with its local job manager to determine whether a job matching the policy schedule attributes assigned to the new policy exists in the job schedule. If there is a job with a matching schedule the job manager adds the new policy to the job with that schedule. If there is not currently a job that matches the policy schedule attributes in the new policy the job manager creates a new job and includes the new job in the schedule to run according to the policy schedule attributes. In one embodiment a default job can be included in the schedule for the user for example a job whereby the policies associated with the job will be triggered every day at midnight. In one embodiment the job manager can only delete a job from the schedule if it is not referenced by a storage efficiency policy regardless of whether that policy is currently assigned to any volumes.

Once a policy has been created the user can assign multiple volumes in the clustered network storage system that policy. This step saves a network administrator considerable time and effort because the policy contains all of the storage efficiency operation attributes that would otherwise have to be individually assigned to each volume. A mapping of the policies to the volumes maintained in each node is stored at the D module in each node.

A user can assign any policy from the policy database to any volume in the cluster from any M host in the cluster. For example a user can assign the nightly policy from the above example to volume from M host or along with any other M host in the cluster. The user can assign the storage efficiency policy nightly on volume through a CLI command an API or a web interface. When a user assigns a policy to a volume the M host checks with the volume location database VLDB not shown to determine the location of the volume i.e. which D module owns that volume. The M host then sends a message to the node where the volume is located. The message includes the policy and can also include the policy attribute values. The D module which owns the volume receives the message through an API and records the policy for the volume in its data structure . The D module can also write the policy information to disk in the volume in a storage efficiency metafile.

After a policy has been assigned to a volume a user can modify the policy. The user can modify the policy for volume using a CLI command an API or the web interface. Assume a user creates a second policy daily and wishes to assign this new policy to volume . The M host checks the VLDB to determine the location of the volume and sends a message to the node where the volume is located. The message includes the volume and the new policy which is being assigned to that volume. The D module of the node where the volume is located changes the entry in the data structure from the nightly policy to the daily policy. The D module can also record the change in the storage efficiency metafile in the volume .

Along with user defined policies the policy based storage efficiency framework can include a number of predefined policies. For example a default policy an auto policy and a blank policy. The default policy can include a default set of attributes and can run on a default schedule. This default policy can be assigned to a volume whenever storage efficiency operations are enabled on the volume but no custom policy has been assigned. The auto policy may be triggered based on the amount of data that has been changed on a volume or another event that would trigger storage efficiency operations. The blank policy could be used by a network administrator if they do not want storage efficiency operations to run on the volume.

If a policy is no longer being used a user can delete the policy. If a policy that is currently assigned to a volume is deleted the storage efficiency operations on that volume would not be performed until the volume is assigned a new policy or a default policy can be applied. However in one embodiment a policy cannot be deleted if it is assigned to any volume.

In response to a scheduled event for example a scheduled job a job manager in the cluster that has sufficient resources available initiates a storage efficiency operation by sending a trigger message from a single M host to each node in the clustered network storage system. Sending a single trigger message per node improves CPU and bandwidth utilization when compared to the conventional method of sending a trigger message for each volume in the node and is particularly advantageous when the communication takes place over a wide area network WAN . For example if a trigger message was sent for each volume in a node containing several hundred volumes several hundred trigger messages would be sent to the node at the same time and consume a large amount of bandwidth. However by sending a single trigger message to each node including which policy the storage efficiency operation is to be operated for only a small fraction of the bandwidth is consumed.

A single trigger message being sent to each node is depicted in . As shown in the job manager is initiating the storage efficiency operation. The arrow represents a single trigger message sent from the job manager to each node in the cluster. Thus in the example of only three trigger messages would need to be sent from the job manager to initiate storage efficiency operations on potentially hundreds or even thousands of volumes in the clustered network storage system. Using conventional methods hundreds or even thousands of trigger messages would need to be sent to initiate the same storage efficiency operations.

The trigger message contains the information necessary for the D module to determine which volumes the storage efficiency operation is initiated for. For example the trigger message can contain the policy name associated with the scheduled storage efficiency operation. The D module can then determine from the information stored in the data structure which volumes have that policy assigned to them and the operation can be carried out on those volumes.

If the D module determines that it owns a volume that has the policy designated in the trigger message assigned to it and if the D module has the resources available to execute the storage efficiency operation the storage efficiency engine executes the operation. However if the D module is busy with other operations the scheduler on the D module places the storage efficiency operation in a job queue for execution when the D module resources become available.

The process A begins at with a user creating a policy In this example assume the user has created three policies P P and P with schedules S S and S respectively.

As part of creating a policy the process A continues at with the job manager scheduling a job. In this example the job manager will schedule one job for schedule S which is associated with policies P and P and another job for schedule S which is associated with P. In one embodiment multiple schedules can be included in a single policy e.g. one for deduplication operations and one for compression operations.

The process continues to where the user assigns a policy to a volume . In this example assume that policy P has been assigned to volume policy P has been assigned to volume and policy P has been assigned to volume .

The process begins with decision block where the job manager waits for the schedule time to arrive. When the schedule time S arrives Yes a job manager in one of the nodes in the cluster that has sufficient resources for example job manager initiates a job at to manage the operations included in the policies associated with schedule S. However the job can be run on any node in the cluster. This feature allows the job manager to run the job on a node that has resources available and thus provides for load balancing. Further if the node on which the job is running fails the job can automatically be taken over by another node in the cluster and restarted. The job run by the job manager checks with the policy database and gets a list of all of the policies associated with schedule S in this example policies P and P. In one embodiment two separate jobs may be created for a single policy for example one job for deduplication operations and one job for compression operations.

The process B continues at with the job manager initiating a storage efficiency operation associated with scheduled policy by using a trigger message sent to each node in the clustered network storage system as shown in . The trigger message includes an indication of one or more policies for which storage efficiency operations are to be run. In this example the trigger message would include information indicating that the storage efficiency operations associated with policies P and P are to be performed. In one embodiment the trigger message includes the type of storage efficiency operation s to be performed according to the policy.

At in response to receiving the trigger message the storage efficiency engine of each node checks with its local data structure to determine if the policy information in the trigger message matches a volume it services and what storage efficiency operations are associated with that policy. In this example storage efficiency engine executes a storage efficiency operation on volume with the attributes included in policy P. Similarly storage efficiency engine executes a storage efficiency operation on volume with the attributes included in policy P. However while the data module receives the trigger as well no storage efficiency operation is executed by the storage efficiency engine because no volume on that node is assigned a policy with schedule S.

Initially at each D module in the cluster receives a trigger from a job manager to perform the operation s associated with the policy on the volumes to which that policy is assigned. In one embodiment the duration is included in the trigger message. In another embodiment the duration is stored in the data structure along with the other policy attributes and is not included in the trigger message. In response to receiving the trigger the D module initializes a duration counter at . The duration counter begins from the time the operation is triggered even if the operation enters a queue due to the D module being busy.

At a scheduler in the D module checks to determine whether the D module has resources available to execute the operation. If the D module has sufficient resources available Yes then at the storage efficiency engine begins execution of the operation. However if the D module does not have sufficient resources available to execute the operation No then the operation is placed in a job queue at to wait for sufficient resources to become available.

After the operation has been queued at the scheduler continuously checks the duration counter to determine whether the duration has expired. If the duration has not expired No then at the scheduler checks to see if sufficient resources have become available to execute the operation. If sufficient resources have become available then at the operation is executed by the storage efficiency engine . However if sufficient resources to execute the operation remain unavailable then the operation remains in the queue and the process above is repeated. If the duration has expired Yes the operation is removed from the queue at and remains unexecuted until the next scheduled process time.

Once an operation has begun execution the scheduler continuously checks the duration counter at to determine whether the duration has expired. If the duration has not expired No the storage efficiency engine continues to execute the operation until either the operation is completed or the duration expires Yes. If the duration expires prior to the operation being completed at the execution of the operation is stopped. If the operation is stopped before it was completed the state of the operation can be saved and the operation can begin at the point where it left off when the next scheduled operation takes place.

In one embodiment a stop time attribute can be substituted for the duration attribute. This embodiment would use the schedule time as a start time and the stop time attribute as a stop time. The operation would be executed between the start time or schedule time and the stop time.

The system of includes a plurality of client systems a plurality of virtual servers and implemented in a clustered network storage system and a computer network connecting the client systems and the clustered network storage system. As shown in each virtual server includes an N module an M host and can include volumes on a plurality of D modules which communicate through the cluster switching fabric . Each virtual server shown in this figure is associated with a separate M host but owns volumes on the plurality of common D modules .

The policy based storage efficiency framework as described above with reference to works essentially the same way in the virtual server environment shown in . However policy based management can also be integrated with a role based access control RBAC model where a virtual server administrator has read only access to a predefined set of storage efficiency policies created by the cluster administrator and cannot create new policies. This allows the cluster administrator to delegate the task of managing storage efficiency to one or more virtual server administrators while maintaining necessary constraints to ensure efficient cluster performance. For example a cluster administrator can create policies which are scheduled to run during off peak times when client traffic is low. Thus a virtual server administrator can assign only policies that are restricted to run storage efficiency operations on a volume in such a way that does not interfere with client traffic.

In another example because some storage efficiency operations require more processing power a cluster administrator can create policies which have those operations disabled. These policies can then be assigned by a virtual server administrator managing volumes on a primary system which has stricter performance requirements such that the performance of the system is not degraded.

The techniques introduced above can be implemented by programmable circuitry programmed or configured by software and or firmware or they can be implemented by entirely by special purpose hardwired circuitry or in a combination of such forms. Such special purpose circuitry if any can be in the form of for example one or more application specific integrated circuits ASICs programmable logic devices PLDs field programmable gate arrays FPGAs etc.

Software or firmware for implementing the techniques introduced here may be stored on a machine readable storage medium and may be executed by one or more general purpose or special purpose programmable microprocessors. A machine readable medium as the term is used herein includes any mechanism that can store information in a form accessible by a machine a machine may be for example a computer network device cellular phone personal digital assistant PDA manufacturing tool any device with one or more processors etc. . For example a machine accessible medium includes recordable non recordable media e.g. read only memory ROM random access memory RAM magnetic disk storage media optical storage media flash memory devices etc. etc.

The term logic as used herein can include for example special purpose hardwired circuitry software and or firmware in conjunction with programmable circuitry or a combination thereof.

Although the present invention has been described with reference to specific exemplary embodiments it will be recognized that the invention is not limited to the embodiments described but can be practiced with modification and alteration within the spirit and scope of the appended claims. Accordingly the specification and drawings are to be regarded in an illustrative sense rather than a restrictive sense.

