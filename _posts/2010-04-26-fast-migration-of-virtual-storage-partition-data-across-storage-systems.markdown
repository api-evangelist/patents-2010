---

title: Fast migration of virtual storage partition data across storage systems
abstract: A method includes reading a superblock of a read-only replica of a source virtual volume in a source virtual storage partition associated with a source aggregate of a source storage system at the destination storage system, modifying the superblock of the read-only replica in a memory of the destination storage system, and associating the modified superblock with one or more virtual volume block number(s) configured to be previously associated with the superblock of the read-only replica of the source virtual volume without initiating a destination consistency point (DCP) at the destination storage system to render the destination virtual volume writable. The method also includes modifying a disk group label to reflect an association of the destination storage disk with the writable destination virtual volume, and initiating the DCP to ensure that the modified superblock and the modified disk group label are flushed to the destination storage disk.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08683152&OS=08683152&RS=08683152
owner: Netapp, Inc.
number: 08683152
owner_city: Sunnyvale
owner_country: US
publication_date: 20100426
---
This application claims priority from Indian provisional application number 626 CHE 2010 titled FAST MIGRATION OF VIRTUAL STORAGE PARTITION DATA ACROSS STORAGE SYSTEMS filed on Mar. 10 2010.

This disclosure relates generally to data migration and more particularly to a method an apparatus and a system of migration of virtual storage partition data between storage systems.

Data migration between storage systems e.g. from a source storage system to a destination storage system may be important during processes such as disaster recovery load balancing system migration and or remote read only data access provisioning. During data migration data in volumes of virtual storage partitions on a source storage system may be mirrored in virtual storage partitions on the destination storage system.

To make a mirrored volume writable relationships between a source data and a minor data at the destination storage system e.g. relationships enforced by a replication engine associated with a data replication process may need to be broken. The process of breaking the relationships between the source data and the minor data at the destination storage system may involve time consuming sub processes e.g. consistency points CPs registry updates . Furthermore the process of breaking the relationships between the source data and the mirror data at the destination may involve operating one volume at a time.

When the time consumed in the abovementioned processes exceeds time limits permitted by application time outs the data migration may lead to application downtime. The inherent latencies in the abovementioned data migration process may result in loss of revenue and or reduced productivity. As a result productivity may suffer and or revenues may be lost.

Disclosed are a method an apparatus and a system of migration of virtual storage partition data between storage systems.

In one aspect a method includes reading through a file system at a destination storage system a superblock of a read only replica of a source virtual volume in a source virtual storage partition associated with a source aggregate of a source storage system at the destination storage system. The read only replica of the source virtual volume is transferred from the source storage system to a destination virtual storage partition associated with a destination aggregate at the destination storage system through a replication engine associated with the source storage system and the destination storage system.

The source virtual volume and the read only replica at the destination virtual storage partition are respectively abstracted from an underlying source storage disk associated with the source storage system and an underlying destination storage disk associated with the destination storage system through the source aggregate and the destination aggregate inside which the source virtual volume and a destination virtual volume signifying the read only replica are created. The source virtual storage partition and the destination virtual storage partition are respectively secure logical partitions of the source storage system and the destination storage system created by an operating system associated therewith.

The method also includes modifying the superblock of the read only replica in a memory of the destination storage system to clear a replica flag associated therewith through the replication engine and associating through the file system at the destination storage system the modified superblock with one or more virtual volume block number s virtual VBNs configured to be previously associated with the superblock of the read only replica of the source virtual volume of the source storage system without initiating a destination consistency point DCP at the destination storage system to render the destination virtual volume writable.

The virtual VBN is configured to index a virtual volume level version of block allocation files of a virtual volume including the source virtual volume and or the destination virtual volume and the DCP defines a process during which an operating system associated with the destination storage system flushes data changes in the memory of the destination storage system to the destination storage disk associated with the destination storage system.

Further the method includes modifying a disk group label indicating an association of the destination storage disk with the read only replica of the source virtual volume of the source storage system to reflect an association of the destination storage disk with the writable destination virtual volume and initiating the DCP to ensure that the modified superblock and the modified disk group label associated with the writable destination virtual volume are flushed to the destination storage disk.

In another aspect a method includes freezing in a number of destination virtual volumes of a destination virtual storage partition associated with a destination aggregate of a destination storage system each destination virtual volume signifying a read only replica of a corresponding source virtual volume in a source virtual storage partition associated with a source aggregate of a source storage system at the destination storage system through a file system associated therewith.

The freezing is configured to queue a subsequent external request through a client device to access data associated with the each destination virtual volume. The read only replica of the corresponding source virtual volume is transferred from the source storage system to the each destination virtual volume through a replication engine associated with the source storage system and the destination storage system. The corresponding source virtual volume and the each destination virtual volume are respectively abstracted from an underlying source storage disk associated with the source storage system and an underlying destination storage disk associated with the destination storage system through the source aggregate and the destination aggregate inside which the corresponding source virtual volume and the each destination virtual volume are created.

The source virtual storage partition and the destination virtual storage partition are respectively secure logical partitions of the source storage system and the destination storage system created by an operating system associated therewith. The method also includes flushing data associated with the each destination virtual volume in a memory of the destination storage system to the destination storage disk reading through the file system at the destination storage system a superblock of the each destination virtual volume and modifying the superblock of the each destination virtual volume in the memory of the destination storage system to clear a replica flag associated therewith through the replication engine.

Further the method includes associating through the file system at the destination storage system the modified superblock with one or more virtual VBN s configured to be previously associated with the read only replica of the corresponding source virtual volume without initiating a DCP at the destination storage system to render the each destination virtual volume writable and modifying a disk group label indicating an association of the destination storage disk with the read only replica of the corresponding source virtual volume to reflect an association of the destination storage disk with the writable each destination virtual volume.

The virtual VBN is configured to index a virtual volume level version of block allocation files of a virtual volume including the corresponding source virtual volume and or the each destination virtual volume. The DCP defines a process during which an operating system associated with the destination storage system flushes data changes in the memory of the destination storage system to the destination storage disk associated with the destination storage system.

Still further the method includes initiating the DCP to ensure that the modified superblock and the modified disk group label associated with the writable each destination virtual volume are flushed to the destination storage disk remounting in parallel the writable each destination virtual volume in a thread associated therewith through the file system associated therewith and unfreezing the remounted writable each destination virtual volume through the file system associated therewith.

In yet another aspect a storage environment includes a destination storage system including a processor a memory and a destination aggregate implemented therein and a destination storage disk associated with the destination storage system. The memory includes storage locations configured to be addressable by the processor. A destination virtual volume of a destination virtual storage partition of the destination storage system signifies a read only replica of a source virtual volume in a source virtual partition of a source storage system.

The source virtual volume is configured to be abstracted from a source storage disk through a source aggregate configured to be associated with the source virtual storage partition and the destination virtual volume is configured to be abstracted from the destination storage disk through the destination aggregate configured to be associated with the destination virtual storage partition. The read only replica of the source virtual volume is transferred from the source storage system to the destination storage system through a replication engine associated with the source storage system and the destination storage system.

Instructions associated with the replication engine are stored in the memory of the destination storage system. The source virtual storage partition and the destination virtual storage partition are respectively secure logical partitions of the source storage system and the destination storage system created by an operating system associated therewith. The operating system of the destination storage system is configured to implement a file system therein. The file system at the destination storage system is utilized to read a superblock of the read only replica of the source virtual volume at the destination storage system through the processor and the memory of the destination storage system.

The superblock of the read only replica is modified in the memory of the destination storage system through the replication engine to clear a replica flag associated therewith. The file system at the destination storage system is utilized to associate the modified superblock with one or more virtual VBN s configured to be previously associated with the superblock of the read only replica of the source virtual volume without initiating a DCP at the destination storage system and to render the destination virtual volume writable. The virtual VBN is configured to index a virtual volume level version of block allocation files of a virtual volume including the source virtual volume and or the destination virtual volume.

The DCP defines a process during which the operating system associated with the destination storage system flushes data changes in the memory of the destination storage system to the destination storage disk. A disk group label indicating an association of the destination storage disk with the read only replica of the source virtual volume is modified at the memory of the destination storage system to reflect an association of the destination storage disk with the writable destination virtual volume. The DCP is initiated to ensure that the modified superblock and the modified disk group label associated with the writable destination virtual volume are flushed to the destination storage disk.

The methods and systems disclosed herein may be implemented in any means for achieving various aspects and may be executed in a form of a machine readable medium embodying a set of instructions that when executed by a machine cause the machine to perform any of the operations disclosed herein. Other features will be apparent from the accompanying drawings and from the detailed description that follows.

Other features of the present embodiments will be apparent from the accompanying drawings and from the detailed description that follows.

Example embodiments as described below may be used to realize migration of virtual storage partition data between storage systems. Although the present embodiments have been described with reference to specific example embodiments it will be evident that various modifications and changes may be made to these embodiments without departing from the broader spirit and scope of the various embodiments.

In one or more embodiments host devices may indicate customers of services provided through network or users associated with an organization e.g. an Information Technology IT organization . In one or more embodiments each host device may have storage associated therewith. For the aforementioned purpose in one or more embodiments isolated logical virtual storage partitions may be created on storage system through an operating system associated with storage system . In one or more embodiments therefore each virtual storage partition may be associated with a host device . In one or more embodiments information on a secured virtual storage partition may solely be accessed by the host device associated therewith.

For example virtual storage partitions may be virtual filers NetApp s vFiler units that are virtual storage controllers operating within NetApp s Data ONTAP operating system. In one or more embodiments the operating system associated with storage system may also be configured to enable migration of virtual storage partitions between storage system and another storage system. In one or more embodiments each virtual storage partition may include data stored in volumes . In one or more embodiments a volume may be an instantiation of an individual file system having a Redundant Array of Independent Disks RAID label associated therewith. In one or more embodiments two or more disks may be part of a RAID group.

In one or more embodiments the root directory of a volume may include a special subdirectory called a quota tree qtree . In one or more embodiments the qtree may be configured to provide flexibility to storage when storage does not require multiple volumes . In one or more embodiments each virtual storage partition may include multiple volumes as shown in . In one or more embodiments each virtual storage partition may also include information e.g. Internet Protocol IP addresses network configuration required to securely route data to the appropriate virtual storage partition .

In one or more embodiments as described above data associated with a virtual storage partition may not be accessed by another virtual storage partition . In one or more embodiments storage system may be configured to map each volume and qtree if applicable to the corresponding virtual storage partition . In one or more embodiments again as described above an entire virtual storage partition may be migrated from storage system to another storage system with minimal disruption of ongoing activity therein.

In one or more embodiments the operating system associated with storage system may support data sets associated with protocols including but not limited to Network File System NFS protocol Common Internet File System CIFS protocol Internet Small Computer System Interface iSCSI protocol Hypertext Transfer HTTP protocol File Transfer Protocol FTP FTP Secure FTPS protocol Secure File Transfer Protocol SFTP and Network Data Management Protocol NDMP .

In one or more embodiments a volume may be created on top of one or more RAID groups where each RAID group may include M data disks and a parity disk. In one or more embodiments the number of bits in data blocks of the M data disks may be added up and stored in the parity disk which may be used in conjunction with the surviving bits on disks to facilitate data recovery during failure of a disk. In one or more embodiments the disk failure may increase the possibility of another failure during the disk rebuild process which may take a lot of time to complete.

In one or more embodiments therefore each RAID group may include M data disks and two parity disks which provides the ability to withstand a loss of up to two disks. Here the additional disk may provide coverage during the abovementioned disk rebuild process. In one or more embodiments each of the disks in the RAID group may include some portion therein allocated for the RAID label to store metadata that indicates the association of the disk with a volume . In one or more embodiments in order to increase the size of volume more disks may need to be added. In one or more embodiments for improved performance full RAID groups may need to be added at a time.

In one or more embodiments for at least the reasons described above the granularity may be fixed i.e. the association of disks and volumes may be inflexible. In one or more embodiments sizing of volumes may be based on physical disk sizes and carving out volumes based on required capacity may not be possible. In one or more embodiments resource utilization may also not be optimal. For example if there is a small sized volume requirement a physical disk may still need to be wasted.

In one or more embodiments a layer of abstraction may be added between the disks and volumes through aggregates. In one or more embodiments smaller utilizable volumes may be created through an aggregate. In one or more embodiments P disks may be allocated to an aggregate which is built RAID groups analogous to volumes . In one or more embodiments an aggregate may not include a file system and may merely include allocatable space therein.

In one or more embodiments aggregate may be analogous to volume and virtual volumes may be analogous to a file within volume . For example creating a 20 GB file in volume may be analogous to creating a 20 GB virtual volume in aggregate . In one or more embodiments aggregate may include one or more files with each file having a virtual volume .

In one or more embodiments as discussed above the operating system associated with storage system may be configured to enable migration of virtual storage partitions between storage system and another storage system. In one or more embodiments transparent migration of virtual storage partitions between storage systems without the infliction of application downtime may be required. Example scenarios associated with transparent migration i.e. migration transparent to applications include but are not limited to migration from a medium end virtual storage partition to a high end virtual storage partition for performance reasons and migration from one aggregate to another aggregate due to space constraints. In addition backups of virtual storage partitions may be created to aid in disaster recovery.

For example if important volume data may be replicated to a different physical location volume data may still be available after a disaster e.g. data corruption natural disaster at source location accidental deletion of data . The client at host device may access the replicated data across network until normalcy is restored at the source location following which data may be transferred back thereto and or retained at the destination.

In one or more embodiments data access may be provided through data migration to local clients e.g. local host devices to enable fast and efficient use of source data through read only versions thereof. In one or more embodiments this may help reduce network utilization. In one or more embodiments load sharing may be implemented whereby all read only activities associated with source data may be out sourced to read only mirrors of the source data at a destination location. Again in one or more embodiments the read only mirrors may be made writable following a disaster at the source location.

In one or more embodiments during the abovementioned transparent migration a virtual storage partition may be replicated to a new storage system and application Input Output I O may be switched to the new storage system. In one or more embodiments in order to achieve transparency data may be periodically replicated to the migration destination asynchronously using a replication engine e.g. NetApp s SnapMirror . In one or more embodiments when conditions are appropriate for switching over to the destination storage system the replication engine may be configured to operate in a semi synchronous mode whenceforth the cutover process may be started. In one or more embodiments therefore the cutover may be defined as the point within the data migration process when the conditions are appropriate for switching over to the destination side.

In one or more embodiments data migration described above may be initiated through a user at a host device . For example the user may press a button in a Graphical User Interface GUI indicating an initiation of the data migration process. In one or more embodiments during the cutover period the corresponding host device may experience a pause. In one or more embodiments at the high level the cutover process may include effectively fencing I O at source volume replicating data to the destination volume storage system converting the read only replica at the destination to a writable volume and then resuming I O on the destination volume. In one or more embodiments the time taken by the cutover process may be governed by application timeouts. Therefore in one or more embodiments there exists an upper limit to the time taken by the cutover process which may be governed by application timeouts.

It is obvious that I O at source volume may be resumed therein after the process of converting the read only replica at the destination to a writable volume is complete if data associated with source volume is to be retained at the source location.

In one or more embodiments converting the read only replica to a writable volume may involve operating on a single volume at a time and may involve a number of consistency points CPs . In one or more embodiments CP may be the process by which the operating system flushes in core storage system data changes to the disk. In one or more embodiments the operation on a single volume at a time may serialize the process of converting the read only replica data to a writable volume.

In one or more embodiments a number of checks may be performed with respect to volume under consideration during the conversion from read only replica to a writable volume. In order to understand the inefficiency of the conversion of a read only replica of a source volume to a writable volume analogous to source volume at the destination it may be prudent to discuss the file system e.g. NetApp s Write Anywhere File Layout WAFL file system associated therewith and the communication between a source storage system including the source volume and the destination storage system including the destination volume. It is obvious source volume and the destination volume are analogous to one another and that the label reference may apply to both for the sake of convenience. shows a tree organization of file system of volume according to one or more embodiments. In one or more embodiments file system may include data structures configured to implement a hierarchical namespace of files and directories.

In one or more embodiments each file of file system may be described by an inode which includes metadata associated with the file and pointers to file data blocks or file indirect blocks . In one or more embodiments for small files the inode may point directly to file data blocks as shown in . In one or more embodiments the inodes of small files may include addresses of all file data blocks that include the file data. In one or more embodiments for large files the inode may point to trees of file indirect blocks . In one or more embodiments as a large file may utilize a lot of data blocks for the corresponding inode to directly address the inode may point to trees of file indirect blocks large enough to include all data block addresses. In one or more embodiments all inodes in file system may be stored in inode file that again may include trees of indirect inode blocks. In one or more embodiments inode data may be associated with inode data blocks . In one or more embodiments the block allocation bitmap may be stored in block map file .

In one or more embodiments superblock may form the topmost level of file system and may include the inode describing inode file . In one or more embodiments the data structures of file system form a tree as shown in with the root of the tree being superblock . shows an expanded view of the communication between source storage system and destination storage system according to one or more embodiments. As the source storage system and destination storage system are analogous to one another the same label specifically may apply thereto.

In one or more embodiments each of source storage system and destination storage system may be a computing device configured to organize information on disks . In one or more embodiments each storage system may include a processor a memory a network adapter a non volatile memory and a storage adapter configured to be interconnected through a system bus . Here the constituents of destination storage system have been left out for clarity sake because destination storage system as discussed above is analogous to source storage system .

In one or more embodiments each storage system may include storage operating system configured to implement file system as a hierarchical structure of files directories and blocks on disks . In one or more embodiments memory may include a buffer cache e.g. volatile memory configured to store data structures passed between disks and network during operation. In one or more embodiments memory may also be configured to store software code e.g. instructions associated with the replication engine and may include storage locations configured to be addressable by processor . In one or more embodiments processor and the adapters may include processing elements and or logic circuitry configured to execute instructions in memory and to manipulate the data structures. In one or more embodiments each storage system may also include non volatile memory e.g. Non Volatile Random Access Memory NVRAM configured to provide fault tolerant back up of data to enable survival of storage system during power failure and or other faults.

In one or more embodiments network adapter may be configured to couple storage system to host devices through network . In one or more embodiments storage adapter may be configured to communicate with storage operating system to access information requested through host devices . In one or more embodiments host devices may be configured to execute applications . In one or more embodiments a host device may be configured to interact with storage system according to a client server model of information delivery. For example host device may request the services of storage system and storage system may return the results e.g. through packets through network .

In one or more embodiments storage of information may be implemented as volumes and may include disks configured to implement a logical arrangement of volume block number VBN space on volumes . In one or more embodiments each logical volume may be associated with file system as discussed above. In one or more embodiments file system may include a continuous range of VBNs starting from 0 to n for a file system of n 1 blocks. In one or more embodiments disks within a volume may be organized as one or more RAID groups again as discussed above. In one or more embodiments to facilitate access to disks storage operating system may implement a write anywhere file system e.g. NetApp s WAFL file system to virtualize the storage space provided by disks .

In one or more embodiments the write anywhere file system may not overwrite data on disks . In one or more embodiments if a data block is retrieved from disk onto memory of storage system and updated modified with new data the data block may thereafter be written to a new location on disk . An example of the write anywhere file system is the WAFL file system available from Network Appliance Inc. Sunnyvale Calif.

In one or more embodiments file system may logically organize information on disks as shown in . In one or more embodiments as soon as the cutover process is initiated source volume I O may be fenced through an Application Programming Interface API associated with file system at source storage system . In one or more embodiments the fencing of source volume I O may lead to subsequent client initiated external requests e.g. requests for source volume data through host devices not being served. For example a communication failure may be indicated to the client e.g. a user of host device of host device initiating the subsequent request. In one or more embodiments at the volume level no further I O through source volume may be possible.

In one or more embodiments the in core data e.g. data in buffer cache non volatile memory and not in disks associated with source volume on source storage system may then be unloaded to the corresponding disk . Meanwhile in one or more embodiments data from source volume may be replicated to destination volume on destination storage system as follows.

In one or more embodiments the first operation in the data replication process prior to the cutover may involve a one time baseline transfer of the entire data associated with source volume . In one or more embodiments the baseline transfer may include creation of a baseline copy of file system associated with source volume which may be a Snapshot copy i.e. a read only point in time image of file system associated with source volume . Snapshot is a trademark of NetApp Inc. in the US and other countries. Then in one or more embodiments all data blocks referenced by the Snapshot copy and any previous copies are transferred and written to the destination file system associated with destination volume . Thus the source and destination file systems may have at least one Snapshot copy in common. In one or more embodiments after the first operation is complete scheduled and or manually triggered updates may occur. Here each update may transfer only new and changed blocks since the previous transfer from the source file system to the destination file system .

Here when the source storage system creates a Snapshot copy the new copy may be compared to the baseline copy to determine the changed new blocks. Thus in one or more embodiments the new changed blocks may be transmitted to the destination and written to the destination file system . Now in one or more embodiments file systems at both the source and the destination have the new Snapshot copy which may now be the baseline copy for a subsequent update.

In one or more embodiments for the abovementioned baseline transfer the replication engine provided in source storage system and destination storage system may operate in an asynchronous mode. In one or more embodiments once the baseline transfer is done the cutover process may begin where the replication engine may switch to a semi synchronous mode of operation. Here in one or more embodiments data replication occurs at the granularity of a CP. In one or more embodiments in the semi synchronous mode of operation a CP may be triggered under certain conditions e.g. non volatile memory journal being half full or 10 seconds having passed since the most recent CP whichever occurs earlier . In one or more embodiments once a CP is triggered storage operating system may utilize transactional data stored in memory e.g. buffer cache to create a list of data block changes that need to be written to disk . In one or more embodiments once the list is ready source file system may transmit the list of data blocks to be written to disk . In one or more embodiments the list of data blocks may also be transmitted e.g. through network to the destination storage system where a data write is initiated too.

Thus in one or more embodiments the list data blocks may be written out to disk at both the source storage system and the destination storage system . In one or more embodiments a block level replication of source volume may be effected at the destination. In one or more embodiments during the data replication process the replication engine may mark the destination volume as a replica through a replica flag e.g. a bit a few bits . In one or more embodiments this may allow for the destination volume to be a read only replica thereby rendering data associated therewith unalterable. In one or more embodiments the relationships between source volume and the destination read only replica may be included in control file s that are part of the replication engine.

In one or more embodiments the process of converting the read only replica at the destination to a writable volume may then be started. For the aforementioned function in one or more embodiments the replication engine relationships between source volume and the destination read only replica may need to be broken. In other words the replica flag associated with the read only replica may need to be cleared.

In one or more embodiments during the process of converting from a read only replica to a writable volume I O e.g. through a client user of host device to the destination volume may be queued until the process is complete and requests may then be served in the order in which they were received in a priority order. Thus in one or more embodiments destination volume may be frozen at the volume level. In one or more embodiments this may be achieved through an API associated with destination file system . In one or more embodiments a client user initiating an external request e.g. requests for data from destination volume may notice a slow down in the request being addressed due to the aforementioned freezing of destination volume . In one or more embodiments the in core data of the read only replica i.e. destination volume in the destination storage system may be unloaded to the disk associated therewith as discussed above.

In one or more embodiments the superblock of destination file system associated with the read only replica may be stored in a VBN associated therewith. In one or more embodiments the superblock associated with the read only replica may also be stored at another VBN for redundancy fault tolerance purposes. In one or more embodiments this superblock may then first be read at the destination storage system . Then in one or more embodiments the replica flag associated therewith may be cleared and the superblock may be modified. For example the modification may be in the form of the replication engine writing metadata indicating the read writable status of destination volume . In one or more embodiments then the modified superblock may be written at the VBN where the superblock of the read only replica was stored. In one or more embodiments then a CP may be triggered e.g. manually to ensure that the modified superblock is unloaded flushed to disk . In one or more embodiments this may ensure that the modification is made permanent. In one or more embodiments the modified superblock may again be written at another VBN space e.g. a VBN at which the superblock of the read only replica was stored for redundancy fault tolerance purposes. In one or more embodiments a CP may again be triggered e.g. manually to ensure that the modified superblock is unloaded flushed to disk .

For example the modified superblocks may be located at two consecutive VBNs. Then in one or more embodiments the RAID label indicating the association of disk with the destination volume may be appropriately modified. In one or more embodiments in order to ensure that the modified RAID label is unloaded flushed to disk a CP may again be triggered e.g. manually . In one or more embodiments the destination volume may then be remounted using the destination file system . After the remounting in one or more embodiments the destination volume may be thawed i.e. unfrozen. Finally the relevant registry entry for the destination volume may be updated. In one or more embodiments the registry associated with destination volumes may be located in non volatile memory of the destination storage system and may include metadata associated therewith. Now the destination volume may be ready to serve the queued requests.

In one or more embodiments operation may involve initiating a CP to ensure that the modified superblock is flushed to disk associated therewith. In one or more embodiments operation may involve writing the modified superblock at a second VBN. In one or more embodiments operation may involve initiating a CP to ensure that the modified superblock is flushed to disk associated therewith. In one or more embodiments operation may involve modifying the RAID label associated with destination volume . In one or more embodiments operation may involve initiating a CP to ensure that the modified RAID label is flushed to disk associated therewith. In one or more embodiments operation may involve remounting destination volume and operation may involve thawing destination volume . Finally operation may involve updating the relevant registry entry for destination volume .

As seen above in one or more embodiments at least 3 CPs may be required during the process of converting the read only replica to a writable destination volume . In an example embodiment of a virtual storage partition supporting 20 volumes there may be at least 60 CPs required for the aforementioned conversion. In one or more embodiments utilizing the replication engine in parallel threads may not mitigate the situation because the number of CPs required is not reduced. Therefore in one or more embodiments the process of converting a read only replica volume to a writable volume may be inefficient.

In one or more embodiments when clients to storage system initiate I O in core modifications i.e. buffer cache modifications non volatile memory modifications and not disk modifications may be performed and logged into non volatile memory . In one or more embodiments whatever is logged into non volatile memory may be accessed from memory and dumped onto disk . This process is called CP. In one or more embodiments non volatile memory may include logs associated with not only the current volume in question but also all other volumes through which 110 was is initiated. Therefore in one or more embodiments a CP may be a time consuming process. The CP may take for example 2 seconds to complete when there is little load on volume and 16 seconds to complete when there is heavy load on volume for a given protocol data set. In one or more embodiments the total time spent in CPs remounting volumes and updating registry entries associated therewith may exceed application timeout limits.

Referring to the sum of storage space consumed by virtual volumes is lesser than or equal to the physical volume analogous to aggregate . In one or more embodiments aggregate may utilize a physical VBN space configured to define a storage space of blocks provided by disks of the physical volume and each virtual volume may utilize a logical virtual VBN space to organize the blocks as files. In one or more embodiments as virtual volume is a logical volume virtual volume may have own block allocation structures in the space associated therewith. In one or more embodiments each virtual volume may be a separate file system coupled onto common storage in aggregate by the storage operating system associated therewith. In one or more embodiments a virtual volume utilizes the storage space of aggregate only when there is data stored therein. Therefore in one or more embodiments the size of virtual volume may be expanded or shrunk according to requirements thereby providing for increased storage efficiency.

In one or more embodiments when a virtual volume is created the container file thereof may be sparsely populated as most of the logical offsets may have no underlying physical storage . In one or more embodiments a file system e.g. WAFL file system associated therewith may allocate physical storage to the container file as virtual volume writes data to new logical offsets. In one or more embodiments the contents of virtual volume may be similar to that of volume . In one or more embodiments analogous to file system there may be a superblock located within the container file space. In one more embodiments standard file system metadata files analogous to those found in volume may be found in virtual volume . In one or more embodiments a virtual volume N may include the same block allocation files as volume . In one or more embodiments aggregate level versions of these files may be indexed by a physical VBN and virtual volume level versions of these files may be indexed by a virtual VBN. Therefore in one or more embodiments files and the processing associated therewith may scale with logical size of virtual volume and not with the physical size of aggregate .

It is obvious that same labels may be utilized for source virtual volume and destination virtual volume for the same reasons described above with respect to source volume and destination volume . In one or more embodiments assuming the destination as having a corresponding block pointer a container map and aggregate blocks analogous to the source during transfer of a block from source to destination the destination storage system may be configured to assign a new physical VBN thereto and to enter the assigned new physical VBN in the destination container map. In one or more embodiments when block pointer may be copied to destination the replication engine associated therewith may preserve the virtual VBN of the source and the block may have the same logical address within source virtual volume and destination virtual volume .

In one or more embodiments physical VBN of the destination block pointer may indicate an unknown state. In one or more embodiments a background process may eventually replace the unknown state with the correct physical VBN. In one or more embodiments if the file system associated therewith needs to resolve the block pointer issue prior to the physical VBN being filled in the file system may look up the physical VBN using the container map.

In one or more embodiments as discussed above the process of converting a read only replica volume at the destination storage system to a writable volume may be inefficient due to the number of CPs involved. Again as discussed above the serialization associated with the ability to only work with one volume at a time may prove to be a performance limitation.

However the flexibility associated with operating virtual volumes may provide for parallelization of the process of converting read only virtual volumes to writable virtual volumes as will be discussed below. In one or more embodiments the aforementioned parallelization may enable reduction in the number of CPs involved. In one or more embodiments data migration associated with virtual volumes in virtual storage partitions and aggregate may be discussed again with reference to source storage system and destination storage system of . Here in one or more embodiments storage operating system of each storage system may be configured to implement file system associated with virtual volume . Again in one or more embodiments the write anywhere file system may be implemented. In one or more embodiments disks may be analogous to disks .

In one or more embodiments data migration associated with virtual volumes e.g. NetApp s FlexVols in a virtual storage partition is understood by one skilled in the art and is analogous to data migration replication with respect to volumes . Therefore the discussion with regard to data migration of virtual storage partitions including virtual volumes will be brief. In one or more embodiments data migration associated with virtual volumes may again be initiated by a user at host device through for example the pressing of a button in a GUI provided on host device .

In one or more embodiments during the abovementioned data migration a virtual storage partition may be replicated to a new storage system and application Input Output I O may be switched to the new storage system. In one or more embodiments the same replication engine e.g. NetApp s SnapMirror used in data migration of virtual storage partitions including volumes may be used in data migration of virtual storage partitions including volumes . However in one or more embodiments conversion of read only replicas to writable volumes may involve a number of CPs as discussed above and may prove to be inefficient.

In one or more embodiments data replication may again begin with the creation of a baseline copy of the file system associated with a virtual volume of virtual storage partition . In one or more embodiments the baseline copy may be transmitted to a destination virtual volume of a destination virtual storage partition associated with the destination storage system . In one or more embodiments scheduled and or manually triggered updates may occur wherein only new and changed blocks since the previous transfer may be transferred from the source file system to the destination file system and written therein. In one or more embodiments once the baseline transfer is complete the cutover process may occur.

Again in one or more embodiments at a high level virtual volume level the cutover process may include effectively fencing I O through source virtual volume replicating data to the destination virtual volume storage system converting the read only replica at the destination to a writable virtual volume and then resuming I O on the destination virtual volume . It is obvious that 110 at source virtual volume may be resumed therein after the process of converting the read only replica at the destination to a writable virtual volume is complete if data associated with source virtual volume is to be retained at source storage system .

In one or more embodiments a set of virtual volumes in a virtual storage partition may be operated on due to the flexibility associated therewith. Therefore in one or more embodiments converting multiple destination read only replicas of source virtual volumes to writable destination virtual volumes may be possible and may reduce the number of CPs involved therein.

In one or more embodiments as soon as the cutover process may be initiated a source virtual volume I O may be fenced through an API associated with a file system at source storage system . In one or more embodiments the in core data associated with source virtual volume on source storage system e.g. data in buffer cache non volatile memory and not in disks may then be unloaded to the corresponding disk . Meanwhile in one or more embodiments data from source virtual volume may be replicated to destination virtual volume on destination storage system as discussed above. In one or more embodiments as soon as the baseline transfer is done the semi synchronous transfer discussed above may begin.

In one or more embodiments during the data replication process the replication engine may mark the destination virtual volume as a replica through a replica flag e.g. a bit a few bits . In one or more embodiments this may allow for the destination virtual volume to be a read only replica thereby rendering data associated therewith unalterable. In one or more embodiments the relationships between source virtual volume and the destination read only replica may be included in control file s that are part of the replication engine.

Then in one or more embodiments the process of converting the read only replica at the destination to a writable volume may be started. For the aforementioned function in one or more embodiments the replication engine relationships between source virtual volume and the destination read only replica may need to be broken. In other words the replica flag associated with the read only replica may need to be cleared. In one or more embodiments during the process of converting from a read only replica to a writable volume I O e.g. through a client user of host device to destination virtual volume may again be queued until the process is complete and requests may then be served in the order in which they were received in a priority order. In one or more embodiments at the volume level destination virtual volume may be frozen. In one or more embodiments this may be achieved through an API associated with destination file system.

In one or more embodiments in contrast to being able to operate on one volume at a time all destination virtual volumes associated with aggregate in virtual storage partition may be frozen and the in core state of all the destination virtual volumes may be unloaded to disks associated therewith. In one or more embodiments for each virtual volume in aggregate the superblock of the destination file system associated with the read only replica may be stored in aggregate at the destination. In one or more embodiments one or more physical VBNs may be associated therewith and one or more virtual VBNs may specify the superblock s offset within the container file associated therewith. In one or more embodiments this superblock may then first be read at the destination storage system . Then in one or more embodiments the replica flag associated therewith may be cleared and the superblock may be modified in core i.e. not at disks .

For example the modification may be in the form of the replication engine writing metadata that indicates the read writable status of destination virtual volume . Then in one or more embodiments the virtual VBN of superblock associated with the read only replica may now be modified to associate with the new modified superblock. In one or more embodiments in contrast with the conversion described above with regard to volumes no CPs may be initiated for reasons that will be described below. In one or more embodiments then the new modified superblock may be associated with another virtual VBN e.g. one of the virtual VBNs specifying the superblock s offset within the container file associated therewith for redundancy fault tolerance purposes. Again in one or more embodiments no CPs may be initiated. Now for example the modified superblocks may have two consecutive virtual VBNs associated therewith.

In one or more embodiments the disk group label e.g. RAID label indicating the association of disks with the destination virtual volume may be appropriately modified in core e.g. at non volatile memory at buffer cache at the destination storage system . In one or more embodiments the abovementioned operations may be repeated for all virtual volumes within aggregate in virtual storage partition . In one or more embodiments in order to ensure that the superblocks for all virtual volumes and the modified disk group labels are unloaded flushed to disk a CP may be triggered e.g. manually . In one or more embodiments virtual VBNs of virtual volumes in aggregate may be translated to physical VBNs specifying the location of virtual volumes within aggregate to enable writing blocks of virtual volumes to disks . Now in one or more embodiments all destination virtual volumes may then be remounted in parallel in separate threads through the destination file system associated therewith. In one or more embodiments the destination virtual volumes may then be thawed i.e. unfrozen. Finally in one or more embodiments the registry entries associated with all virtual volumes in non volatile memory of the destination storage system may be updated in a single registry transaction.

In one or more embodiments the number of CPs may be reduced to 1 for all virtual volumes within an aggregate associated with a virtual storage partition . In one or more embodiments therefore the number of CPs may be independent of the number of virtual volumes . Also in one or more embodiments remounting virtual volumes in parallel in separate threads may reduce the overall time required for remounting virtual volumes . Therefore in one or more embodiments the overall time may be independent of the number of virtual volumes . In one or more embodiments a registry update may be a costly operation associated non volatile memory . In one or more embodiments the abovementioned batching of registry operations i.e. by updating the registry entries for all virtual volumes in one registry transaction may be independent on the number of virtual volumes . Thus in one or more embodiments the total time spent in the aforementioned processes may not exceed the time limit imposed by application timeouts.

In one or more embodiments as the time required for remounting virtual volumes is independent of the number of virtual volumes the number of virtual volumes supported by a data migration process including the abovementioned process of converting a read only replica at the destination to a writable volume may be increased. Also in one or more embodiments virtual storage partition N data migration may be faster than in the process described in .

In one or more embodiments as discussed above after modifying the superblock in core on the destination the superblock may be written at two VBNs. In one or more embodiments assuming that the file server panics in between the following situations may occur viz. i both superblocks at the two VBNs were flushed to disk ii one of the superblocks was flushed to disk but not the other iii none of the superblocks were flushed to disk and iv both superblocks were partially written to disk . In one or more embodiments in the case of virtual volumes i and ii may result in the file system associated therewith selecting the correct superblock for a virtual volume . Therefore in one or more embodiments virtual volume at the destination may become writable. In one or more embodiments in the case of virtual volumes iii and iv may result in aggregate still being at the previous CP as the aggregate superblock was not flushed to disk . Thus in one or more embodiments virtual volume at the destination may still remain a replica.

In one or more embodiments in the case of volumes iv may result in both superblocks being corrupt due to the CPs involved therein. Therefore in one or more embodiments the destination file system may not be able to mount destination volume . Thus in one or more embodiments if pre checks carried out on all virtual volumes fail for any of virtual volumes all virtual volumes may still remain replica virtual volumes .

In one or more embodiments the source virtual volume and the read only replica at the destination virtual storage partition are respectively abstracted from an underlying source storage disk associated with the source storage system and an underlying destination storage disk associated with the destination storage system through the source aggregate and the destination aggregate inside which the source virtual volume and the destination virtual volume signifying the read only replica are created. In one or more embodiments the source virtual storage partition and the destination virtual storage partition are respectively secure logical partitions of the source storage system and the destination storage system created by an operating system associated therewith.

In one or more embodiments operation may involve modifying the superblock of the read only replica in a memory of the destination storage system to clear a replica flag associated therewith through the replication engine. In one or more embodiments operation may involve associating through the file system at the destination storage system the modified superblock with one or more virtual VBNs configured to be previously associated with the superblock of the read only replica of the source virtual volume of the source storage system without initiating a destination consistency point DCP at the destination storage system to render the destination virtual volume writable.

In one or more embodiments the virtual VBN may be configured to index a virtual volume level version of block allocation files of a virtual volume including the source virtual volume and or the destination virtual volume and the DCP may define a process during which an operating system associated with the destination storage system flushes data changes in the memory of the destination storage system to the destination storage disk associated with the destination storage system .

In one or more embodiments operation may involve modifying a disk group label indicating an association of the destination storage disk with the read only replica of the source virtual volume of the source storage system to reflect an association of the destination storage disk with the writable destination virtual volume . In one or more embodiments operation may then involve initiating the DCP to ensure that the modified superblock and the modified disk group label associated with the writable destination virtual volume are flushed to the destination storage disk .

In one or more embodiments the freezing is configured to queue a subsequent external request through a client device to access data associated with the each destination virtual volume . In one or more embodiments the read only replica of the corresponding source virtual volume may be transferred from the source storage system to the each destination virtual volume through a replication engine associated with the source storage system and the destination storage system . The corresponding source virtual volume and the each destination virtual volume are respectively abstracted from an underlying source storage disk associated with the source storage system and an underlying destination storage disk associated with the destination storage system through the source aggregate and the destination aggregate inside which the corresponding source virtual volume and the each destination virtual volume are created.

In one or more embodiments the source virtual storage partition and the destination virtual storage partition are respectively secure logical partitions of the source storage system and the destination storage system created by an operating system associated therewith. In one or more embodiments operation may involve flushing data associated with the each destination virtual volume in a memory of the destination storage system to the destination storage disk . In one or more embodiments operation may involve reading through the file system at the destination storage system a superblock of the each destination virtual volume . In one or more embodiments operation may involve modifying the superblock of the each destination virtual volume in the memory of the destination storage system to clear a replica flag associated therewith through the replication engine.

In one or more embodiments operation may involve associating through the file system at the destination storage system the modified superblock with one or more virtual VBNs configured to be previously associated with the read only replica of the corresponding source virtual volume without initiating a DCP at the destination storage system to render the each destination virtual volume writable. In one or more embodiments operation may involve modifying a disk group label indicating an association of the destination storage disk with the read only replica of the corresponding source virtual volume to reflect an association of the destination storage disk with the writable each destination virtual volume .

In one or more embodiments the virtual VBN is configured to index a virtual volume level version of block allocation files of a virtual volume including the corresponding source virtual volume and or the each destination virtual volume . The DCP defines a process during which an operating system associated with the destination storage system flushes data changes in the memory of the destination storage system to the destination storage disk associated with the destination storage system .

In one or more embodiments operation may involve initiating the DCP to ensure that the modified superblock and the modified disk group label associated with the writable each destination virtual volume are flushed to the destination storage disk . In one or more embodiments operation may involve remounting in parallel the writable each destination virtual volume in a thread associated therewith through the file system associated therewith. In one or more embodiments operation may involve unfreezing the remounted writable each destination virtual volume through the file system associated therewith.

Although the present embodiments have been described with reference to specific example embodiments it will be evident that various modifications and changes may be made to these embodiments without departing from the broader spirit and scope of the various embodiments. Also for example the various devices and modules described herein may be enabled and operated using hardware circuitry e.g. CMOS based logic circuitry firmware software or any combination of hardware firmware and software e.g. embodied in a machine readable medium . For example the various electrical structure and methods may be embodied using transistors logic gates and electrical circuits e.g. application specific integrated ASIC circuitry and or in Digital Signal Processor DSP circuitry .

In addition it will be appreciated that the various operations processes and methods disclosed herein may be embodied in a machine readable medium and or a machine accessible medium compatible with a data processing system e.g. a computer devices and may be performed in any order e.g. including using means for achieving the various operations . Accordingly the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.

