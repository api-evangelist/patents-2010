---

title: Asynchronous future based API
abstract: An apparatus and a method for operating on data at a cache node of a data grid system is described. An asynchronous future-based interface of a computer system receives a request to operate on a cache node of a cluster. An acknowledgment is sent back upon receipt of the request prior to operating on the cache node. The cache node is then operated on based on the request. The operation is replicated to other cache nodes in the cluster. An acknowledgment that the operation has been completed in the cluster is sent back.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08402106&OS=08402106&RS=08402106
owner: Red Hat, Inc.
number: 08402106
owner_city: Raleigh
owner_country: US
publication_date: 20100414
---
Embodiments of the present invention relate to computing systems and more particularly to networked storage.

Highly concurrent systems often require high throughput of certain data structures. Traditional locks can be used to enforce mutual exclusion and implement operations on concurrent data structures.

Because changes are not propagated quickly enough rehashing cannot be accomplished without a blocking process of cluster cache nodes. As such access to a cluster of cache nodes may be blocked when a cache node is leaving or joining the cluster.

Described herein is an apparatus and a method for operating on data at a cache node of a data grid system. An asynchronous future based interface of a computer system receives a request to operate on a cache node of a cluster. An acknowledgment is sent back upon receipt of the request prior to operating on the cache node. The cache node is then operated on based on the request. The operation is replicated to other cache nodes in the cluster. An acknowledgment that the operation has been completed in the cluster is sent back.

In one embodiment cache node belongs to one or more data grids. Data grids are highly concurrent distributed data structures. They typically allow one to address a large amount of memory and store data in a way that it is quick to access. They also tend to feature low latency retrieval and maintain adequate copies across a network to provide resilience to server failure.

One example of a data grid uses INFINISPAN. INFINISPAN is an extremely scalable highly available data grid platform written in Java. The purpose of INFINISPAN is to expose a data structure that is highly concurrent designed ground up to make the most of modern multi processor multi core architectures while at the same time providing distributed cache capabilities. At its core INFINISPAN exposes a Cache interface. It is also optionally is backed by a peer to peer network architecture to distribute state efficiently around a data grid.

High availability is offered via making replicas of state across a network as well as optionally persisting state to configurable cache stores. INFINISPAN also offers enterprise features such as efficient eviction algorithms to control memory usage as well as Java Transaction API JTA compatibility.

In addition to the peer to peer architecture of INFINISPAN INFINISPAN has the ability to run farms of INFINISPAN instances as servers and connecting to them using a plethora of clients both written in Java as well as other popular platforms.

As such client connects to a cache node to read write data from data grids formed by INFINISPAN. In one embodiment client communicate with cache nodes using an asynchronous future based API interface.

One way to do this is just to block the thread that calls set until the replication has been performed synchronously to the other node and returns however that will involve a network roundtrip per set. illustrates a communication with a blocking approach where S represents Set in cache and A represents acknowledgment of replication of Set in cache node. Thus the thread has to wait for the acknowledgment of the set before calling the next set which involves a network roundtrip per set. It would be thus desirable to be able to get acknowledgments of replication back asynchronously in a difference stream.

For example calls S S are placed in quick succession without waiting for an acknowledgment of replication of set in cache node . Acknowledgments A A are sent back as they are being processed. Since there is no blocking one can use the throughput of the network without being limited by its latency. As such when someone sends a load of messages one by one the system doesn t individually do a network RTT per message because it would be too slow to replicate them. However client still needs the guarantee that the message has reached the all the cache nodes before they get the acknowledgment of send back.

In one embodiment server comprises a processing device and a local cache node . The processing device has an asynchronous future based API Application Programming Interface and a cache nodes engine . Cache nodes engine is configured to operate and communicate with a data grid formed from cache node cluster . Cache node cluster can also include local cache node . For example server may include an INFINISPAN module that can be run on one or several computing machines. Client can then communicate with cluster via asynchronous future based API .

Processing device represents one or more general purpose processing devices such as a microprocessor central processing unit or the like. More particularly the processing device may be complex instruction set computing CISC microprocessor reduced instruction set computing RISC microprocessor very long instruction word VLIW microprocessor or processor implementing other instruction sets or processors implementing a combination of instruction sets. Processing device may also be one or more special purpose processing devices such as an application specific integrated circuit ASIC a field programmable gate array FPGA a digital signal processor DSP network processor or the like.

In one embodiment asynchronous future based API allows INFINISPAN module and client to perform other operations or processes without having to wait for data to be completely operated on in cluster . In other words a thread is not needed to be kept open until the operation is finished.

In one embodiment asynchronous future based API enables non blocking access to data in data grids formed form cluster . Conventionally a thread is needed for each connection e.g. 100 threads for 100 connections . However with a non blocking interface a smaller number of threads can be used for more connections e.g. 3 threads for 100 connections .

Server comprises a computer system within which a set of instructions for causing the machine to perform any one or more of the methodologies discussed herein may be executed. In alternative embodiments the machine may be connected e.g. networked to other machines in a LAN an intranet an extranet or the Internet. Further while only a single machine is illustrated the term machine shall also be taken to include any collection of machines that individually or jointly execute a set or multiple sets of instructions to perform any one or more of the methodologies discussed herein.

Cache nodes of cluster may reside on computer accessible storage medium of one or more data storage devices of one or more servers. In one embodiment local cache node data resides in local storage device of server . Storage device can include a memory or a data storage device. Memory can include a read only memory ROM flash memory dynamic random access memory DRAM such as synchronous DRAM SDRAM or a static memory e.g. flash memory static random access memory SRAM etc. . Data storage device may include a computer accessible storage medium on which is stored one or more sets of instructions embodying any one or more of the methodologies or functions described herein.

Asynchronous future based API and cache nodes engine may also reside completely or at least partially within a main memory and or within the processing device during execution thereof by the computer system the main memory and the processing device also constituting computer accessible storage media. The software may further be transmitted or received over a network via the network interface device.

While the computer accessible storage medium is shown in an exemplary embodiment to be a single medium the term computer accessible storage medium should be taken to include a single medium or multiple media e.g. a centralized or distributed database and or associated caches and servers that store the one or more sets of instructions. The term computer accessible storage medium shall also be taken to include any medium that is capable of storing encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present invention. The term computer accessible storage medium shall accordingly be taken to include but not be limited to solid state memories optical and magnetic media.

A Future represents the result of an asynchronous computation. Methods are provided to check if the computation is complete to wait for its completion and to retrieve the result of the computation. The result can only be retrieved using method get when the computation has completed blocking if necessary until it is ready. Cancellation is performed by the cancel method. Additional methods are provided to determine if the task completed normally or was cancelled. Once a computation has completed the computation cannot be cancelled.

As illustrated above these methods do not block. They return immediately. If return values are needed one simply waits until the operation completes. A Future.get will block until the call completes. This is useful because in the case of clustered caches it allows one to get the best of both worlds when it comes to synchronous and asynchronous mode transports.

Synchronous transports are normally recommended because of the guarantees they offer the caller always knows that a call has properly propagated across the network and is aware of any potential exceptions. However asynchronous transports give greater parallelism. One can start on the next operation even before the first one has made it across the network. But this is at a cost losing out on the knowledge that a call has safely completed. However with the present future based cache API it is possible to know that the call has been safely completed. The following is an example of the asynchronous future based API 

The network calls possibly the most expensive part of a clustered write involved for the 3 put calls can now happen in parallel. This is even more useful if the cache is distributed and k1 k2 and k3 map to different nodes in the cluster the processing required to handle the put operation on the remote nodes can happen simultaneously on different nodes. And all the same when calling Future.get it is blocked until the calls have completed successfully. And the system is aware of any exceptions thrown. With this approach elapsed time taken to process all 3 puts should only be as slow as the single slowest put .

At a server receives a request to operate on a cache node of a cluster at an asynchronous future based interface of a computer system. An acknowledgment is sent at upon receipt of the request prior to operating on the nodes. At the cache node are operated on based on the request. The operation is replicated to the other cache nodes in the cluster. At an acknowledgment that the operation has been completed in the cluster is returned.

In the above description numerous details are set forth. It will be apparent however to one skilled in the art that the present invention may be practiced without these specific details. In some instances well known structures and devices are shown in block diagram form rather than in detail in order to avoid obscuring the present invention.

Some portions of the detailed descriptions above are presented in terms of algorithms and symbolic representations of operations on data bits within a computer memory. These algorithmic descriptions and representations are the means used by those skilled in the data processing arts to most effectively convey the substance of their work to others skilled in the art. An algorithm is here and generally conceived to be a self consistent sequence of steps leading to a desired result. The steps are those requiring physical manipulations of physical quantities. Usually though not necessarily these quantities take the form of electrical or magnetic signals capable of being stored transferred combined compared and otherwise manipulated. It has proven convenient at times principally for reasons of common usage to refer to these signals as bits values elements symbols characters terms numbers or the like.

It should be borne in mind however that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussion it is appreciated that throughout the description discussions utilizing terms such as operating or copying or receiving or the like refer to the action and processes of a computer system or similar electronic computing device that manipulates and transforms data represented as physical electronic quantities within the computer system s registers and memories into other data similarly represented as physical quantities within the computer system memories or registers or other such information storage transmission or display devices.

The present invention also relates to apparatus for performing the operations herein. This apparatus may be specially constructed for the required purposes or it may comprise a general purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer readable storage medium such as but is not limited to any type of disk including floppy disks optical disks CD ROMs and magnetic optical disks read only memories ROMs random access memories RAMs EPROMs EEPROMs magnetic or optical cards or any type of media suitable for storing electronic instructions and each coupled to a computer system bus.

The algorithms and displays presented herein are not inherently related to any particular computer or other apparatus. Various general purpose systems may be used with programs in accordance with the teachings herein or it may prove convenient to construct more specialized apparatus to perform the required method steps. The required structure for a variety of these systems will appear from the description below. In addition the present invention is not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings of the invention as described herein.

It is to be understood that the above description is intended to be illustrative and not restrictive. Many other embodiments will be apparent to those of skill in the art upon reading and understanding the above description. The scope of the invention should therefore be determined with reference to the appended claims along with the full scope of equivalents to which such claims are entitled.

