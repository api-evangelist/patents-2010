---

title: Consistent cluster operational data in a server cluster using a quorum of replicas
abstract: A method and system for increasing server cluster availability by requiring at a minimum only one node and a quorum replica set of replica members to form and operate a cluster. Replica members maintain cluster operational data. A cluster operates when one node possesses a majority of replica members, which ensures that any new or surviving cluster includes consistent cluster operational data via at least one replica member from the immediately prior cluster. Arbitration provides exclusive ownership by one node of the replica members, including at cluster formation, and when the owning node fails. Arbitration uses a fast mutual exclusion algorithm and a reservation mechanism to challenge for and defend the exclusive reservation of each member. A quorum replica set algorithm brings members online and offline with data consistency, including updating unreconciled replica members, and ensures consistent read and update operations.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=07984155&OS=07984155&RS=07984155
owner: Microsoft Corporation
number: 07984155
owner_city: Redmond
owner_country: US
publication_date: 20100614
---
This is a continuation of U.S. patent application Ser. No. 11 225 222 filed Sep. 12 2005 which is a continuation in part of U.S. patent application Ser. No. 11 203 489 filed Aug. 12 2005 which is a continuation of U.S. Pat. No. 6 938 084 filed Jun. 28 2001 which is a continuation in part of U.S. Pat. No. 6 401 120 filed Mar. 26 1999 each of which is incorporated herein by reference.

A server cluster ordinarily is a group of at least two independent servers connected by a network and utilized as a single system. The clustering of servers provides a number of benefits over independent servers. One important benefit is that cluster software which is run on each of the servers in a cluster automatically detects application failures or the failure of another server in the cluster. Upon detection of such failures failed applications and the like can be terminated and restarted on a surviving server.

Other benefits of clusters include the ability for administrators to inspect the status of cluster resources and accordingly balance workloads among different servers in the cluster to improve performance. Such manageability also provides administrators with the ability to update one server in a cluster without taking important data and applications offline for the duration of the maintenance activity. As can be appreciated server clusters are used in critical database management file and intranet data sharing messaging general business applications and the like.

When operating a server cluster the cluster operational data i.e. state of any prior incarnation of a cluster needs to be known to the subsequent incarnation of a cluster otherwise critical data may be lost. For example if a bank s financial transaction data are recorded in one cluster but a new cluster starts up without the previous cluster s operational data the financial transactions may be lost. To avoid this prior clustering technology required that each node server of a cluster possess its own replica of the cluster operational data on a private storage thereof and that a majority of possible nodes along with their private storage device of a cluster be operational in order to start and maintain a cluster.

However requiring a quorum of nodes has the drawback that a majority of the possible nodes of a cluster have to be operational in order to have a cluster. A recent improvement described in U.S. patent application Ser. No. 08 963 050 entitled Method and System for Quorum Resource Arbitration in a Server Cluster assigned to the same assignee of the present invention provides the cluster operational data on a single quorum device typically a storage device for which cluster nodes arbitrate for exclusive ownership. Because the correct cluster operational data is on the quorum device a cluster may be formed as long as a node of that cluster has ownership of the quorum device. Also this ensures that only one unique incarnation of a cluster can exist at any given time since only one node can exclusively own the quorum device. The single quorum device solution increases cluster availability since at a minimum only one node and the quorum device are needed to have an operational cluster. While this is a significant improvement over requiring a majority of nodes to have a cluster a single quorum device is inherently not reliable and thus to increase cluster availability expensive hardware based solutions are presently employed to provide highly reliable single quorum device for storage of the operational data. The cost of the highly reliable storage device is a major portion of the cluster expense.

Briefly the present invention provides a method and system wherein at least three storage devices replica members are configured to maintain the cluster operational data and wherein the replica members may be independent from any given node independent replica member or directly attached to a node as local storage local replica member . A cluster may operate as long as either 1 one node possesses a quorum e.g. a simple majority of configured independent replica members or 2 one or more communicating nodes possess a quorum e.g. a simple majority of configured replica members among those that are independent local or a combination of both. For example in a cluster having three replica members configured at least two replica members need to be available and controlled by a node to have an operational cluster. Because a replica member can be controlled by only one node at a time only one unique incarnation of a cluster can exist at any given time since only one node may possess a quorum of members. The quorum requirement further ensures that a new or surviving cluster has at least one replica member that belonged to the immediately prior cluster and is thus correct with respect to the cluster operational data.

A quorum arbitration algorithm is provided by which any number of nodes may arbitrate for exclusive ownership of the independent replica members or a single quorum device . Note that the arbitration process described in the algorithms generally applies to the members of the replica set that are independent of any cluster node since any local replica members directly attached to a node tend to be by definition controlled by the node to which they are attached. The quorum arbitration algorithm ensures that only one node may have possession of the independent replica members in the quorum replica set when a cluster is formed and also enables another node to represent the cluster when a node having exclusive possession of the quorum replica members fails. Arbitration may thus occur when a node first starts up including when there is no cluster yet established because of a simultaneous startup of the cluster s nodes. Arbitration also occurs when a node loses contact with the owner of the quorum replica set such as when the owner of the replica set fails or the communication link is broken as described below.

In one implementation arbitration is based on challenging or defending for an exclusive reservation of each independent replica member and a method for releasing an exclusive reservation is provided. In this implementation the arbitration process leverages the SCSI command set in order for systems to exclusively reserve the SCSI replica members resources and break any other system s reservation thereof. A preferred mechanism for breaking a reservation is the SCSI bus reset while a preferred mechanism for providing orderly mutual exclusion is based on a modified fast mutual exclusion algorithm in combination with the SCSI reserve command. Control of the cluster is achieved when a quorum of replica members is obtained by a node. The algorithm enables any number of nodes to arbitrate for any number of replica members or for a single quorum device .

A quorum replica set algorithm is also provided herein to ensure the consistency of data across replica members in the face of replica or node failures. The quorum replica set algorithm provides a database that is both fault tolerant and strongly consistent. The quorum replica set algorithm ensures that changes that were committed in a previous incarnation of the cluster remain committed in the new incarnation of the cluster. Among other things the quorum replica set algorithm maintains the consistency of data across the replica set as replica members become available online or unavailable offline to the set. To this end the quorum replica set algorithm includes a recovery process that determines the most up to date replica member from among those in the quorum and reconciles the states of the available members by propagating the data of that most up to date replica member to the other replica members when needed to ensure consistency throughout the replica set. For example the quorum replica set algorithm propagates the data to update replica members following a cluster failure and restart of the cluster when a replica member becomes available for use in the replica set upon the failure and recovery of one or more members or a change in node ownership of the replica set. The quorum replica set algorithm also handles reads and updates in a manner that maintains consistency such as by preventing further updates when less than a majority of replica members are successfully written during an update.

The method and system of the present invention require only a small number of relatively inexpensive components to form a cluster thereby increasing availability relative to a quorum of nodes solution while lowering cost and increasing reliability relative to a single quorum device solution.

Other benefits and advantages will become apparent from the following detailed description when taken in conjunction with the drawings in which 

With reference to an exemplary system for implementing the invention includes a general purpose computing device in the form of a conventional personal computer or the like acting as a node i.e. system in a clustering environment. The computer includes a processing unit a system memory and a system bus that couples various system components including the system memory to the processing unit . The system bus may be any of several types of bus structures including a memory bus or memory controller a peripheral bus and a local bus using any of a variety of bus architectures. The system memory includes read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within the personal computer such as during start up is stored in ROM . The personal computer may further include a hard disk drive for reading from and writing to a hard disk not shown a magnetic disk drive for reading from or writing to a removable magnetic disk and an optical disk drive for reading from or writing to a removable optical disk such as a CD ROM or other optical media. The hard disk drive magnetic disk drive and optical disk drive are connected to the system bus by a hard disk drive interface a magnetic disk drive interface and an optical drive interface respectively. The drives and their associated computer readable media provide non volatile storage of computer readable instructions data structures program modules and other data for the personal computer . Although the exemplary environment described herein employs a hard disk a removable magnetic disk and a removable optical disk it should be appreciated by those skilled in the art that other types of computer readable media which can store data that is accessible by a computer such as magnetic cassettes flash memory cards digital video disks Bernoulli cartridges random access memories RAMs read only memories ROMs and the like may also be used in the exemplary operating environment.

A number of program modules may be stored on the hard disk magnetic disk optical disk ROM or RAM including an operating system which may be considered as including or operatively connected to a file system one or more application programs other program modules and program data . A user may enter commands and information into the personal computer through input devices such as a keyboard and pointing device . Other input devices not shown may include a microphone joystick game pad satellite dish scanner or the like. These and other input devices are often connected to the processing unit through a serial port interface that is coupled to the system bus but may be connected by other interfaces such as a parallel port game port or universal serial bus USB . A monitor or other type of display device is also connected to the system bus via an interface such as a video adapter . In addition to the monitor personal computers typically include other peripheral output devices not shown such as speakers and printers.

The personal computer operates in a networked environment using logical connections to one or more remote computers . At least one such remote computer is another system of a cluster communicating with the personal computer system over the networked connection. Other remote computers may be another personal computer such as a client computer a server a router a network PC a peer device or other common network system and typically includes many or all of the elements described above relative to the personal computer although only a memory storage device has been illustrated in . The logical connections depicted in include a local area network LAN and a wide area network WAN . Such networking environments are commonplace in offices enterprise wide computer networks Intranets and the Internet. The computer system may also be connected to system area networks SANS not shown . Other mechanisms suitable for connecting computers to form a cluster include direct connections such as over a serial or parallel cable as well as wireless connections. When used in a LAN networking environment as is typical for connecting systems of a cluster the personal computer is connected to the local network through a network interface or adapter . When used in a WAN networking environment the personal computer typically includes a modem or other means for establishing communications over the wide area network such as the Internet. The modem which may be internal or external is connected to the system bus via the serial port interface . In a networked environment program modules depicted relative to the personal computer or portions thereof may be stored in the remote memory storage device. It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

A preferred system further includes a host adapter or the like which connects the system bus to a SCSI Small Computer Systems Interface bus for communicating with a quorum replica set comprising one or more persistent memory storage devices referred to herein as replica members e.g. of . Other ways of connecting cluster systems to storage devices including Fibre Channel are equivalent. Indeed one alternative way to connect storage devices is via a network connection as described in U.S. patent application Ser. No. 09 260 194 entitled Method and System for Remote Access of Computer Devices assigned to the assignee of the present invention.

As used herein an independent replica member is a storage device that is not integral to any specific node but rather is separate from any node of the cluster and accessible to nodes in the cluster at various times. In other words an independent replica member can operate in a cluster regardless of which node or nodes are in that particular incarnation thereof. Each independent replica member may be a simple disk or some or all of them may be a hardware based redundant array of devices or the like although as will become apparent a benefit of the present invention is that such hardware based redundancy is unnecessary. Note that any number of independent replica members may be configured in a given cluster configuration however for purposes of simplicity only three are shown in . In any event as shown in the computer system may comprise the node of a cluster while one of the remote computers may be similarly connected to the SCSI bus and comprise the node and so on.

As used herein a local replica member is a storage device that is integral to a specific node and is thus controlled by that node to which it is a part. Other nodes may communicate with a local replica member through the node which controls it. Such a node is considered a communicating node. Each local replica member may be a simple disk or it may be a hardware based redundant array of devices or the like. A node may include zero or more local replica members. In one example there tends to one local replica member per node.

Quorum may be achieved using a replica set composed of local replica members independent replica members or a combination of both local and independent replica members. illustrates quorum achieved using a combination of both local and independent replica members where all components are functioning properly. shows quorum maintained in the face of failure of one of the independent replica members. shows quorum maintained despite failure of one of the nodes and the local replica member to which it is attached.

The term replica member as used herein without further qualification by either local or independent generally refers to an independent replica member.

An administrator typically works with groups each group being a collection of resources e.g. cluster application resources names addresses and so forth organized to allow an administrator to combine resources into larger logical units and manage them as a unit. Group operations performed on a group affect all resources contained within that group. Usually a group contains all of the elements needed to run a specific application and for client systems to connect to the service provided by the application. For example a group may include an application that depends on a network name which in turn depends on an Internet Protocol IP address all of which are collected in a single group. In a preferred arrangement the dependencies of all resources in the group are maintained in a directed acyclic graph known as a dependency tree. Dependency trees are described in more detail in U.S. patent application Ser. No. 08 963 049 entitled Method and System for Resource Monitoring of Disparate Resources in a Server Cluster assigned to the same assignee as the present invention.

A cluster service controls the cluster operation on a server cluster e.g. and is preferably implemented as a Windows NT service. The cluster service includes a node manager which manages node configuration information and network configuration information e.g. the paths between nodes . The node manager operates in conjunction with a membership manager which runs the protocols that determine what cluster membership is when a change e.g. node failure occurs. A communications manager kernel driver manages communications with other nodes of the cluster via one or more network paths. The communications manager sends periodic messages called heartbeats to counterpart components on the other nodes of the cluster to provide a mechanism for detecting that the communications path is good and that the other nodes are operational. Through the communications manager the cluster service is essentially in constant communication with the other nodes of the cluster . In a small cluster communication is fully connected i.e. all nodes of the cluster are in direct communication with all other nodes. In a large cluster direct communication may not be possible or desirable for performance reasons.

Nodes in the cluster have the same view of cluster membership and in the event that one node detects a communication failure with another node the detecting node broadcasts a message to nodes of the cluster causing other members to verify their view of the current cluster membership. This is known as a regroup event during which writes to potentially shared devices are disabled until the membership has stabilized. If a node does not respond it is removed from the cluster and its active groups are failed over pulled to one or more active nodes. Note that the failure of the cluster service also causes its locally managed resources to fail.

The cluster service also includes a configuration database manager which implements the functions that maintain a cluster configuration database on local storage devices such as a disk and or memory and configuration databases on each of the replica members . The databases maintain cluster operational data i.e. information about the physical and logical entities in the cluster as described below. In one embodiment the cluster operational data may be split into core boot data and cluster configuration data and is maintained in two cluster databases as described in the copending U.S. patent application Ser. No. 09 277 503 entitled Data Distribution in a Server Cluster filed on Mar. 26 1999 assigned to the same assignee as the present invention. As described therein the core boot data is stored in a database maintained on quorum replica members while the cluster configuration data is stored in a database on a higher performance lower cost storage mechanism such as a mirror set of storage elements. Note that the cluster software is aware that the core boot data is replicated to multiple storage devices and that the core boot data has a log per storage device as described below. However in such an embodiment the cluster software views the mirror set storage as a single storage device and is generally not cognizant of the replication which is maintained at a lower level . Thus the cluster configuration information is viewed by the cluster software as a single database with a single log. The database manager may cooperate with counterpart database managers of nodes in the cluster to maintain certain cluster information consistently across the cluster . Global updates may be used to ensure the consistency of the cluster database in each of the replica members and nodes .

A logging manager provides a facility that works with the database manager of the cluster service to maintain cluster state information across a situation in which a cluster shuts down and a new cluster is later formed with no nodes necessarily being common to the previous cluster known as a temporal partition. The logging manager operates with the log file preferably maintained in the replica members to unroll logged state changes when forming a new cluster following a temporal partition.

A failover manager makes resource group management decisions and initiates appropriate actions such as startup restart and failover. The failover manager is responsible for stopping and starting the node s resources managing resource dependencies and for initiating failover of groups.

The failover manager receives resource and node state information from at least one resource monitor and the node manager for example to make decisions about groups. The failover manager is responsible for deciding which nodes in the cluster should own which groups. Those nodes that own individual groups turn control of the resources within the group over to their respective failover managers .

An event processor connects the components of the cluster service via an event notification mechanism. The event processor propagates events to and from cluster aware applications e.g. and to and from the components within the cluster service . An object manager maintains various cluster objects. A global update manager operates to provide a global atomic and consistent update service that is used by other components within the cluster service . The global update protocol GLUP is used by the global update manager to broadcast updates to each node in the cluster . GLUP generally comprises a standard global update message format state information maintained in each node and a set of rules that specify how global update should be processed and what steps should be taken when failures occur.

In general according to the GLUP protocol one node e.g. of serves as a locker node. The locker node ensures that only one global update is in progress at any given time. With GLUP a node e.g. wishing to send an update to other nodes first sends a request to the locker node . When any preceding updates are complete the locker node gives permission for this sender node to broadcast its update to the other nodes in the cluster . In accordance with GLUP the sender node sends the updates one at a time to the other nodes in a predetermined GLUP order that is ordinarily based on a unique number assigned to each node. GLUP can be utilized to replicate data to the machines of a cluster including at least some of the cluster operational data as described below. A more detailed discussion of the GLUP protocol is described in the publication entitled Tandem Systems Review Volume 1 Number 2 June 1985 pp. 74 84.

A resource monitor runs in one or more processes that may be part of the cluster service but are shown herein as being separate from the cluster service and communicating therewith via RPC or the like. The resource monitor monitors the health of one or more resources e.g. via callbacks thereto. The monitoring and general operation of resources is described in more detail in the aforementioned U.S. patent application Ser. No. 08 963 049.

The resources e.g. are implemented as one or more Dynamically Linked Libraries DLLs loaded into the address space of the Resource Monitor . For example resource DLLs may include physical disk logical volume consisting of one or more physical disks file and print shares network addresses and names generic service or application and Internet Server service DLLs. The resources run in the system account and are considered privileged code. Resources may be defined to run in separate processes created by the cluster service when creating resources or they may be run in a common process.

Resources expose interfaces and properties to the cluster service and may depend on other resources with no circular dependencies allowed. If a resource does depend on other resources the resource is brought online after the resources on which it depends are already online and is taken offline before those resources. Moreover each resource has an associated list of nodes in the cluster on which this resource may execute. For example a disk resource may only be hosted on nodes that are physically connected to the disk. Also associated with each resource is a local restart policy defining the desired action in the event that the resource cannot continue on the current node.

Nodes in the cluster need to maintain a consistent view of time. A time function suitable for this purpose is available in the Windows 2000 operating system however in other implementations one of the nodes may include a resource that implements a time service.

From the point of view of other nodes in the cluster and management interfaces nodes in the cluster may be in one of three distinct states offline online or paused. These states are visible to other nodes in the cluster and thus may be considered the state of the cluster service . When offline a node is not a fully active member of the cluster . The node and its cluster service may or may not be running. When online a node is a fully active member of the cluster and honors cluster database updates maintains heartbeats and can own and run groups. Lastly a paused node is a fully active member of the cluster and thus honors cluster database updates and maintains heartbeats. Online and paused are treated as equivalent states by most of the cluster software however a node that is in the paused state cannot honor requests to take ownership of groups. The paused state is provided to allow certain maintenance to be performed.

Note that after initialization is complete the external state of the node is offline. To join a cluster following the restart of a node the cluster service is started automatically. The node configures and mounts local non shared devices. Cluster wide devices are left offline while booting because they may be in use by another node. The node tries to communicate over the network with the last known members of the cluster . When the node discovers any member of the cluster it performs an authentication sequence wherein the existing cluster node authenticates the newcomer and returns a status of success if authenticated or fails the request if not. For example if a node is not recognized as a member or its credentials are invalid then the request to join the cluster is refused. If successful the newcomer may be sent an updated copy of the shared database or databases. The joining node may use the one or more databases to find shared resources and to bring them online as needed and also to find other cluster members. If a cluster is not found during the discovery process a node will attempt to form its own cluster by acquiring control of a quorum of the replica devices in accordance with one aspect of the present invention as described below.

Once online a node can have groups thereon. A group can be owned by only one node at a time and the individual resources within a group are present on the node that currently owns the group. As a result at any given instant different resources within the same group cannot be owned by different nodes across the cluster . Groups can be failed over or moved from one node to another as atomic units. Each group has a cluster wide policy associated therewith comprising an ordered list of owners. A group fails over to nodes in the listed order.

For example if a resource e.g. an application fails the failover manager may choose to restart the resource or to take the resource offline along with any resources dependent thereon. If the failover manager takes the resource offline the group is restarted on another node in the cluster known as pushing the group to another node. A cluster administrator may also manually initiate such a group transfer. Both situations are similar except that resources are gracefully shutdown for a manually initiated failover while they are forcefully shut down in the failure case.

When an entire node in the cluster fails its groups are pulled from the failed node to another node. This process is similar to pushing a group but without the shutdown phase on the failed node. To determine what groups were running on the failed node the nodes maintain group information on each node of the cluster in a database or the like in memory or persistent to track which nodes own which groups. To determine which node should take ownership of which groups those nodes capable of hosting the groups negotiate among themselves for ownership based on node capabilities current load application feedback and or the group s node preference list. Once negotiation of a group is complete all members of the cluster update their databases to properly reflect which nodes own which groups.

When a previously failed node comes back online the failover manager decides whether to move some groups back to that node in an action referred to as failback. To automatically failback groups require a defined preferred owner. There may be an ordered list of preferred owners in a cluster of more than two nodes. Groups for which the newly online node is the preferred owner are pushed from the current owner to the new node. Protection in the form of a timing window is included to control when the failback occurs.

In accordance with one aspect of the present invention the information needed to form and operate a cluster i.e. the cluster operational data is replicated to a quorum replica set of the replica members e.g. of . Such information generally includes node information information regarding the replica members of the quorum replica set and other critical information. A node of the cluster e.g. needs to obtain exclusive ownership control of a quorum replica set of replica members in order to form and maintain a cluster. Control of a quorum replica set establishes a cluster and guarantees that the cluster incarnation is unique because only one node can have control over the quorum replica set at any one time. Updates to this operational data are replicated to each member of the quorum replica set by the node that has exclusive ownership thereof. Note that if another node wants to access some information in the quorum replica set it does so through the node that owns the replica set.

To create a new cluster a system administrator runs a cluster installation utility on a system node that then becomes a first configured member of the cluster . For a new cluster a total replica set of replica members is created each member including a database e.g. . As described below to ensure that each replica member is consistent with the state of the previous cluster a quorum replica set algorithm is executed to select the most updated replica member of the set and propagate any needed logged information therefrom to other replica members. The administrator then configures any resources that are to be managed by the cluster software possibly including other storage devices. In general a first system forms a cluster as generally described below with reference to . At this time a cluster exists having a single node e.g. after which an installation procedure may be run to add more nodes and resources. Each added node e.g. receives at least a partial copy of the current cluster operational data e.g. the cluster database . This copy includes the information necessary to identify and access the members of the total replica set and the identity of the other known member nodes of the cluster e.g. . This information is stored on the added node s local storage e.g. .

More particularly as shown in beginning at step a node that has been configured to be part of a cluster but which is not currently participating in an operational instance of that cluster first assumes that some instance of the cluster is operational and attempts to join that existing cluster as described previously. If not successful as determined by step the node will attempt to form a new instance of the cluster by arbitrating for control of a quorum e.g. a majority of the total replica set members as described below with reference to . If successful as determined by step the node joins the existing cluster and performs some work as specified by the cluster i.e. as set by an administrator as described below with reference to . The node continues to perform work until it is shut down fails or some event occurs such as the node stops communicating with the cluster or a replica member fails as described below.

In accordance with one aspect of the present invention to form a cluster when a plurality of replica members are configured a node has to obtain access to a quorum of the replica members e.g. at least a simple majority of the total configured replica set . As described above the replica members include the cluster operational data on respective databases . The quorum requirement ensures that at least one replica member is common to the previous cluster whereby via the common member or members and the quorum replica set algorithm described below the cluster will possess the latest cluster operational data. The quorum further ensures that only one unique cluster may be formed at any given time. As a result the node owning the quorum replica set possesses the information necessary to properly configure a new cluster following a temporal partition.

By way of example shows two quorum replica sets and which may be formed from the total number of replica members configured i.e. three in the present example . Replica Set represented by the surrounding dashed line was the prior quorum replica set used by the immediately prior cluster for recording cluster operational data and included replica members and . Some time later a new cluster is formed with Replica Setas the quorum replica set which as represented by the surrounding solid line includes replica members and . Since more than half two or more in the present example of the total members configured are required to form a cluster at least one replica member is common to any previous cluster. In the present example the replica member is common to both replica sets and thus maintains the correct cluster operational data from the prior cluster. Note that any permutation of the server nodes may have been operating in the previous cluster as long as one node was present. Indeed a significant benefit of the present invention is that at a minimum only one node need be operational to form and or maintain a cluster which greatly increases cluster availability. In addition even though multiple replica members e.g. disks are used to back up the cluster operational data to provide high availability only a majority of the replica members is required to be functional in order to operate a cluster.

Each of the three local replica members is typically integral to their respective nodes. That is each replica forms a part of the node and is controlled by the node. If the node fails then it is generally assumed that the replica fails as well. Each such node is considered a communicating node and tends to provide access to its local replica member for any other nodes in the cluster.

Some time later as represented in the independent replica member has failed leaving only the local replica members surviving along with a modified quorum replica set comprising a majority three members of the four possible replica members.

In another failure condition as represented in node along with its integral local replica member has failed. This failure leaves only two local replica members surviving along with independent replica member forming a modified quorum replica set comprising a majority three members of the four possible replica members.

In keeping with the invention any node may form a cluster following a temporal partition regardless of the number of functioning nodes since by effectively separating the cluster operational data from the nodes there is no requirement that a majority of nodes be operational. Thus for example in the node may have formed the latest cluster by first having obtained exclusive control described below of the replica members and of the quorum replica set . To this end as shown in the node attempting to form a cluster first arbitrates via for control of a quorum replica set e.g. of replica members from the total replica set configured to operate in the cluster as described below beginning at step .

More particularly because only one node may have possession of the quorum replica set when a cluster is formed and also because a node having exclusive possession thereof may fail there is provided a method for arbitrating for exclusive ownership of the replica members typically by challenging or defending for an exclusive reservation of each member. A method for releasing an exclusive reservation may also be provided. Arbitration may thus occur when a node first starts up including when there is no cluster yet established because of a simultaneous startup of the cluster s nodes. Arbitration also occurs when a node loses contact with the owner of the quorum replica set such as when the owner of the replica set fails or the communication link is broken as described below. Arbitration for and exclusive possession of a single quorum device by two nodes are described in detail in the aforementioned U.S. patent application Ser. No. 08 963 050.

In accordance with another aspect of the present invention the arbitration exclusive ownership process has been extended to accommodate a cluster of more than two nodes. Although the algorithm described herein is capable of arbitrating for control of a replica set with a plurality of members it should be noted that the multiple node arbitration algorithm is applicable to clusters having a single quorum device as the resource. For example in such an event the majority can be considered as one member available out of a total configured set of one and although some simplification to the algorithm is possible when there is only one device in contention the general principles are essentially the same.

In general to obtain control over the members of the quorum replica set an arbitration process leverages a resource reservation mechanism such as the SCSI command set or the like in order for systems to exclusively reserve the e.g. SCSI replica members resources and break any other system s reservation thereof. Control is achieved when a quorum of replica members is obtained by a node. A preferred mechanism for breaking a reservation is the SCSI bus reset while a preferred mechanism for providing orderly mutual exclusion is based on a modified fast mutual exclusion algorithm in combination with the SCSI reserve command. One such algorithm is generally described in the reference entitled Leslie Lamport 5 1 February 1987 although such an algorithm needs to be modified among other things to make it work properly in an asynchronous system such as a cluster.

Step of begins the process for arbitrating for the replica set by initializing some variables e.g. setting a loop counter RetryCount to zero and a delay interval variable equal to an initial value. Similarly step initializes some additional variables setting the current member according to the known ordering to the first member of the replica set and zeroing a count that will be used for tracking the number of owned members against the quorum requirement. Step also sets entries in an array that track which members are owned by the node to false since no members are owned at this time. Step then tests the current member against the order number of the last member in the total replica set to determine whether arbitration has been attempted on each member in the total set of replica members. At this time the first member is still the current member and thus step branches to arbitrate for this current member as represented in the steps beginning at step of .

A first possible outcome to the read request is that the read will fail as detected at step because another node e.g. has previously placed and not released a reservation on the quorum member . At this time there is a possibility that the other node that has exclusive control of the quorum replica member has stopped functioning properly and consequently has left the replica member in a reserved locked state. Note that the nodes and are not communicating and thus there is no way for node to know why the communication has ceased e.g. whether the other node has crashed or whether the node itself has become isolated from the cluster due to a communication break. Thus in accordance with another aspect of the present invention the arbitration process includes a challenge defense protocol to the ownership of the members of the quorum replica set that can shift representation of the cluster from a failed node to another node that is operational.

To accomplish the challenge portion of the process if the read failed at step the challenging node first uses the SCSI bus reset command to break the existing reservation of the quorum replica member held by the other node . Next after a bus settling time e.g. two seconds at step the node saves the unique myseq identifier to a local variable old y and attempts to write the myseq identifier to the y variable location on the replica member . Note that the write operation may fail even though the reservation has been broken because another node may have exclusively reserved the replica member via its own arbitration process between the execution of steps and by the node . If the write fails at step the node knows that another node is competing for ownership whereby the node backs off by failing the arbitration and appropriately returning with a FALSE success code. Note that the write may also fail if the replica member has failed in which event it cannot be owned as a quorum member whereby the FALSE return is also appropriate.

However if the write was successful as determined at step the arbitration process of the node continues to step where the challenging node delays for a time interval equal to at least two times a predetermined delta value. As described below this delay gives a defending node an opportunity to persist its reservation of the replica member and defend against the challenge. Because nodes that are not communicating cannot exchange node time information the delta time interval is a fixed universal time interval previously known to the nodes in the cluster at present equal to a three second arbitration time and a bus settling time of two seconds. Note however that one bus settling time delay was already taken at step and thus step delays for double the arbitration time but only one additional bus settling time e.g. eight more seconds. After this delay step again attempts to read the y variable from the replica member .

Returning to step if the reading of the y variable was successful then no node had a reservation on the replica member and the local variable old y is set to the y variable step that was read. However it is possible that the read was successful because it occurred just after another arbitrating node broke the exclusive reservation of a valid operational owner. Thus before giving the node exclusive control ownership of the replica member step branches to step to delay for a period of time sufficient to enable the present exclusive owner if there is one enough time e.g. the full two delta time of ten seconds to defend its exclusive ownership of the current member. After the delay step continues to step to attempt to re read the y variable.

Regardless of the path taken to reach step if the read at step failed as determined by step then the arbitration is failed because some node reserved the replica member . Alternatively if at step the member s y variable that was read changed from its value preserved in the local old y variable then a competing node appears to be ahead in the arbitration process and the node backs off as described below so that the other node can obtain the quorum. However if the y variable has not changed it appears that no node is able to defend the replica member and that the node may be ahead in the arbitration whereby at step the arbitration process continues to step of .

Note that it is possible for a plurality of nodes to successfully complete the challenge procedure of and reach step of . In accordance with one aspect of the present invention a mutual exclusion algorithm is executed to ensure that only one of the plurality of nodes succeeds in completing the arbitration process. In accordance with the principles of a fast mutual exclusion algorithm at step of an attempt is made to write an identifier unique from other nodes to a second location x on the replica member . Note that as shown in for purposes of simplicity any time a read or write operation fails the arbitration is failed and thus only successful operations will be described in detail herein. Then steps and again test whether y s value on the replica member still equals the old y variable since it may have just been changed by another node e.g. node wrote to y while the operation of writing the x value by the node was taking place. If changed at least one other node is apparently contending for ownership and thus step backs off i.e. fails the arbitration process.

If y is still unchanged at step step generates a new unique myseq sequence identifier for the node into the y location on the replica member and if successful continues to step where the value at the x location is read. If at step the x location still maintains the my id value written at step then this node has won the arbitration reserves the disk at step and returns with a success return code of TRUE. Alternatively if at step the x location no longer maintains the ID of the node then apparently another node e.g. is also challenging for the right to obtain exclusive control. However it is possible that the other node has changed the x value but then backed off because the y value was changed e.g. at its own steps whereby the node is still the leader. Thus after a delay at step to give the other node time to write to the y location or back off the y value is read and if the y value is changed at step then the arbitration was lost. Note that a node which wins the arbitration writes the y location immediately thereafter as described below with reference to .

Conversely if the y value is still equal to the unique sequence ID myseq of the node at step then this node has won the arbitration and returns with the TRUE success return code. Note that the mutual exclusion mechanism of steps run by each competing node ordinarily ensures that only one node may ever reach step to persist the reservation because only the node having its ID in the y location can enter this critical section while the x location is used to determine if any other nodes are competing for the y location. However there is a non zero probability that more than one node will successfully complete the arbitration procedure given arbitrary processing delays. This is because fast mutual exclusion depends on the delay at step being long enough to guarantee that the participants that evaluated the condition at step as true are able to write down their sequence number to the disk at step . However if an unexpected delay occurs between steps and that is larger than the delay of step then more than one node could have successfully complete the arbitration procedure. This unlikely problem is made even less likely by the fact that a node places a SCSI reservation on a replica set member after successfully completing arbitration as will be discussed later with reference to .

Returning to step evaluates the code returned for the current member from the single member arbitration algorithm of . If not successful step branches to step to determine whether the failure to obtain control was caused by the member being owned by another node or whether the member was inaccessible e.g. crashed or not properly connected to the challenging node . If owned by another node step branches to to determine whether the challenging node already has a quorum or should back off and relinquish any members controlled thereby as described below. If the failure occurred because the member was not accessible as opposed to owned step branches to step to repeat the process on the next member as described below.

If at step it is determined that the challenging node was successful in obtaining control of the replica member step branches to step . At step the array tracking the node s control of this member is set to TRUE the count used for determining a quorum is incremented and the replica member is set to be defended by the node if the node is able to achieve control over a quorum of the members. Defense of an owned member is described below with reference to . Then at step the current member is changed to the next member if any and the process returns to step to again arbitrate for control of each remaining member of the total replica set of configured replica members.

Step of is executed when the replica members have all been arbitrated step of or if an arbitrated replica member was owned by another node step of as described above. Step tests whether the count of members owned by the challenging node achieved a quorum. If so step returns to its calling location with a TRUE success code whereby the next step in forming a cluster will ultimately take place at step of as described below.

If a quorum is not achieved step branches to step to relinquish control of any replica members that the node obtained ownership over recompute the delay interval and increment the retry loop counter. Step then repeats the process after the delay interval at step by returning to step of until a maximum number of retries is reached. Typically the delay calculation in step uses a well known exponential backoff as follows BackoffTime BackoffTime0 Rand BackoffTimeMin 

where BackoffTime0 is the maximum backoff time for the first try E is a number greater than 1 typically 2 for convenience n is the retry number 0 based represents exponentiation raised to the power BackoffTimeMin is the smallest practical backoff time and Rand is a function that returns a random number between 0 and 1.

If no quorum is achieved after retrying the process ultimately returns to step with a failure status. Steps and will repeat the attempt to join an existing cluster or start the formation attempt over again until some threshold number of failures is reached whereby some action such as notifying an administrator of the failure may take place.

It should be noted that describe a probabilistic algorithm. In general the ordering requirement the restart of the process upon failure to control a member and the random exponential backoff when taken together provide some non zero probability that one of a plurality of independent non communicating arbitrating nodes will successfully gain control of a quorum of the members in the set. The probability may be adjusted by tuning various parameters of the algorithm. Note that the use of exponential backoff techniques in arbitration algorithms is well known to those skilled in the art e.g. it is the basis for CSMA CD networks such as Ethernet. Moreover note that the probabilistic nature of the overall algorithm is different than the probability that more than one node will successfully complete the arbitration procedure given arbitrary processing delays as described above.

Returning to step of when a quorum is achieved an attempt is made to reconcile the replica members so that the correct cluster operational data may be determined. As described above a requirement on any mechanism for maintaining the cluster operational data is that a change made to the data by a first instance of a cluster be available to a second instance of the cluster that is formed at a later time. In other words no completed update may be lost. In order to meet these requirements for a set of replica members changes pertaining to the update are applied to a quorum of the replica members and an update is not deemed to be complete until this is successfully accomplished thereby guaranteeing that at least one member of any quorum set has the latest data. In general one way to accomplish this goal is to use a distributed consensus algorithm such as one similar to the algorithm generally described in the reference entitled Leslie Lamport 16 2 May 1998 133 169. In order to reconcile the states of different members of a replica set a quorum replica set algorithm described below is executed. In accordance with another aspect of the present invention as part of the quorum replica set algorithm a recovery process is initiated whenever a replica member becomes available and a majority of members are available. To determine the most updated member and thereby accomplish consistent reconciliation an epoch number is stored on the log header of a log maintained on each replica member. The epoch on the log header is incremented during the recovery process and corresponds to the epoch that begins with that recovery process. In addition every update is originally associated with an epoch number and a sequence number. These are stored on each replica member as part of the log record associated with this update. The epoch in the log records correspond to the epoch in which the update was made.

The failure of any read or write operation on a quorum replica set member during this recovery procedure is treated as a failure of the replica member although the operation may be optionally retried some number of times before declaring failure . A failed replica member is removed from the quorum replica set as described below with reference to . The cluster may continue operating despite the failure of a member of a quorum replica set at any point as long as the remaining set still constitutes a quorum. If the remaining set does not constitute a quorum then the cluster must cease operating at least with respect to allowing updates to the cluster operational data as described below. If the quorum requirement is still met after a replica member failure any update or reconciliation procedure that was in progress when the member failed continues forward unaltered after the failed member has been removed from the quorum replica set. This procedure guarantees that all updates to the cluster operational data are sequentially consistent that no committed update is ever lost and that any cluster instance which controls a quorum of the total replica set members will have the most current cluster operational data.

If the reconciliation of the members at step is determined to be successful at step the process returns to step of with a TRUE success status otherwise it returns with a FALSE status. As described above based on the status step either allows the cluster to operate or restarts the join formation attempt up to some threshold number of times.

Step of represents the performing of work by the cluster. In general the work continues until some event occurs or a time of delta elapses where delta is the arbitration time e.g. three seconds described above. Preferably the node continues to perform work and runs a background process when an event time interval is detected. Events may include a graceful shutdown a failure of a replica member and a failure of a node. Step tests if a shutdown has been requested whereby if so step returns to step of with a TRUE shutdown status. Step performs various cleanup tasks and step tests the shutdown status ending operation of the node if TRUE.

If not a shutdown event step of branches to step where the node makes a decision based on whether the node is the owner of the quorum of replica members. If so step branches to step of described below while if not step branches to step where the quorum owner s communication with the node is evaluated. If the quorum owning node is working step returns to step to resume performing work for the cluster. Otherwise step branches to step of as described below.

Turning to when a node e.g. represents the cluster at step the node tests whether an event corresponded to a failure of one or more of the replica members. If so step is executed to determine if the node still has control of a quorum of replica members. If not step returns to step of with a FALSE shutdown status whereby the cleanup operation will take place and the cluster join formation process will be repeated for this node . However if the node still has a quorum of members step branches to step to defend ownership of each of the members as described below. Note that the defense of the members is essentially performed on each member in parallel.

As shown at step of to defend each of the owned replica members the node first sets a loop counter for a number of write attempts to zero and then attempts to exclusively reserve that member e.g. via the SCSI reserve command. If unsuccessful another node has won control of this disk whereby the node re evaluates at step of whether it still possesses a quorum. If the node has lost the quorum the node will ultimately return to step of and repeat the join formation process.

If successful in reserving the disk step is next executed where a new myseq value is generated for this node and an attempt is made to write to write the y variable used in the arbitration process as described above. The y variable is essentially rewritten to cause other nodes that are monitoring the y value after breaking the previous reservation to back off as also described above. If the write succeeds the replica member was successfully defended and the process returns to step of with a TRUE success status. If the write failed steps and cause the write attempt to be repeated some maximum number of times until the process either successfully defends the replica member or fails to do so whereby the node needs to re evaluate whether it still has a quorum as described above. Note that an added benefit to using the SCSI reservation mechanism is that if a former owning node malfunctions and loses control of a member it is prevented from accessing that member by the SCSI reservation placed by the new owner. This helps prevent against data corruption caused by write operations as there are very few times that the members of the quorum replica set will not be exclusively reserved by a node e.g. only when a partition exists and the reservation has been broken but not yet persisted or shifted .

Returning to step after attempting to defend the members if the node no longer has a quorum the node returns to step of to cleanup and then repeat the join formation process. Conversely if the node still possesses a quorum of the members step is next executed to test whether the node that represents the cluster owns all the members of the total replica set of configured members. If so step returns to step of . However if not all the members are owned for reliability and robustness the node representing the cluster attempts to obtain control of as many of the operational replica members as it can. Thus at step the node attempts to gain control of any member M for which OwnedMember M FALSE using the single member arbitration algorithm of described above. If there are multiple members that are not owned the node may attempt to gain control of them in any order or in parallel.

Alternatively if at step the node successfully acquired control over a quorum of replica members step is executed to reconcile the quorum members and form the cluster as described above. If successful in reconciling the members the node returns to to perform work for the cluster it now represents including executing the steps of as appropriate otherwise the node returns to step of to cleanup and restart the joining formation process as described herein.

In alternative implementations not all of the cluster operational data need be maintained in the replica members only the data needed to get the cluster up and running as described in the aforementioned copending U.S. patent application entitled Data Distribution in a Server Cluster. In one such alternative implementation the replica members maintain this core boot data and also maintain information regarding the state of the other cluster operational data e.g. configuration information about the applications installed on the cluster and failover policies . The state information ensures the integrity of the other cluster operational data while the other storage device or devices e.g. a mirror set of storage elements that store this data provide a relatively high performance and or lower cost storage for this additional cluster configuration information with high reliability. In any event as used herein the replica members maintain at least enough information to get a cluster up and running but may store additional information as desired.

Note that a quorum need not be a simple majority but may for example be some other ratio of operational members to the total number such as a supermajority e.g. three of four or four of five . However a primary benefit of the present invention is to provide availability with the minimum number of components and such a supermajority requirement would tend to reduce availability.

Instead cluster availability may be increased by requiring only a simple majority while using a larger number of devices. For example three replica members may be configured for ordinary reliability in which two disks will have to fail to render the cluster unavailable. However the more that reliability is desired the more replica members may be used at a cost tradeoff e.g. three of five failures is less likely than two of three and so on. Note that SCSI limitations as to the number of replica members and their physical separation need not apply as described in U.S. patent application Ser. No. 09 260 194 entitled Method and System for Remote Access of Network Devices assigned to the same assignee as the present invention.

While having a set of multiple replica members increases cluster availability and reliability over a single quorum device having a replica set requires providing consistency across the members of the replica set. This consistency needs to be maintained even though individual replica members can fail and recover at various times.

In accordance with another aspect of the present invention to keep a replica set consistent in view of replica failures and recoveries and also following a temporal partition a quorum replica set QRS algorithm is provided that among other things performs a recovery process whenever a change to a replica set occurs that is any time a formerly unavailable replica member becomes available . The QRS algorithm also prevents updates when less than a quorum of replica members is available. To this end as part of the QRS algorithm any time a write occurs to a replica member described below with respect to the success of that write determines whether the replica member is available or has failed. If failed the remaining available set is checked for a majority and no updates are allowed unless there is a majority.

The QRS algorithm is capable of being run on any node that is capable of representing the cluster via ownership of the quorum replica set . The QRS algorithm may be run during startup as replica members are detected e.g. to bring those members online and is also run during normal execution such as by the node that possesses the quorum replica set to ensure that replica members that come online or go offline are properly dealt with and to ensure that data updates only occur when a quorum of configured members exists. For purposes of simplicity the QRS algorithm will be primarily described with respect to its operation after a cluster has already been formed.

The QRS algorithm includes three properties. A first property is that configuration information updates that are applied to a majority of the members of a replica set will never be undone or lost despite the subsequent failure and recovery of replica set members and or nodes executing the QRS algorithms. A second property is that an update that was recorded to some of the replica members but not committed in a previous recovery or an update that was made without the knowledge of a previously committed update in a later epoch will not get committed during recovery. A third property is that an update is reported to have succeeded if and only if the update was applied to a quorum of the replica members. Thus if an update was in progress when a failure occurred but had not yet been applied to a quorum of the replica members then its fate cannot be known until recovery is complete. Such an update may be either committed or discarded during the recovery procedure. When an update is committed to at least the quorum the update is reported to the cluster as having been successfully committed. Such reports commit notifications are generated in the same order in which the updates occur.

In order to ensure replica consistency the QRS algorithm uses a log a standard database technique that logs the updates in records on each replica member including three variables associated with the logged information. For example in the three member configured replica set generally represented in each replica member e.g. 0 1 and 2 includes a respective log . In each log a first variable is a replica epoch number respectively which is a number stored in a header of the log on each replica. The replica epoch number also referred to herein as a current replica epoch is associated with a recovery session as described below and always moves in one direction e.g. increases by at least one during the recovery process .

A second variable used by the QRS algorithm is an update epoch number. The update epoch number is stored with each logged record to associate that update record with the current replica epoch value at the time the update record was logged. In the log sequence number is represented by the box in each record e.g. Rec under the italicized letter E. 

A third variable is a log sequence number that tracks the relative sequence of each logged update record with respect to other logged update records. In the record epoch is represented by the box in each record e.g. Rec under the italicized letter S. After a successful recovery the sequence numbers are guaranteed to be the same for the logs on every replica member that is part of the currently available set of replica members. In particular it must not be possible for two different update records that were applied by two different cluster instances to have the same update epoch.sequence number. Note that in addition to the update epoch number and log sequence number each record also will typically e.g. except for certain NULL data instances contain the update data that describes the update.

The QRS algorithm will be described herein with reference to the general flow diagrams of and the above described replica epoch update epoch and sequence number variables. One part of the QRS algorithm is directed to handling replicas that are configured for cluster operation but were unavailable for some reason and then become available for operation. For example having another replica member become available may cause a quorum of replica members to be achieved where there previously was less than a majority of members available whereby updates then become possible. Another part of the QRS algorithm operates during a requested data update. Only when a majority of replica members have committed an update is an update reported as having been successful. Alternatively reads and updates may be prevented from even being attempted if the QRS algorithm has detected that a majority of replica members are not available. In addition an update attempt may fail because a replica member has failed in which event a majority may no longer be available and further data updates need to be prevented.

Step prevents updates from occurring during operation of the replica online process such as by setting an update variable to FALSE. For example as will be described below the update process of exits if updates are not allowed. Note that the replica online process of will re enable updates at step if certain conditions described below are met.

Step increments a count of the number of available replicas to reflect the detection of the newly available replica that triggered operation of . Step adds an identifier of this replica to a set that maintains which replicas are currently available. Then step represents the test for whether a quorum e.g. majority has been achieved based on the actual available count versus the number required for a majority which is known to the cluster nodes . If there is no majority step branches ahead to step to release the update lock after which the replica online process ends. Note that via the above described step updates are precluded in this situation.

If however a majority of replicas are available step then a recovery process is started as generally represented via . The recovery part of the QRS algorithm is thus executed when a majority of replica members are available from those that are configured for cluster operation. In general the recovery process operates to make the replica members consistent with one another so that possession by a node of any majority of replica members ensures that the latest changes are known to the cluster in any given quorum replica set. Note that although not shown in for purposes of simplicity if an operation fails at an appropriate place for example a write to log opening of a log propagating a record to a log or the like recovery is aborted via described below and a FALSE status is returned to as the recovery status to indicate the lack of success. Further note that not shown in each possible instance this inherent abort on failure situation applies when appropriate with respect to which are part of the recovery process.

Following recovery step of will test for success and if success status is FALSE regardless of where in the recovery process it was generated step will prevent updates essentially by bypassing step which if executed would re enable updates and instead branching ahead to step to release the update lock. Step is thus only executed to re enable updates if the recovery process was successful. Note however that it is alternatively feasible for a cluster to allow updates as long as a majority of replicas is still available e.g. there is no reason to halt updates when a majority exists before a new replica member is detected but that new replica fails during recovery as long as a majority still exists afterward. For simplicity only successful operations will be described hereinafter in the recovery process except as otherwise noted.

If the replica is not already initialized at step then the log file is opened at step . If the log is not new then it is mounted via steps and by reading the log header e.g. as a variable into the owning node s local storage verifying the validity of the log records by evaluating checksums maintained with each record or the like and then setting a sequence number e.g. as a variable in the owning node s local storage equal to the sequence number of the last valid record in the log. Note that if the read at step fails step aborts the recovery and takes this replica offline as described below with reference to . Further note that if the read at step is successful the log of each quorum replica set member is replayed via step during initialization to ensure that the replica member s data is self consistent. Any partially written records are discarded undone . Following step which also can have a read failure the process then advances to step entry Point of which sets the log opened variable to TRUE for this log. Step sets a recovery log header variable maintained in node local storage for this particular replica equal to the log header variable.

If the log was just created then it is initialized via steps and including initializing its local replica epoch and sequence variables to zero and writing the epoch data to the replica log header. The process then advances to step entry Point of .

At step of a starter record is prepared for the log on the replica with a record header epoch equal to the local log header epoch variable initialized to zero the local record header sequence equal to the log header sequence variable initialized to zero and NULL record data. Step attempts to write this record to the log. If the write fails step calls the abort recovery process of described below. If successful the local log header sequence variable is incremented for the next update . As described above once the log for this replica member is initialized the log opened variable is set to TRUE for this log at step and step sets a recovery log header variable maintained in node local storage for this particular replica equal to the log header variable. The process then returns to step of .

Step of is thus executed following the various log initialization operations of . Based upon the epoch numbers recorded in the header of each of the available replicas a maximum epoch number is determined at step . A current replica epoch is established by adding one to the maximum at step and the current replica epoch is written to the log headers on all the replicas in the availability set such that they are updated to the current replica epoch. Note that although not specifically shown a write failure results in the abort recovery process of being executed.

Step represents the reading of the last two valid records one record if only one record exists e.g. the starter record from the log of each available replica. Again although not specifically shown a read failure results in the abort recovery process of being executed.

At step from among the replicas the replica or replicas having a last record with the highest update epoch number is chosen as a candidate for leader. If at step only one replica has the highest epoch number in its last record there is only one candidate for the leader replica and it is selected as the leader at step . In the event of a tie in record epochs at step a leader is selected from the leader candidates based on the highest log sequence number in its last record. In other words the leader is a replica member having in its last record an epoch.sequence number greater than or equal the maximum epoch.sequence number of the last record on the available replicas. Note that if two or more candidates replicas have the same sequence number any one of those can serve as the leader replica since each have the same last record however another tiebreaker may be used based on some other criteria if desired. For example if an epoch.sequence tie exists the replica with the log having the lowest log identifier becomes the leader replica. Further if all available replicas each the same epoch and sequence number for their respective last record then no propagation of records is needed and these steps can be avoided. In the present example for purposes of explanation it is assumed that this is not the situation at this time.

Once a leader replica is selected the recovery process continues to to propagate any needed records from the leader to other replicas. At step the last record in the replica log of the leader replica is retagged with the current replica epoch.

Step selects a replica that is not the leader for updating. Based on the last two records therein previously read via step the records that are needed to update that non leader replica relative to the leader replica are determined at step . These records referred to as the set of records to update or recordset will be propagated to the selected non leader replica via the process of . In other words the necessary records from the leader replica greater than or equal to the Epoch.Sequence of the second last record on a non leader replica are propagated from the leader replica to the other replicas.

During the propagation the last two records on the non leader replicas need to be examined with respect to the records being propagated by the leader replica because the last record may correspond to an update that was made to this replica but that was not committed to a majority of the replicas and now conflicts with an update committed in a later epoch while the second last record may have been retagged in a previous unsuccessful recovery session. This part of the QRS algorithm shown in essentially determines whether to discard or retag the records in the selected non leader replica by comparing the first two records in the recordset sent by the leader replica against the last two records on that selected non leader replica. Thereafter any remaining records in the recordset sent by the leader replica are applied to the selected non leader replica to make it consistent.

More particularly step of first tests whether there is any second to last record on the selected non leader replica. If not step branches to step where the last record is evaluated at least the starter record will exist against the first record in the set of propagated records. If the records are not the same at step the last record in the non leader is replaced atomically with the first record in the propagated recordset from the leader. At step the remaining records in the recordset propagated from the leader replica are applied whereby this selected non leader replica is consistent. Although not specifically shown as mentioned above if any read or write failures occur the recovery process is aborted via described below.

If instead step determines that the second to last record on the selected non leader replica exists then step branches to step where the second to last record in the selected non leader replica is evaluated against the first record of the leader s propagated recordset. If the same step branches to step to evaluate the last record of the selected non leader replica against the second record of the leader s propagated recordset. If these are not the same then the last record of the non leader replica is atomically replaced by the second record of the leader s propagated recordset at step . Any remaining propagated records are then applied via step . If instead step determines that the epoch and sequence for the records match step branches to step wherein any remaining propagated records are then applied.

Returning to step if the second to last record in the selected non leader replica is not the same as the first record of the leader s propagated recordset step branches to step where the last record of the non leader replica is discarded. Step then replaces the second to last record of the non leader replica with the first record in the leader s propagated recordset and then step applies any remaining propagated records from the leader s propagated recordset to the selected non leader replica.

The process of ultimately returns to step of such as to determine whether another non leader replica needs to be updated. If so step selects that non leader replica as the selected non leader replica and the process of is similarly executed therefor. Note that steps to are generally represented as showing the propagation of the leader s records to each of the non leader replicas to one non leader replica at a time. However as can be readily appreciated some or all of these propagation related steps may be performed to multiple non leader replicas in parallel.

When the non leader replicas have been made consistent with the leader replica step is performed to report generate the commit notifications for the successful committing of the last record transmitted from the leader replica. For efficiency such commit notifications only have to be generated for records propagated since the last recovery.

At this time recovery is complete and step returns to step of where the success of the recovery process is evaluated. If successful step is executed to allow updates and the replica online including recovery process completes by releasing the update lock step .

As mentioned above if any read or write failure to a replica occurs during the recovery process the abort recovery process of is called with the identity of that replica. This function is called with the replica id of the bad replica if the recovery process fails. Note that the update lock is held when this function is invoked. At step a count of the number of available replicas is decremented and step removes the identifier of this replica from the set that tracks which replicas are currently available to reflect that this replica is no longer available. Step forces the log to be initialized again when the replica subsequently comes online by setting the log opened variable to FALSE for this replica. As described above this variable is evaluated at step of prior to initialization. A variable indicative of success evaluated at step of may also be set at step to indicate that recovery failed. Step then generates an event that will ordinarily cause other processes in the system to try and get this replica member online again check for its integrity and so forth. Step generates another event a recovery event which will restart recovery if a majority of replica members is present. Generating this recovery event guarantees that if this replica does not recover or come online the recovery process will be retried again as long as majority of replicas exists. Note that it is alternatively feasible to have test for whether a majority of replica members is consistent and if so to not consider the recovery to have completely failed which requires a restart of the recovery process.

In read operations acquire the update lock at step and prevent read operations at any time that updates are not allowed via step . An attempt to read while updates are not allowed is considered an error via step . If the read attempt is allowed step attempts to read the requested recordset and returns a status value equal to the success or failure of the read attempt. Note that if a read failure occurred this replica is taken offline as described below with respect to and this read can be retried on another member if a quorum still exists. Before returning to the process that requested the read the read operation releases the update lock at step . represents a replica update write request handled in conjunction with the QRS algorithm and its properties. At step a counter that tracks the number of successful writes is initialized to zero and at step the update lock is acquired as described above. Step then tests whether updates are currently allowed. As described above updates are not allowed unless a quorum of consistent replica members is available. If not allowed step branches to step where the update lock is released and an error is returned via step .

If updates are allowed step instead branches to step wherein an attempt to make the update is made e.g. a data write attempt to each available replica member. represents the actions taken on each replica member in the write update attempt. Note that the write attempts may be made in parallel.

At step of the log header variable of the replica member is set to equal the recovery log header variable for this replica consistent with step described above and the sequence number variable is increased at step . To build the update record the epoch number for the record is set to equal the epoch number stored in the local node s log header for this recovery epoch as described above. Similarly the sequence number for the record is set to equal the just incremented sequence number stored in the log header. Lastly the record s data field is set to include the data that is to be written at step . Note that any checksums or the like can be calculated and added to the record at this time. When ready an attempt to write the record is made at step .

Step evaluates whether the write was successful. Note that although not shown any writes to the replica member are not to be cached but instead written through to the disk. If the record is successfully written and flushed to the disk a TRUE status is returned to step of as the status of the operation. If either the write or any flush operation was not successful then FALSE is returned to step of as the status.

Steps through of work with the returned write status from each replica and thus are executed for each of the replicas. Step evaluates the write status for a given replica. If not successful updates to any replica are prevented via step and the particular replica on which the write failed is declared offline at step e.g. by generating an offline event or the like that will cause the offline process of to be called. The process for handling an offline replica is described below with respect to however at this time it should be pointed out that among other things when a replica goes offline the offline handling process re enables updates if a majority of replicas are still available. For a write that was successful the write counter is incremented at step .

When a write status has been returned from for each replica step compares the number of successful writes in the counter against the majority number that is required for a quorum. If a majority was not successfully written then a FALSE status is returned as the update status via step to the process that requested the update. Note that when step is executed updates are not allowed via step . The update lock is released via step .

If instead at step a majority was successfully written step is executed which reports that this record was successfully committed. Step re enables further updates since a majority of writes were known to be successful. A TRUE is returned via step and the update lock is released via step .

Step decrements a count of the number of available replicas and step removes the identifier of this replica from the set that tracks which replicas are currently available to reflect that this replica is no longer available. Note that steps and are essentially counter to the steps and that are described above for when a replica becomes available. Step forces the log variable in the recovery structure to be initialized again when the replica subsequently comes online by setting a variable or the like for this replica. As described above this variable is evaluated at step of prior to initialization.

Step represents the test for whether a quorum e.g. majority still exist based on the count that remains versus the number required for a majority. If there is not a majority step branches to step to disable updates. If there is a majority step instead branches to step to allow updates. After either step the offline process continues to step to release the update lock after which the replica offline process ends.

Returning to an example will now be provided of the general operation of the QRS algorithm as described above. In two replica members are available from a configured set of three replica members wherein the logs replica epochs and headers of each have the replica member number as a subscript. Note that in the large diagonally crossed lines indicate the unavailability of whichever replica member is crossed out.

In the current replica epochs and in respective headers and are both at 1. As also shown in update . has been logged in both replica logs and and thus this update is considered successfully committed. Update . has not been committed to a majority and thus is not reported as being successfully committed. In the present example at this time assume that the node controlling the replica members dies or shuts down unexpectedly whereby the update . is not recorded to a majority of replicas and is thus not reported as having successfully committed.

As also represented in while later operating replica members and both commit a record record . to their respective logs and . Because this record was successfully written flushed to a majority of total configured members the update is considered successful as described above with respect to . Still later an update record . is written to replica member but not to replica member as in this example the node owning and controlling the replica member dies or shuts down unexpectedly. Again since this update was not recorded to a majority the change corresponding to this update record is not acknowledged as having been committed.

Sometime later as generally represented in replica member comes online whereby the replica majority is achieved via members and and recovery is initiated via the online process as described above. In this next replica epoch replica and have their replica epochs and in respective headers and both set to 3 since the largest previous epoch number in any record record . in replica was 2 as apparent from . In addition as described above replica becomes the leader since it had the record therein with a record epoch equal to 2 whereas replica s largest record epoch number was a . As also shown in the changes from to during recovery the record . of replica is discarded because this last record was determined via described above to have not been committed to the quorum replica set prior to propagated retagged record . of replica having been committed. Replica record . thus overwrites this record in the log and the recovery process reports the update as being successfully committed. In the example replica member then goes offline without any other updates having occurred.

In the last part of the example generally represented in replica member comes online whereby the replica quorum is now achieved via members and and recovery is initiated. In this next replica epoch replica and have their replica epochs and both set to 4 since the recovery process determines that the largest previous epoch number in any record was 3. In keeping with the present invention replica is chosen as the leader since it had the record therein with a record epoch equal to 3 whereas replica s largest record epoch number was a 2. As also shown in during recovery the second to last record . of replica is kept and retagged to . while the last record . is discarded as being not having been committed prior to a subsequent record having been committed. As can be readily appreciated regardless of which replica fails and or when it fails the QRS algorithm ensures that no record which is successfully committed is ever lost. At the same time the QRS algorithm ensures that records that were not committed to a majority are not kept if a subsequent update was committed first. Lastly reports of successfully committed updates commit notifications are generated in the same order in which the updates occur.

The above description and accompanying examples are directed to handling replica members becoming available or unavailable when the total configured replica set is constant. However the QRS algorithm can also handle the situation wherein new previously unknown replica members are added to the total configured replica set or when previously configured members are removed from the total configured replica set.

As can be readily appreciated changing the number of replica members in the total configured replica set changes the majority requirement which if done incorrectly could cause a significant problem. When adding replica members care must be taken to ensure that in the event of a cluster or replica member failure during the addition process a subsequent majority cannot be allowed without at least one member present from the prior epoch. For example it cannot be possible to change from a two of three requirement to a three of five requirement prior to making the new replicas consistent otherwise data could be lost. By way of example if a first quorum set is operating with only replica members A and B available out of a total configured replica set of A B and C replica member C is inconsistent. If new replica members D and E are then added and the majority requirement becomes three of five forming a new cluster with only replica members C D and E cannot be allowed unless at least one of C D and E are first updated to include A and B s data.

Also when adding a replica member and thus changing the majority requirement the change needs to be done such that a majority can later be achieved regardless of failures. For example if only two replicas A B are available out of three replicas A B and C configured and the number of the total configured replica set is increased to four by the addition of replica member D then three replicas will be needed for a majority. If however after increasing the majority requirement the cluster and the replica D fail while making D consistent then only A and B may be available which will not achieve a quorum.

At step the update lock is acquired and the update process of described above is called to make a change to the quorum configuration information maintained in the replica set namely to record that a new replica is being added to the total configured replica set. At step further updates are prevented until re enabled as described above. If the update was successful as evaluated at step the new replica is recognized is brought online at step similar to the online process described above with respect to . If the update failed a recovery event is issued which among other this will re enable updates if a majority of configured replicas is available as described above.

If the update is successful step and the replica is now online step then the above described recovery process of is started to make the new replica member consistent with the set. If recovery is successful at step then the status is set to TRUE at step the update lock is released at step and the status returned step .

If either the update failed step or recovery failed step the status is set to FALSE at step . The update lock is then released at step and the status returned step . Note that if the update was not successful this change might get committed to the current majority during the next recovery. If so recovery can keep track of this special update and reenter recovery. Further note that failure during the writing of the epoch metadata 1 1 will leave the new replica in an uninitialized state so it will never be visible to the cluster. Still further note that failure during the update may leave the system in a state where some of replicas are aware of the new member or members while others are not. If during subsequent recovery a replica member that is aware of a new member is operating in the quorum set the new member information will get propagated and the new member will be included and made consistent. If no member of a new quorum replica set is aware of the new member an extra member will be visible but will be ignored by the cluster since it will not be part of the total configured replica set.

When removing decommissioning a replica member from the total configured replica set care similar to that described above is taken to ensure that the problems above are not encountered in the event of failures namely that data is not lost and that a majority can still be achieved after removal. describes the removal of a replica member or members from the total configured replica set in a manner that handles failures. At step the update lock is acquired and at step the replica is tested for whether it is part of the available set i.e. is online. If so step branches to step which takes the replica offline.

Next the update process of described above is called to make a change to the quorum configuration information maintained in the replica set that is to record that a replica is being removed from the total configured replica set. One reason that the update may fail is that bringing the replica member offline causes the majority to be lost. However if the update was successful the recovery process will be correct since the change will be on the old majority of replicas and consequently will be on a new majority of replicas.

At step further updates are prevented until re enabled as described above. If the update was successful as evaluated at step then the above described recovery process of is started to ensure that the remaining replica members are consistent in the set. If recovery is successful at step then the status is set to TRUE at step the update lock is released at step and the status returned step .

If either the update failed step or recovery failed step the status is set to FALSE at step . The update lock is then released at step and the status returned step . Note that if the update was not successful this change might get committed to the current majority during the next recovery. If so recovery can keep track of this special update and can reenter recovery.

As can be seen from the foregoing detailed description there is provided a method and system for increasing the availability of a server cluster while reducing its cost. By requiring a server node to own a quorum of replica members in order to form or continue a cluster and maintaining the consistency of the replica members integrity of the cluster data is ensured.

While the invention is susceptible to various modifications and alternative constructions certain illustrated embodiments thereof are shown in the drawings and has been described above in detail. It should be understood however that there is no intention to limit the invention to the specific forms disclosed but on the contrary the intention is to cover all modifications alternative constructions and equivalents falling within the spirit and scope of the invention.

