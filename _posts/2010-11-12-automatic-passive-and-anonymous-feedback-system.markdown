---

title: Automatic passive and anonymous feedback system
abstract: A system for generating passive and anonymous feedback of multimedia content viewed by users is disclosed. The multimedia content may include recorded video content, video-on-demand content, television content, television programs, advertisements, commercials, music, movies, video clips, and other on-demand media content. One or more of the users in a field of view of a capture device connected to the computing device are identified. An engagement level of the users to multimedia content being viewed by the users is determined by tracking movements, gestures, postures and facial expressions performed by the users. A report of the response to viewed multimedia content is generated based on the movements, gestures, postures and facial expressions performed by the users. The report is provided to rating agencies, content providers and advertisers. In one embodiment, preview content and personalized content related to the viewed multimedia content is received from the content providers and advertisers based on the report. The preview content and personalized content are displayed to the users.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08667519&OS=08667519&RS=08667519
owner: Microsoft Corporation
number: 08667519
owner_city: Redmond
owner_country: US
publication_date: 20101112
---
Television rating systems rely on self recorded paper diaries or electronic metering technology to measure the number of people watching a television program or show. For example rating systems typically utilize electronic meters that are placed near television sets to determine when a television set is on what channel the television set is tuned to and how many people are watching a particular television program. Rating systems typically utilize a representative sample of a population to gather a certain amount of data about a general population. In addition paper diaries utilized by rating systems are generally affected by response biases by viewers of television shows or programs.

Disclosed herein is a method and system that generates passive and anonymous feedback of multimedia content viewed by users by tracking movements gestures postures vocal responses and facial expressions performed by the users while the users view the multimedia content. The multimedia content may include recorded video content video on demand content television content television programs advertisements commercials music movies video clips and other on demand media content. In one embodiment of the disclosed technology a user s presence in a field of view while the user views program content via the user s computing device is detected. The type of program content being viewed by the user the user s demographic information and the user s program viewing history is determined. The detection of the user s presence the program content the user s demographic information and the user s program viewing history is utilized by rating agencies content providers and advertisers to provide preview content and personalized content related to the program viewed by the user.

In another embodiment a user s engagement level to a viewed program is determined by tracking movements gestures postures and facial expressions performed by the user. The user s movements gestures postures and facial expressions are provided to one or more of rating agencies content providers and advertisers. In one embodiment the user s gestures postures movements and facial expressions are utilized by content providers and advertisers to provide preview content and personalized content related to a program viewed by a user. The preview content and the personalized content are displayed to the user via a display device. In another embodiment the disclosed technology enables the polling and aggregation of responses to viewed multimedia content from a large number of households to generate a large user response data set for analysis by content providers and advertisers.

In one embodiment a method for generating passive and anonymous feedback of multimedia content viewed by users is disclosed. The method includes receiving and displaying multimedia content associated with a current broadcast. The method includes identifying one or more of the users in a field of view of a capture device connected to a computing device and automatically determining an engagement level of the users to the multimedia content being viewed by the users. In one embodiment the engagement level of the users is determined by tracking the movements gestures postures audio responses and facial expressions performed by the users. The method further includes automatically generating a report of a response to the multimedia content viewed by each user identified by the capture device based on the movements gestures postures audio responses and facial expressions performed by the users. The report is transmitted to a remote computing system for analysis.

In one embodiment preview content related to a program viewed by the user is received from content providers and advertisers based on the report. The preview content is displayed to the user via an audio visual device connected to a computing device. In another embodiment personalized content related to a program viewed by the user is received from the content providers and advertisers based on the report. The personalized content is displayed to the user via an audio visual device connected to a computing device.

This summary is provided to introduce a selection of concepts in a simplified form that are further described below in the detailed description. This summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter. Furthermore the claimed subject matter is not limited to implementations that solve any or all disadvantages noted in any part of this disclosure.

Technology is disclosed by which a user s response to viewed multimedia content is obtained by tracking the user s movements gestures postures and facial expressions while viewing the multimedia content. Multimedia content may be received at a computing device or at an audiovisual device connected to the computing device. A capture device connected to the computing device identifies one or more users in a field of view and tracks the users movements gestures postures and facial expressions while the users view the multimedia content. In one embodiment the computing device determines a user s engagement level to a television program being viewed by the user based on the user s movements gestures postures and facial expressions. The computing device generates a user specific report of a response to the viewed program for each user identified by the capture device based on the gestures postures movements and facial expressions performed by each of the users. The user specific report is provided to one or more rating agencies content providers and advertisers. In one embodiment the computing device receives preview content related to a viewed program or personalized content related to a viewed program for the users based on the user specific report from the content providers or advertisers. In another embodiment the computing device may also receive preview content or personalized content related to a program viewed by a user based on detecting the user s presence in a field of view of the user s computing device while the user views the program the type of program viewed by the user the user s demographic information and the user s program viewing history. For example if it is determined that a male user in the age group 30 35 is watching a science fiction program and the user s program viewing history indicates the user s preference for science fiction then the user may receive preview content that contains a snippet of the next episode of the science fiction program or personalized content that contains a selection of a set of science fiction programs that the user would like to view. The responses to viewed multimedia content may be polled from multiple users in multiple households and the aggregated responses of the multiple users may be transmitted to a remote computing system for analysis by content providers and advertisers.

As shown in the tracking system may further include a capture device . The capture device may be for example a camera that may be used to visually monitor one or more users such as the users and such that movements postures and gestures performed by the users may be captured and tracked by the capture device .

According to one embodiment computing device may be connected to an audiovisual device such as a television a monitor a high definition television HDTV or the like that may provide visuals and or audio to users and . For example the computing device may include a video adapter such as a graphics card and or an audio adapter such as a sound card that may provide the audiovisual signals to a user. The audiovisual device may receive the audiovisual signals from the computing device and may output visuals and or audio associated with the audiovisual signals to the users and . According to one embodiment the audiovisual device may be connected to the computing device via for example an S Video cable a coaxial cable an HDMI cable a DVI cable a VGA cable or the like.

In one embodiment capture device tracks one or more movements gestures and postures performed by users within a field of view of the capture device . Lines and denote a boundary of the field of view . In one embodiment computing device determines a user s response to multimedia content being viewed via the audio visual device based on the user s movements postures and gestures tracked by the capture device . Multimedia content can include any type of audio video and or image media content received from media content sources such as content providers broadband satellite and cable companies advertising agencies the internet or video streams from a web server. As described herein multimedia content can include recorded video content video on demand content television content television programs advertisements commercials music movies video clips and other on demand media content. Other multimedia content can include interactive games network based applications and any other content or data e.g. program guide application data user interface data advertising content closed captions data content metadata search results and or recommendations etc. . The operations performed by the capture device and the computing device are discussed in detail below.

As shown in the capture device may include an image camera component . According to one embodiment the image camera component may be a depth camera that may capture a depth image of a scene. The depth image may include a two dimensional 2 D pixel area of the captured scene where each pixel in the 2 D pixel area may represent a depth value such as a distance in for example centimeters millimeters or the like of an object in the captured scene from the camera.

As shown in the image camera component may include an IR light component a three dimensional 3 D camera and an RGB camera that may be used to capture the depth image of a capture area. For example in time of flight analysis the IR light component of the capture device may emit an infrared light onto the capture area and may then use sensors to detect the backscattered light from the surface of one or more targets and objects in the capture area using for example the 3 D camera and or the RGB camera . In some embodiments pulsed infrared light may be used such that the time between an outgoing light pulse and a corresponding incoming light pulse may be measured and used to determine a physical distance from the capture device to a particular location on the targets or objects in the capture area. Additionally the phase of the outgoing light wave may be compared to the phase of the incoming light wave to determine a phase shift. The phase shift may then be used to determine a physical distance from the capture device to a particular location on the targets or objects.

According to one embodiment time of flight analysis may be used to indirectly determine a physical distance from the capture device to a particular location on the targets or objects by analyzing the intensity of the reflected beam of light over time via various techniques including for example shuttered light pulse imaging.

In another example the capture device may use structured light to capture depth information. In such an analysis patterned light i.e. light displayed as a known pattern such as grid pattern or a stripe pattern may be projected onto the capture area via for example the IR light component . Upon striking the surface of one or more targets or objects in the capture area the pattern may become deformed in response. Such a deformation of the pattern may be captured by for example the 3 D camera and or the RGB camera and may then be analyzed to determine a physical distance from the capture device to a particular location on the targets or objects.

According to one embodiment the capture device may include two or more physically separated cameras that may view a capture area from different angles to obtain visual stereo data that may be resolved to generate depth information. Other types of depth image sensors can also be used to create a depth image.

The capture device may further include a microphone . The microphone may include a transducer or sensor that may receive and convert sound into an electrical signal. According to one embodiment the microphone may be used to reduce feedback between the capture device and the computing device in the target recognition analysis and tracking system . Additionally the microphone may be used to receive audio signals that may also be provided by the user to control applications such as game applications non game applications or the like that may be executed by the computing device .

In one embodiment the capture device may further include a processor that may be in operative communication with the image camera component . The processor may include a standardized processor a specialized processor a microprocessor or the like that may execute instructions that may include instructions for storing profiles receiving the depth image determining whether a suitable target may be included in the depth image converting the suitable target into a skeletal representation or model of the target or any other suitable instruction.

The capture device may further include a memory component that may store the instructions that may be executed by the processor images or frames of images captured by the 3 D camera or RGB camera user profiles or any other suitable information images or the like. According to one example the memory component may include random access memory RAM read only memory ROM cache Flash memory a hard disk or any other suitable storage component. As shown in the memory component may be a separate component in communication with the image capture component and the processor . In another embodiment the memory component may be integrated into the processor and or the image capture component . In one embodiment some or all of the components and of the capture device illustrated in are housed in a single housing.

The capture device may be in communication with the computing device via a communication link . The communication link may be a wired connection including for example a USB connection a Firewire connection an Ethernet cable connection or the like and or a wireless connection such as a wireless 802.11b g a or n connection. The computing device may provide a clock to the capture device that may be used to determine when to capture for example a scene via the communication link .

The capture device may provide the depth information and images captured by for example the 3 D or depth camera and or the RGB camera to the computing device via the communication link . The computing device may then use the depth information and captured images to perform one or more operations of the disclosed technology as discussed in detail below.

In one set of operations performed by the disclosed technology multimedia content associated with a current broadcast is initially received from one or more media content sources such as content providers broadband satellite and cable companies advertising agencies the internet or video streams from a web server. The multimedia content may be received at the computing device or at the audiovisual device connected to the computing device . The multimedia content may be received over a variety of networks. Suitable types of networks that may be configured to support the provisioning of multimedia content services by a service provider may include for example telephony based networks coaxial based networks and satellite based networks. In one embodiment the multimedia content is displayed via the audiovisual device to the users. The multimedia content can include recorded video content video on demand content television content television programs advertisements commercials music movies video clips and other on demand media content.

In one embodiment multimedia content associated with the current broadcast is identified. In one example the multimedia content identified may be a television program movie a live performance or a sporting event. For example the multimedia content may be identified as a television program by identifying the channel and the program that the television set is tuned to during a specific time slot from metadata embedded in the content stream or from an electronic program guide provided by a service provider. In one embodiment the audio visual device connected to the computing device identifies the multimedia content associated with the current broadcast. In another embodiment computing device may also identify the multimedia content associated with the current broadcast.

In one embodiment capture device initially captures one or more users viewing multimedia content in a field of view of the capture device. Capture device provides a visual image of the captured users to the computing device . Computing device performs the identification of the users captured by the capture device . In one embodiment computing device includes a facial recognition engine to perform the identification of the users. Facial recognition engine may correlate a user s face from the visual image received from the capture device with a reference visual image to determine the user s identity. In another example the user s identity may be also determined by receiving input from the user identifying their identity. In one embodiment users may be asked to identify themselves by standing in front of the computing system so that the capture device may capture depth images and visual images for each user. For example a user may be asked to stand in front of the capture device turn around and make various poses. After the computing system obtains data necessary to identify a user the user is provided with a unique identifier identifying the user. More information about identifying users can be found in U.S. patent application Ser. No. 12 696 282 Visual Based Identity Tracking and U.S. patent application Ser. No. 12 475 308 Device for Identifying and Tracking Multiple Humans over Time both of which are incorporated herein by reference in their entirety. In another embodiment the user s identity may already be known by the computing device when the user logs into the computing device such as for example when the computing device is a mobile computing device such as the user s cellular phone. In another embodiment the user s identity may also be determined using the user s voice print.

In one embodiment the user s identification information may be stored in a user profile database in the computing device . The user profile database may include information about the user such as a unique identifier associated with the user the user s name and other demographic information related to the user such as the user s age group gender and geographical location in one example. The user profile database may also include information about the user s program viewing history such as a list of programs viewed by the user and recent movies or songs purchased by the user.

In one set of operations performed by the disclosed technology capture device tracks the users movements gestures postures and facial expressions while the users view multimedia content via the audio visual device . In one example the gestures postures and movements tracked by the capture device may include detecting if a user moves away from the field of view of the capture device or turns away from the audio visual device while viewing the program stays within the field of view of the capture device faces the audio visual device or leans forward or talks to the display screen of the audio visual device while viewing the program. Similarly facial expressions tracked by the capture device may include detecting smiles laughter cries frowns yawns or applauses from the user while the user views the program.

In one embodiment computing device includes a gestures library and a gesture recognition engine . Gestures library includes a collection of gesture filters each comprising information concerning a movement gesture or posture that may be performed by the user. In one embodiment gesture recognition engine may compare the data captured by the cameras and device in the form of the skeletal model and movements associated with it to the gesture filters in the gesture library to identify when a user as represented by the skeletal model has performed one or more gestures or postures. Computing device may use the gestures library to interpret movements of the skeletal model to perform one or more operations of the disclosed technology. More information about the gesture recognition engine can be found in U.S. patent application Ser. No. 12 422 661 Gesture Recognition System Architecture filed on Apr. 13 2009 incorporated herein by reference in its entirety. More information about recognizing gestures and postures can be found in U.S. patent application Ser. No. 12 391 150 Standard Gestures filed on Feb. 23 2009 and U.S. patent application Ser. No. 12 474 655 Gesture Tool filed on May 29 2009 both of which are incorporated by reference herein in their entirety. More information about motion detection and tracking can be found in U.S. patent application Ser. No. 12 641 788 Motion Detection Using Depth Images filed on Dec. 18 2009 and U.S. patent application Ser. No. 12 475 308 Device for Identifying and Tracking Multiple Humans over Time both of which are incorporated herein by reference in their entirety.

Facial recognition engine in computing device may include a facial expressions library . Facial expressions library includes a collection of facial expression filters each comprising information concerning a user s facial expression. In one example the facial recognition engine may compare the data captured by the cameras in the capture device to the facial expression filters in the facial expressions library to identify a user s facial expression. In another example facial recognition engine may also compare the data captured by the microphone in the capture device to the facial expression filters in the facial expressions library to identify one or more vocal or audio responses such as for example sounds of laughter or applause from a user. Audio responses may also include for example singing saying lines with a character appearing in the program content commentary from the user etc.

In another embodiment the user s movements gestures postures and facial expressions may also be tracked using one or more additional sensors that may be positioned in a room in which the user is viewing multimedia content via the audiovisual device or placed for example on a physical surface such as a tabletop in the room. The sensors may include for example one or more active beacon sensors that emit structured light pulsed infrared light or visible light onto the physical surface detect backscattered light from the surface of one or more objects on the physical surface and track movements gestures postures and facial expressions performed by the user. The sensors may also include biological monitoring sensors user wearable sensors or tracking sensors that can track movements gestures postures and facial expressions performed by the user.

In one embodiment the disclosed technology provides a mechanism by which a user s privacy concerns are met while interacting with the target recognition and analysis system by anonymizing the user s profile information prior to tracking the user s movements gestures postures and facial expressions. In one example an opt in by the user to the tracking of the user s movements gestures postures and facial expressions while viewing a program is also obtained from the user before implementing the disclosed technology. The opt in may display an option with text such as Do you consent to the tracking of your movements gestures postures and facial expressions The option may be displayed to the user during initial set up of the user s system each time the user logs into the system or during specific sessions such as just before the user starts watching a movie or a program.

In another set of operations performed by the disclosed technology computing system determines a user s engagement level to multimedia content viewed by the user such as a television program based on the user s movements gestures postures audio responses and facial expressions tracked by the capture device . In one embodiment capture device may track a user s gestures postures movements and facial expressions during consecutive time intervals that span the length of the duration of the program and computing device may determine an engagement level of the program viewed by the user during the consecutive time intervals based on the gestures postures movements and facial expressions performed by the user during each consecutive time interval. It is to be appreciated that the tracking of a user s gestures postures movements and facial expressions during consecutive time intervals that comprise the duration of a program enables the determination of a user s engagement level to specific portions of the program and also the determination of the specific portions of the viewed program that caused a specific engagement level from the user.

In one embodiment computing device includes a user specific response tracking module . User specific response tracking module determines a user s engagement level to a program being viewed by the user based on the gestures postures audio responses movements and facial expressions performed by the user while viewing the program content. In one example the engagement level of the user may be determined to be one of positive satisfactory or negative based on the types of movements gestures postures audio responses and facial expressions performed by the user while viewing the program.

In one example the engagement level of the user is determined to be negative if the user s postures or gestures indicate that the user moved away from the field of view of the capture device or if the user s head was turned away from the audio visual device while viewing the program or if the user was using another device such as the user s phone lap top or personal computer while viewing the program. Similarly the engagement level of the user is determined to be negative if the user s facial expression indicated one of boredom or if a user s vocal or audio response indicated a yawn. The user s engagement level to a program viewed by a user may be determined to be satisfactory if the gestures and postures performed by the user indicate that the user faced the display and was in the field of view for a threshold percentage of time while viewing the program in one example. The threshold percentage of time may be pre determined by the computing device in one implementation. Similarly the engagement level to a program viewed by a user may be determined to positive if the user was within the field of view of the capture device for the entire duration of the program faced the audio visual device or leaned forward while viewing the program. It is to be appreciated that the types of gestures postures movements and facial expressions utilized to determine the user s engagement level while viewing a program as discussed above are for illustrative purposes and different combinations of gestures postures audio responses movements and facial expressions may also be utilized to determine a user s engagement levels to a viewed program in other embodiments. For example an engagement level of the user may also be determined by detecting the user s presence in the field of view of the computing device as soon as the user hears a sound of laughter or applause from the program content being displayed via the user s computing device.

In another example the engagement level of the user may also be determined based on detecting the duration of time that the user was engaged while viewing the program for example by detecting the duration of time that the user faced the display while viewing the program content . For example the user may watch a program for the first five minutes perform other activities on other devices such as the user s phone or personal computer for fifteen minutes and then watch the program again for another ten minutes and so forth.

In one embodiment the user specific response tracking module generates a user specific report of a response to a viewed program for each user identified by the capture device based on the gestures postures movements and facial expressions performed by each of the users. In one example the user specific report of a response to a viewed program may be implemented as a table with fields such as one or more time intervals that comprise the length of the duration of the program the movement postures or gestures and facial expressions performed by a user during each time interval and the user s engagement level to the viewed program during each time interval. An exemplary illustration of a user specific report of a response to a viewed program is illustrated in Table 1 as shown below 

In another embodiment the user specific response tracking module may also generate a user specific report of the average response to one or more viewed programs by each user identified by the capture device . In one example the user specific report of the average response to programs viewed by a user may include information such as one or more programs viewed by a user over a period of time the program genre the percentage of program episodes viewed by the user and the user s average engagement level while watching each program episode. In one embodiment the user s average engagement level may be determined by assigning numeric values to the positive satisfactory or negative engagement levels obtained from the user over the duration of the viewed program determining the average of the numeric values corresponding to the engagement levels and computing the average engagement level by determining if the average of the numeric values falls within a pre defined range of values.

For example suppose a negative satisfactory and a positive engagement level is obtained during consecutive time intervals and the numeric values assigned to a negative engagement level is 1 a satisfactory engagement level is 5 and a positive engagement level is 10 and the pre defined range of values for a negative engagement level is 1 4 a satisfactory engagement level is 4 6 and a positive engagement level is 6 10 then the user s average engagement level to the viewed program is determined to be satisfactory in one implementation based on the average of the numeric values which in this example is 5.3 and the pre defined range of numeric values which in this example is 4 6 . In one example the user s average engagement level to a viewed program may be displayed as a list of the user s engagement levels to each program episode viewed by the user. An exemplary illustration of a user specific report of the average response to a set of viewed programs is illustrated in Table 2 as shown below 

It is to be appreciated that the user specific reports generated in Table 1 and Table 2 provide passive and anonymous feedback of program content viewed by a user by providing a detailed analysis of the gestures postures movements facial expressions performed by the user while viewing the program content. In one embodiment computing device may provide the user specific reports to a remote computing system for analysis. For example in one embodiment the user specific reports may be utilized by television rating agencies to determine more accurate ratings of viewed program content in one embodiment.

In one embodiment computing device may receive preview content related to a viewed program for one or more users based on the user specific reports from a remote computing system. For example in one embodiment the user specific reports may be utilized by one or more content providers or advertisers to generate preview content related to a viewed program for the users. Preview content may include for example content that is currently in development for a program that may be initially presented to a selected subset of users before the commencement of a public presentation of the content. For example preview content may include alternate endings of a program a snippet of the next episode of the program or a preview to an upcoming movie or show.

In another embodiment computing device may receive personalized content related to a viewed program for one or more users based on the user specific reports from the content providers or advertisers. Personalized content may include for example a selection of a set of most relevant programs that a user would like to view or customization of the type and amount of information to be conveyed to a user while the user views a program.

In one embodiment computing device may also provide information about the user s presence in a field of view while viewing the multimedia content the type of program viewed by the user the user s demographic information and the user s program viewing history to one or more content providers and advertisers to receive preview content and personalized content related to the viewed program from the content providers and advertisers. For example if it is determined that a male user in the age group 30 35 is watching a science fiction program and the user s program viewing history indicates the user s preference for science fiction then the user may receive preview content related to a snippet of the next episode of the science fiction program or personalized content related a selection of a set of science fiction programs that the user would like to view from the content providers and advertisers.

The preview content and the personalized content may be stored in a user preferences module in the computing system . In one embodiment the user preferences module includes a user specific personalized content module and a user specific preview content module . The user specific personalized content module stores personalized content related to a viewed program for one or more users interacting with the computing device. The user specific preview content module stores preview content related to a viewed program for one or more users interacting with the computing device. The display module displays the personalized content and preview content to the users via the audio visual device connected to the computing device .

In one embodiment and as discussed above computing device may directly provide the user specific reports to one or more rating agencies content providers and advertisers and receive preview content and personalized content from the content providers or advertisers. In an alternate embodiment computing device may also provide the user specific reports to a user response aggregation service which may then provide the user specific reports to the rating agencies content providers and advertisers. The operations performed by the user response aggregation service are discussed in detail in .

CPU memory controller and various memory devices are interconnected via one or more buses not shown . The details of the bus that is used in this implementation are not particularly relevant to understanding the subject matter of interest being discussed herein. However it will be understood that such a bus might include one or more of serial and parallel buses a memory bus a peripheral bus and a processor or local bus using any of a variety of bus architectures. By way of example such architectures can include an Industry Standard Architecture ISA bus a Micro Channel Architecture MCA bus an Enhanced ISA EISA bus a Video Electronics Standards Association VESA local bus and a Peripheral Component Interconnects PCI bus also known as a Mezzanine bus.

In one implementation CPU memory controller ROM and RAM are integrated onto a common module . In this implementation ROM is configured as a flash ROM that is connected to memory controller via a PCI bus and a ROM bus neither of which are shown . RAM is configured as multiple Double Data Rate Synchronous Dynamic RAM DDR SDRAM modules that are independently controlled by memory controller via separate buses not shown . Hard disk drive and portable media drive are shown connected to the memory controller via the PCI bus and an AT Attachment ATA bus . However in other implementations dedicated data bus structures of different types can also be applied in the alternative.

A graphics processing unit and a video encoder form a video processing pipeline for high speed and high resolution e.g. High Definition graphics processing. Data are carried from graphics processing unit to video encoder via a digital video bus not shown . An audio processing unit and an audio codec coder decoder form a corresponding audio processing pipeline for multi channel audio processing of various digital audio formats. Audio data are carried between audio processing unit and audio codec via a communication link not shown . The video and audio processing pipelines output data to an A V audio video port for transmission to a television or other display. In the illustrated implementation video and audio processing components are mounted on module .

In the implementation depicted in console includes a controller support subassembly for supporting four controllers . The controller support subassembly includes any hardware and software components needed to support wired and wireless operation with an external control device such as for example a media and game controller. A front panel I O subassembly supports the multiple functionalities of power button the eject button as well as any LEDs light emitting diodes or other indicators exposed on the outer surface of console . Subassemblies and are in communication with module via one or more cable assemblies . In other implementations console can include additional controller subassemblies. The illustrated implementation also shows an optical I O interface that is configured to send and receive signals that can be communicated to module .

MUs and are illustrated as being connectable to MU ports A and B respectively. Additional MUs e.g. MUs are illustrated as being connectable to controllers and i.e. two MUs for each controller. Controllers and can also be configured to receive MUs not shown . Each MU offers additional storage on which games game parameters and other data may be stored. In some implementations the other data can include any of a digital game component an executable gaming application an instruction set for expanding a gaming application and a media file. When inserted into console or a controller MU can be accessed by memory controller . A system power supply module provides power to the components of gaming system . A fan cools the circuitry within console .

An application comprising machine instructions is stored on hard disk drive . When console is powered on various portions of application are loaded into RAM and or caches and for execution on CPU wherein application is one such example. Various applications can be stored on hard disk drive for execution on CPU .

Gaming and media system may be operated as a standalone system by simply connecting the system to monitor a television a video projector or other display device. In this standalone mode gaming and media system enables one or more players to play games or enjoy digital media e.g. by watching movies or listening to music. However with the integration of broadband connectivity made available through network interface gaming and media system may further be operated as a participant in a larger network gaming community.

Computer typically includes a variety of computer readable media. Computer readable media can be any available media that can be accessed by computer and includes both volatile and nonvolatile media removable and non removable media. By way of example and not limitation computer readable media may comprise computer storage media and communication media. Computer storage media includes volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory or other memory technology CD ROM digital versatile disks DVD or other optical disk storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can accessed by computer . Communication media typically embodies computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media includes wired media such as a wired network or direct wired connection and wireless media such as acoustic RF infrared and other wireless media. Combinations of the any of the above should also be included within the scope of computer readable media.

The system memory includes computer storage media in the form of volatile and or nonvolatile memory such as read only memory ROM and random access memory RAM . A basic input output system BIOS containing the basic routines that help to transfer information between elements within computer such as during start up is typically stored in ROM . RAM typically contains data and or program modules that are immediately accessible to and or presently being operated on by processing unit . By way of example and not limitation illustrates operating system application programs other program modules and program data .

The computer may also include other removable non removable volatile nonvolatile computer storage media. By way of example only illustrates a hard disk drive that reads from or writes to non removable nonvolatile magnetic media a magnetic disk drive that reads from or writes to a removable nonvolatile magnetic disk and an optical disk drive that reads from or writes to a removable nonvolatile optical disk such as a CD ROM or other optical media. Other removable non removable volatile nonvolatile computer storage media that can be used in the exemplary operating environment include but are not limited to magnetic tape cassettes flash memory cards digital versatile disks digital video tape solid state RAM solid state ROM and the like. The hard disk drive is typically connected to the system bus through a non removable memory interface such as interface and magnetic disk drive and optical disk drive are typically connected to the system bus by a removable memory interface such as interface .

The drives and their associated computer storage media discussed above and illustrated in provide storage of computer readable instructions data structures program modules and other data for the computer . In for example hard disk drive is illustrated as storing operating system application programs other program modules and program data . Note that these components can either be the same as or different from operating system application programs other program modules and program data . Operating system application programs other program modules and program data are given different numbers here to illustrate that at a minimum they are different copies. A user may enter commands and information into the computer through input devices such as a keyboard and pointing device commonly referred to as a mouse trackball or touch pad. Other input devices not shown may include a microphone joystick game pad satellite dish scanner or the like. These and other input devices are often connected to the processing unit through a user input interface that is coupled to the system bus but may be connected by other interface and bus structures such as a parallel port game port or a universal serial bus USB . A monitor or other type of display device is also connected to the system bus via an interface such as a video interface . In addition to the monitor computers may also include other peripheral output devices such as speakers and printer which may be connected through an output peripheral interface .

The computer may operate in a networked environment using logical connections to one or more remote computers such as a remote computer . The remote computer may be a personal computer a server a router a network PC a peer device or other common network node and typically includes many or all of the elements described above relative to the computer although only a memory storage device has been illustrated in . The logical connections depicted in include a local area network LAN and a wide area network WAN but may also include other networks. Such networking environments are commonplace in offices enterprise wide computer networks intranets and the Internet.

When used in a LAN networking environment the computer is connected to the LAN through a network interface or adapter . When used in a WAN networking environment the computer typically includes a modem or other means for establishing communications over the WAN such as the Internet. The modem which may be internal or external may be connected to the system bus via the user input interface or other appropriate mechanism. In a networked environment program modules depicted relative to the computer or portions thereof may be stored in the remote memory storage device. By way of example and not limitation illustrates remote application programs as residing on memory device . It will be appreciated that the network connections shown are exemplary and other means of establishing a communications link between the computers may be used.

In another embodiment computing device such as computing device described in may be implemented as a mobile computing device. Mobile computing device may include for example a cell phone a web enabled smart phone a personal digital assistant a palmtop computer a laptop computer or any similar device which communicates via wireless signals. In one embodiment the disclosed technology may also be implemented using a head mounted display HMD device connected to a mobile computing device. The HMD device in one example may be in the shape of glasses and worn on the head of a user so that the user can see through a display and thereby have an actual direct view of the space in front of the user. In one example the HMD device may communicate wirelessly e.g. WiFi Bluetooth infra red or other wireless communication means to a mobile computing device via a processing unit in the HMD device.

In one approach of the disclosed technology a user may view multimedia content via the HMD device connected to the user s mobile computing device. The mobile computing device may determine a user s response to multimedia content being viewed by the user by tracking the user s movements gestures postures and facial expressions via a camera in the mobile computing device or the HMD device.

In one embodiment user response aggregation service receives a user specific report of the response to a viewed program e.g. illustrated in Table 1 and a user specific report of the average response to viewed programs e.g. illustrated in Table 2 corresponding to one or more users from the individual processing devices A B and X. In one embodiment the user response aggregation service generates an aggregated user response report based on the user specific reports received from the individual processing devices A B and X. In one embodiment the aggregated user response report includes responses to a viewed program received from all of the users via processing devices A B and X. It is to be appreciated that the user response aggregation service may aggregate user responses of any number of users interacting with the processing devices A B and X thereby enabling the collection of a large user response data set for analysis by content providers and advertisers. In one example the aggregated user response report may include information such as a list of users with anonymized user profiles the top viewed program by each of the users the program genre associated with the top viewed program and the average engagement level to each program episode of the top viewed program. The user profiles for each user may be anonymized by computing system prior to providing the user profiles to the user response aggregation service in one embodiment. Alternatively the user profiles may also be anonymized by the user response aggregation service in another embodiment. An exemplary illustration of the aggregated user response report is illustrated in Table 3 as shown below 

In the example shown in Table 3 the top viewed program and the average engagement level to each program episode may be derived from the user specific reports illustrated in . For example the top viewed program may be derived based on the percentage of program episodes of each program viewed by a user as shown in Table 2 . In one embodiment the user response aggregation service provides the user specific reports shown in Table 1 and Table 2 and the aggregated user response report shown in Table 3 to one or more of rating agencies content providers and advertisers .

In certain embodiments user response aggregation service may also generate a user specific report for all the users identified by the capture device . In one example the user specific report for all identified users may include information such as the duration of time that each user was engaged while watching the program content for example by detecting the duration of time that the user faced the display while viewing the program content when each user entered the field of view when each user left the field of view and the user s level of engagement while viewing the program content derived from Tables 1 2 and 3 . The user specific report for all identified users is illustrated in Table 3 below 

In another embodiment user response aggregation service receives preview content related to a viewed program for one or more users based on the user specific reports and the aggregated user response report from the content providers or advertisers. The preview content may be stored in a preview content module . In another embodiment user response aggregation service receives personalized content related to a viewed program for one or more users based on the user specific reports and the aggregated user response report from the content providers or advertisers. The personalized content may be stored in a personalized content module . In one embodiment user response aggregation service delivers the preview content and personalized content to the processing devices A B and X which may then be displayed via an audio visual device in the processing devices to the users. As further illustrated user response aggregation service also includes a global user profile database . The global user profile database includes information about a user s account such as a unique identifier and password associated with the user and a console identifier that uniquely identifies a user of a processing device such as A B or X and the user s profile information.

The hardware devices of discussed above can be used to implement a system that generates passive and anonymous feedback of program content viewed by users by tracking movements gestures postures and facial expressions performed by the users while viewing the program content. The passive and anonymous feedback of program content may be provided to one or more of rating agencies content providers and advertisers in one embodiment. In another embodiment the hardware devices of can also be used to implement a system that provides preview content and personalized content to one or more users based on the movements gestures postures and facial expressions performed by the users.

In step multimedia content associated with the current broadcast is identified. In one embodiment the multimedia content identified may include a television program movie a live performance or a sporting event. The multimedia content may be identified by the audio visual device connected to the computing device in one embodiment. Alternatively the multimedia content may also be identified by the computing device . In step one or more users in a field of view of the capture device connected to the computing device are identified. In one embodiment the computing device may determine a user s identity by receiving input from the user identifying their identity. In another embodiment and as discussed in the facial recognition engine in computing device may also perform the identification of users. In step the user s identification information is anonymized. As discussed in in one embodiment a user s privacy concerns are met by anonymizing the user s profile information prior to implementing the disclosed technology.

In step a user s movements gestures and postures in a field of view of the capture device are tracked while the user views the program. The process by which a user s movements gestures and postures may be captured and tracked by the capture device is discussed in . In step the user s facial expressions are also tracked while the user views the program. In one embodiment and as discussed in the user s gestures postures movements and facial expressions may be tracked during consecutive time intervals that comprise the length of duration of the program. In step an engagement level to the program viewed by the user is determined based on the gestures postures movements and facial expressions performed by the user. In one embodiment and as discussed in the engagement level to a program viewed by a user may be determined to be one of positive satisfactory or negative based on the types of movements gestures postures and facial expressions performed by the user while viewing the program.

In step it is determined if the current broadcast has ended. If the current broadcast has not yet ended multimedia content associated with the current broadcast is received at step as discussed above. If the current broadcast has ended then a user specific report of the response to a viewed program and a user specific report of the average response to programs viewed by the user are generated in step . In one embodiment the reports provide passive and anonymous feedback of program content viewed by a user by providing a detailed analysis of the user s gestures postures movements and facial expressions while viewing the program content. The generation of the user specific reports is discussed in detail above with respect to . In step the user specific reports are transmitted to a remote system for analysis. In one embodiment the user specific reports are provided to rating agencies content providers and advertisers. In one embodiment the user specific reports are utilized by television rating agencies to determine more accurate ratings of viewed program content.

At step depth information corresponding to the visual image and depth image are determined. The visual image and depth image received at step can be analyzed to determine depth values for one or more targets within the image. Capture device may capture or observe a capture area that may include one or more targets.

At step the capture device determines whether the depth image includes one or more human targets. In one example each target in the depth image may be flood filled and compared to a pattern to determine whether the depth image includes a human target. In one example the edges of each target in the captured scene of the depth image may be determined. The depth image may include a two dimensional pixel area of the captured scene. Each pixel in the 2D pixel area may represent a depth value such as a length or distance for example as can be measured from the camera. The edges may be determined by comparing various depth values associated with for example adjacent or nearby pixels of the depth image. If the various depth values being compared are greater than a pre determined edge tolerance the pixels may define an edge. The capture device may organize the calculated depth information including the depth image into Z layers or layers that may be perpendicular to a Z axis extending from the camera along its line of sight to the viewer. The likely Z values of the Z layers may be flood filled based on the determined edges. For instance the pixels associated with the determined edges and the pixels of the area within the determined edges may be associated with each other to define a target or an object in the capture area.

At step the capture device scans the human target for one or more body parts. The human target can be scanned to provide measurements such as length width or the like that are associated with one or more body parts of a user such that an accurate model of the user may be generated based on these measurements. In one example the human target is isolated and a bit mask is created to scan for the one or more body parts. The bit mask may be created for example by flood filling the human target such that the human target is separated from other targets or objects in the capture area elements.

At step one or more models of the one or more human targets is generated or updated based on the scan performed at step . The bit mask may be analyzed for the one or more body parts to generate a model such as a skeletal model a mesh human model or the like of the human target. For example measurement values determined by the scanned bit mask may be used to define one or more joints in the skeletal model. The bitmask may include values of the human target along an X Y and Z axis. The one or more joints may be used to define one or more bones that may correspond to a body part of the human.

According to one embodiment to determine the location of the neck shoulders or the like of the human target a width of the bitmask for example at a position being scanned may be compared to a threshold value of a typical width associated with for example a neck shoulders or the like. In an alternative embodiment the distance from a previous position scanned and associated with a body part in a bitmask may be used to determine the location of the neck shoulders or the like.

In one embodiment to determine the location of the shoulders the width of the bitmask at the shoulder position may be compared to a threshold shoulder value. For example a distance between the two outer most Y values at the X value of the bitmask at the shoulder position may be compared to the threshold shoulder value of a typical distance between for example shoulders of a human. Thus according to an example embodiment the threshold shoulder value may be a typical width or range of widths associated with shoulders of a body model of a human.

In one embodiment some body parts such as legs feet or the like may be calculated based on for example the location of other body parts. For example as described above the information such as the bits pixels or the like associated with the human target may be scanned to determine the locations of various body parts of the human target. Based on such locations subsequent body parts such as legs feet or the like may then be calculated for the human target.

According to one embodiment upon determining the values of for example a body part a data structure may be created that may include measurement values such as length width or the like of the body part associated with the scan of the bitmask of the human target. In one embodiment the data structure may include scan results averaged from a plurality depth images. For example the capture device may capture a capture area in frames each including a depth image. The depth image of each frame may be analyzed to determine whether a human target may be included as described above. If the depth image of a frame includes a human target a bitmask of the human target of the depth image associated with the frame may be scanned for one or more body parts. The determined value of a body part for each frame may then be averaged such that the data structure may include average measurement values such as length width or the like of the body part associated with the scans of each frame. In one embodiment the measurement values of the determined body parts may be adjusted such as scaled up scaled down or the like such that measurements values in the data structure more closely correspond to a typical model of a human body. Measurement values determined by the scanned bitmask may be used to define one or more joints in a skeletal model at step .

At step the model s created or updated in step is are tracked using skeletal mapping. For example the skeletal model of the user may be adjusted and updated as the user moves in physical space in front of the camera within the field of view. Information from the capture device may be used to adjust the model so that the skeletal model accurately represents the user. In one example this is accomplished by one or more forces applied to one or more force receiving aspects of the skeletal model to adjust the skeletal model into a pose that more closely corresponds to the pose of the human target and physical space.

At step the motion determined based on the skeletal mapping is used to generate a user motion capture file. In one embodiment of step the determining of the motion may include calculating the position direction acceleration and curvature of one or more body parts identified by the scan. The position of the body part is calculated in X Y Z space to create a three dimensional positional representation of the body part within the field of view of the camera. The direction of movement of the body part is calculated based upon the position. The directional movement may have components in any one of or a combination of the X Y and Z directions. The curvature of the body part s movement in the X Y Z space is determined for example to represent non linear movement within the capture area by the body part. The velocity acceleration and curvature calculations are not dependent upon the direction. It is to be appreciated that the use of X Y Z Cartesian mapping is provided only as an example. In other embodiments different coordinate mapping systems can be used to calculate movement velocity and acceleration. A spherical coordinate mapping for example may be useful when examining the movement of body parts which naturally rotate around joints.

Once all body parts in the scan have been analyzed the user motion capture file generated in step may be updated for the target. In one example the user motion capture file is generated or updated in real time based on information associated with the tracked model. For example in one embodiment the motion capture file may include the vectors including X Y and Z values that define the joints and bones of the model as it is being tracked at various points in time. As described above the model being tracked may be adjusted based on user motions at various points in time and a motion capture file of the model for the motion may be generated and stored. The user motion capture file may capture the tracked model during natural movement by the user interacting with the target recognition analysis and tracking system. For example the user motion capture file may be generated such that the user motion capture file may naturally capture any movement or motion by the user during interaction with the target recognition analysis and tracking system. The user motion capture file may include frames corresponding to for example a snapshot of the motion of the user at different points in time. Upon capturing the tracked model information associated with the model including any movements or adjustment applied thereto at a particular point in time may be rendered in a frame of the user motion capture file. The information in the frame may include for example the vectors including the X Y and Z values that define the joints and bones of the tracked model and a time stamp that may be indicative of a point in time in which for example the user performed the movement corresponding to the pose of the tracked model.

In one embodiment steps are performed by capture device . In other embodiments various ones of steps may be performed by other components such as by computing device . For example the capture device may provide the visual and or depth images to the computing device which will determine depth information detect the human target scan the target generate and track the model and capture motion of the human target.

Skeletal model includes joints n n and in some embodiments additional joints . Each of the joints n n may enable one or more body parts defined there between to move relative to one or more other body parts. A model representing a human target may include a plurality of rigid and or deformable body parts that may be defined by one or more structural members such as bones with the joints n n located at the intersection of adjacent bones. The joints n n may enable various body parts associated with the bones and joints n n to move independently of each other or relative to each other. For example the bone defined between the joints n and n corresponds to a forearm that may be moved independent of for example the bone defined between joints n and n that corresponds to a calf. It is to be understood that some bones may correspond to anatomical bones in a human target and or some bones may not have corresponding anatomical bones in the human target.

The bones and joints may collectively make up a skeletal model which may be a constituent element of the model. An axial roll angle may be used to define a rotational orientation of a limb relative to its parent limb and or the torso. For example if a skeletal model is illustrating an axial rotation of an arm a roll joint may be used to indicate the direction the associated wrist is pointing e.g. palm facing up . By examining an orientation of a limb relative to its parent limb and or the torso an axial roll angle may be determined. For example if examining a lower leg the orientation of the lower leg relative to the associated upper leg and hips may be examined in order to determine an axial roll angle.

In one embodiment user eligibility to participate in preview content related to a viewed program may be determined by the content providers and advertisers based on the percentage of program episodes viewed by a user the average engagement level to each program episode and based on the movements gestures postures and facial expressions performed by the user while the user viewed the program as determined from the user specific reports. In step the user is provided with an option to participate in a preview study based on the user eligibility determined in step . In step an opt in by the user to participate in a preview study may be obtained. In one example obtaining an opt in by a user may include prompting a user to select an option displayed via the audio visual device connected to the computing device . In one embodiment the audiovisual device may display a menu having two options to the user. A first option may display text such as Do you want to participate in a preview study A second option may display text such as Do you want to continue watching the current program 

In step upon receiving an opt in from the user to participate in a preview study preview content for a viewed program is received and displayed to the user. In one embodiment the preview content may be received from various media content sources such as content providers broadband satellite and cable companies the internet video streams from a web server or advertising agencies. The preview content is received at the audiovisual device in one embodiment. In another embodiment the preview content is received at the computing device connected to the audio visual device. In step the preview content is displayed via the audio visual device to the user.

At step if it is determined that the user is watching the personalized content then the user s movements gestures postures and facial expressions are tracked in step . In step the user s engagement level to the program being viewed is determined based on the user s movements gestures postures and facial expressions. In step the user s engagement level is provided to the content providers and advertisers. The user s engagement level may be determined as discussed in . In step new or modified personalized content for the user may be received from the content providers and advertisers based on the user s engagement level as discussed above. For example if it is determined that the user s engagement level while viewing the personalized content is positive then the content providers and advertisers may generate additional personalized content that includes a new or updated set of the most relevant programs that the user would like to view or provide the user with additional alternate endings of the next episode of the program. At step the new or updated personalized content is displayed to the user.

Although the subject matter has been described in language specific to structural features and or methodological acts it is to be understood that the subject matter defined in the appended claims is not necessarily limited to the specific features or acts described above. Rather the specific features and acts described above are disclosed as example forms of implementing the claims. It is intended that the scope of the invention be defined by the claims appended hereto.

