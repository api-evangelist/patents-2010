---

title: Timeline alignment for closed-caption text using speech recognition transcripts
abstract: Method, systems, and computer program products for synchronizing text with audio in a multimedia file, wherein the multimedia file is defined by a timeline having a start point and end point and respective points in time therebetween, wherein an N-gram analysis is used to compare each word of a closed-captioned text associated with the multimedia file with words generated by an automated speech recognition (ASR) analysis of the audio of the multimedia file to create an accurate, time-based metadata file in which each closed-captioned word is associated with a respective point on the timeline corresponding to the same point in time on the timeline in which the word is actually spoken in the audio and occurs within the video.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08281231&OS=08281231&RS=08281231
owner: Digitalsmiths, Inc.
number: 08281231
owner_city: Durham
owner_country: US
publication_date: 20100913
---
This application claims the benefit under 35 U.S.C. 119 e of U.S. Provisional Patent Application Ser. No. 61 241 450 entitled Timeline Alignment for Closed Captions Using Speech Recognition Transcripts filed Sep. 11 2009 which is incorporated herein by reference as if set forth herein in its entirety.

The present invention relates generally to video and associated audio transcripts and closed captioned text and more particularly to methods and systems for automatically and correctly aligning the timeline of audio transcripts with corresponding video for indexing accurate metadata capture and usage and for improved searching and playback presentation purposes.

Closed captioning is a term describing several systems developed to display text on a television or video screen to provide additional or interpretive information to viewers who wish to access it. Closed captions typically display a transcript of the audio portion of a program as it occurs either verbatim or in edited form sometimes including non speech elements. Most commonly closed captions are used by deaf or hard of hearing individuals to assist comprehension. However audio transcripts associated with video are also an important tool used for creating indexes or underlying metadata associated with video that can be used for many different purposes.

When indexing and associating metadata with videos it is very important that the video and audio be correctly and closely aligned in order for the underlying metadata of each frame or scene of the video to match up correctly. Unfortunately audio transcripts obtained from known sources like human generated closed captions or automated speech recognition ASR software almost always introduce time lags. It has been observed in the industry with production level data that typical time lags associated with closed caption text while often accurate can cause audio transcripts to shift as much as 30 seconds or more with respect to the corresponding visuals. Such time lags introduce errors in time based indexing and can create errors in the underlying metadata associated with a video especially if the timeline of the audio transcript is relied upon and assumed to synch correctly with the timeline of the actual audio.

On the other hand automated speech recognition ASR software when used alone to try to generate an audio transcript of a corresponding video usually captures the correct timeline and time location for each word or sound associated with the video but ASR software still generates a number of errors in transcription and tends to miss some text especially when there is a lot of background noise.

For these and many other reasons there is a need for systems and methods that correctly and accurately calibrate the timeline for audio transcripts with the underlying audio and video which improves not only the underlying metadata created by video indexing systems but also provides an improved and automated system for synching closed captioned text with the actual audio and corresponding video for later playback and use.

It will be understood that the present methods may also include and encompass computer readable media having computer executable instructions for performing steps or functions of the methods described herein and that the systems described herein may include computer networks and other systems capable of implementing such methods.

The above features as well as additional features and aspects of the present invention are disclosed herein and will become apparent from the following description of preferred embodiments of the present invention.

The present invention relates generally to video and associated audio transcripts and closed captioned text and more particularly to methods and systems for automatically and correctly aligning the timeline of each word of an audio transcript with a specific point in time of corresponding video for indexing accurate metadata capture and usage for improved searching and playback presentation and for numerous other purposes. Briefly described aspects of the present invention include the following.

In a first aspect of the present invention a method of synchronizing text with audio in a multimedia file is disclosed and described wherein the multimedia file includes previously synchronized video and audio wherein the multimedia file has a start time and a stop time that defines a timeline for the multimedia file wherein the frames of the video and the corresponding audio are each associated with respective points in time along the timeline comprising the steps of receiving the multimedia file and parsing the audio therefrom but maintaining the timeline synchronization between the video and the audio receiving closed captioned data associated with the multimedia file wherein the closed captioned data contains closed captioned text wherein each word of the closed captioned text is associated with a corresponding word spoken in the audio wherein each word of the closed captioned text has a high degree of accuracy with the corresponding word spoken in the audio but a low correlation with the respective point in time along the timeline at which the corresponding word was spoken in the audio using automated speech recognition ASR software generating ASR text of the parsed audio wherein each word of the ASR text is associated approximately with the corresponding words spoken in the audio wherein each word of the ASR text has a lower degree of accuracy with the corresponding words spoken in the audio than the respective words of the closed captioned text but a high correlation with the respective point in time along the timeline at which the corresponding word was spoken in the audio thereafter using N gram analysis comparing each word of the closed captioned text with a plurality of words of the ASR text until a match is found and for each matched word from the closed captioned text associating therewith the respective point in time along the timeline of the matched word from the ASR text corresponding therewith whereby each closed captioned word is associated with a respective point on the timeline corresponding to the same point in time on the timeline in which the word is actually spoken in the audio and occurs within the video.

In a feature the closed captioned text and the ASR text represent only a portion of the audio of the multimedia file. In another feature the closed captioned text and the ASR text represent all of the audio of the multimedia file.

In another feature the step of comparing each word of the closed captioned text with a plurality of words of the ASR text until a match is found further comprises the step of moving on to the next respective word of the closed captioned text for comparison purposes if the prior word of the closed captioned text is not matched with any of the plurality of words of the ASR text.

In yet a further feature for any unmatched word in the closed captioned text identifying the closest matched words in the closed captioned text on either side of the unmatched word along the timeline and then comparing the unmatched word with words of the ASR text between the two points on the timeline and selecting the most likely match or matches thereto.

In another feature the step of comparing comprises matching strings of characters between the words of the closed captioned text with the words of the ASR text to attempt to find exact or phonetically similar matches.

In a further feature the step of comparing comprises matching strings of characters between the words of the closed captioned text with the words of the ASR text to attempt to find approximate matches based on the proximity of the respective points on the timeline of the respective words.

In yet a further feature of the first aspect of the present invention the method further comprises the step of creating a time based metadata file that contains all of the correct words associated with the audio of the multimedia file and wherein each of the correct words is associated with the respective point in time along the timeline of the matched word from the ASR text corresponding therewith. Preferably the time based metadata file is associated with the corresponding multimedia file.

In a second aspect of the present invention a computer program product comprising a computer readable medium and computer program instructions stored on the computer readable medium that when processed by a computer instruct the computer to perform a process of synchronizing text with audio in a multimedia file is disclosed and described wherein the multimedia file includes previously synchronized video and audio wherein the multimedia file has a start time and a stop time that defines a timeline for the multimedia file wherein the frames of the video and the corresponding audio are each associated with respective points in time along the timeline the process comprising receiving the multimedia file and parsing the audio therefrom but maintaining the timeline synchronization between the video and the audio receiving closed captioned data associated with the multimedia file wherein the closed captioned data contains closed captioned text wherein each word of the closed captioned text is associated with a corresponding word spoken in the audio wherein each word of the closed captioned text has a high degree of accuracy with the corresponding word spoken in the audio but a low correlation with the respective point in time along the timeline at which the corresponding word was spoken in the audio using automated speech recognition ASR software generating ASR text of the parsed audio wherein each word of the ASR text is associated approximately with the corresponding words spoken in the audio wherein each word of the ASR text has a lower degree of accuracy with the corresponding words spoken in the audio than the respective words of the closed captioned text but a high correlation with the respective point in time along the timeline at which the corresponding word was spoken in the audio thereafter using N gram analysis comparing each word of the closed captioned text with a plurality of words of the ASR text until a match is found and for each matched word from the closed captioned text associating therewith the respective point in time along the timeline of the matched word from the ASR text corresponding therewith whereby each closed captioned word is associated with a respective point on the timeline corresponding to the same point in time on the timeline in which the word is actually spoken in the audio and occurs within the video.

In a feature the closed captioned text and the ASR text represent only a portion of the audio of the multimedia file. In another feature the closed captioned text and the ASR text represent all of the audio of the multimedia file.

In another feature within the process the step of comparing each word of the closed captioned text with a plurality of words of the ASR text until a match is found further comprises the step of moving on to the next respective word of the closed captioned text for comparison purposes if the prior word of the closed captioned text is not matched with any of the plurality of words of the ASR text.

In yet a further feature for any unmatched word in the closed captioned text the process further comprises identifying the closest matched words in the closed captioned text on either side of the unmatched word along the timeline and then comparing the unmatched word with words of the ASR text between the two points on the timeline and selecting the most likely match or matches thereto.

In another feature of this second aspect of the invention within the process the step of comparing comprises matching strings of characters between the words of the closed captioned text with the words of the ASR text to attempt to find exact or phonetically similar matches.

In a further feature within the process the step of comparing comprises matching strings of characters between the words of the closed captioned text with the words of the ASR text to attempt to find approximate matches based on the proximity of the respective points on the timeline of the respective words.

In a further feature the process further comprises creating a time based metadata file that contains all of the correct words associated with the audio of the multimedia file and wherein each of the correct words is associated with the respective point in time along the timeline of the matched word from the ASR text corresponding therewith.

In another feature the process further comprises associating the time based metadata file with the corresponding multimedia file.

It will be understood by those skilled in the art that the present methods and systems and computer program product may also include and encompass computer readable media having computer executable instructions for performing steps or functions of the methods described herein and that the systems described herein may include computer networks and other systems capable of implementing such methods.

The above features as well as additional features and aspects of the present invention are disclosed herein and will become apparent from the following description of preferred embodiments of the present invention.

For the purpose of promoting an understanding of the principles of the present disclosure reference will now be made to the embodiments illustrated in the drawings and specific language will be used to describe the same. It will nevertheless be understood that no limitation of the scope of the disclosure is thereby intended any alterations and further modifications of the described or illustrated embodiments and any further applications of the principles of the disclosure as illustrated therein are contemplated as would normally occur to one of ordinary skill in the art to which the disclosure relates.

The present system and methods calibrate the timeline of human speech obtained from audio tracks of a video to correctly follow the timeline of the visual frames and scenes while at the same time maintaining a high level of quality associated with the transcription. The human speech for the audio tracks is generally available from two sources i automated speech recognition ASR software and ii closed captions CC created manually by human interpreters. Unfortunately neither of the two sources by itself typically provides a successful or reliable method to align an accurate audio transcription with the video on a precise timeline basis. This creates issues with indexing and the accurate creation of time based metadata associated with specific frames and scenes within a video which relies upon and requires more precise synchronization of the audio and the video with the underlying timeline of the multimedia matter such as a movie or TV show.

While ASR is able to capture the timeline accurately with respect to the visuals assuming that the audio is correctly synched with the video which is typically the case with any high quality multimedia matter its failure rate to recognize or spell words correctly is very high. On the other hand closed captions tend to capture the words correctly but frequently shift them from the time they were actually spoken since proper synching of the text may not be possible or important in typical use of closed captioning. For example it may be more desirable to slow down the typical closed captioned text long enough for it to be read more easily and it may be less important for the written text to match up precisely with the visually spoken words. Further closed caption text is typically grouped together as a block of text that is associated with a particular scene or between a start time and an end time of the underlying video. Rarely are closed captions presented in a format that precisely matches each word of text presented on screen with the spoken or sung word and underlying video other than in specific applications such as karaoke videos. Introduction of ad breaks inside videos is another reason why audio transcripts are or need to be shifted. The present systems and methods are used for traditional closed captioned text that are associated with a block of video or a scene and where there is no need or reason for the text to be synched precisely with the underlying audio and at times where it may be preferable for them not to be so precisely aligned so that the text may be more easily read by the viewer.

As described herein the present methods and systems adjust and align the timeline of text within a video by comparing closed captioned words against the text output of ASR speech software run against the same video using an N gram model algorithm for matching strings. The systems and methods thus enable identifying correct text and identifying the precise point in time that such text occurs as audio within the synched video. An N gram is a subsequence of N items from a given sequence of words. In this case the sequence of words is ASR generated audio transcriptions from videos. Typically an N gram model is a type of probabilistic model for predicting the next item in the sequence. Here an N gram model was used by placing an input pattern over another text piece at its extreme left and scanned to the right for a mismatch. If a mismatch occurs the input pattern is shifted one position to the right and the scan is restarted at the leftmost position of the pattern. This process is repeated until a match occurs or the end of the input pattern is reached.

The system also includes a parser that is adapted to receive the multimedia file and forward the audio portion of the multimedia file on to an automated speech recognition ASR module component or software system . As will be appreciated the timeline or synchronization between the audio and the video is not lost as part of this parsing or in other words the synchronization of the audio with the video is maintained. The output of the ASR component is ASR text . As will be appreciated by those skilled in the art each word of the ASR text is associated approximately with the corresponding word or words spoken in the audio wherein each word of the ASR text has a lower degree of accuracy with the corresponding word or words spoken in the audio than the respective word of the closed captioned text but has a high correlation with the respective point in time along the timeline at which the corresponding word was spoken in the audio.

The closed caption text and the ASR text are each provided to a synchronization comparator component . As will be described in greater detail hereinafter the synchronization comparator component compares the closed caption text and the ASR text in order to generate as an output the best possible version of the text that matches the audio word for word generally based upon the closed captioned text and wherein each word is associated with the accurate point on the timeline corresponding to the same point in time on the timeline in which the word is actually spoken in the audio and occurs within the video.

The accurate time based text output is then provided to a re combiner or re association component along with the original multimedia file so that an accurate multimedia file with the accurate time based text is now available for further use. The accurate time based text output is a type of metadata associated with the multimedia file . Such metadata may be linked into a single file output that may optionally be provided back to the database for later use. In alternative embodiments not shown the accurate time based text output may be provided back to the database and associated with the original multimedia file or may be provided to a different database or to a software module that uses the metadata for searching analysis or other processing.

Turning now to an exemplary timeline for a multimedia file as described in is illustrated. The timeline of the multimedia file is represented by a plurality of variable specific sub timelines and that are shown one above the other and are assumed to be synchronized aligned. Sub timeline is the main timeline of the multimedia file and represents the time based variable associated with the multimedia file. The sub timeline includes a start time St and an end or stop time Et and is divided into time increments . In this example and at the shown resolution sub timeline is arbitrarily divided into one 1 minute time increments. As will be appreciated one could zoom into the sub timeline and focus on sub one minute time increments or conversely could zoom out and focus on greater than one minute time increments.

Sub timeline represents the video component portion of the multimedia file and is divided into a plurality of scenes or camera shot segments separated by dividers. Sub timeline represents the audio component portion of the multimedia file wherein the audio is shown as an analog wave form with an amplitude positive or negative indicative of the occurrence some form of sound noise music dialogue etc occurring during the video and with zero amplitude shown when there is silence on the audio file. Again it is assumed that the video and audio are synchronized with each other and with the main timeline. In some embodiments the audio file and sub timeline may be limited to just the dialogue portion of the audio of the multimedia file with other non dialogue sounds being separated out onto their own sub timelines not shown . Finally sub timeline represents the closed captioned text associated with the multimedia file. The closed captioned text is represented as a series of blocks in which each block represents one or more words that are displayed on screen of the video starting at the point of time on the timeline at the leftmost side of the block. In some embodiments of use of closed captioning the text remains on screen until the next block of text takes its place. In other embodiments as contemplated and shown in the block of text is assumed to stay on screen until the point in time represented by the rightmost edge of the respective block. Regardless it is impossible solely by examining the closed captioning time block to determine when each word within a respective closed captioning block matches up at an exact point in time on the timeline with the same words actually spoken in the audio. Such words can be approximately but not precisely matched with a point in time. Blow up focuses on a portion of the closed captioned text of sub timeline and is discussed in greater detail in .

Turning now to a zoomed in portion of the exemplary timeline for the multimedia file as described in is illustrated. Sub timeline is still representative of the main timeline of the multimedia file and represents the time based variable associated with the multimedia file. The sub timeline includes a start time St and in this example and at the shown blown up resolution only focuses on the first one 1 minute of the multimedia file and is arbitrarily divided into 0.25 one quarter minute time increments . The blown up portion of sub timeline from still represents the closed captioned text associated with the multimedia file wherein blocks and represent the four closed captioned blocks that occur in the first minute of the video. Block is shown to start at time t 0.075 and includes the text My name is Lester Burnham. . Block is shown to start at time t 0.310 and includes the text This is my neighborhood. This is my street. This . . . is my life. . Block is shown to start at time t 0.732 and includes the text I m forty two years old. . Block is shown to start at time t 0.890 and includes the text In less than a year I ll be dead. . A blown up portion of sub timeline not shown in represents the ASR text generated by the ASR component from . Each ASR text represents a word and has a corresponding time shown as specific points in time along the timeline . As can be seen the ASR text is close but does not perfectly match up word for word with the closed captioned text . However as described herein the closed captioned text represents the most accurate text corresponding with the dialogue portion of the audio or the multimedia file. Further the ASR text provides the most accurate time points associated with each word. The system and processes described above and hereinafter create the accurate time based text by comparing and taking the most accurate information from these two sources of data.

First the process assumes that there are Wwords in the closed caption file and Wwords in the ASR file. Then a CCF pointer is pointed step to the first word in the closed caption file. From the ASR file starting from the first word a window with M words is selected steps . The value of M is modifiable and for purposes of testing of the system by Applicant was set arbitrarily to 1310. Starting from the word indicated by the CCF pointer the process then looks for an N gram match within the ASR window step .

If the process cannot find an N gram matching the first N words in the closed caption file if the result of step is a NO the process then points to the second word in the closed caption file and repeats the N gram matching process step . The ASR window remains unchanged until a match is found i.e. the ASR window of size M will still point to the first word of the ASR file at step . If the process finds an N gram within the ASR window matching the first N word in the closed caption file at step the N word of the closed captioned file is deemed a match and the time associated with the matching ASR text word is assigned to the N word and such N word and corresponding time is included in the accurate time based text file step . The process then will point to the N 1position in the closed caption file step . The ASR window is then updated to form a new window of size M starting from the next word following the current N gram step . The process will continue until all Wwords in the closed caption file have been compared against the ASR file step .

The following example illustrates how the words in the closed caption file are aligned based on the timeline of the ASR file. The two lines shown below are an example of an extracted N gram where N 5. The first line is from the ASR file and shows word time pairs. The second line is from the closed caption file showing the corresponding word time pairs.

As shown in a e above words inside an N gram have already been time aligned by the extracted N gram. Only words between two extracted N grams are needed to do time alignment including the words before the first N gram and the words after the last N gram . Suppose there are t1 a number N gram extracted these N grams separate the whole CCF into the following t1 1 parts and it is necessary to do t1 1 time alignments.

It is to be noted that the accuracy of the time alignment process strongly depends on the accuracy of the ASR software. For example during experimentation by Applicant it was seen that the ASR software used had better recognition for broadcast news programs in the studio but showed poorer performance when analyzing a typical TV series with background noises. To handle the errors in the ASR the following processes were used to compute the timelines.

Suppose there T N grams extracted. Then the closed caption file can be divided into t 1 blocks. The general form each of the T 1 parts is 

Specifically the process was tested using 24 episodes from the series Friends 2 episodes from Gilmore Girls 3 episodes from VeronicaMars and 2 episodes from Smallville. A total of 31 episodes were used for testing the time alignment system described above.

To verify the results of the time alignment process the audio from the videos were manually listened to checking the words in the aligned closed caption every 50 seconds. If the same words appear in the aligned closed caption file at the same time as the words occurred while listening to the audio they are counted as correct ones. On the average each episode lasts about 2500 seconds only 50 words on the average on the 50th second 100th second 2500th second were checked. No errors were detected in the experiment results.

In conclusion precise video indexing requires that the speech transcripts from the audio uses the correct time alignment with respect to the visuals. Lack of proper alignment results in time lags between occurrence of visuals and the transcript which introduces errors during the video indexing and retrieval process.

The N gram match process described herein was able to identify matches between closed captioned files and automated speech recognition transcripts. Even though the automated speech recognition ASR transcripts showed high error rates for detecting the correct word spoken in the videos of selected popular TV series that had background noise the above described process showed very high accuracy in aligning the time of words in the closed captioned files. Best performance for the present system was obtained by running the software and comparison processes on a Linux machine.

In view of the foregoing detailed description of preferred embodiments of the present invention it readily will be understood by those persons skilled in the art that the present invention is susceptible to broad utility and application. While various aspects have been described in the context of screen shots additional aspects features and methodologies of the present invention will be readily discernable therefrom. Many embodiments and adaptations of the present invention other than those herein described as well as many variations modifications and equivalent arrangements and methodologies will be apparent from or reasonably suggested by the present invention and the foregoing description thereof without departing from the substance or scope of the present invention. Furthermore any sequence s and or temporal order of steps of various processes described and claimed herein are those considered to be the best mode contemplated for carrying out the present invention. It should also be understood that although steps of various processes may be shown and described as being in a preferred sequence or temporal order the steps of any such processes are not limited to being carried out in any particular sequence or order absent a specific indication of such to achieve a particular intended result. In most cases the steps of such processes may be carried out in various different sequences and orders while still falling within the scope of the present inventions. In addition some steps may be carried out simultaneously. Accordingly while the present invention has been described herein in detail in relation to preferred embodiments it is to be understood that this disclosure is only illustrative and exemplary of the present invention and is made merely for purposes of providing a full and enabling disclosure of the invention. The foregoing disclosure is not intended nor is to be construed to limit the present invention or otherwise to exclude any such other embodiments adaptations variations modifications and equivalent arrangements the present invention being limited only by the claims appended hereto and the equivalents thereof.

