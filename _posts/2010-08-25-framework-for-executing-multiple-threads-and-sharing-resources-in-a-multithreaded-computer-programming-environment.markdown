---

title: Framework for executing multiple threads and sharing resources in a multithreaded computer programming environment
abstract: Techniques for execution of multiple threads in a multithreaded computing programming environment are disclosed. The techniques are especially well suited for environments that use multilayered programming architecture where a higher layer can build on the functions provided by a lower layer where the delay time is an important consideration. In one aspect, the conceptual notion of a “Worker” effectively serves to represent the thread-specific execution context for a thread of execution (“thread”) in a multithreaded computing environment. Another aspect, provides the notion of an Exclusion Area (EA) as logical lock that serves to protect shared resources in a multithreaded environment. The combination of the worker and EA are used to provide a powerful framework that, among other things, allows minimizing of the delay time.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08327374&OS=08327374&RS=08327374
owner: Real-Time Innovations, Inc.
number: 08327374
owner_city: Santa Clara
owner_country: US
publication_date: 20100825
---
This application is a divisional of and claims priority under 35 U.S.C. 120 to U.S. patent application Ser. No. 11 410 563 filed on Apr. 24 2006 and entitled FRAMEWORK FOR EXECUTING MULTIPLE THREADS AND SHARING RESOURCES IN A MULTITHREADED COMPUTER PROGRAMMING ENVIRONMENT which is related to i U.S. application Ser. No. 11 379 986 filed on Apr. 24 2006 now U.S. Pat. No. 7 783 853 issued on Aug. 24 2010 entitled MEMORY USAGE TECHNIQUES IN MIDDLEWARE OF A REAL TIME DATA DISTRIBUTION SYSTEM and ii U.S. patent application Ser. No. 11 410 511 filed on Apr. 24 2006 and entitled FLEXIBLE MECHANISM FOR IMPLEMENTING THE MIDDLEWARE OF A DATA DISTRIBUTION SYSTEM OVER MULTIPLE TRANSPORT NETWORKS which are incorporated by reference herein for all purposes.

In multithread computing environments multiple threads of execution can effectively be executed. Multithreaded programs present many difficult challenges as for example multithreaded applications require careful design and coordination of resources among multiple threads.

Referring to consider an example of computer code that computes the cumulative sum of numbers passed to it by the calling thread. shows the pseudo code for a function cumulative sum value that computes prints and returns the running total of the numbers passed to it by a calling thread. In a single threaded environment this function works as expected thus 

Now consider a multithreading environment where two threads A and B concurrently invoke cumulative sum . One possible execution scenario is shown in . The diagram shows the program code line numbers and sequence in which they are executed by the two threads. In this scenario thread B executes a line of code right after thread A has executed it thus interleaving the execution of the two threads A Line 1 B Line 1 A Line 2 B Line 2 A Line 3 B Line 3 A Line 4 B Line 4 A Line 5 B Line 5. As a consequence both threads clobber each other s execution by modifying the shared static variable running total such that both threads produce incorrect results.

The conventional way to prevent the two threads from clobbering or adversely interfering with each other is to protect access to the shared variable running total as shown in . Referring to a lock is taken before modifying the value of the shared variable running total and unlocked after changing the shared variable. This guarantees that only one thread is modifying the shared variable at a time. However as shown in this approach still does not guarantee that results of the two independent threads will not interfere with each other since the running total variable is still shared among the threads.

In view of the foregoing an improved framework for executing multiple threads and sharing resources in a multithreaded programming environment would be highly useful.

A scenario where multithreading presents additional challenges includes implementing the middleware for a data distribution system. depicts a data distribution middleware that decouples producers and consumers of Data in a domain. Data Distribution Service DDS is a formal standard from the Object Management Group OMG popular in embedded systems especially in industrial automation aerospace and defense applications. DDS specifies an API designed for enabling real time data distribution. It uses a publish subscribe communication model and supports both messaging and data object centric data models.

DDS uses a publish subscribe P S communication model. The P S communication model uses asynchronous message passing between concurrently operating subsystems. The publish subscribe model connects anonymous information producers with information consumers. The overall distributed system is composed of processes each running in a separate address space possibly on different computers. We will call each of these processes a participant application . A participant may be a producer or consumer of data or both.

Data producers declare the topics on which they intend to publish data data consumers subscribe to the topics of interest. When a data producer publishes some data on a topic all the consumers subscribing to that topic receive it. The data producers and consumers remain anonymous resulting in a loose coupling of sub systems which is well suited for data centric distributed applications. DDS targets real time systems the API and Quality of Service QoS are chosen to balance predictable behavior and implementation efficiency performance.

The DCPS model builds on the idea of a global data space of data objects that any entity can access. Applications that need data from this space declare that they want to subscribe to the data and applications that want to modify data in the space declare that they want to publish the data. A data object in the space is uniquely identified by its keys and topic and each topic must have a specific type. There may be several topics of a given type. A global data space is identified by its domain id each subscription publication must belong to the same domain to communicate.

The data distribution middleware handles the actual distribution of data on behalf of a user application. The distribution of the data is controlled by user settable Quality of Service QoS .

In view of the foregoing an improved framework for executing multiple threads and sharing resources in a multithreaded computing environment would be highly useful.

Broadly speaking the invention relates to techniques for execution of multiple threads in a multithreaded computing programming environment. It will be appreciated that the techniques are especially well suited for environments that use multilayered programming architecture where a higher layer can build on the functions provided by a lower layer. One example is an environment where a multilayered middleware software acts as an mediator between an application program and an operating system.

In one aspect the invention provides the conceptual notion of a Worker which can effectively serves to represent the thread specific execution context for a thread of execution thread in a multithreaded computing environment. In one embodiment the Worker is effectively used to store all thread specific context needed by operational computer code to execute e.g. a function method procedure module . In addition a Worker can include or be associated with one or more objects that are in effect thread specific and can be used to define the execution context as well as other applications e.g. time stamping to determined how much time a thread spends in a particular layer or module . The Worker can be passed between various operational code portions e.g. functions in order to provide the execution context of the thread. This allows execution of multiple portions of operational code for a thread without having to lock any of the code activated e.g. called invoked by the thread. As a result the latency experienced in conventional systems where a single thread may effectively lock various portions of operational code can be minimized. Those skilled in the art will appreciate that the notion of a Worker allows writing computer programming source code without having to make assumption about the number of threads that would be supported by the code. As an example middleware software can be written without regard to the number of threads that would be configured for a particular application or the number of threads that can actually be supported by various platforms operating systems . Furthermore a Worker can be associated e.g. bounded to a thread at the entry point of middleware software thereby allowing the number of threads for the application layer to be determined in a dynamic manner and by users. It will be appreciated that this delayed association of actual threads configured for applications enables an implementation to be effectively mapped to single or multithread environments that are supported by various platforms. As a result software can be written in a simpler manner and the same software can effectively be used on various platforms with different multithreading configuration and support.

In another aspect the invention provides the conceptual notion of an Exclusion Area EA to effectively protect resources that could be shared shared resources between multiple threads. Generally a Worker can conceptually interact with an Exclusion Area EA and shared resources can be protected based on this interaction. In one embodiment an Exclusion Area EA can only be visited by a single Worker at a given time. This effectively protects the shared resource associated with the Exclusion Area EA from multiple threads accessing it because each Worker can be associated with a thread and effectively provide the execution context for the thread. In another embodiment a numbering or priority scheme is effectively provided for the Exclusion Area EA . By way of example a number N can be provided as a property for the Exclusion Area i.e. EA.N . Based on the numbers assigned to each Exclusion Area EA it can be determined whether a Worker can visit multiple Exclusion Areas EAs at the same time. For example while visiting a first Exclusion Area first EA a Worker may be allowed to visit a second Exclusion Area second EA if the number assigned to the second EA is higher than the number assigned to the first EA being visited by the Worker and so on. Thus a Worker can effectively visit multiple EAs allowing a thread of execution to safely use multiple resources if they are not used by another thread. Further similar to Worker associating e.g. binding of an EA to a physical lock can be delayed and for example performed at the entry point to a middleware. As a result one physical lock may be used to effectively protect numerous shared resources thereby minimize delays experienced in conventional system where multiple physical locks would be used.

The Workers and EA framework is applied to the construction of multi layered real time middleware. The EAs protecting the shared resources in the internal middleware layers are mapped across the layers such that for a Data Reader Data Writer the incoming outgoing data path effectively uses a single physical lock. Furthermore the incoming outgoing data paths of Data Readers Data Writers are independent of each other and can proceed concurrently as parallel threads of execution. The EA mapping scheme lends itself to mapping the EAs across the layers to use a single physical lock for a group of Data Readers Data Writers based on a user specified configuration policy.

The invention can be implemented in numerous ways including a method an apparatus a computer readable medium a computing device. Several embodiments of the invention are discussed below.

Other aspects and advantages of the invention will become apparent from the following detailed description taken in conjunction with the accompanying drawings illustrating by way of example the principles of the invention.

As noted in the background section multithread computer environments present many difficult challenges. One way to avoid resource contention between multiple threads is to attach some storage that is local to a thread. Many modern operating OS systems provide the notion of a thread and the notion of associating a thread specific storage. However the thread specific storage provided by the OS is a limited resource and generally not enough to store significant state. Also different operating systems implement thread specific storage in different ways and some operating environments may not support this concept at all or may not even support multi threading. Accordingly an improved framework for executing multiple threads and sharing resources in a multithreaded programming environment would be highly useful.

Accordingly the invention pertains to techniques for execution of multiple threads in a multithreaded computing programming environment. It will be appreciated that the techniques are especially well suited for environments that use multilayered programming architecture where a higher layer can build on the functions provided by a lower layer. One example is an environment where a multilayered middleware software acts as an mediator between an application program and an operating system.

In one aspect the invention provides the conceptual notion of a Worker which can effectively serves to represent the thread specific execution context for a thread of execution thread in a multithreaded computing environment. In one embodiment the Worker is effectively used to store all thread specific context needed by operational computer code to execute e.g. a function method procedure module . In addition a Worker can include or be associated with one or more objects that are in effect worker specific and can be used to define the execution context as well as other applications e.g. time stamping to determine how much time a thread spends in a particular layer or module . The Worker can be passed between various operational code portions e.g. functions in order to provide the execution context of the thread. This allows execution of multiple portions of operational code for a thread without having to lock any of the code activated e.g. called invoked by the thread. As a result the latency experienced in conventional systems where a single thread may effectively lock various portions of operational code can be minimized. Those skilled in the art will appreciate that the notion of a Worker allows writing computer programming source code without having to make assumption about the number of threads that would be supported by the code. As an example middleware software can be written without regard to the number of threads that would be configured for a particular application or the number of threads that can actually be supported by various platforms operating systems . Furthermore a Worker can be associated e.g. bounded to a thread at the entry point of middleware software thereby allowing the number of threads for the application layer to be determined in a dynamic manner and by users. It will be appreciated that this delayed association of actual threads configured for applications enables an implementation to be effectively mapped to single or multithread environments that are supported by various platforms. As a result software can be written in a simpler manner and the same software can effectively be used on various platforms with different multithreading configuration and support.

In another aspect the invention provides the conceptual notion of an Exclusion Area EA to effectively protect resources that could be shared shared resources between multiple threads. Generally a Worker can conceptually interact with an Exclusion Area EA and shared resources can be protected based on this interaction. In one embodiment an Exclusion Area EA can only be visited by a Worker at a given time. This effectively protects the shared resource associated with the Exclusion Area EA from multiple threads accessing it because each Worker can be associated with a thread and effectively provide the execution context for the thread. In another embodiment a numbering or priority scheme is effectively provided for the Exclusion Area EA . By way of example a number N can be provided as a property for the Exclusion Area i.e. EA.N . Based on the numbers assigned to each Exclusion Area EA it can be determined whether a Worker can visit multiple Exclusion Areas EAs at the same time. For example while visiting a first Exclusion Area first EA a Worker may be allowed to visit a second Exclusion Area second EA if the number assigned to the second EA is higher than the number assigned to the first EA being visited by the Worker and so on. Thus a Worker can effectively visit multiple EAs allowing a thread of execution to safely use multiple resources if they are not used by another thread. Further similar to Worker associating e.g. binding of an EA to a physical lock can be delayed and for example performed at the entry point to a middleware. As a result one physical lock may be used to effectively protect numerous shared resources thereby minimize delays experienced in conventional system where multiple physical locks would be used.

Embodiments of these aspects of the invention are discussed below with reference to . However those skilled in the art will readily appreciate that the detailed description given herein with respect to these figures is for explanatory purposes as the invention extends beyond these limited embodiments.

In general the computer code A can be represented as a mathematical function e.g. f x that may receive input e.g. x and produce output e.g. y f x . As such the interaction between the first thread and the computer code A can be generally thought of as invocation or call to a generic function e.g. f x . This interaction can be characterized as execution of computer code A by the first thread of execution or first thread . The execution of computer code A can effectively provide as input one or more parameters e.g. x to the function and receive as output one or more values or results. Moreover a first thread specific execution context or Worker W can be effectively provided to the computer code A in accordance with one aspect of the invention. It should be noted that the first thread specific execution context W can be provided in addition to any parameter s that are normally provided to computer code A. Furthermore it will be appreciated that the first thread specific execution context Worker W is execution context that is specifically provided for execution of computer code e.g. computer code A by the first thread . As such the thread specific execution context Worker W effectively defines the execution context in order to execute computer code A when the first thread activates e.g. invokes or calls computer code A e.g. when thread A calls a particular function method procedure or module .

It will be appreciated that the thread specific context Worker W among other things can effectively serve as a work sheet that is provided by or on behalf of the first thread . The work sheet or Worker can be provided to computer code A and used to do work for or in the context of the first thread . Similarly a second thread specific execution context Worker W can be provided for the second thread allowing execution of computer code A for the second thread in the context defined by the second thread specific context Worker W .

Those skilled in the art will appreciate that providing thread specific execution context e.g. W W for threads e.g. first and second threads and allows multiple threads to effectively access the same computer code without requiring each thread to lock the computer code. Referring to first and second threads and can effectively access the computer code A at the same time. Moreover first thread need not effectively lock the computer code A to prevent the second thread from accessing the computer code A when the first thread is executing the computer code A. Similarly the computer code A need not be locked for execution by the second thread . This can be accomplished because computer code A can be executed in different contexts for different threads thus allowing multiple threads to execute computer code A without interfering with each other in a safe manner without having to implement conventional locking mechanisms that typically adversely affect performance.

In accordance with another aspect of the invention a thread specific execution context or Worker W can be effectively passed between computer codes that interact with each other. Referring back to the first thread specific execution context or first Worker W is effectively passed to a second computer code B which may somehow interact with the first computer code A. The interaction of computer code A and B can for example be the result of invocation or calling of the second computer code B from or by code of the computer code A. By way of example computer code A can be a first function F that calls another function F such as a function F F . In any case passing the first thread specific execution context W through multiple interrelated portions of computer code allows the first thread to effectively use multiple pieces of computer code e.g. functions blocks of code modules procedures methods without having to lock any of them so that the same pieces of code can be used by the second thread. Similarly the execution context for the second thread or Worker W can be effectively passed through multiple pieces of code that may be interrelated for example in a chain of calls or invocations.

Those skilled in the art will also appreciate that the execution context or Worker can effectively include or reference one or more objects . In effect these objects are worker specific objects that can be provided for a particular thread namely the first thread as a part of the thread specific execution context or Worker . In order to execute computer code for the thread one or more objects can be used to define the context of execution and or manipulated by one or more pieces of computer code e.g. functions as it is passed through various pieces of computer code effectively activated e.g. invoked or called by the thread. It should be noted that one or more objects are not accessible to the second thread .

To further elaborate depicts a conceptual model of a multilayered software in accordance with one embodiment of the invention. The general concept of multilayered software architecture is known to those skilled in the art. In brief this model allows designing software in multiple layers that relate to each other. Generally a top layer A directly interacts with a lower layer B and vice versa. Referring now to three layers A B and C are depicted in a hierarchical relationship that respectively defines them as top middle and a bottom layer layer A B and C . Further each layer can include computer code that effectively builds on or uses computer code in a lower layer. For illustrative purposes computer code is represented as a generic logical or mathematical function e.g. Fwhich is a function in the top layer A . As such a function F in the top layer A can for example build on functions Fand or Fprovided or implemented by the middle layer B and so on. Thus activation e.g. calling invocation of operational code in Fcan in turn activate functions Fand Fin the middle layer B. Similarly activation of operational code in Fand Fcan for example activate operational code of function F F Fof the bottom layer C.

Those skilled in the art will appreciate that the thread specific execution context Worker W also shown in can effectively be passed through the functions of the different layers A B and C . More particularly function Fcan pass the execution context W to the functions it builds upon as it effectively activates them. As an example function Fcan activate functions Fand Fin the middle layer B and so on. It should also be noted that the interaction between the layers may be extremely complex where for example a function Fin the bottom layer may be called by multiple activated functions Fand F . Nevertheless the thread specific execution context for different threads may be passed through various functions in multiple layers thereby allowing multiple threads to effectively use various functions in different layers at the same time without interfering with each other. It should be noted that the functions depicted in need not be locked for execution by other threads as would be required by conventional multi threading techniques where a single thread of execution may effectively lock numerous functions in a multilayer software architecture. As such the invention is especially useful for multilayered software where multiple functions may be called by a single thread of execution because among other things the latency of the conventional systems primarily caused by numerous locks taken by a single thread can be minimized. Further concurrency can be maximized as a Worker can be assigned to each of any number of threads as will be discussed in greater detail below.

It should be noted that threads may be executed on the same processor or multiple processors. In any case an execution context or Worker can be effectively associated with each thread of execution and passed through as operational or active code is activated in various paths of execution taken by the thread. depicts a multithreaded computing environment in accordance with one embodiment of the invention. As shown in multiple Central Processing Units CPUs can be used to execute threads in parallel and or multiple threads can be executed on the same CPU e.g. time slicing as known to those skilled in the art . In any case for each thread a thread specific execution context or Worker is provided that is generally not made available or accessible to other threads of execution. Thus although threads 2 and 3 may be executed on the same CPU a separate Worker 2 and 3 can be provided for each thread and used to execute code separately in the context of each thread.

The association between a thread and an execution context or Worker can for example be made by effectively binding a Worker to the thread in accordance with one embodiment of the invention. To further elaborate depicts a computing environment where the multilayered software also shown in is provided as intermediary component e.g. middleware between two other computing components namely a first computing component e.g. an application program and a second computing component. The first computing component uses an interface e.g. application programming interface to interact with the intermediary component . The intermediary component is connected to the second computing component e.g. operating system OS hardware virtual machine VM firmware . As such the intermediary component allows the first component e.g. application program to access the second computing component indirectly i.e. via the multiple layers of the intermediary component . Referring to a function Fin a layer A of the intermediary component is implemented with a parameter W that can for example be an input output parameter to have a reference or pointer to the execution context or Worker W as a parameter W . In the described embodiment the execution context or Worker W is represented as the last parameter or a parameter N of parameters that can be numbered from 1 to N P . . . P .

However it will be appreciated that Worker can be any one of the parameters P . . . P where N 1 for any of the functions implemented in the multilayered architecture depicted for the intermediary . As such functions F F F Fand Fcan also be implemented to receive the execution context W for a thread as a parameter for example as a reference or pointer W . Thus each one of the threads T Tand Tcan execute a function F . . . W at the first layer by providing their associated Worker parameter to the function Fwhich in turn would pass the Worker W as a parameter to lower layers. Moreover it will be appreciated that this association can be made in a dynamic manner when the number of threads that are needed for a particular application becomes known. In other words the intermediary component e.g. a multilayered middleware can be written without making assumption about the number of threads that are or can be supported by either the first or second computing components around the intermediary component . As a result implementation of the intermediary component for multi threading applications can be immensely simplified. Further the intermediary component can easily be ported to various environments with different levels of multi threading support ranging from those that provide no multi threading support to those that can support numerous threads at the same time. By way of example when it is determined that the first computing entity e.g. a first application program requires three 3 threads three 3 execution contexts or Workers can be respectively associated e.g. bounded to each thread of execution T Tand T . Similarly for another application program not shown that requires five 5 threads five Workers can be provided and respectively associated with the threads such that each thread has its own associated Worker. However it should be noted that actual multi threading support provided by the second computing component e.g. operating system may differ than the number of threads or Workers that can be effectively configured for the first computing component e.g. application program . By way of example three 3 threads may be effectively associated with three 3 Workers for an application program as the first component depicted in . However there may be no actual multithreading support for a particular operating system provided as the second computing component that supports the application program so the application program can effectively be treated by the operating system as a single thread. The same intermediary component and mechanisms can be used to effectively configure the application with three 3 threads on another operating system that provides multi threading support for these threads. Accordingly those skilled in the art will appreciate that intermediary component e.g. middleware software can be written without making any assumption about the multithreading component interact with it. Further the same intermediary component can support i.e. serve as an intermediary other computing components irrespective of their different and seemingly incompatible levels of multithreading configuration or support.

As noted above a thread may be associated e.g. bounded to an execution context or Worker . Moreover this association can be made at the top layer of a multilayered architectural design. Referring to the association between the three 3 threads T Tand Tand Workers W Wand W can be effectively made at the top layer A. Moreover this association can be made in a dynamic manner when the number of threads to be configured for a particular application becomes known. To further elaborate the process of associating a thread to an execution context or Worker is described below.

Referring now to a first thread T which has been bounded to a first execution context or first Worker W is depicted. Those skilled in the art will know that a particular thread e.g. first thread T can be associated with a thread ID e.g. a number . Further a thread specific storage TSS can be effectively allocated for use of the first thread T e.g. thread Tcan be bounded to a thread specific storage . As such the thread specific storage TSS can serve as a storage area for the first thread T. Typically other threads e.g. a second thread T are excluded from accessing the thread specific storage TSS allocated for the first thread. In the described embodiment shown in a reference to the execution context or Worker W is allocated in the thread specific storage TSS . Further a reference is allocated to one or more objects associated with the first execution context or Worker W. Those skilled in the art will appreciate that virtually anything can be represented by the object s that are associated or included in the first execution context or Worker W bounded to the first thread. Generally these object s can be used to define or provide the context of the execution for a particular thread e.g. first thread T . Moreover it should be noted that objects bounded to a particular thread can have many additional benefits and applications. By way of example a timestamp object is depicted with 3 fields each representing a layer of the intermediary entity . Timestamp object also provides functions or methods e.g. get put which operate to get a timestamp e.g. current time and put or write it into the fields as a particular thread traverses the layers of the intermediary component e.g. a middleware . It will be appreciated that the timestamp object allows measuring the amount of time a particular thread spends in each layer of the intermediary component .

As noted above activation of execution of a first operational computer code can effectively activate additional operational code. By way of example a function or method in a first code portion can call another function or method in a second code portion. As such it is determined whether to activate additional operational code. Accordingly the Worker can be passed to additional operational code as many times as it may be necessary thereby allowing additional operational code to be executed in the context of the thread. The Worker is used to execute additional operational code s for the thread. Finally the execution of the operational code for the thread is completed and the execution method ends.

Referring back to some of the challenges and problems associated with conventional multithread techniques are illustrated. To overcome these limitations and to make the code independent of a particular operating system or environment the notion of a Worker can be used as shown in in accordance with one embodiment of the invention. In the embodiment shown a worker is an object in program memory that represents a thread specific execution context and can be thought of as a logical thread in which a block of code executes. In this example the worker is passed in as the last parameter of the function e.g. cumulative sum3 value worker and provides the logical execution context in which the function executes. A physical operating system thread is bound to one and only one worker as shown in . This binding happens before calling program code uses a worker.

The program code using the worker execution context or logical thread can associate worker specific objects with a worker to store state that must be maintained on a per worker basis. Thus as shown in the cumulative sum3 value worker stores the running total in an object attached to the worker that is passed in as a parameter. This allows two concurrent invocations of the code to produce the correct results see Figure since each invocation maintains its own running total there is no danger of clobbering or interfering with one another.

To overcome the limitations of the conventional multithreading environments those skilled in the art should observe the following. A Worker can be provided as a logical abstraction of the notion of a thread. The Worker represents the context in which a method is invoked and can be passed as a parameter to all the internal methods in the middleware. A Worker can be associated with named objects to store additional Worker specific state. For example a time stamping object can be attached to a worker to record the time stamps as a thread traverses the different middleware layers. A Worker can be bound to a specific thread at the entry points into the middleware from user code and callbacks. This can for example be done by storing the Worker pointer in thread specific storage provided by the Operating System OS . This design allows the thread specific storage to be effectively quite large.

It should be noted that the concept of a Worker and the notion of associating objects per Worker enables several benefits not afforded by conventional approaches. These include 1 the program code is independent of assumption about the operating system or environment in which it operates 2 the worker can be bound to physical OS threads at the user interface layers in possibly different ways depending on the operating system or environment 3 attaching objects per worker allows the program code to store unlimited amounts of execution context specific state thereby solving two related issues preventing resource contention and preventing interference by putting resources on a per worker basis. Since locks do not needed to be taken for resources that are only used on a per execution context worker basis the extra locking latency is avoided and concurrency is maximized.

Another aspect of the invention pertains to techniques for protecting resources of a computing environment from multiple threads. In multithreaded programming environment a resource can be shared between multiple threads i.e. a shared resource . It will be appreciated that an Exclusion Area EA can serve as a logical lock to protect a shared resource in a multithreaded environment.

Referring now to an Exclusion Area EA is conceptually depicted as an area or boundary that protects a shared resource R . The Exclusion Area EA is provided as a logical mechanism for protecting a Shared Resource R in a multi threaded programming environment. Those skilled in the art will appreciate that the logical Exclusion Area EA can be combined with the notion of a thread specific context or Worker to effectively implement a multi threaded environment. More particularly multiple threads that are each respectively associated with a Worker can interact with the Exclusion Areas EAs provided to protect shared resources R s . In other words the notion of a Worker and an Exclusion Area EA can be used to implement a multi threaded environment where multiple threads can safely share resources yet the benefits of using Workers can be utilized as well as additional benefits as will be discussed below.

In general an EA can be provided used or passed to a Shared Resource R in order to protect the shared resource. As such each Shared Resource R of a computing environment can be passed an EA to effectively protect the shared resource this is depicted as R . . . EA . Typically a particular Shared Resource e.g. R is protected by an EA e.g. EA . However an EA may be used to protect multiple resources and can be passed as a resource parameter from one layer to another in a multi layered software architecture. In general multiple EAs EA. . . EA may effectively protect multiple Shared Resource R. . . R .

Referring to an EA is associated with a number e.g. level priority N. Those skilled in the art will appreciate that this can for example be accomplished by an EA with a property or parameter N EA.Number . As such each EA protecting a shared resource can be assigned a Number e.g. 1 2 3 4 5 . This number can be used to determine whether a Worker can visit multiple EAs. Referring now to Worker Wis depicted while visiting an EA e.g. EA . In this situation the Worker may visit EAs that are for example assigned numbers that are higher than N e.g. 4 and 5 3 . However the Worker Wwould not be able to visit an EA with assigned numbers that are lower than or equal to N k N .

Those skilled in the art will appreciate that similar to the conceptual notion of a Worker the notion of an Exclusion Area can be used in a multilayered programming environment. depicts a computing environment that uses the intermediary component e.g. middleware also shown in as an intermediary component between a first and second computing components. The intermediary component is implemented in multiple layers where a shared resource R in layer A is protected by EA and is parametrically depicted as R . . . EA . Similarly shared resources Rand Rcan be in a middle layer B and so on. Each of the shared resources can be effectively protected by an Exclusion Area EA as depicted in and parametrically depicted as R . . . EA . It will be appreciated that a Worker Wcan visit the EA at the top layer A if no other Worker is visiting the EA as discussed above. Moreover the Worker Wcan visit EAs in a lower layers EA EA and the one lower than that EA EA EA and so on. The EAmay be passed down to the lower layers as a parameter as the thread effectively accesses resources. Thus multiple resources in a lower layer which are protected by distinct EAs can be mapped to a single EA passed down as a parameter from a higher layer. Thus a single physical lock associated with EAmay actually be used to protect all of the resources that are effectively used by the thread as the Worker visits the EA.

It should be noted that the determination of whether the Worker can visit multiple EAs can be made for example based on a numbering or prioritizing scheme used for the EAs and thereby their respective shared resources. As such multiple EAs can be organized so that only a single physical lock is taken by a Worker W thus the thread associated with the Worker Wcan effectively use a single physical lock to access multiple shared resources. This allows a desired binding of shared resources to physical exclusion locks thereby allowing users to control the amount of resource contention that can be tolerated in the system as desired.

However if it is determined that the Worker is trying to visit the EA it is determined whether another Worker is visiting the EA that is protecting the shared resource. In effect it is determined whether another context of execution is accessing the shared resource. Accordingly if it is determined that another Worker is visiting the EA the Worker is not allowed to visit or enter the EA. In effect the thread associated with the Worker is caused to wait to access the shared resource.

On the other hand if it is determined that another Worker is not visiting the EA the Worker is allowed to visit the EA thereby allowing the thread associated with the Worker to access the shared resource. Thereafter it is determined whether the Worker is trying to access another shared resource protected by a different EA while the Worker is visiting the EA. In other words it is determined whether the Worker is attempting to visit another EA effectively protecting a second shared resource while the Worker is still visiting the EA protecting the first shared resource. If it is determined that the Worker is trying to visit another EA while visiting the EA the numbers assigned to each of the EAs can be compared . Accordingly based on the comparison of the numbers assigned to each EA it can be determined whether to allow the Worker to visit another EA. Thus the Worker may be allowed to visit the other EA i.e. access a second shared resource or the Worker may not be allowed to visit the EA thereby preventing the associated thread from accessing a second shared resource. In effect the Worker may attempt to visit or enter several EAs and permission to enter can be granted or denied in a similar manner as discussed above. It should be noted that if it is determined that the Worker is not trying to visit another EA it is determined whether the Worker is to leave the one or more EAs it has visited. Accordingly the Worker can leave the one or more EAs and the execution method can end. The execution method ends when it is determined to end the execution of a thread or threads.

Conventional resource contention is typically resolved by protecting shared resources by mutual exclusion semaphores. An EA provides a logical abstraction of the notion of a mutual exclusion semaphore. An EA can be associated with a layer in a multilayered architecture. Each shared resource can be passed an EA that will be used to protect it. The same EA can be used to protect another resource multiple EAs may be used to protect different resources. The decision of how many EAs to use and the binding of EAs to physical locks can be made in higher layers and can be configurable by the user. This approach allows the user to tune the amount of resource contention in the system. Conceptually an EA may be visited by a Worker. A worker may be visiting zero one or more EA at any given time. The Worker and EA keep track of each other. A Worker can only enter an EA if its level is higher than the level of the EA if any the Worker is in currently. This approach prevents deadlocks by avoiding a situation where a worker is effectively waiting on itself.

There are situations where the worker mechanism can be combined with the EA mechanism. For example when there are common shared resources that need to be accessed by multiple execution contexts or workers. As such the Exclusion Area EA mechanism can work in conjunction with the worker mechanism to address resource sharing among workers. In one embodiment An Exclusion Area EA is a logical abstraction with the following properties.

As noted above deadlock conditions between Workers trying to access Exclusion Areas EAs can be resolved by assigning a number e.g. level priority in accordance with one aspect of the invention. By way of example a deadlock condition would arise when a first Worker enters a first EA and waits for a second EA at the same time when a second Worker enters the second EA and waits for the first EA. This deadlock condition is resolved by assigning a number e.g. level priority to the first and second EAs.

In another aspect a Worker provides a programming context that represents a logical thread. The Worker can for example be passed explicitly as a parameter in the call chain up and down the stack. In another aspect an Exclusion Area EA represents a programming construct that can for example be explicitly associated with a level. The EA can work hand in hand with a worker to provide the following functionality a the Worker maintains a record of the Exclusion Areas EAs it has currently visited and b the EA knows the Worker that is visiting it if any and the level and the number of times it has been entered by the worker repetitive entry counter . These two objects representative of the Worker and the EA can for example collaborate following these rules 

If a Worker tries to re enter an EA it is already visiting it is allowed to do so and the repetitive entry counter is incremented 

Otherwise the enter operation succeeds and acquires an underlying operating system lock or a similar construct.

Otherwise the repetitive entry counter is decremented and in case it is zero i.e. the Worker left the EA as many times as it was entered the underlying operating system lock or similar construct is released.

Variations contemplate the case where the underlying OS resource handles the repetitive acquisition count. In this case the EA has less to do but it provides the same service to the Worker.

The Workers and EA framework is applied to the construction of multi layered real time data middleware. For example the DDS specification does not specify an implementation model. Instead it focuses on the logical model of data distribution for real time systems. The combination of Workers and EA provides a flexible and general purpose mechanism for multi threading and resource sharing that can be used to realize a high performance real time data distribution middleware implementation. These techniques can be used to ensure minimal end to end latency and maximal concurrency in a system.

In one embodiment Workers are associated with individual threads that interact with the data distribution middleware. At the top layer of a data distribution middleware there are user threads. At the bottom layer of the data distribution middleware there are receive threads that are waiting for incoming messages on transport plugin resources and storing the information in the internal middleware buffers. Other layers in the data distribution middleware may create and use internal threads to implement the functionality offered by the middleware.

Further different middleware layers can be configured to use the same Exclusion Area EA to protect all the internal shared resources that are visited in connection with writing data for a single Data Writer or receiving data from a single Data Reader. The internal shared resources necessary to write or read a different Data Writer or Data Reader can also be effectively protected it is typically protected with a different EA. Those skilled in the art will appreciate that the same EA can be used for the resources associated with a single Data Writer or Data Reader entity across the layers of the middleware. This is illustrated in for Data Readers and for Data Writers. illustrates that the EAs in the incoming data path for a single Data Reader can be mapped across the middleware layers to a single logical EA at the top layer which can be bound to a single physical lock per Data Reader . illustrates that the EAs in the outgoing data path for a single Data Writer can be mapped across the middleware layers to a single logical EA at the top layer which can be bound to a single physical lock per Data Writer . Each Data Reader Data Writer entity is associated with a different Exclusion Area EA . The EAs protecting the shared resources used by the Data Reader Data Writer in the internal middleware layers are mapped across the layers such that for a Data Reader Data Writer the incoming outgoing data path effectively uses a single physical lock. Furthermore the incoming outgoing data paths of Data Readers Data Writers are independent of each other and can deliver data concurrently in parallel threads of execution. This in effect creates what can be depicted as a latency swim lane whereby incoming messages intended for a Data Reader can proceed up the Data Reader s swim lane in parallel concurrently with incoming messages intended for another Data Reader. Similarly for Data Writers outgoing messages can proceed down the Data Writer s swim lane in parallel concurrently with other Data Writers. Since the Data Readers and Data Writers use different EAs the middleware can concurrently send and receive data. Also since the EAs in a data path are mapped across layers there is no need to take multiple locks thus minimizing the latency in the data path.

It should also be noted that groups of Data Reader or Data Writer entities can be made to use the same Exclusion Area EA in effect making the write or read exclusive from each other i.e. it is no longer possible to use Data Writer 1 and Data Writer 2 concurrently from different user threads . The EAs protecting resources in a data path across the different layers can be mapped to a single logical EA for a group of Data Readers Data Writers based on a user specified configuration policy. The single EA per group of Data Readers Data Writers can in turn be bound to a single physical lock. This is illustrated in for a group of Data Readers and in for a group of Data Writers. illustrates that the EAs in the incoming data path for a group of Data Readers say belonging to a Subscriber can be mapped across the middleware layers to a single logical EA at the top layer which can be bound to a single physical lock per group of Data Readers Subscriber . illustrates that the EAs in the outgoing data path for a group of Data Writers say belonging to a Publisher can be mapped across the middleware layers to a single logical EA at the top layer which can be bound to a single physical lock per group of Data Writers Publisher . In one embodiment a single EA is associated with a Subscriber for the group of Data Readers created from the Subscriber and a single EA is associated with a Publisher for the group of Data Writers created from the Publisher .

The selection of whether to use a single physical lock per Data Reader Data Writer or whether to use a single lock per Data Reader Data Writer group such as a Subscriber Publisher can be made by a user configurable policy that can be specified in the user application code .

Those skilled in the art will appreciate that the invention can be used to allow the layers to be developed independently without making assumptions about the threads being used or the resource sharing scheme being used. Instead the techniques discussed in the invention delay the resource locking and the thread assignment until the very top layers. The techniques also enable a flexible remapping to alternate resource sharing and threading schemes without changing existing code.

The invention has many advantages. One or more embodiments of the invention can provide one or more of the advantages highlighted below.

CPU is also coupled to a variety of input output devices such as display keyboard mouse and speakers . In general an input output device may be any of video displays track balls mice keyboards microphones touch sensitive displays transducer card readers magnetic or paper tape readers tablets styluses voice or handwriting recognizers biometrics readers or other computers. CPU optionally may be coupled to another computer or telecommunications network using network interface . With such a network interface it is contemplated that the CPU might receive information from the network or might output information to the network in the course of performing the above described method steps. Furthermore method embodiments of the present invention may execute solely upon CPU or may execute over a network such as the Internet in conjunction with a remote CPU that shares a portion of the processing.

In addition embodiments of the present invention further relate to computer storage products with a computer readable medium that have computer code thereon for performing various computer implemented operations. The media and computer code may be those specially designed and constructed for the purposes of the present invention or they may be of the kind well known and available to those having skill in the computer software arts. Examples of computer readable media include but are not limited to magnetic media such as hard disks floppy disks and magnetic tape optical media such as CD ROMs and holographic devices magneto optical media such as floptical disks and hardware devices that are specially configured to store and execute program code such as application specific integrated circuits ASICs programmable logic devices PLDs and ROM and RAM devices. Examples of computer code include machine code such as produced by a compiler and files containing higher level code that are executed by a computer using an interpreter. Computer readable media may also be computer code transmitted by a computer data signal embodied in a carrier wave and representing a sequence of instructions that are executable by a processor.

