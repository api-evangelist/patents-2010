---

title: Techniques for synchronizing application object instances
abstract: Techniques for synchronizing data object instances between applications/processes in an efficient manner. In one set of embodiments, the techniques described herein can be implemented in one or more network routers to synchronize data between a process running on an active management processor and a process running on a standby management processor, thereby facilitating features such as non-stop routing (NSR).
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08769155&OS=08769155&RS=08769155
owner: Brocade Communications Systems, Inc.
number: 08769155
owner_city: San Jose
owner_country: US
publication_date: 20100624
---
The present application claims the benefit and priority under 35 U.S.C. 119 e of U.S. Provisional Application No. 61 315 757 filed Mar. 19 2010 entitled TECHNIQUES FOR SYNCHRONIZING APPLICATION OBJECT INSTANCES the entire contents of which are incorporated herein by reference for all purposes.

Embodiments of the present invention relate in general to data synchronization and in particular to techniques for efficiently synchronizing data object instances between applications processes.

Data synchronization refers to the process of keeping multiple copies of a dataset in coherence with each other. Data synchronization techniques are commonly used in a variety of different computing scenarios that require consistency between redundant replicated data stores such as multi level cache architectures distributed filesystems high availability database clusters and the like.

In the field of computer networking data synchronization techniques can be used to facilitate non stop routing NSR . Generally speaking NSR enables a network router to gracefully handle the failure of an active management processor active MP within the router by failing over to a standby management processor standby MP without disrupting routing protocol interactions with other routers and without dropping any packets known as hitless failover . NSR also allows for software upgrades to be performed on an active MP in the same hitless fashion.

To implement NSR a router typically maintains data structures in a memory accessible by a standby MP that replicate data structures e.g. routing table neighbor database etc. used by a process running on an active MP in carrying out routing functions. Thus if the active MP fails the standby MP can automatically access the information it needs via the replicated data structures to take over routing functions in a seamless manner. As part of this implementation data synchronization techniques are needed to ensure that the respective data accessible by the active and standby MPs remain in sync with each other. For example while the active MP is available the process running on the active MP can receive messages from other routers e.g. link state advertisements etc. that require changes to its routing information. These changes need to be replicated in a consistent manner to the standby MP so that the standby has the most up to date routing data in case of a subsequent failure in the active MP .

Unfortunately existing data synchronization techniques have a number of limitations that limit their usefulness in this and other similar contexts. Merely by way of example existing data synchronization techniques generally require creating an intermediate copy of the data to be synchronized in the memory accessible by the active MP thereby consuming memory resources and decreasing performance. As another example existing data synchronization techniques cannot easily support synchronization of different types of data objects as may be needed for supporting NSR with respect to different routing protocols .

Embodiments of the present invention provide a framework referred to herein as sync library for synchronizing data object instances between applications processes in an efficient manner. In one set of embodiments the sync library can be implemented in one or more network routers to synchronize data between a process running on an active MP e.g. the master application and a process running on a standby MP e.g. the slave application thereby facilitating features such as non stop routing NSR .

In one embodiment the sync library can synchronize data between a master application and a slave application without creating a temporary copy of the data. In another embodiment the sync library can support parallel synchronization of different types of data objects e.g. link state advertisements multicast cache entries etc. . In another embodiment the sync library can enable the master application to check the synchronization status and receive an indication of a successful end to end synchronization for each data object instance. In another embodiment the sync library can allow the master and slave applications to define functions for packing and unpacking data into synchronization buffers e.g. inter process communication or IPC buffers used to transmit data to the slave application. In another embodiment the sync library can support multiple virtual synchronization instances. In another embodiment the sync library can perform baseline synchronization in the event that the slave application is unavailable for a period of time and is restarted.

According to one embodiment of the present invention a method is provided that comprises synchronizing by a network device a data object instance between a first application running on a first processor of the network device and a second application executing on a second processor. In certain embodiments the data object instance is resident in a first memory accessible by the first processor and the synchronizing does not require a copy of the data object instance to be created in the first memory.

In one embodiment the synchronizing causes the data object instance to be replicated in a second memory accessible by the second processor.

In one embodiment the synchronizing comprises if the second processor is available adding the data object instance to a first linked list the first linked list including data object instances of the first application that are intended to be sent to the second application and invoking a first function for packing the data object instance into a synchronization buffer.

In one embodiment the first function is a callback function that is registered by the first application.

In one embodiment the synchronizing further comprises if the second processor is available transmitting the synchronization buffer to the second application and invoking a second function for unpacking the data object instance from the synchronization buffer.

In one embodiment the second function is a callback function that is registered by the second application.

In one embodiment the synchronizing further comprises if the second processor is available moving the data object instance from the first linked list to a second linked list the second linked list including data object instances of the first application that have been sent to the second application but have not yet been acknowledged as being received.

In one embodiment the synchronizing further comprises if the second processor is available determining whether an acknowledgment is received from the second application within a predetermined time interval the acknowledgement indicating that the data object instance has been received by the second application.

In one embodiment the synchronizing further comprises if the second processor is available and if the acknowledgement is received within the predetermined time interval invoking a third function for notifying the first application that synchronization of the data object instance is successful and moving the data object instance to a third linked list the third linked list including data object instances of the first application that have been sent to the second application and acknowledged.

In one embodiment the synchronizing further comprises if the second processor is available and if the acknowledgement is not received within the predetermined time interval moving the data object instance to the end of the first linked list.

In one embodiment the synchronizing further comprises if the second processor is unavailable adding the data object instance to the third linked list and invoking the third function.

In one embodiment the synchronizing further comprises once the second processor becomes available moving the data object instance from the third linked list to a fourth linked list the fourth linked list including data object instances of the first application that are intended to be sent to the second application in a bulk fashion.

In one embodiment the network device is a network router the first processor is an active management processor and the second processor is a standby management processor.

According to another embodiment of the present invention a network device is provided. The network device comprises a first processor configured to perform management functions of the network device and a first memory accessible by the first processor. In certain embodiments the network device is configured to synchronize data between the first processor and a second processor where the second processor is communicatively coupled with a second memory where the synchronizing comprising replicating a data object instance from the first memory to the second memory and where the synchronizing does not require a copy of the data object instance to be created in the first memory.

In one embodiment the synchronizing further comprises synchronizing a plurality of data object instances between the first processor and the second processor in a single synchronization transaction.

In one embodiment the network device is a network router the first processor is an active management processor and the second processor is a standby management processor.

In one embodiment the first and second processors are configured to execute a plurality of virtual routing protocol instances and the network device is configured to synchronize data between the first processor and the second processor for each virtual routing protocol instance.

According to another embodiment a computer readable storage medium having stored thereon program code executable by a first processor of a network device is provided. The program code comprises code that causes the first processor to synchronizing a data object instance between a first application executing on the first processor and a second application executing on a second processor where the data object instance is resident in a first memory accessible by the first processor and where the synchronizing does not require a copy of the data object instance to be created in the first memory.

A further understanding of the nature and advantages of the embodiments disclosed herein can be realized by reference to the remaining portions of the specification and the attached drawings.

In the following description for the purposes of explanation specific details are set forth in order to provide a thorough understanding of embodiments of the invention. However it will be apparent that the invention may be practiced without these specific details.

Embodiments of the present invention provide a framework referred to herein as sync library for synchronizing data object instances between applications processes in an efficient manner. In one set of embodiments the sync library can be implemented in one or more network routers to synchronize data between a process running on an active MP e.g. the master application and a process running on a standby MP e.g. the slave application thereby facilitating features such as non stop routing NSR .

In certain embodiments the sync library can be implemented as a set of application programming interfaces APIs . As described in further detail below these APIs can be invoked by the master and or slave application to initiate and carry out the synchronization process.

As shown in the embodiment of router can include one or more management cards A B and one or more linecards coupled via a switch fabric . Each management card linecard A B can be inserted into or removed from one of a plurality of modular slots in the chassis of router . Accordingly router can accommodate any number of management cards and linecards as needed for different network topologies and different switching routing requirements. It should be appreciated that the particular configuration depicted in is meant for illustrative purposes only and is not intended to limit the scope of the present invention. For example alternative embodiments can have more or less components than those shown in .

Generally speaking linecards represent the data forwarding plane of router . Each linecard can include one or more input output ports that are used by router to send and receive data packets. Ports can send and or receive various types of data traffic at different speeds including 1 Gigabit sec 10 Gigabits sec or more. In some embodiments multiple ports can be logically grouped into one or more trunks.

Management cards A B represent the control plane of router . Each management card can include a management processor MP e.g. A B that executes management and or control functions of router . In one set of embodiments the MP can be a general purpose microprocessor such as a PowerPC Intel AMD or ARM microprocessor that operates under the control of software stored in a computer readable storage medium e.g. RAM ROM etc. . For example the computer readable storage medium can store program code which when executed by MP A or B carries out the various data synchronization techniques described herein.

In one set of embodiments management cards A B can support non stop routing NSR with respect to one or more routing protocols functions e.g. Open Shortest Path First OSPF Intermediate System to Intermediate System IS IS Border Gateway Protocol BGP multicast tree management etc. . In these embodiments MP A of management card A referred to as the active MP can operate in an active mode and carry out the routing control functions of router . MP B of management card B referred to as the standby MP can operate in a standby or waiting mode. When a failure or some other event such as a software upgrade causes MP A to become deactivated or otherwise unavailable router can automatically fail over control plane functionality from MP A to MP B without disrupting routing control interactions with other routers and without dropping any packets referred to as a hitless failover . Once the failover is complete MP B can become the new active MP and MP A can become a standby MP.

As part of this NSR implementation management card A can maintain routing data in a memory A is that used by active MP A in executing routing control functions and management card B can maintain a synchronized copy of the routing data in a memory B that is accessible by standby MP B. Examples of such routing data can include e.g. link state advertisements received from peer routers a neighbor database and the like. With this mirrored configuration when a failover occurs from active MP A to standby MP B MP B can automatically access the information it needs via memory B to take over the routing functions of MP A in a seamless manner.

To ensure that the routing data stored in memory A remains in sync with the routing data stored in memory B router can make use of a sync library . In various embodiments sync library can provide a set of synchronization data structures and APIs that enable a process running on active MP A referred to herein as the master application to synchronize data object instances with a corresponding process running on standby MP B referred to herein as the slave application . In this manner any changes to the routing data in memory A can be replicated in a consistent manner to memory B.

In certain embodiments sync library can provide a number of advantages over existing data synchronization techniques. For instance sync library is not limited to synchronizing specific types of data and can be used to synchronize any type of data structure that may be used by the master and slave applications e.g. in the case OSPF sync library can be used to synchronize LSAs and neighbor information in the case of Multicast sync library can be used to synchronize multicast cache entries and so on .

In addition sync library can minimize memory overhead by avoiding making intermediary copies of the data to be synchronized. For example sync library can synchronize data from memory A to memory B without creating an intermediate or temporary copy of the data in memory A. Rather sync library can operate directly on the data object instances instantiated by the master application in memory A.

In addition sync library can enable the master application to check the synchronization status and receive an indication of a successful end to end synchronization for each data object instance being synchronized.

In addition sync library can allow the master and slave applications to define functions for packing and unpacking data into the synchronization buffers e.g. IPC buffers used to transmit data to the slave application. This enables sync library to support the synchronization of different data object types since the logic for packing unpacking a particular data object type is provided by the master and or slave applications rather than being handled by the sync library . This also allows for parallel synchronization of different data object types since the master slave applications can specify a different pack unpack function for each data structure type.

In addition the sync library can support multiple virtual synchronization instances. This can be useful for example if MPs A and B each support the parallel execution of multiple virtual routing protocol instances e.g. multiple OSPF instances . In this case a separate synchronization instance can be created and maintained for each routing protocol instance.

In addition sync library can perform baseline synchronization in the event that the slave application is down for a period of time. For example if standby MP B is unavailable sync library can queue all of the routing data updates received from the process running on active MP A. Once the standby MP B become available again sync library can automatically synchronize all of the data object instances in the queue so that routing data B is brought up to the same baseline state as routing data A.

It should be appreciated that is illustrative and not intended to limit embodiments of the present invention. For example network router can have other capabilities or include other components that are not specifically described. In a particular embodiment management cards A B and thus MPs A B and memories A B can be resident in different network routers such that routing data is synchronized across routers rather than within a single router . One of ordinary skill in the art will recognize many variations modifications and alternatives.

As shown includes a sync library global data array that specifies one more application IDs . In various embodiments array can be created when sync library is first initialized and each array index can correspond to an identifier of an application that is using the sync library. For example if a process running on active MP A of initializes sync library array index can correspond the application ID for that process.

Each array value for array can point to a linked list of sync instances e.g. . This linked list can identify all of the sync instances currently being used by the application specified by the corresponding array index. As described above an application can be composed of multiple virtual instances each of which require synchronization services. Accordingly a separate sync instance can be created for each virtual application instance. Although only two sync instances are shown for app ID any number of sync instances can be created.

Each sync instance e.g. can point to a linked list of sync entities e.g. . In one set of embodiments a sync entity can maintain synchronization information for a specific type of data object. For example a process running on active MP A may want to synchronize link state advertisements LSAs as well neighbor database updates with standby MP B. In this case a separate link entity can by created for the LSA data type and the neighbor data type. Since each link entity can maintain its own synchronization state this allows different types of data objects to be synchronized in parallel for a given application.

In one set of embodiments each sync entity e.g. can include pointers to four different types of linked lists bulk TBS To Be Sent dynamic TBS SNA Sent but Not Acknowledged windows and SAA Sent And Acknowledged . Each of these linked lists can comprise sync nodes e.g. that correspond to data object instances that need to be or have been synched between the master and slave application. Using these lists sync library can keep track of for example which data object instances need to be sent synchronized to the slave application which instances have been sent but not acknowledged and which instances have been sent and acknowledged. The logic for populating and removing nodes from each of these lists is discussed in greater detail with respect to below.

In one set of embodiments the sync nodes pointed to via lists and can directly correspond to the data object instances created by the master application. Thus sync library does not need to create a temporary or working copy of the data object instances for synchronization purposes rather sync library can operate directly on the instances used by the master application.

In certain embodiments sync library can send a plurality of data object instances from the master application to the slave application in a single transaction via a synchronization buffer e.g. an IPC buffer . In these embodiments SNA Windows can point to a number of sub lists that each have an SNA Window head node . These sub lists can be used to keep track of the data object instances that are sent to the slave application in a single buffer.

At block master application can call one or more sync library APIs to create a sync instance and a sync entity for synchronizing data with slave application . For example master application can create a sync instance for an OSPF routing protocol running on active MP A and can create a sync entity for syncing LSAs. In response to block sync library can instantiate the sync instance and sync entity data structures as described with respect to block .

At block master application can instantiate data object instances that are used by the application. The data object instances can include for example routing data for facilitating routing via a particular protocol e.g. OSPF IS IS BGP etc. . As part of this process master application can allocate for each data object instance a memory portion for storing a sync node pointer used by sync library . This enables sync library to directly access these data object instances when building the TBS SNA and SAA lists.

At block master application can call a sync library API that specifies a particular data object instance to be synched. In response sync library API can add the sync node correspond to the specified data object instance to dynamic TBS of block for the purposes of process it is assumed that the standby processor on which slave application is running e.g. MP B is available process of illustrates an alternative process that is performed when the standby processor is not available . Blocks and can be repeated any number of times to add additional sync nodes to the dynamic TBS list.

At blocks sync library can initiate synchronization of the sync nodes data object instances added to the dynamic and bulk TBS lists. In one set of embodiments the processing of blocks can be automatically initiated at a recurring time interval that is specified by master application . In another set of embodiments this processing can be initiated by a specific command received from master application .

At block sync library can invoke a callback function i.e. a pack function registered by the master application to pack the data object instance for the sync node into an synchronization buffer. As part of this invocation sync library can pass as a parameter to the function a pointer to the start of the buffer. At block master application can execute the pack function e.g. pack the data object instance for the sync node into the buffer and provide an indication to sync library whether the packing was successful. If the packing was successful sync library can transmit the buffer to slave application invoke a callback function i.e. an unpack function registered by slave application to unpack the instance in the buffer and move the sync node to an appropriate SNA window sub list block . In one set of embodiments the particular SNA window sub list that the node is moved to can correspond to the buffer that was used to transmit the instance.

At block slave application can unpack the data object instance from the buffer in response to the invocation of the unpack function at block and copy it to memory B of . Slave application can then acknowledge receipt of the entire buffer to sync library . Once the acknowledgment is received sync library can move all of the sync nodes in the SNA window sub list corresponding to the buffer to the SAA list block . In this manner the nodes in the buffer can be identified as being both synchronized and acknowledged.

In some cases sync library may not receive an acknowledgement from slave application for a long period of time or at all . To address these situations illustrates a process in which sync library can determine whether an acknowledgement has been received from slave application within a predetermined period of time block . If an acknowledgement is received within this time frame processing can proceed to block of block . If an acknowledgement is not received within this time frame sync library can automatically move the sync nodes in the SNA window sub list to the tail end of the dynamic TBS list block . Accordingly those nodes can be retransmitted. In a particular embodiment the predetermined time period can be configurable.

Returning to at block sync library can invoke a callback function i.e. an acknowledgement function registered by master application to inform the master that the data object instance s were successfully synched.

It should be appreciated that process is illustrative and that variations and modifications are possible. Steps described as sequential may be executed in parallel order of steps may be varied and steps may be modified combined added or omitted. One of ordinary skill in the art would recognize other variations modifications and alternatives.

As described above process of assumes that the standby MP e.g. MP B on which slave application is running is available. is a flow diagram of a process illustrating steps performed by master application and sync library when the standby MP is not available. Process can be implemented in software hardware or a combination thereof. As software process can be encoded as program code stored on a computer readable storage medium.

The processing performed at blocks is substantially similar to blocks of process . At block upon receiving a command to synchronize a particular sync node i.e. add the sync node to the TBS list sync library can add the node to the SAA list and immediately invoke the callback function indicating that the synchronization was acknowledged by the slave. In this embodiment master application does not receive any indication that the slave is unavailable and believes that the synchronization of the instance completed successfully. This can be repeated any number of times while the standby MP is unavailable and while master application requests additional nodes to be synched.

When the standby MP and thus slave application becomes available e.g. is restarted sync library can move all of the nodes in the SAA list to the bulk TBS list block . The nodes in the bulk TBS list can then be synchronized in parallel with new sync nodes added to the dynamic TBS list as part of the processing of . In this manner slave application can be made consistent with master application without any intervention on the part of the master. While this bulk synchronization is occurring any new nodes i.e. nodes not already in the bulk TBS list can be added to the dynamic TBS list. If an update is received for a node that is already in the bulk TBS list that node can be moved from the bulk TBS list to the dynamic TBS list.

In certain embodiments when a node in the bulk TBS list is sent to slave application and acknowledged by the slave the sync library will not invoke the callback acknowledgement function described at block since the synchronization of that node was already acknowledged at block of .

If the standby MP becomes unavailable during the execution of process all of the nodes in the TBS and SNA lists can be moved to the SAA list. These nodes can then be moved to the bulk TBS list when the standby MP becomes available again.

It should be appreciated that process is illustrative and that variations and modifications are possible. Steps described as sequential may be executed in parallel order of steps may be varied and steps may be modified combined added or omitted. One of ordinary skill in the art would recognize other variations modifications and alternatives.

Although specific embodiments of the invention have been described various modifications alterations alternative constructions and equivalents are also encompassed within the scope of the invention. For example in one set of embodiments the synchronization techniques described above can be used to synchronize application object instances between a master application running on an active management processor and a slave application running on a standby management processor where the active and standby processors reside in different network devices. In these embodiments sync library can use some form of inter machine communication such as a socket based buffer or API rather than an IPC buffer to synchronize data between the remote processors. Additionally although the present invention has been described using a particular series of transactions and steps it should be apparent to those skilled in the art that the scope of the present invention is not limited to the described series of transactions and steps.

Further while the present invention has been described using a particular combination of hardware and software it should be recognized that other combinations of hardware and software are also within the scope of the present invention. The present invention may be implemented only in hardware or only in software or using combinations thereof.

The specification and drawings are accordingly to be regarded in an illustrative rather than a restrictive sense. It will however be evident that additions subtractions deletions and other modifications and changes may be made thereunto without departing from the broader spirit and scope of the invention as set forth in the claims.

