---

title: Interactive manual, system and method for vehicles and other complex equipment
abstract: A method and system of providing an interactive manual, including a speech engine to receive and process speech from a user, convert the speech into a word sequence, and identify meaning structures from the word sequence, a structured manual including information related to an operation of a device, a visual model to relate visual representation of the information, a dialog management arrangement to interpret the meaning structures in a context and to extract pertinent information and the visual representation from the structured manual and the visual model, and an output arrangement to output the information and visual representation.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09263037&OS=09263037&RS=09263037
owner: Robert Bosch GmbH
number: 09263037
owner_city: Stuttgart
owner_country: DE
publication_date: 20100415
---
This application is a continuation of and claims priority under 35 U.S.C. 120 to U.S. patent application Ser. No. 10 871 249 filed on Jun. 17 2004 now U.S. Pat. No. 7 720 680 which relates to U.S. patent application Ser. No. 10 427 601 filed on Apr. 30 2003 now U.S. Pat. No. 7 197 457 and to U.S. patent application Ser. No. 10 613 366 filed on Jul. 3 2003 now U.S. Pat. No. 7 324 927 the disclosures of each of which are incorporated by reference herein in their entirety.

Manuals may be too generic and cumbersome for many people and it is believed that there are people who do not like to read manuals. While some devices may be usable without a manual other devices may be more complicated requiring a manual to operate them. This problem may become worse since automotive industry trends may indicate that more devices are being integrated into vehicles.

Those who use manuals for vehicles or other complex equipment or devices may complain that it is tedious to read these manuals. In particular when traveling for example they may rent a car which is unfamiliar and may have little time to peruse the manual due to for example a tight travel schedule.

The reluctance and or unwillingness to use manuals may be explained in that these manuals may only utilize written languages which may be adequate in some circumstances for example to explain abstract concepts but not in other instances for example to explain intuitive objects or process. Additionally manuals may be designed for general users and not for each individual need at each specific occasion.

U.S. Pat. No. 6 330 499 refers to a vehicle diagnosis and health monitoring system that uses a client server architecture to keep vehicle relevant data in a database and to check the status of the vehicle remotely using a voice interface. U.S. Pat. No. 6 330 499 indicates that the diagnosis and health monitoring system may be used to update user manuals but there is no direct link to any manual construction process.

U.S. Pat. No. 6 526 335 refers to an automobile personal computer system that allows a user to wirelessly interact with merchants communications facilities information providers computers at the home or office and other entities. Products may be purchased using voice commands or by interacting with displays in the automobile.

U.S. Patent Publication 20020004719 refers to a machine interface for interfacing a user to a machine that receives input words from the user. Scores for at least some of the input words are determined using scores for words appearing in each passages of the text. A passage in the text having the highest combined score for the input words is identified and a corresponding dialogue instruction set is executed to generate words for output and to respond to input words from the user. In this regard the system of U.S. Patent Publication 20020004719 merely returns a text passage with the highest score without further representation. No preprocessing or structural investigation of the document occurs.

An exemplary embodiment and or method of the present invention may be used to integrate spoken language and a visual display including for example a 2 D or 3 D visual display into an interactive manual to provide a more natural and intuitive communication tool for humans. This may significantly reduce the amount of time and may greatly facilitate the process in learning the operation of various devices. The exemplary embodiment and or method of the present invention may be applied not only to manuals for in car devices but also to manuals for other devices as well including sophisticated devices.

An exemplary embodiment and or method of the present invention may use advanced language understanding techniques and visual modeling to construct an interactive manual for complex devices including for example devices that reside in or are integrated into a vehicle.

An exemplary interactive manual according to the present invention may provide both verbal and visual information to guide the user in learning the operation of a particular device and or vehicle. For example the exemplary interactive manual may provide users with the ability to query information in natural language such as for example plain English or any other suitable spoken language. The exemplary interactive manual may also provide users with the ability the display information into one or more perspective views with zoom in and zoom out capabilities depending upon the user query.

An exemplary embodiment and or method of the present invention may be used to provide an interactive manual that may better facilitate the cognitive learning process of each individual so that people may efficiently and effectively acquire the required operating information.

An exemplary interactive manual according to the present invention may return a verbal message together with visual information such as for example a 2 D or 3 D graphic representation or a sequence of graphic representations. The verbal message and or visual information may directly target at the question from the user. The 2 D or 3 D graphic representation may be vehicle specific instead of generic so that the user may more easily locate the corresponding object and learn the operation of certain devices.

An exemplary interactive manual according to the present invention may be configured for each individual need on each individual occasion.

An exemplary interactive manual according to the present invention may be configured to understand that different phrases may have the same meaning or relate to the same concept. In this regard the exemplary interactive manual may permit users to use different phrases to denote the same object or the same process e.g. the opening of gas tank or the lid for gas tank .

An exemplary interactive manual according to the present invention may permit users to see precisely where the object is and what it looks like in one or more views and or perspectives. In this regard the exemplary interactive manual may use verbal and or visual cues to guide the user to the appropriate location.

An exemplary interactive manual according to the present invention may permit users to understand how to operate a device in an interactive way particularly if the device operation involves multiple steps. An exemplary interactive manual system may therefore show the user how to operate the device in a manner that the user feels that they are involved in the learning process.

An exemplary interactive manual according to the present invention may involve elements that may be preprocessed and structured based on human cognition for better conversation and learning.

An exemplary embodiment and or exemplary method of the present invention may demonstrate device functions by answering user questions in a visual and verbal manner. In this regard the functions of the device may be understood in a more intuitive interactive format.

An exemplary embodiment and or exemplary method of the present invention may involve the construction of one or more device models which are modular in nature to the overall system so that a common format of presentation may be provided to the user despite differing devices. In this regard the overall system may be configurable with an appropriate device model with little or no modification of the software.

An exemplary embodiment and or exemplary method may allow users to explore and navigate the model so that a simulation of the device environment may provide where users may look around and play with the controls.

An exemplary embodiment and or exemplary method may use animation clips to demonstrate device functions and respond to queries including for example how to queries.

An exemplary embodiment and or exemplary method may use a Text To Speech TSS application to provide answers to questions including for example what is questions.

According to an exemplary embodiment and or exemplary method at any given time a user may be watching an animation clip or a non animation frame. An animation may include a sequence of frames. Each frame may be narrated by a Text To Speech TTS application and each frame may be shown for only a fixed amount of time before the next frame is loaded. A non animation frame may be displayed for an unlimited amount of time until the user requests a change.

The automatic speech recognition arrangement transforms speech signals into discrete representations such as for example word sequences which may be assigned proper meaning at a later stage. In particular the automatic speech recognition arrangement receives the user s speech spectrally analyzes the speech for salient features and converts the speech into digitally encoded feature vectors which are examined to extract phone sequences e.g. simple vowel or consonant sounds using knowledge about acoustic environments gender dialect differences and phonetics. The phone sequences are then converted into corresponding word sequences using for example knowledge of what constitutes a possible word what words are likely to occur and in what sequence called language models LM . In this regard an exemplary embodiment and or method of the present invention may use techniques described for example in U.S. patent application Ser. No. 10 427 601 entitled Method for Statistical Language Modeling in Speech Recognition which was filed Apr. 30 2003 the disclosure of which is incorporated by reference herein in its entirety.

The natural language understanding arrangement receives the word sequences from the automatic speech recognition arrangement and identifies the meaning structures from sequences based on either statistical or rule based knowledge. In this regard an exemplary embodiment and or method of the present invention may use techniques described for example in U.S. application Ser. No. 10 613 366 entitled A Fast Feature Selection Method and System for Maximum Entropy Modeling which was filed Jul. 3 2003 the disclosures of which is incorporated by reference herein in its entirety.

The dialog management arrangement uses the meaning structures generated by the natural language arrangement to examine a structured manual for relevant information and an inter related visual model of the device s for graphic images thereof. The relevant information from the structured manual and graphic images from the visual model are supplied to a response generation arrangement which formats the information and graphic images in a suitable form for output via the speech synthesis arrangement and visual display arrangement . In this regard the visual display arrangement may provide for example 2 D or 3 D images which may be displayed for example in a static or dynamic manner.

The dialog management arrangement provides coordination of interpreting a user s speech and preparing a response. For example using the question what does the left object do the natural language understanding arrangement is configured to understand this as a what is question and the dialog management arrangement uses this understanding to interact with the structured manual and visual model to identify the object retrieve certain information regarding the object and the current display and then send the information to the response generation arrangement .

The structured manual and visual model together form a model package that includes for example a GSL grammar package and a grammar table. The GSL grammar package includes grammars which specify for example a set of phrases and sentences for the automatic speech recognition arrangement to recognize such as for example questions and or commands such as stop start how to go to a city etc. The grammar table may include each animation clip and non animation frame that is associated with a particular grammar defined for example using a Grammar Specific Language GSL . In this regard when a spoken utterance is given the automatic speech recognition arrangement may use the corresponding grammar to find its corresponding sentence. Based on the output from the automatic speech recognition arrangement model objects or a user s intended actions may be identified so that an appropriate response to assist the user may be provided.

The model package formed by the structure manual and visual model may also include an objects file to define the spatial and graphical information related to a device s model including for example the location and color of certain device components. In this regard the model package may include various tables such as for example a surrounding table a slide table a how to table a what is table and a select it table. In particular the surrounding table may define the adjacent spatial information of an object which together with information provided by the objects file may define the physical model and relations. The slide table may store animation sequences and associated information including for example information regarding the duration of the display and text captions. In this regard the animation sequences may be interrupted at any point in time. The how to table may link animation clips with objects so that if a how to action is prompted e.g. detecting the utterance how to do this the appropriate animation clip is displayed on the screen. The what is table may include a text describing what a particular feature does so that if a what is action is prompted e.g. detecting the utterance what does this do the text may be displayed and or output as an audio response. The select it table may facilitate the display of a frame to show the result of a control that is enabled including for example the result if a particular button were pushed.

Among these lookup tables objects may be consistently identified with unique Ids. The structured manual provides an application programming interface API for lookup in each table using this unique ID. The natural language understanding arrangement will identify each object in a speech and reference it with its object ID in the further analysis. The dialog management arrangement may use this ID in its operation.

When the display needs to be updated the current content will also be localized. First the operation locates a slide corresponding to the new view in the structured manual. Then all objects in the slide are found and a search of each table is performed using the object s ID to construct localized content. In this regard the localized content may be used for display and fact lookups.

The model package formed by the structured manual and the visual model may be vehicle specific and or device specific instead of generic so that the user may more easily locate the corresponding object or learn the operation of certain devices including vehicle related and non vehicle related devices. For example the exemplary system may involve the construction of one or more device models which are modular in nature to the overall system so that a common format of presentation may be provided to the user despite differing devices. In this regard the overall system may be configurable with an appropriate device model with little or no modification of the software.

The speech synthesis arrangement provides an audio response to the user. In particular the speech synthesis arrangement may include for example a text to speech TTS application to provide the audio response to the user. In this regard the text to speech TTS application may take English text and generate a natural and intelligible acoustic speech signal by using a set of rules or models derived from speech data.

The visual display arrangement provides visual information to the user in three dimensional perspective. In this regard the visual display arrangement may include for example a Cathode Ray Tube CRT monitor liquid crystal display or other suitable graphic display. It will be appreciated that the visual display arrangement may provide 2 D or 3 D displays. It will also be appreciated that the visual model may be implemented in 2 D or 3 D or a combination of 3 D and 2 D so that for example depending on a user s input sequence and the orientation of looking angle the view may be different. Accordingly a model may be appropriately transformed into 2D to accommodate the user s perspective.

Portions of the exemplary interactive manual system may be implemented using Microsoft Visual C 6.0 Nuance Vocalizer 3.0 and Nuance Recognizer 8.0. The exemplary interactive manual system may include for example any suitable processing arrangement such as for example a personal computer with an Intel based microprocessor. The suitable processing arrangement may be supported for example by a wide variety of operating and or application development environments including for example a Microsoft Windows version 200 environment. It will be appreciated that components to of the exemplary interactive manual system may be collocated on a common platform or alternatively some or all of these components may reside separately. For example the automatic speech recognition arrangement the natural language understanding arrangement the dialog arrangement the response generation arrangement the speech synthesis arrangement and the visual display arrangement may all reside in a vehicle whereas the structured manual and visual mode may reside outside the vehicle at a remote location to be accessible for example via a suitable wireless interface. In this regard the structured manual and visual model may be uploaded and or downloaded for example as needed so that expanded updated and or corrected versions of these components may be supported.

According to an exemplary operation of the interactive manual system when a user encounters problems and or difficulties in operating a device such as locating a certain device or understanding its operation the user may talk to the interactive manual which may respond in an audio and or visual manner. In this regard the interactive manual may conduct a simple dialog with the user and may show the user an animation about the operation of that device. The animation may include a sequence of frames containing so called animation objects associated with each graphical representation in the frame. Each frame may store the information displayed at a give time. The information may include a text description and an index table for object lookup. The description may be read using a text to speech TTS application during a demonstration. During the process of playing a frame is displayed for a pre defined period of time and its description may be prompted to read out by the text to speech TTS application. The current frame may include the information of a slide at the current time. In addition to duplicating a slide s content an animation object may be highlighted and or moved for example to the position of the selection cursor. For example the interactive manual may confirm with the user about the device the user is having trouble with and may display the device on a screen with flashing symbols and or objects.

The content in each frame may include for example a list of animation objects. In particular each graphic representation in the display may be stored as an animation object. Moreover each instance of an animation object may have a unique name layout position and color specification.

Each object may store immediate surrounding information relative to its location. For example each object may store information relative to the left right top bottom front back etc. Additionally an adjacent object may specify the most likely to be asked surrounding object. Moreover when a direction command such as north south up down left right etc. is given the system may look up the currently selected object s surrounding objects. If no object is found there is no neighbor object that is available.

The exemplary system architecture may be message based. In this regard when a component hooks onto a top and bottom layer that component may post messages upward or downward. Components in the same pier e.g. the Display Manager component and Speech Engine component may not pass messages directly. Through connectors messages are distributed to the designated components. If a message is specified with a receiving component s name the connector sends the message to that component only. Otherwise the message is broadcasted to all components.

Upon the start of the application the Speech Engine component receives a user s utterance which may be for example a command or a question. The inputted speech is interpreted and the results are sent to the Interaction Manager component .

The Interaction Manager component takes corresponding actions such as for example loading a demo animation. In this regard during the process of playing a demo the user may interrupt at any point to ask questions regarding the current frame issue commands to navigate between frames or jump to another topic.

The Display Manager component may provide a camera view for users to observe the physical models which are generated by the Model Manager component . In this regard users may verbally control the camera for zooming in or out rotation and shifting. In a 3 D environment for example the camera view may allow users to view different sides of the object.

The Display Manager component displays the current frame on the screen. It may also handle User Interface UI inputs such as for example the pressing of a key. The Display Manager component includes a model of the current display which may be updated by the Model Manager component via the Interaction Manager component . The displayer may convert the 3 D model to a 2D display in user s view when needed.

The Model Manager component contains the content of the objects frames and animations and hash tables that provide indexing for fast lookups. In this regard the Model Manager may include numerous different files which such information so that a suitable model package may be loaded the appropriate search tables may be constructed and the resulting device model may be assembled.

The Model Manager component may also provide application programming interfaces APIs for spatial and temporal manipulation of the objects and sequences such as for example playback controls or movement controls. In this regard the playback control may include for example rewind forward play reverse etc. and the movement controls may include for example up down left right in out etc.

The Speech Engine component receives voice input attempts to understand the user s meaning and passes the meaning to the Interaction Manager component . The Speech Engine component may also receive commands from the Interaction Manager component to generate speech. The Text To Speech TTS component converts text to speech. The Speech Engine component may include for example a Language Understanding Processor.

The Interaction Manager component provides overall control of the system and may be implemented via a finite state machine finite state grammar a pushdown automata context free grammar or more complicated automata in Chomsky hierarchy. In this regard the Interaction Manager component may maintain various system states. The Interaction Manager component receives meaning input from the Speech Engine component and the current system states and provides a sequence of commands to the other components and adjusts the system states accordingly. The knowledge base for the set of decisions may be described through an interaction description language IDL .

The interaction description language IDL is programmed as pairs of rules and actions. A rule is defined with one or more conditions. If all conditions in a rule are satisfied the actions associated with the rule are invoked. For example when the utterances What is the red button and What is this are compared the Speech Engine component generates different meaning structures and therefore different rules are applied. In particular the Speech Engine component labels the first utterance a What is question and a specific object. The associated action retrieves a description for the red button . For the second utterance the Speech Engine component does not provide a specific object. Instead there are two associated actions. In the first action a lookup is performed for the currently selected object in the current frame. If it exists a second lookup is performed for the description of the selection. Otherwise the input is ignored.

The rules may be classified into four categories and may be extended based on application needs. In this regard the rules may be 1 Demo Action Control rules that handle animation sequences such as for example fast forward rewind or stop 2 Cursor Movement rules that handle item marking using a cursor 3 Interface Manipulation rules that handle a user action for exploring the target device such as for example selecting a menu item or pressing a button and 4 Object Query rules that handle questions such as what is and how to .

