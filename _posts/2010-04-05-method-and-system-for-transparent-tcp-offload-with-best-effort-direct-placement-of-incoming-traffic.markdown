---

title: Method and system for transparent TCP offload with best effort direct placement of incoming traffic
abstract: Certain aspects of a method and system for transparent transmission control protocol (TCP) offload with best effort direct placement of incoming traffic are disclosed. Aspects of a method may include collecting TCP segments in a network interface card (NIC) processor without transferring state information to a host processor every time a TCP segment is received. When an event occurs that terminates the collection of TCP segments, the NIC processor may generate a new aggregated TCP segment based on the collected TCP segments. If a placement sequence number corresponding to the generated new TCP segment for the particular network flow is received before the TCP segment is received, the generated new TCP segment may be transferred directly from the memory to the user buffer instead of transferring the data to a kernel buffer, which would require further copy by the host stack from kernel buffer to user buffer.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08416768&OS=08416768&RS=08416768
owner: Broadcom Israel Research Ltd.
number: 08416768
owner_city: Ramat Gan
owner_country: IL
publication_date: 20100405
---
This patent application is a continuation of U.S. application Ser. No. 11 489 389 filed on Jul. 18 2006 now U.S. Pat. No. 7 693 138 which application makes reference to claims priority to and claims benefit from U.S. Provisional Patent Application Ser. No. 60 700 544 filed on Jul. 18 2005.

U.S. application Ser. No. 11 489 388 filed on Jul. 18 2006 issued as U.S. Pat. No. 7 684 344 on Mar. 23 2010 

U.S. application Ser. No. 11 489 300 filed on Jul. 18 2006 issued as U.S. Pat. No. 7 596 638 on Sep. 29 2009 

Each of the above referenced applications is hereby incorporated herein by reference in its entirety.

Certain embodiments of the invention relate to processing of TCP data and related TCP information. More specifically certain embodiments of the invention relate to a method and system for transparent TCP offload with best effort direct placement of incoming traffic.

There are different approaches for reducing the processing power of TCP IP stack processing. In a TCP Offload Engine TOE the offloading engine performs all or most of the TCP processing presenting to the upper layer a stream of data. There may be various disadvantages to this approach. The TTOE is tightly coupled with the operating system and therefore requires solutions that are dependent on the operating system and may require changes in the operating system to support it. The TTOE may require a side by side stack solution requiring some kind of manual configuration either by the application for example by explicitly specifying a socket address family for accelerated connections. The TTOE may also require some kind of manual configuration by an IT administrator for example by explicitly specifying an IP subnet address for accelerated connections to select which of the TCP flows will be offloaded and the offload engine is very complex as it needs to implement TCP packet processing.

Large segment offload LSO transmit segment offload TSO may be utilized to reduce the required host processing power by reducing the transmit packet processing. In this approach the host sends to the NIC bigger transmit units than the maximum transmission unit MTU and the NIC cuts them to segments according to the MTU. Since part of the host processing is linear to the number of transmitted units this reduces the required host processing power. While being efficient in reducing the transmit packet processing LSO does not help with receive packet processing. In addition for each single large transmit unit sent by the host the host would receive from the far end multiple ACKs one for each MTU sized segment. The multiple ACKs require consumption of scarce and expensive bandwidth thereby reducing throughput and efficiency.

In large receive offload LRO a stateless receive offload mechanism the TCP flows may be split to multiple hardware queues according to a hash function that guarantees that a specific TCP flow would always be directed into the same hardware queue. For each hardware queue the mechanism takes advantage of interrupt coalescing to scan the queue and aggregate subsequent packets on the queue belonging to the same TCP flow into a single large receive unit.

While this mechanism does not require any additional hardware from the NIC besides multiple hardware queues it may have various performance limitations. For example if the number of flows were larger than the number of hardware queues multiple flows would fall into the same queue resulting in no LRO aggregation for that queue. If the number of flows is larger than twice the number of hardware queues no LRO aggregation is performed on any of the flows. The aggregation may be limited to the amount of packets available to the host in one interrupt period. If the interrupt period is short and the number of flows is not small the number of packets that are available to the host CPU for aggregation on each flow may be small resulting in limited or no LRO aggregation even if the number of hardware queues is large. The LRO aggregation may be performed on the host CPU resulting in additional processing. The driver may deliver to the TCP stack a linked list of buffers comprising of a header buffer followed by a series of data buffers which may require more processing than in the case where all the data is contiguously delivered on one buffer.

Accordingly the computational power of the offload engine needs to be very high or at least the system needs a very large buffer to compensate for any additional delays due to the delayed processing of the out of order segments. When host memory is used for temporary storage of out of order segments additional system memory bandwidth may be consumed when the previously out of order segments are copied to respective buffers. The additional copying provides a challenge for present memory subsystems and as a result these memory subsystems are unable to support high rates such as 10 Gbps.

In general one challenge faced by TCP implementers wishing to design a flow through NIC is that TCP segments may arrive out of order with respect to the order placed in which they were transmitted. This may prevent or otherwise hinder the immediate processing of the TCP control data and prevent the placing of the data in a host buffer. Accordingly an implementer may be faced with the option of dropping out of order TCP segments or storing the TCP segments locally on the NIC until all the missing segments have been received. Once all the TCP segments have been received they may be reordered and processed accordingly. In instances where the TCP segments are dropped or otherwise discarded the sending side may have to re transmit all the dropped TCP segments and in some instances may result in about a fifty percent 50 decrease in throughput or bandwidth utilization.

There are different approaches for reducing the processing power of TCP IP stack processing. In a TCP Offload Engine TOE the offloading engine performs all or most of the TCP processing presenting to the upper layer a stream of data. There may be various disadvantages to this approach. The TOE is tightly coupled with the operating system and therefore requires solutions that are dependent on the operating system and may require changes in the operating system to support it. The TOE may require a side by side stack solution requiring some kind of manual configuration either by the application for example by explicitly specifying a socket address family for accelerated connections. The TOE may also require some kind of manual configuration by an IT administrator for example by explicitly specifying an IP subnet address for accelerated connections to select which of the TCP flows will be offloaded and the offload engine is very complex as it needs to implement TCP packet processing.

Large segment offload LSO transmit segment offload TSO may be utilized to reduce the required host processing power by reducing the transmit packet processing. In this approach the host sends to the NIC bigger transmit units than the maximum transmission unit MTU and the NIC cuts them to segments according to the MTU. Since part of the host processing is linear to the number of transmitted units this reduces the required host processing power. While being efficient in reducing the transmit packet processing LSO does not help with receive packet processing. In addition for each single large transmit unit sent by the host the host would receive from the far end multiple ACKs one for each MTU sized segment. The multiple ACKs require consumption of scarce and expensive bandwidth thereby reducing throughput and efficiency.

During conventional TCP processing each of the plurality of TCP segments received would have to be individually processed by a host processor in the host system. TCP processing requires extensive CPU processing power in terms of both protocol processing and data placement on the receiver side. Current processing systems and methods involve the transfer of TCP state to a dedicated hardware such as a NIC where significant changes to host TCP stack and or underlying hardware are required.

The host processing power may be consumed by the copying of data between user space and kernel space in the TCP IP stack. Some solutions have been proposed to reduce the host processing power. For example utilizing remote direct memory access RDMA avoids memory copy in both transmit and receive directions. However this requires a new application programming interface API a new wire protocol and modifications to existing applications at both sides of the wire. A local DMA engine may be utilized to offload memory copy in both transmit and receive directions. Although a local DMA engine may offload copying operations from the CPU it does not relieve the memory bandwidth required. The memory bandwidth may be a severe bottleneck in high speed networking applications as platforms are shifting to multiple CPU architectures with multiple cores in each CPU architecture all sharing the same memory.

When the host processor has to perform a read write operation a data buffer has to be allocated in the user space. A read operation may be utilized to copy data from the file into this allocated buffer. A write operation may be utilized to transmit the contents of the buffer to a network. The OS kernel has to copy all data from the user space into the kernel space. Copy operations are CPU and memory bandwidth intensive limiting system performance.

Further limitations and disadvantages of conventional and traditional approaches will become apparent to one of skill in the art through comparison of such systems with some aspects of the present invention as set forth in the remainder of the present application with reference to the drawings.

A method and or system for transparent TCP offload with best effort direct placement of incoming traffic substantially as shown in and or described in connection with at least one of the figures as set forth more completely in the claims.

These and other advantages aspects and novel features of the present invention as well as details of an illustrated embodiment thereof will be more fully understood from the following description and drawings.

Certain embodiments of the invention may be found in a method and system for transparent TCP offload with best effort direct placement of incoming traffic. Aspects of the method and system may comprise collecting TCP segments in a network interface card NIC processor without transferring state information to a host processor every time a TCP segment is received. When an event occurs that terminates the collection of TCP segments the NIC processor may generate a new aggregated TCP segment based on the collected TCP segments. If a placement sequence number corresponding to the generated new TCP segment for the particular network flow is received before the TCP segment is received the generated new TCP segment may be transferred directly from the memory to the user buffer instead of transferring the data to a kernel buffer which would require further copy by the host stack from kernel buffer to user buffer.

The network subsystem may comprise a processor . Although illustrated for example as a CPU and an Ethernet the present invention need not be so limited to such examples and may employ for example any type of processor and any type of data link layer or physical media respectively. Accordingly although illustrated as coupled to the Ethernet the TTEEC or the TTOE of may be adapted for any type of data link layer or physical media. Furthermore the present invention also contemplates different degrees of integration and separation between the components illustrated in . For example the TTEEC TTOE may be a separate integrated chip from the chip set embedded on a motherboard or may be embedded in a NIC. Similarly the processor may be a separate integrated chip from the chip set embedded on a motherboard or may be embedded in a NIC. In addition the dedicated memory may be integrated with the chip set or may be integrated with the network subsystem of .

In accordance with certain embodiments of the invention providing a single aggregated TCP segment to the host for TCP processing significantly reduces overhead processing by the host . Furthermore since there is no transfer of TCP state information dedicated hardware such as a NIC may assist with the processing of received TCP segments by coalescing or aggregating multiple received TCP segments so as to reduce per packet processing overhead.

In conventional TCP processing systems it is necessary to know certain information about a TCP connection prior to arrival of a first segment for that TCP connection. In accordance with various embodiments of the invention it is not necessary to know about the TCP connection prior to arrival of the first TCP segment since the TCP state or context information is still solely managed by the host TCP stack and there is no transfer of state information between the hardware stack and the software stack at any given time.

The frame parser may comprise suitable logic circuitry and or code that may enable L2 Ethernet processing including for example address filtering frame validity and error detection of the incoming frames . Unlike an ordinary Ethernet controller the next stage of processing may comprise for example L3 such as IP processing and L4 such as TCP processing within the frame parser . The TTEEC may reduce the host CPU utilization and memory bandwidth for example by processing traffic on coalesced TCP IP flows. The TTEEC may detect for example the protocol to which incoming packets belong based on the packet parsing information and tuple . If the protocol is TCP then the TTEEC may detect whether the packet corresponds to an offloaded TCP flow for example a flow for which at least some TCP state information may be kept by the TTEEC . If the packet corresponds to an offloaded connection then the TTEEC may direct data movement of the data payload portion of the frame. The destination of the payload data may be determined from the flow state information in combination with direction information within the frame. The destination may be a host memory for example. Finally the TTEEC may update its internal TCP and higher levels of flow state without any coordination with the state of the connection on the host TCP stack and may obtain the host buffer address and length from its internal flow state.

The receive system architecture may comprise for example a control path processing and data movement engine . The system components above the control path as illustrated in upper portion of may be designed to deal with the various processing stages used to complete for example the L3 L4 or higher processing with maximal flexibility and efficiency and targeting wire speed. The result of the stages of processing may comprise for example one or more packet identification cards that may provide a control structure that may carry information associated with the frame payload data. This may have been generated inside the TTEEC while processing the packet in the various blocks. A data path may move the payload data portions or raw packets of a frame along from for example an on chip packet frame buffer and upon control processing completion to a direct memory access DMA engine and subsequently to the host buffer via the host bus that was chosen via processing. The data path to the DMA engine may comprise packet data are optional headers .

The receiving system may perform for example one or more of the following parsing the TCP IP headers associating the frame with a TCP IP flow in the association block fetching the TCP flow context in the context fetch block processing the TCP IP headers in the RX processing block determining header data boundaries and updating state mapping the data to a host buffers and transferring the data via a DMA engine into these host buffers . The headers may be consumed on chip or transferred to the host buffers via the DMA engine .

The packet frame buffer may be an optional block in the receive system architecture. It may be utilized for the same purpose as for example a first in first out FIFO data structure is used in a conventional L2 NIC or for storing higher layer traffic for additional processing. The packet frame buffer in the receive system may not be limited to a single instance. As control path processing is performed the data path may store the data between data processing stages one or more times.

In an exemplary embodiment of the invention at least a portion of the coalescing operations described for and or for may be implemented in the RX processing block in . In this instance buffering or storage of TCP data may be performed by for example the frame buffer . Moreover the flow lookup table FLT utilized by the RX processing block may be implemented using the off chip storage and or the on chip storage for example.

In an embodiment of the invention a new flow may be detected at some point during the flow lifetime. The flow state is unknown when the new flow is detected and the first packets are utilized to update the flow state until the flow is known to be in order. A device performing TTO may also support other offload types for example TOE RDMA or iSCSI offload. In this case the FLT for TTO may be shared with the connection search for other offload types with each entry in the FLT indicating the offload type for that flow. Packets that belong to flows of other offload types may not be candidates for TTO. Upon detecting a new flow the flow may be initiated with the basic initialization context. An entry in the FLT with a flow ID may be created.

In another embodiment of the invention a plurality of segments of the same flow may be aggregated in TTO up to a receive aggregation length RAL presenting to the host a bigger segment for processing. If aggregation is allowed the received packet may be placed in the host memory but will not be delivered to the host. Instead the host processor may update the context of the flow this packet belongs to. The new incoming packet may either cause the packet to be delivered immediately alone if there were no prior aggregated packets that were not delivered or as a single packet that represents both that packet and the previously received packets. In another embodiment of the invention the packet may not be delivered but may update the flow s context.

A termination event may occur and the packet may not be aggregated if at least one of the following occurs at the TCP level 1 the data is not in order as derived from the received sequence number SN and the flow s context 2 at least one packet with TCP flags other than ACK flag for example a PUSH flag is detected 3 at least one packet with selective acknowledgement SACK information is detected or 4 if the ACK SN received is bigger than the delivered ACK SN and requires stopping the aggregation. Similarly a termination event may occur and the packet may not be aggregated if at least one of the following occurs at the IP level 1 the type of service TOS field in the IP header is different than the TOS field of the previous packets that were aggregated or 2 the received packet is an IP fragment.

When aggregating a plurality of packets to a single packet the aggregated packet s header may contain the aggregated header of all the individual packets it contains. In an exemplary embodiment of the invention a plurality of TCP rules for the aggregation may be as follows. For example 1 the SN in the aggregated header is the SN of the first or oldest packet 2 the ACK SN is the SN of the last or youngest segment 3 the length of the aggregated header is the sum of the lengths of all the aggregated packets 4 the window in the aggregated header is the window received in the last or youngest aggregated packet 5 the time stamp TS in the aggregated header is the TS received in the first or oldest aggregated packet 6 the TS echo in the aggregated header is the TS echo received in the first or oldest aggregated packet and 7 the checksum in the aggregated header is the accumulated checksum of all aggregated packets.

In an exemplary embodiment of the invention a plurality of IP field aggregation rules may be provided. For example 1 the TOS of the aggregated header may be that of all the aggregated packets 2 the time to live TTL of the aggregated header is the minimum of all incoming TTLs 3 the length of the aggregated header is the sum of the lengths in the aggregated packets 4 the fragment offset of the aggregated header may be zero for aggregated packets and 5 the packet ID of the aggregated header is the last ID received.

The received packets may be aggregated until the received packet cannot be aggregated due to the occurrence of a termination event or if a timeout has expired on that flow or if the aggregated packet exceeds RAL. The timeout may be implemented by setting a timeout to a value timeout aggregation value when the first packet on a flow is placed without delivery. The following packets that are aggregated may not change the timeout. When the packets are delivered due to timeout expiration the timeout may be canceled and may be set again in the next first packet that is not delivered. Notwithstanding other embodiments of the invention may provide timeout implementation by periodically scanning all the flows.

In an exemplary embodiment of the invention the received ACK SN may be relevant to determine the rules to aggregate pure ACKs and to determine the rules to stop aggregation of packets with data due to the received ACK SN. The duplicated pure ACKs may never be aggregated. When duplicated pure ACKs are received they may cause prior aggregated packets to be delivered and the pure ACK may be delivered immediately separately. The received ACK SN may also be utilized to stop the aggregation and deliver the pending aggregated packet to the host TCP IP stack.

In an exemplary embodiment of the invention a plurality of rules may be provided for stopping the aggregation according to the ACK SN. For example 1 if the number of acknowledged ACKed bytes that are not yet delivered taking into account the received segments and the prior segments that were not delivered exceeds a threshold ReceiveAckedBytesAggretation for example in bytes or 2 the time from the arrival of the first packet that advanced the received ACK SN exceeds a threshold TimeoutAckAggregation for example. For this purpose a second timer per flow may be required or other mechanisms such as periodically scanning the flows may be implemented.

In another exemplary embodiment of the invention the flows may be removed from the host memory if one of the following occurs 1 a reset RST flag was detected in the receive side 2 a finish FIN flag was detected in the receive side 3 there was no receive activity on the flow for a predefined time TerminateNoActivityTime for example 4 a KeepAlive packet in the receive direction was not acknowledged. A least recently used LRU cache may be used instead of a timeout rule to remove the flows from the host memory.

In another exemplary embodiment of the invention the flows may be removed from the host memory if the flow was closed due to a retransmission timeout that requires information from the transmitter. In one exemplary embodiment of the invention retransmission timeout may comprise periodically scanning all the flows to determine if any flow is closed. The period for scanning may be low for example 5 seconds. In each scan if there is unacknowledged data that was transmitted by the NIC the maximum transmitted sequence number SN may be recorded. Additionally if there is unacknowledged data that was transmitted by the peer side the maximum received SN may be recorded. If in two consequent scans there is pending data on same flow of the same type with the recorded number unchanged pending data that was not acknowledged for the entire scan period may be indicated. In this case the flow may be removed.

With the send zero copy approach the data may be read immediately from the disk into the OS cache memory using direct memory access DMA hardware. Data to be transferred may be taken directly from system buffers without context switching and without trashing the cache. Thus the usage of send zero copy in server applications may significantly reduce CPU load.

The user buffer may be posted to the NIC for DMA transfer and the user pages may be marked with a copy on write COW attribute so that if the user attempts writing to them the kernel may trap these writes. The page may be copied to a different location and the new page may be remapped to the user application. Zero copy may be implemented not only on SendFile operations but rather on send operations by detecting the application usage pattern and selecting zero copy for applications that do not attempt writing to send buffers immediately.

The system call may post the I O user space corresponding to the user buffer and aggregation rules with placement sequence number SN to the NIC driver . The NIC driver may further post the I O user space corresponding to the user buffer and placement sequence number SN to the NIC which may or may not trigger the NIC . The NIC may receive an aggregated TCP segment from the remote peer and write the received data to a pre posted buffer according to the TCP SN and the placement SN. When the NIC receives subsequent incoming TCP segments from the remote peer it may determine whether they are in order. If subsequent incoming TCP segments are in order and the NIC has received the placement information for the specific flow and the specific sequence numbers the NIC may place the received data in the user buffer and the headers in a kernel buffer. The NIC driver may mark the user buffer by setting a flag for example a copy already done flag and may pass it to the stack for further processing. The stack may perform TCP processing but may skip the copying of data to the kernel buffer. The user application may be accessed by the system call and the user buffer may be unpinned before control is returned to the user.

A primary difference between a synchronous API mode and an asynchronous mode in receive operation is that in asynchronous API the application may post multiple receive buffers in advance while for the synchronous API the application may only post one buffer at a time.

In accordance with an embodiment of the invention a method and system for transparent TCP offload with best effort direct placement of incoming traffic may include determining whether to perform a zero copying operation based on receiving a particular TCP segment from a remote peer after receiving the placement information for the specific flow and the specific sequence numbers from the host processor or system call . The NIC may place the received data in the user buffer and the headers in a kernel buffer. The dynamic zero copy operation may avoid the intermediate placement of data in kernel buffers which requires further copy of the kernel buffers to user buffers.

In accordance with an embodiment of the invention a NIC processor may enable collection of at least one transmission control protocol TCP segment received for a particular network flow without transferring state information regarding the particular network flow to a host processor every time the TCP segment is received. The NIC processor may enable generation of a new TCP segment comprising the collected TCP segments after a termination event occurs. The NIC processor may enable receiving of at least one placement sequence number corresponding to the generated new TCP segment for the particular network flow. The NIC processor may enable transferring the generated new TCP segment directly to the user buffer instead of transferring the data to a kernel buffer based on the received at least one placement sequence number which would require further copy by the host stack from kernel buffer to user buffer .

The NIC processor may enable placement of payload data corresponding to the generated new TCP segment in a user buffer if at least one placement sequence number corresponding to the generated new TCP segment is received before the new TCP segment is received. The host stack may enable pinning at least one page in the user buffer to skip copying of the payload data corresponding to the generated new TCP segment to a kernel buffer. The NIC processor may enable placement of header information corresponding to the generated new TCP segment in the kernel buffer if at least one placement sequence number corresponding to the generated new TCP segment is received before the generated new TCP segment. The host stack may enable unpinning of at least one page in the user buffer after the host processor receives the generated new TCP segment. The NIC processor may enable marking of the generated new TCP segment as copy already done after zero copying of the generated new TCP segment. The NIC processor may enable generation of the new TCP segment by aggregating at least a portion of a plurality of the collected TCP segments for the particular network flow. The NIC processor may enable detection of a state of the particular network flow to be at least one of an in order state an out of order state and an unknown state. The NIC processor may enable updating of the detected state based on the collected TCP segments.

Another embodiment of the invention may provide a machine readable storage having stored thereon a computer program having at least one code section executable by a machine thereby causing the machine to perform the steps as described above for performing transparent TCP offload with best effort direct placement of incoming traffic.

Accordingly the present invention may be realized in hardware software or a combination of hardware and software. The present invention may be realized in a centralized fashion in at least one computer system or in a distributed fashion where different elements are spread across several interconnected computer systems. Any kind of computer system or other apparatus adapted for carrying out the methods described herein is suited. A typical combination of hardware and software may be a general purpose computer system with a computer program that when being loaded and executed controls the computer system such that it carries out the methods described herein.

The present invention may also be embedded in a computer program product which comprises all the features enabling the implementation of the methods described herein and which when loaded in a computer system is able to carry out these methods. Computer program in the present context means any expression in any language code or notation of a set of instructions intended to cause a system having an information processing capability to perform a particular function either directly or after either or both of the following a conversion to another language code or notation b reproduction in a different material form.

While the present invention has been described with reference to certain embodiments it will be understood by those skilled in the art that various changes may be made and equivalents may be substituted without departing from the scope of the present invention. In addition many modifications may be made to adapt a particular situation or material to the teachings of the present invention without departing from its scope. Therefore it is intended that the present invention not be limited to the particular embodiment disclosed but that the present invention will include all embodiments falling within the scope of the appended claims.

