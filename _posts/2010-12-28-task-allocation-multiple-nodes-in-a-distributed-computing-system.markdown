---

title: Task allocation multiple nodes in a distributed computing system
abstract: Work is distributed amongst a plurality of nodes. A first plurality of tasks is extracted, where the number of tasks is selected in correspondence to the number of nodes, and where sizes of the tasks are sized based on a job load metric. The first plurality of tasks is distributed. A determination is made whether a time difference between a response from a node that is first to complete its task and a response from a node that is last to complete its task exceeds a predefined threshold. In response to a determination that the time difference exceeds the predefined threshold, the job load metric is adjusted. A second plurality of tasks is extracted, where the number of tasks is selected in correspondence to the number of nodes, and wherein sizes of the tasks are sized based on the adjusted job load metric. The second plurality of tasks is distributed.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08645454&OS=08645454&RS=08645454
owner: Canon Kabushiki Kaisha
number: 08645454
owner_city: Tokyo
owner_country: JP
publication_date: 20101228
---
The present disclosure relates to distributed computing and more specifically relates to allocation of tasks amongst nodes in a distributed computing environment.

In the field of distributed computing individual computers are typically linked together to form a computing cluster for the purpose of solving computationally extensive problems. A computing cluster typically consists of a front end node and multiple compute nodes. Under this arrangement the front end node typically allocates a problem to the compute nodes by dividing the problem into multiple tasks and distributing those tasks amongst the compute nodes for processing.

According to one technique for task allocation the front end node divides a problem into equally sized tasks and distributes the tasks to the compute nodes for processing. Upon distribution of the tasks the front end node waits for the compute nodes to complete their assigned tasks and to transmit the results back to the front end node for further processing.

One difficulty with such implementations typically arises when a computing cluster consists of compute nodes that vary in terms of hardware and software resources. In such clusters all compute nodes are typically assigned a task of the same size irrespective of the hardware and software capabilities of the individual compute nodes. Naturally resource rich compute nodes complete their tasks faster than other compute nodes. As a consequence the front end node must wait for the slower compute nodes to finish their tasks before the next set of tasks can be assigned to the compute nodes. Meanwhile the faster compute nodes which have already completed their tasks remain idle thus reducing the overall utilization of the computing cluster.

One technique for overcoming this issue is to assign new tasks to the faster compute nodes immediately upon completion of their previously assigned tasks thereby preventing the faster nodes from remaining idle. However this solution is often inadequate because completion of the overall problem is still constrained by the slowest node which might have been assigned a task that is too large for its capabilities.

The foregoing situation is addressed herein by sizing tasks based on a dynamically adjusted job load metric where the job load metric is dynamically adjusted responsive to a time difference between a response from a node that is first to complete its task and a response from a node that is last to complete its task.

Thus in an example embodiment described herein work is distributed amongst a plurality of nodes in a distributed computing environment. A first plurality of tasks is extracted from the work where the number of tasks in the first plurality of tasks is selected in correspondence to the number of nodes and where sizes of the tasks in the first plurality of tasks are sized based on a job load metric of each of the nodes. The first plurality of tasks is distributed to at least some of the nodes for processing. A determination is made whether a time difference between a response from a node that is first to complete its task and a response from a node that is last to complete its task exceeds a predefined threshold. In response to a determination that the time difference exceeds the predefined threshold the job load metric of at least some of the nodes is adjusted.

Thereafter a second plurality of tasks is extracted from the work where the number of tasks in the second plurality of tasks is selected in correspondence to the number of nodes and where sizes of the tasks in the second plurality of tasks are sized based on the adjusted job load metric of each of the nodes. The second plurality of tasks is distributed to at least some of the nodes for processing.

By sizing tasks based on a dynamically adjusted job load metric it is ordinarily possible to size the tasks such that after several iterations all tasks tend to complete at around the same time within the predefined threshold. As a consequence there is a reduction in the amount of time a front end node must wait between responses from the compute nodes thereby resulting in a decrease in compute node idle times and an increase in computing cluster utilization.

In another example embodiment a qualification task is transmitted for execution by each node where the qualification task measures capabilities of core elements in a node where an influence factor is assigned to each core element and where the influence factor is used to adjust the weight of the core element on a capability measurement of the node. In particular the results from the executed qualification task from each node are received and the job load metric for each node is calculated by using the results from the executed qualification task.

In another example embodiment the distribution of work amongst a plurality of nodes in a distributed computing environment comprises a repetition of the steps of determining whether a time difference between a response from a node that is first to complete its task and a response from a node that is last to complete its task exceeds a predefined threshold adjusting the job load metric of at least some of the nodes responsive to a determination that the time difference exceeds the predefined threshold extracting a second plurality of tasks from the work where the number of tasks in the second plurality of tasks is selected in correspondence to the number of nodes and where sizes of the tasks in the second plurality of tasks are sized based on the adjusted job load metric of each of the nodes and distributing the second plurality of tasks to at least some of the nodes for processing.

In still another example embodiment the work available for distribution is dynamic in nature such that the amount of work available for distribution is received on an ongoing basis and where the work available for distribution is distributed and processed in the order received.

In yet another example embodiment responsive to a determination that the time difference does not exceed the predefined threshold a second plurality of tasks is extracted from the work where the number of tasks in the second plurality of tasks is selected in correspondence to the number of nodes and where sizes of the tasks in the second plurality of tasks are based on the unadjusted job load metric of each of the nodes and the second plurality of tasks is distributed to some of the nodes for processing.

In a further example embodiment adjusting the job load metric comprises dynamically adjusting capacity utilizations for at least some of the core elements in a node during runtime and adjusting the job load metric using the dynamically adjusted capacity utilizations of the core elements in the node.

In another example embodiment dynamically adjusting capacity utilizations for at least some of the core elements in a node during runtime comprises reducing the capacity utilizations for a plurality of core elements in the worst performing nodes and increasing the capacity utilizations for a plurality of core elements in the remaining nodes wherein adjustments to the capacity utilizations for the plurality of core elements are made in descending order of the influence factor assigned to the core elements.

In yet another example embodiment dynamically adjusting capacity utilizations for at least some of the core elements in a node during runtime comprises reducing the capacity utilization for a core element assigned the highest influence factor in the node that is last to complete its task by a predetermined amount and increasing the capacity utilization for the core element assigned the highest influence factor in the remaining nodes wherein the amount increased in each node is proportionate to the existing capacity utilizations for the core element assigned the highest influence factor in the remaining nodes and wherein the cumulative increase amongst the remaining nodes is equal to the amount that was reduced in the node that is last to complete its task.

This brief summary has been provided so that the nature of this disclosure may be understood quickly. A more complete understanding can be obtained by reference to the following detailed description and to the attached drawings.

As shown in front end node is connected to compute nodes to via network . The front end node extracts and distributes tasks to the compute nodes to . As depicted diagrammatically in front end node is provided with multiple time sequential frames of image data in a moving image such as frames to for which extraction and distribution to compute nodes to is effected. The frames to are received in a work queue where they are distributed and processed in the order received.

The present disclosure contemplates various numbers of compute nodes. Accordingly more or fewer compute nodes may be implemented in the computing cluster and the number of participating compute nodes might change during work.

In front node is depicted as receiving work such as a moving image via the Internet or intranet for processing. The front end node divides each frame to into multiple tasks for distribution and processing by multiple compute nodes to such that each frame to is processed by more than one compute node. Thus as depicted in with respect to frame the front end node divides a plurality of tasks to and distributes the divided tasks to to the compute nodes to for processing. As also depicted in compute node is assigned task compute node is assigned task compute node is assigned task and compute node is assigned task . The size of each divided task corresponds to a job load metric of the compute nodes that is assigned to execute that task as discussed more fully below in connection with .

The compute nodes to transmit the results of processing to the front end node upon completion of their assigned tasks. The front end node receives responses from the compute nodes to including results of processing where the results may be transmitted to a server for further processing. According to the embodiment shown in the front end node transmits the results of processing to server for display by the high resolution output device .

The particular implementation scale and hardware of network may vary according to different embodiments. Thus for example network could be the Internet a Local Area Network LAN Wide Area Network WAN Metropolitan Area Network MAN Personal Area Network PAN or combinations thereof. Network can be wired or wireless and can be implemented for example as an Ethernet Infiniband Myrinet Optical fiber or Wireless LAN network. In addition the network topology of network may also vary. Communication between the front end node and the compute nodes to may be accomplished using an application programming interface API such as the Message Passing Interface MPI that allows computers to communicate with one another.

As shown in front end node includes central processing unit CPU which interfaces with computer bus . Also interfacing with computer bus are fixed disk e.g. a hard disk or other nonvolatile storage medium network interface keyboard interface mouse interface random access memory RAM for use as a main run time transient memory read only memory ROM and work distribution module . Front end node uses network interface to interface to network .

RAM interfaces with computer bus so as to provide information stored in RAM to CPU during execution of the instructions in software programs such as an operating system application programs the work distribution module and device drivers. More specifically CPU first loads computer executable process steps from fixed disk or another storage device into a region of RAM . CPU can then execute the stored process steps from RAM in order to execute the loaded computer executable process steps. Data or other information is stored in RAM and the data is accessed by CPU during the execution of the computer executable software programs to the extent that such software programs have a need to access and or modify the data.

As also shown in fixed disk is an example of a computer readable storage medium that stores computer executable process steps for operating system and application programs such as moving image manipulation programs. Fixed disk also stores computer executable process steps for device drivers for software interface to devices such as input device drivers output device drivers and other device drivers . Image files and other files are stored for distribution to compute nodes for processing.

Fixed disk also includes a work queue that receives and stores work to be processed. The work stored in work queue is distributed and processed in the order it is received.

Work distribution module comprises computer executable process steps stored on a computer readable storage medium for execution by a processor such as CPU to extract tasks from work that is received by front end node and to distribute those tasks to compute nodes for processing. Work distribution module generally comprises an extraction module a distribution module a receiving module a determination module and a job load adjuster module . These modules are described in greater detail below in connection with .

The computer executable process steps for work distribution module may be configured as part of operating system as part of an output device driver such as a work distribution driver or as a stand alone application program. Work distribution module may also be configured as a plug in or dynamic link library DLL to the operating system a device driver or an application program.

As shown in a compute node includes central processing unit CPU which interfaces with computer bus . Also interfacing with computer bus are fixed disk e.g. a hard disk or other nonvolatile storage medium network interface keyboard interface mouse interface random access memory RAM for use as a main run time transient memory read only memory ROM and a work processing module . Network interface is provided to interface to network .

RAM interfaces with computer bus so as to provide information stored in RAM to CPU during execution of the instructions in software programs such as an operating system application programs a work processing module and device drivers. More specifically CPU first loads computer executable process steps from fixed disk or another storage device into a region of RAM . CPU can then execute the stored process steps from RAM in order to execute the loaded computer executable process steps. Data or other information is stored in RAM and the data is accessed by CPU during the execution of the computer executable software programs to the extent that such software programs have a need to access and or modify the data.

As also shown in fixed disk is an example of a computer readable storage medium that stores computer executable process steps for operating system and application programs such as moving image manipulation programs. Fixed disk also stores computer executable process steps for device drivers for software interface to devices such as input device drivers output device drivers and other device drivers .

Work processing module comprises computer executable process steps stored on a computer readable storage medium for execution by a processor such as CPU to receive and execute tasks. Work processing module generally comprises a receiving module an execution module and an output module . These modules are described in greater detail below in connection with .

The computer executable process steps for work processing module may be configured as part of operating system as part of an output device driver such as a work processing driver or as a stand alone application program. Work processing module may also be configured as a plug in or dynamic link library DLL to the operating system a device driver or an application program.

In particular illustrates one example of the work distribution module in which the sub modules of work distribution module are stored in fixed disk . The example work distribution module is constructed to distribute work from the front end node to the compute nodes to over a network for processing. As shown in the work distribution module comprises an extraction module a distribution module a receiving module a determination module and a job load metric adjuster module . Each of these modules are computer executable software code or process steps executable by a processor such as CPU and are stored on a computer readable storage medium such as fixed disk or RAM . More or fewer modules may be used than shown in and other architectures are possible.

As shown in the work distribution module includes an extraction module constructed to extract tasks from the work received by front node where the number of tasks corresponds to the number of nodes and where the tasks are sized based on a job load metric of each of the nodes. The extraction module is further constructed to extract tasks from the work received by front node where the tasks are sized based on an adjusted job load metric of each of the compute nodes.

Work distribution module also includes a distribution module constructed to distribute the tasks to at least some of the nodes for processing.

In one example embodiment the work available for distribution is dynamic in nature such that the amount of work available for distribution is received in work queue on an ongoing basis and where the distribution module is further constructed to distribute the work available for distribution in the order received. For example as shown in the front end node receives a moving image via the Internet or intranet on an ongoing basis and stores the received frames to in work queue . The front end node distributes the frames to from the work queue for processing by the compute nodes to in the order received.

In another example embodiment the distribution module of front end node is further constructed to transmit a qualification task for execution by each node where the qualification task measures capabilities of core elements in a node where an influence factor is assigned to each core element and where the influence factor is used to adjust the weight of the core element on a capability measurement of the node. Qualification tasks are described in more detail below in connection with .

The work distribution module includes a receiving module constructed to receive results outputted by the compute nodes upon execution of their assigned tasks.

Work distribution module further includes a determination module constructed to determine a time difference between a response from a node that is first to complete its task and a response from a node that is last to complete its task and to determine whether such time difference exceeds a predefined threshold.

In addition work distribution module includes a job load metric adjuster module constructed to adjust the job load metric of at least some of the nodes in response to a determination that the time difference between a response from a node that is first to complete its task and a response from a node that is last to complete its task exceeds a predefined threshold.

In one example embodiment the job load metric adjuster module is further constructed to calculate an initial job load metric for each compute node using the results from a qualification task.

In another example embodiment the job load metric adjuster module is further constructed to leave the job load metric unadjusted in response to a determination that the time difference between a response from a node that is first to complete its task and a response from a node that is last to complete its task does not exceed a predefined threshold.

As shown in the work processing module includes a receiving module constructed to receive tasks from front end node for processing.

The work processing module also includes an execution module constructed to execute tasks received from the front end node .

The work processing module also includes an output module constructed to output the results of the executed tasks to the front end node .

Briefly in work is distributed amongst a plurality of nodes in a distributed computing environment. A first plurality of tasks is extracted from the work where the number of tasks in the first plurality of tasks is selected in correspondence to the number of nodes and where sizes of the tasks in the first plurality of tasks are sized based on a job load metric of each of the nodes. The first plurality of tasks is distributed to at least some of the nodes for processing. A determination is made whether a time difference between a response from a node that is first to complete its task and a response from a node that is last to complete its task exceeds a predefined threshold. The job load metric of at least some of the nodes is adjusted in response to a determination that the time difference exceeds the predefined threshold. A second plurality of tasks is extracted from the work where the number of tasks in the second plurality of tasks is selected in correspondence to the number of nodes and where sizes of the tasks in the second plurality of tasks are sized based on the adjusted job load metric of each of the nodes. The second plurality of tasks is distributed to at least some of the nodes for processing.

In more detail the process in the front end node starts with step where the front end node receives work to be processed by the compute nodes. As shown in the front end node receives work via a network such as the Internet or intranet . The front end node may receive work from a single source or from multiple sources and each source may transmit different types of work to be processed. For example one source may transmit multiple time sequential 4K sized frames of a video stream for post processing another source may transmit a stream of images from security cameras for face detection purposes and third source may transmit a high definition video stream for special effects processing. Each stream of work is collected in a work queue and is distributed by the front end node to the compute nodes to on a first in first out FIFO basis.

In step the distribution module of front end node transmits a qualification task for execution by each compute node to where the qualification task measures capabilities of core elements in a compute node. In this example six different capabilities are measured the type of network interconnect I the nature and power of the CPU C the nature and power of the GPU G the capabilities of storage devices S the capabilities of the system memory M and system bus performance B . More precisely the type of network interconnect Imay distinguish between Ethernet Infiniband Myrinet Optical fiber or copper wire. The nature and power of the CPU Cmay be determined using the clock frequency cache size and processing cores. Similarly the nature and power of the GPU Gmay be determined using the clock frequency device memory size and processing cores. The capabilities of the storage devices Smay be established based on storage device capacity buffer size and data throughput. System memory Mcapabilities may be based on the clock frequency and storage capacity of the system memory. Finally the performance of the system bus Bmay be based on the maximum throughput of the bus.

The present disclosure contemplates modifying the qualification task to measure capabilities of other core elements in addition to the core elements described above thereby allowing measurement of core elements that are relevant to a particular type of work.

In one example embodiment the distribution module of front end node tailors the influence factors in correspondence to the nature of the work. For example a compute environment requiring a large number of CPU intensive transactions would typically place most of the demand on the CPU and the system memory followed by network bandwidth and bus speed with the storage device and the GPU having the least amount of influence. As discussed in more detail below in connection with step in this example Cand Mwould be assigned the highest influence factors with Iand Bhaving smaller influence factors and with Sand Ghaving the lowest influence factors. According to the embodiment the value of each influence factor is set manually. However the present disclosure also contemplates determining and setting each influence factor automatically using heuristic methods such as machine learning and data mining.

In step the receiving module of front end node receives the results from the executed qualification task from each compute node to . The qualification task results are stored for use in calculating a job load metric for the compute nodes as discussed more fully in step . The following table illustrates example core element capability measurements that were generated upon execution of a qualification task by compute nodes to 

In step the job load metric adjuster module of front end node ranks the core elements in proportion to one another as shown in Table 2.

The following table illustrates an example proportionate ranking of core elements C M I and Busing the core element capability measurements determined in steps and as shown in Table 1.

In step the job load adjuster module of front end node computes a capability metric CM for each core element using the proportionate ranking data for the core elements shown in Table 2. The capability metric CM is determined as follows CM Equation 1

where CM is capability metric Xis the core element proportionate ranking and Iis the influence factor assigned to the core element.

Using Equation 1 the front end node determines a capability metric for core elements C M I and Busing the core element data from Table 2 and an influence factor I . For simplicity purposes the present example omits core element data for Sand Gfrom Table 2 in order to deemphasize their role in the allocation of tasks in a CPU intensive environment where the capabilities of Sand Gwould typically be less significant.

In one example embodiment an influence factor is assigned to each core element where the influence factor is used to adjust the weight of the core element on a capability measurement of the compute node.

The influence factor is tailored to allow the front end node to assign appropriate weights to the core elements thereby allowing the front end node to manipulate the amount of influence a particular core element has on the capability metric and ultimately the job load metric in correspondence to the nature of the work. For example a compute environment requiring a large number of CPU intensive transactions would typically place most of the demand on the CPU and the system memory. Additional factors such as network bandwidth and bus speed would also play a role in such a scenario since the ability to transfer data quickly between processors and compute nodes is needed. In such an environment the highest influence factor would be assigned to the CPU C and system memory M core elements followed by the network interconnects I core element and bus speed B . The following table shows example capability metric values for core elements C M I and B that were calculated using Equation 1 and the core element data from Table 2 where core elements Cand Mwere assigned an influence factor of 0.3 core elements Iand Bwere assigned influence factors 0.2 and 0.15 respectively.

Once the capability metrics have been determined in step the front end node determines the capacity utilization for core elements C M I and Busing the following equation 

where CU is the capacity utilization CM is the capability metric and Xis the core element proportionate ranking.

The front end node uses the capacity utilization metrics to determine a job load metric for each compute node. The job load metric of each compute node is used to allocate the tasks to the compute nodes where the allocation is performed so as to achieve near full utilization of the core elements. As discussed above an environment having a large number of CPU intensive transactions would typically be heavily dependent on the CPU C system memory M network interconnects I and bus speed B . Thus the capacity utilization metric would aid in allocating the tasks so that the C M I and Bcore elements are utilized at or near 100 percent. The following table shows example capacity utilization metrics for core elements C M I and Bthat were calculated using Equation 2 in combination with capability metric data from Table 3.

In step the front end node calculates a job load metric for each compute node to by using the results from the executed qualification task. Specifically a job load metric is approximated using the following equation 

where JL is the job load metric CU is the capacity utilization Xis the core element proportionate ranking Iis the influence factor assigned to the core element and where the summation is taken over 6 since in this example there are six core elements.

A job load metric is calculated for each compute node to using Equation 3 and the capacity utilization values that were determined in step . In particular the job load metrics for the compute nodes to are used by the front end node to size the tasks extracted from the work.

In step the extraction module of front end node extracts a first plurality of tasks where the number of tasks in the first plurality of tasks is selected in correspondence to the number of nodes and where sizes of the tasks in the first plurality of tasks are sized based on a job load metric of each of the compute nodes. In one example the extraction module of front end node divides the work into four tasks one for each compute node to . The sizes of the divided tasks correspond to the job load metric of the compute node assigned to execute that task. In other words the size of a task assigned to compute node would be based on a job load metric that was determined for compute node . Similarly the sizes of tasks assigned to compute nodes and is based on a job load metric that was determined for compute nodes and respectively.

In step the distribution module of front end node distributes the first plurality of tasks to at least some of the nodes for processing.

In step the receiving module of front end node receives the executed task results that are outputted by output module of compute nodes to . The front end node may then store the task results in a database or may send the results to another server for further processing and display as depicted in .

In step the determination module of front end node determines whether a time difference between a response from a node that is first to complete its task and a response from a node that is last to complete its task exceeds a predefined threshold. If the time difference exceeds the predefined threshold the front end node proceeds to step . Otherwise if the time difference does not exceed the predefined threshold the front end node proceeds to step where the extraction module of front end computer continues processing work from work queue .

As suggested above the time difference between a response from a node that is first to complete its task and a response from a node that is last to complete its task is a measurement that is used to gauge how far apart in time the responses are for the best performing compute node and for the worst performing compute node. Ideally the time difference between responses from the best performing compute node and the worst performing compute node should be minimal which would indicate that all of the compute nodes are being utilized at near full potential. However a significant time difference between responses from the best performing node and the worst performing node would suggest that the best performing compute node may be under worked as indicated by its fast response or that the worst performing compute node may be over worked as indicated by its slow response.

The predefined threshold is used to determine whether the time difference between responses from the best performing compute node and the worst performing compute node falls within an acceptable duration. A time difference that exceeds the predefined threshold will trigger an adjustment of the job load metric for the compute nodes. Therefore setting the predefined threshold too high may result in the job load metric not being adjusted and may therefore result in a sub optimal utilization of the compute nodes. For example if the predefined threshold is defined as 2.0 seconds and the time difference between the first node to respond and the last node to respond never exceeds 2.0 seconds then adjustment of the job load metric will not be triggered. In this example the front end node will size the tasks using the unadjusted job load metric each time tasks are extracted from the work which may result in a sub optimal sizing of the tasks. Alternatively setting the predefined threshold too low may result in the job load metric being adjusted too often. Since adjusting the job load metric requires the front end node to perform additional operations frequent adjustment of the job load metric may disrupt efficient operation of the computing cluster. For example if the predefined threshold is defined as 0.1 seconds and the time difference between the first node to respond and the last node to respond frequently exceeds 0.1 seconds then adjustment of the job load metric would be triggered each time tasks are extracted from the work. Such a situation is ordinarily not desirable because it affects the efficient operation of the computing cluster. Thus setting the predefined threshold value to an acceptable value is helpful in maintaining efficiency. According to the embodiment the threshold is set manually. However the present disclosure also contemplates determining and setting the predefined threshold automatically using heuristic methods such as machine learning and data mining.

In steps and the job load adjuster module of front end node adjusts the job load metric of at least some of the nodes in response to a determination that the time difference between a response from a node that is first to complete its task and a response from a node that is last to complete its task exceeds the predefined threshold.

In the example embodiment described herein adjusting the job load metric comprises dynamically adjusting capacity utilizations for at least some of the core elements in a node during runtime and adjusting the job load metric using the dynamically adjusted capacity utilizations of the core elements in the node.

Accordingly in step the job load adjuster module of front end node dynamically adjusts capacity utilizations for at least some of the core elements in each compute node. The adjusted capacity utilizations are used to adjust the job load metric for at least some of the nodes as discussed more fully in step .

According to the example embodiment described herein dynamically adjusting capacity utilizations for at least some of the core elements in a node during runtime comprises reducing the capacity utilization for a core element assigned the highest influence factor in the node that is last to complete its task by a predetermined amount and increasing the capacity utilization for the core element assigned the highest influence factor in the remaining nodes where the amount increased for each node is proportionate to the existing capacity utilizations for the core element assigned the highest influence factor in the remaining nodes and where the cumulative increase amongst the remaining nodes is equal to the amount that was reduced in the node that is last to complete its task.

The capacity utilization adjustment is initially made in the node that is last to complete its task i.e. the last responding node as determined by the timing of the responses that were received in step . In particular the capacity utilization for the core element assigned the highest influence factor is reduced by a predetermined amount in the last responding node. According to this example embodiment in situations where the highest influence factor is shared amongst multiple core elements the capacity utilization adjustment is performed for only one of those core elements. In addition the predetermined amount which can be any desired value is typically set to a low number in order to prevent excessive fluctuations in the job load metrics of the compute nodes.

The amount reduced from the capacity utilization for the core element assigned the highest influence factor is redistributed to the remaining compute nodes by increasing the capacity utilization for the same core element in the remaining compute nodes. In particular the amount increased in each compute node is proportionate to the existing capacity utilization for the core element assigned the highest influence factor in each compute node as explained further in the example below. In addition the cumulative increase amongst the remaining compute nodes is equal to the amount that was reduced in the last responding node.

For demonstrative purposes the following example table shows a list of compute nodes along with their task completion times and the capacity utilization for the core element assigned the highest influence factor the CPU C 

As shown in Table 5 the task completion times for Nodes 1 to 4 are 0 seconds 2 seconds 3 seconds and 10 seconds respectively. According to Table 5 Node 4 which took 10 seconds to complete its assigned task is the last responding node. Therefore the capacity utilization for the core element assigned the highest influence factor in Node 4 i.e. C is reduced by a predetermined amount which in this example is 5 percent. Consequently the capacity utilization for Cin Node 4 is reduced to 45 percent and the amount that was reduced from Node 4 i.e. 5 percent is redistributed to the remaining compute nodes. The redistribution is accomplished by adjusting the capacity utilization for the core element Cin the remaining nodes in proportion to the existing capacity utilization for core element C. Since Nodes 1 to 3 have an existing Ccapacity utilization of 30 percent 10 percent and 10 percent respectively the redistribution of the amount reduced in the last responding node i.e. 5 percent is performed by distributing 3 percent to Node 1 1 percent to Node 2 and 1 percent to Node 3. The example table below shows the adjusted capacity utilizations for C 

These dynamically adjusted capacity utilizations are used to adjust the job load metric of each compute node as discussed in step so that the next set of tasks can be sized and distributed in a manner that facilitates near full utilization of the compute nodes.

The present disclosure contemplates dynamically adjusting the capacity utilization for a plurality of core element in each compute node where the dynamically adjusted capacity utilizations of the plurality of core elements are used to adjust the job load metric of each compute node.

In step the job load adjuster module of front end node adjusts the job load metric for at least some of the nodes. In particular the job load adjuster module of front end node re computes the job load metric for the compute nodes to using the dynamically adjusted capacity utilizations of the core elements.

The re computed job load metrics for the compute nodes to are used in step where the extraction module of front end node extracts a second plurality of tasks from the work where the number of tasks in the second plurality of tasks is selected in correspondence to the number of nodes and where sizes of the tasks in the second plurality of tasks are sized based on the adjusted job load metric of each of the nodes. In step the distribution module of front end node distributes the second plurality of tasks to at least some of the nodes for processing. Steps to are performed in the same manner as described above.

In one example embodiment described herein in response to a determination that the time difference does not exceed the predefined threshold a second plurality of tasks is extracted from the work where the number of tasks in the second plurality of tasks is selected in correspondence to the number of nodes and where sizes of the tasks in the second plurality of tasks remain based on the unadjusted job load metric of each of the nodes and the second plurality of tasks is distributed to some of the nodes for processing.

Thus referring back to step if the time difference does not exceed the predefined threshold the front end node proceeds to step where the extraction module of front end node extracts a second plurality of tasks from the work where the number of tasks in the second plurality of tasks is selected in correspondence to the number of nodes and where sizes of the tasks in the second plurality of tasks remain based on the unadjusted job load metric of each of the nodes. In step the distribution module of front end node distributes the second plurality of tasks to some of the nodes for processing. Steps and are performed in the same manner as described above.

The compute nodes to begin the task execution process with step where the receiving module of the compute node receives a qualification task from front end node . As discussed in connection with the qualification task is used to measure capabilities of core elements in a node.

In step the compute nodes to execute the qualification task and transmit results of the qualification task to front end node .

Once a task has completed the process continues to step where the receiving module of compute nodes to determines if the compute node should continue to receive and process further tasks. Specifically the process makes this determination by evaluating whether the compute node is functionally able to continue receiving and processing tasks. For example the compute node may not be functionally able to continue receiving and processing tasks if the compute node needs to be taken offline for repair. If a determination is made that the compute node is functionally able to receive and process further tasks the compute node proceeds to step and waits to receive the next task for processing. Otherwise the compute node ends the reception process.

As shown in the front end node is connected to compute nodes to . The front end node extracts and distributes tasks to the compute nodes to . As depicted diagrammatically in front end node has multiple time sequential frames of a moving image for which extraction and distribution to compute nodes to is effected. also depicts a job load metric table a task distribution diagram and a time difference monitor .

In the front end node begins by extracting a first plurality of tasks to from frame where the number of tasks in the first plurality of tasks is selected in correspondence to the number of compute nodes to and where sizes of the tasks in the first plurality of tasks are sized based on a job load metric of each of the compute nodes to . The front end node distributes the extracted tasks to to the compute nodes to for processing.

The sizes of tasks to are based on a job load metric of each of the compute nodes to . The calculation of a job load metric for compute nodes to is discussed more thoroughly in step of . As shown in the job load metric table the job load metric for compute nodes to is 8 26 24 and 42 respectively. Accordingly the front end node sizes tasks to as 8 26 24 and 42 respectively such that the size of each task corresponds to the job load metric for the compute node assigned to execute that task. Thus the size of task corresponds to the job load metric of compute node i.e. 8 the size of task corresponds to the job load metric of compute node i.e. 26 the size of task corresponds to the job load metric of compute node i.e. 24 and the size of task corresponds to the job load metric of compute node i.e. 42 .

Upon completion of tasks to the front end node determines whether a time difference between a response from a node that is first to complete its task and a response from a node that is last to complete its task exceeds a predefined threshold. As shown in the time difference monitor in connection with frame the time difference between a response from a node that is first to complete its task and a response from a node that is last to complete its task has exceeded the predefined threshold T. In this example the time difference between a response from compute node which was the first to complete its task and a response from compute node which was the last to complete its task exceeded the predefined threshold T. Accordingly in response to the time difference exceeding the predefined threshold T the front end node adjusts the job load metric of at least some of the compute nodes. Specifically the job load metric of the compute nodes is adjusted by dynamically adjusting the capacity utilizations of each core element in each compute node during runtime and adjusting the job load metric using the dynamically adjusted capacity utilizations of the core elements in the node so that the next set of tasks can be sized and distributed in a manner that facilitates near full utilization of the compute nodes. Next the front end node extracts a second plurality of tasks to not shown from frame where the number of tasks in the second plurality of tasks is selected in correspondence to the number of compute nodes and where sizes of the tasks in the second plurality of tasks are sized based on the adjusted job load metric of each of the compute nodes. The sizes of tasks to are based on an adjusted job load metric of each of the compute nodes to . Accordingly the front end node sizes tasks to as 10 23 24 and 43 respectively. The front end node distributes the second plurality of tasks to to compute nodes to for processing.

Once execution of the second plurality of tasks to is complete the front end node again determines whether a time difference between a response from a compute node that is first to complete its task and a response from a compute node that is last to complete its task exceeds a predefined threshold. In this example the time difference between a response from compute node which was the first to complete its task and a response from compute node which was the last to complete its task exceeded the predefined threshold T as shown in the time difference monitor in connection with frame . Accordingly the front end node adjusts the job load metric of at least some of the compute nodes. Specifically the job load metric of the compute nodes is adjusted by dynamically adjusting the capacity utilizations of each core element in each compute node during runtime and adjusting the job load metric using the dynamically adjusted capacity utilizations of the core elements in the node so that the next set of tasks can be sized and distributed in a manner that facilitates near full utilization of the compute nodes. Next the front end node extracts a third plurality of tasks to not shown from frame where the number of tasks in the third plurality of tasks is selected in correspondence to the number of compute nodes and where sizes of the tasks in the third plurality of tasks are sized based on the adjusted job load metric of each of the compute nodes. Accordingly the front end node sizes tasks to as 6 23 24 and 47 respectively. The front end node distributes the third plurality of tasks to to compute nodes to for processing.

Upon completion of the third plurality of tasks to the front end node again determines whether a time difference between a response from a compute node that is first to complete its task and a response from a compute node that is last to complete its task exceeds a predefined threshold. As shown in the time difference monitor in connection with frame in this example the time difference between a response from compute node which was the first to complete its task and a response from compute node which was the last to complete its task exceeded the predefined threshold T. Accordingly the front end node adjusts the job load metric of at least some of the compute nodes. Specifically the job load metric of the compute nodes is adjusted by dynamically adjusting the capacity utilizations of each core element in each compute node during runtime and adjusting the job load metric using the dynamically adjusted capacity utilizations of the core elements in the node so that the next set of tasks can be sized and distributed in a manner that facilitates near full utilization of the compute nodes. Next the front end node extracts a fourth plurality of tasks to not shown from frame where the number of tasks in the fourth plurality of tasks is selected in correspondence to the number of compute nodes and where sizes of the tasks in the fourth plurality of tasks are sized based on the adjusted job load metric of each of the compute nodes. Accordingly the front end node sizes tasks to as 6 24 25 and 45 respectively. The front end node distributes the fourth plurality of tasks to to compute nodes to for processing.

Once execution of the fourth plurality of tasks to is complete the front end node again determines whether a time difference between a response from a compute node that is first to complete its task and a response from a compute node that is last to complete its task exceeds a predefined threshold. In this example the time difference between a response from compute node which was the first to complete its task and a response from compute node which was the last to complete its task did not exceed the predefined threshold T as shown in the time difference monitor in connection with frame . Accordingly the front end node does not adjust the job load metric and extracts a fifth plurality of tasks to not shown from frame where the number of tasks in the fifth plurality of tasks is selected in correspondence to the number of compute nodes and where sizes of the tasks in the fifth plurality of tasks are sized remain based on the unadjusted job load metric of each of the compute nodes. Accordingly the front end node sizes tasks to as 6 24 and 45 respectively which are the same sizes that were used to size tasks to . The front end node distributes the fifth plurality of tasks to to compute nodes to for processing and awaits responses from the compute nodes to to determine if an adjustment to the job load metric is needed.

This disclosure has provided a detailed description with respect to particular representative embodiments. It is understood that the scope of the appended claims is not limited to the above described embodiments and that various changes and modifications may be made without departing from the scope of the claims.

