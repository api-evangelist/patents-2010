---

title: Cloud computing: unified management console for services and resources in a data center
abstract: A system and a method to enable visual management of a service is disclosed. The method includes, for example, presenting a visual representation of a candidate concrete type of an instance of a model resource, the candidate concrete type of the instance of the model resource derived from a filtering of concrete types of resources, the filtering based on an abstract type of an instance of a model resource corresponding to the service and an instance of an additional model resource corresponding to the service. The method also includes receiving a command to establish a mapping between the instance of the model resource and the candidate concrete type of the instance of the model resource.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09442810&OS=09442810&RS=09442810
owner: PayPal, Inc.
number: 09442810
owner_city: San Jose
owner_country: US
publication_date: 20100226
---
This application claims the priority benefit of U.S. Provisional Application No. 61 230 584 entitled Management of Services and Resources filed Jul. 31 2009 which is hereby incorporated herein by reference in its entirety.

The present application relates generally to the technical field of management of data centers. In one specific example embodiments of the inventive subject matter relate to a method and system that unifies in a management system operational lifecycle management OLCM and service level management SLM of services and resources in an infrastructure environment such as a cloud computing environment or data center environment.

A large infrastructure such as the infrastructure utilized by eBay s Marketplace may span multiple infrastructure environments. The multiple infrastructure environments may include a data center environment a cloud computing environment or another environment that incorporates compute network storage operating system application and other resources. Various services deployed on the infrastructure such as search and messaging services may use these resources to accomplish various business objectives. The services themselves may span multiple infrastructure environments. To manage the operational lifecycle of these services and resources many tools and scripts may be built or brought in over a period of time in an ad hoc manner by different groups of people. As a result of for example the tools and scripts being set up over time many problems related to managing such services and resources may arise. Thus there is a need for a management system that enables cohesive and systemic management including OLCM and SLM of services and resources that span multiple infrastructure environments.

The description that follows includes illustrative systems methods techniques instruction sequences and computing machine program products that embody the inventive subject matter presented herein. In the following description for purposes of explanation numerous specific details are set forth in order to provide an understanding of various embodiments of the inventive subject matter. It will be evident however to those skilled in the art that embodiments of the inventive subject matter may be practiced without these specific details. Further well known instruction instances protocols structures and techniques have not been shown in detail.

As used herein the term or may be construed in an inclusive or exclusive sense. Further the term data center may refer to a data center a cloud or any infrastructure environment having a resource on which a service can be deployed. Thus any specific reference to a data center or cloud is provided merely for clarity. It should be understood that a service may be deployed in the data center the cloud or an infrastructure environment or split between them. Furthermore the data center may be located locally to or remotely from an enterprise and is still to be considered as being within a scope of various embodiments of the present invention. Additionally the data center may include services and resources located in proximity to one another or alternatively the services and resources may be geographically distributed. Furthermore the terms modeled topology and logical topology may be used interchangeably.

In an example embodiment a system to manage a data center including a cloud or another infrastructure environment is disclosed. The system includes for example a profile retrieval engine to retrieve a deployment profile of a service. An abstract resource type determination engine determines from the deployment profile an abstract type of a resource for the service. A modeled topology editing engine presents a representation of the abstract type to be selected by a user. A selection of the representation of the abstract type is to add a first modeled instance of the resource to a modeled topology of the service with the first modeled instance having the abstract type.

In another example embodiment a method to manage a data center including a cloud or another infrastructure environment is disclosed. The method includes for example retrieving a deployment profile of a service. An abstract type of a resource for the service is determined from the deployment profile. A representation of the abstract type is presented and is to be selected by a user. A selection of the representation of the abstract type adds a first modeled instance of the resource to a modeled topology of the service the first modeled instance having the abstract type.

In another example embodiment a machine readable storage medium embodying a set of instructions that when executed by a processor causes the processor to perform a method of managing a data center including a cloud or another infrastructure environment is disclosed. The method includes for example retrieving a deployment profile of a service. An abstract type of a resource for the service is determined from the deployment profile. A representation of the abstract type is presented and is to be selected by a user. A selection of the representation of the abstract type adds a first modeled instance of the resource to a modeled topology of the service the first modeled instance having the abstract type.

In an example embodiment the services and the resources are coupled to the management system such that the management system can manage the services and the resources using the OLCM engine and the SLM engine . In an example embodiment ones of the services and one or more of the resources are coupled such that one or more of the resources are allocated for ones of the services or ones of the services are deployed such that they use one or more of the resources .

Each of the service managers receives or obtains a state s not shown of the system e.g. in this case a service at time step t. If needed one or more of the service managers makes decisions based on policies and consequently takes action at action a not shown . As a result of the actions taken or some other notable event happening for example a failure the system state changes at time step t 1 and the cycle repeats. Both state and action are vectors. State is specified in terms of system configuration system performance operational mode e.g. maintenance mode and operational status e.g. serving traffic marked down etc. . Action is specified in terms of life cycle actions such as software provisioning addition of capacity e.g. compute network storage failure recovery e.g. auto reboots powering off a server etc.

Thus the service managers can be considered system controllers and it is here that a feedback loop closes. More precisely the controllers not shown inside each of the service managers close individual feedback loops where each loop can pertain to a single managed entity. Each controller also maintains the state of the resources that it manages where state is a vector defined in terms of the performance configuration operational mode and operational status of the managed resource.

To perform the OLCM operations and in particular to perform operations on various hardware and software resources a controller uses an adaptor not shown explicitly . The adaptor provides a transformation of commands given from the controller to the native commands exposed by the managed resource. A controller may also use other internal management services as well for example a provisioning manager . A controller is specific to an abstract type such as a load balancer whereas an adaptor is specific to a concrete type such as for example a Citrix NetScaler load balancer. To support new resources corresponding adaptors are implemented whereas to support new resource or service types corresponding controllers are implemented. The OLCM operations involve sequencing of various tasks. The sequencing may be accomplished by an orchestrator not shown explicitly inside each of the service managers . The orchestrator takes as an input a Directed Acyclic Graph DAG of tasks and executes the tasks with the objective of minimizing a total completion time. As such the orchestrator identifies independent tasks if any and executes them concurrently.

With continuing reference to the management services are shown to include a resource manager a provisioning manager a dispatcher a lock manager and a configuration manager . In this example embodiment the dispatcher is the point of entry into the management system . The dispatcher provides user authentication and authorization and creates and maintains user sessions. In an example embodiment user authentication is delegated to a separate lightweight directory access protocol LDAP server. User authorization is done internally based on roles and privileges. On successful authentication and authorization a user session is created and the session persists until it times out tunable or when the user logs out. The dispatcher sends the user requests such as that for service deployment to an appropriate one of the management services or the service managers e.g. the service manager . The dispatcher first looks up the endpoint of the service in its internal cache if this first look up fails the dispatcher does a service lookup with the microkernel . Thereafter the dispatcher directly communicates with an appropriate one of the management services or the service managers .

The configuration manager stores the profiles and the topologies both logical and physical in its configuration repository not shown explicitly . To maintain independence from both the technology and a vendor of the configuration repository the configuration manager has a service layer that abstracts the configuration repository. To maintain model independence the configuration manager may operate on a meta model. Specifically the configuration manager may transform a graph as for example the graph manifested in an XML document for a topology into relational database tables using meta model constructs. Each meta object except one relating to dynamic values such as performance data has a corresponding table in the database. Analogous to a source code management system known independently in the art the configuration manager may create or maintain version vectors for the profiles logical topologies physical topologies or constituent objects in the logical topologies or the physical topologies. Thus a logical topology may be realized from a particular version of a profile. Furthermore a physical topology may be created from a specific version of a logical topology. Additionally the system can roll back or forward a service to a previous or later version of its deployed topology. The configuration manager also verifies the deployed configurations to ensure that there is no configuration drift e.g. an out of band or otherwise unexpected configuration change made to a service or resource . The specified configuration is called What It Should Be WISB and the configuration as read from the managed resources is called What It Really Is WIRI . When a drift is detected a relevant one of the Service Managers can take appropriate corrective actions based on policies set by for example a system administrator or other person responsible for the infrastructure.

The provisioning manager deploys an operating system application stack e.g. Java Virtual Machine JVM or servlet container and application code to compute elements e.g. servers . Since a variety of specialized products and tools are available off the shelf for these specialized tasks the provisioning manager provides a service layer that abstracts the underlying tools using adaptors. Examples of such tools include but are not limited to Altiris by Symantec for OS deployment and Tivoli Provisioning Manager by IBM for application stack and code deployment. The clients of the provisioning manager e.g. the relevant controllers in the service manager are not exposed to the underlying tool and instead deal with the normalized interface exposed by the provisioning manager .

The resource manager reserves and allocates resources. Reservations are leased that is they expire if they are not used by a service. The reservations can be made permanent or up to a specified maximum duration by doing a resource allocation. The resource allocation happens when a service deployment command is issued. The resource manager may also dynamically assign resources at an initial resource allocation for example if initial resource allocation is automated or during maintenance of SLOs when a service has been deployed. The resource manager may adjust perhaps periodically the resource allocation between services to for example minimize resource consumption while maintaining Service Level Agreements SLAs . For example the resource manager may invoke or provide functionality equivalent to the service level objective violation remedying engine .

Resources can be shared between different services if appropriate. If multiple ones of the service managers access a shared resource concurrently there is a chance of creating inconsistency in the configuration of the resource as not all resources enforce or provide serialized access. Therefore to prevent concurrent access to a shared resource an out of band synchronization or mutual exclusion mechanism can be provided. The exclusion mechanism can be achieved by the lock manager . To access any resource one of the service managers first obtains a lock from the lock manager and relinquishes it after its session with the managed resource is completed. Locks are leased that is they expire after the duration specified in the request. Using leased locks allows the system to recover gracefully from failures of lock holders. A lease can be extended subject to some tunable maximum to prevent a thread from waiting indefinitely for a lock lock starvation and ensure locks are issued in as close to arrival order as possible in a manner that is specified consistent and in accordance with the needs of a program lock fairness . Locks are also reentrant. Further the locks are persistent that is they survive restart or failure of both the lock manager and service managers .

In an example embodiment the architecture may include at least two additional subsystems an event system and a monitoring system neither of which is shown . The event system is used to transport and process events e.g. alerts notifications and performance metrics transported as events from managed resources to the subscribed controllers in various ones of the service managers . The monitoring system is used to enable monitoring for all managed ones of the services and resources and abstract various monitoring tools.

Each management service except the microkernel and each of the service managers has an associated topology. The topologies for the management services and associated ones of the Service Managers are bundled with the system whereas those for the Service Managers that manage services deployed in the data center are constructed on demand. Service orientation and declared configurations in the form of topologies allow the system to be bootstrapped and essentially self managing.

When the system is started only the microkernel gets started. At startup the microkernel creates the relevant controllers. Using the controllers the microkernel deploys the service managers for the management services based on the profiles stored in the system. The controllers register their respective ones of the service managers with a registry component in the microkernel . Thereafter the microkernel asks each of the service managers to deploy respective ones of the management services by passing in the relevant respective topologies. There is a respective topology stored each for the dispatcher the configuration manager the resource manager the lock manager and the provisioning manager . On successful deployment the microkernel registers the respective ones of the management services with its registry component.

The microkernel uses the same mechanism to deploy a service in an infrastructure environment. For example the microkernel deploys a service manager for the service . Then the service manager deploys the service . Recall that in an example embodiment each controller inside each of the service managers manages the OLCM and SLM for its managed entity and that each of the service managers has controllers one per each type of service or resource being managed. When a managed service fails or its performance starts lagging its service manager via its controllers can take failure recovery or compensating actions based on pre determined policies. The same approach or mechanism is also used internally in the system for the management services the dispatcher the resource manager etc. . The service managers being services themselves are managed by their respective controllers in the microkernel .

This recursive model of service orientation and service management enables the system to be self managing to a large extent. An exception to this recursive model could be the microkernel since this is the point at which the recursion stops. As such in an example embodiment the microkernel is made highly available for example using high availability HA clustering solutions .

In the OLCM engine is also shown to include a concrete resource type determination engine to determine from the physical topology a concrete type of a resource that the service requires.

In the OLCM engine is also shown to include an actual resource instance automatic selection engine to automatically select one or more actual instances of the resources in the data center for allocation to the service using specified policies and constraints to for example minimize resource consumption while maintaining SLAs.

In the OLCM engine is also shown to include a service deployment engine to deploy a service such that the service uses the selected resources. Once the service has been approved for deployment the deployment happens in this phase. The corresponding physical topology of a specified version is retrieved from the management system and the system provides the deployment. The management system verifies that the resource reservations done in the resource allocation phase are still valid and accordingly makes the allocations permanent. The management system auto generates an orchestration plan from the given topology based on pre determined rules. The management system then performs a dry run informing the user of the impending actions and on due approval proceeds with the deployment. During deployment the user is kept fully in the loop with deployment status and progress updates. In an example embodiment deployments are intended to be performed by operations personnel typically system administrators. However deployment of code is typically intended to be performed by developers.

In the OLCM engine is also shown to include a service controlling engine . Once a service has been deployed the service can be controlled. Control actions may include start stop activate and deactivate operations. There may be a symmetrical reverse operation e.g. stop deactivate or undeploy for each respective forward operation e.g. start activate and deploy . Furthermore the symmetrical reverse operation may be invoked substantially immediately after the respective forward operation is invoked or the forward operation may be invoked substantially immediately after the respective reverse operation is invoked. The management system provides capabilities to the user or another software tool to initiate a control action on the service as a whole or constituent resources or groups thereof. The management system auto generates the orchestration plan from the given topology based on rules if not already completed. For every control action the management system performs a dry run informing the user of the impending actions and resultant changes and on due approval proceeds with the execution of the control action. During execution the user is kept fully in the loop with status and progress updates. In an example embodiment control actions are intended to be performed by operations personnel typically network operations center NOC personnel and system administrators.

With continuing reference to the OLCM engine is also shown to include an updating engine to update a deployment profile modeled deployment topology physical topology or service. Updates to profiles can be made simply by saving a new version of the profile. Updates to existing topologies can be made by using the editing facilities provided by the management system. Updates to a deployed service are made simply by instructing the management system to deploy a physical topology of a different version. The management system is smart enough to determine a difference the delta between the deployed version and the to be deployed version and then perform actions corresponding to the computed delta. As before management system performs a dry run and informs the user of the impending actions and the resultant changes.

The OLCM engine may also include a service undeployment engine not shown to undeploy a service or a resource deallocation engine not shown to deallocate or free resources. The undeployment engine and the resource deallocation engine may act concurrently or serially to deallocate resources for a service. Furthermore the undeployment engine may include a safety state in which a service cannot be undeployed if it is running or otherwise active.

In an example embodiment clients communicating with the management system may present in a user interface a summary of performance information and corresponding color coded markers in context with a displayed topology. For example a green marker associated with a service or resource indicates all relevant KPIs are within expected ranges an amber marker indicates there is cause for some concern and red indicates an unexpected or abnormal condition including a failure or an SLO violation. Using visual indicators in context provides a mechanism for operations personnel to quickly determine if there is a problem and if so the chance of propagation of problem to other resources or services.

With continuing reference to the SLM engine is also shown to include a service level objective violation remedying engine that may remedy a SLO violation upon detection. For example the service level objective violation remedying engine may dynamically allocate resources for a service. The dynamic allocation may be performed according to a physical topology of the service such that a cost of the resources is balanced with a benefit of meeting the service level objective. For example high service levels play a role in consumer retention and contribute to higher revenue. However there is an inherent tradeoff between the service levels that the system provides to its consumers and the cost that it incurs in providing those service levels. The system cost is at least partially specified in terms of resources consumed. Therefore the management system may strike a balance between the two to for example minimize resource consumption while maintaining SLAs. Mathematically such balancing can be stated as System Profit Service Level Objectives System Cost 

Stated differently the management system may allocate resources to service s to provide maximum utility to a business over an infinite time horizon. Thus to do true service level management the management system may have a capability to allocate judiciously one or more resources to various services. In particular the service level objective violation remedying engine may have a decision making capability to allocate or deallocate resources to or from services based on the SLOs. That is the service level objective violation remedying engine may judiciously allocate resources to newly deployed services as well as reallocate resources between services during runtime to meet the SLOs. The service level objective violation remedying engine may consider the state of the managed services at the current time and determine if their states are good states. If the states are not good then the dynamic resource allocation engine may proactively move the managed services to a better state through a system reconfiguration. In an example embodiment the system reconfiguration requires due operator approval. While allocating resources to different services the service level objective violation remedying engine may minimize resource consumption while maintaining SLAs. The decision making capability may be based on one or more policies. For example a failure of a service or a resource e.g. a load balancer may require a resource allocation a deployment a start or an activation of another service or resource e.g. another load balancer . As another example a failure of a running application e.g. a tomcat server may require a restarting of the running application and if the condition persists after for example 3 retries then the failure of the running application may further require a generating of an alert to be sent to a network operations center NOC . As another example if a load on a service increases and is for example within allowed limits then additional capacity may be added. The service level objective violation remedying engine or a controller such as a controller in one of the service managers may retrieve the one or more policies from a policy repository. The policy repository may be associated or managed by the management system or programmatically exposed to one of the various clients for management by the one of the various clients or a user.

Referring now to a flowchart of an example embodiment of a method of managing a data center including a cloud or another infrastructure environment. The method includes retrieving a physical topology of a service determining from the physical topology a concrete type of a resource for the service and selecting an actual instance of the resource in the data center the actual instance having the concrete type the actual instance selected such that a consumption of the actual instance does not violate at least one of a constraint and a policy. The constraint or the policy may be included in the policy repository.

The profile retrieval engine retrieves a deployment profile profile of a service. A deployment profile is a substantially complete specification for a deployment architecture of a service. The deployment architecture may include a given architecture domain such as messaging search etc. . The deployment profile specifies entities such as the service itself the resource types that the service consumes such as application servers compute servers load balancers etc. their cardinalities their configuration parameters and their inter relationships. This profile is then a model that is used to drive all subsequent operational lifecycle actions such as resource allocation deployment etc. It is expected that each architecture domain may have at least a few profiles. In an example embodiment to facilitate development of profiles a base information model called base module is provided that is common across all domains. The base module specifies for example compute network storage IP address virtual local area network VLAN operating system and application resources. To create a profile a specific architecture domain such as a messaging domain leverages relevant aspects of the base module and will add domain specific constructs. In an example embodiment the profile is created in a Unified Modeling Language UML tool. To make the profile executable by a software program the profile is transformed into an XML schema. In an example embodiment clients such as a command line interface console or other software tools can use programmatic mechanisms for example Web Services or RESTful approach to communicate with the management system. These clients can provide a user interface for performing profile operations such as creating retrieving updating and deleting profiles. For example in a specific example embodiment MagicDraw is used as the UML tool and to transform the model to XML schema. In an example embodiment profiles are built by systems or application architects with input from operations architects. Profiles may be created outside of the management system . Furthermore the console may include a profile editing engine that provides create retrieve update and delete operations on profiles. Profile schemas for example XML profile schemas may be stored in the management system .

The abstract resource type determination engine parses and interprets the schema of a deployment profile to determine the abstract resource types associated with a service. These abstract resource types may correspond to the nouns in the schema for example server operating system load balancer layer 2 connections etc.

The modeled topology editing engine provides create retrieve update and delete operations on logical topologies. Every service to be deployed may have an associated topology. A deployment topology topology is a realization of the profile and as such specifies an actual type and number of resources consumed. The topology may have one of two types logical or modeled and physical. The logical topology specifies the abstract types of resources used and their cardinalities. The logical topology thus does not have any bindings to actual physical resources. The advantage of separation between the logical topology and the physical topology is that the same logical topology can be deployed in different environments at the same time or at different times by binding to different resource types or instances. For example the same logical topology can be deployed in a quality assurance QA environment and later on in a production environment. As another example the same logical topology can be deployed in an eBay data center and at a later time or simultaneously in either an internal or an external cloud. The console through the modeled topology editing may also provide a visual association between the logical topology and a related specification namely its profile. For example the modeled topology editing engine may present a representation of abstract types determined from the deployment profile. For example the modeled topology editing engine may dynamically populate a service and resource palette such as service and resource palette see discussed below with the correct icons. These icons may correspond to the nouns in the schema for example server operating system load balancer layer 2 connections etc. The user may then be able to drag and drop one of these icons to add an instance of a resource of a particular type to a modeled topology of the service. In an example embodiment the modeled topology editing engine may be part of a freestanding application that can be started from or integrated into the console . In an example embodiment both logical and physical topologies are intended to be built by operations or application architects. In an example embodiment once created a topology can be saved and versioned in the management system . Furthermore the modeled topology editing engine may include a safety state in which a topology cannot be deleted if a corresponding service is deployed.

The layout topology retrieval engine retrieves the topology of the data center cloud or another infrastructure environment. The topology of the data center may be retrieved from storage or directly from a server such as the management system through communication between the console and the server.

The concrete resource type association engine analyzes the topology of the data center to determine concrete types of resources in the data center that may be associated with the abstract types of resources specified in the deployment profile or modeled topology of the service.

The physical topology editing engine provides create retrieve i.e. read update delete and save operations on physical topologies. Physical topologies may be created from logical topologies. A physical topology contains the binding from the abstract to the concrete types e.g. a server which is an abstract type is bound to for example an IBM LS20 which is a concrete type . The console through the physical topology editing engine may present a visual association between the two types of topologies logical and physical of a service such that a user can navigate from the logical topology to the physical topology and vice versa. The console through the physical topology editing engine may also provide a visual association between the topology either logical or physical and a related specification namely its profile. Furthermore the physical topology editing engine may include a safety state in which a topology cannot be deleted if a corresponding service is deployed.

The actual resource instance manual selection engine presents available and relevant concrete resources types in an infrastructure environment for selection by the user to add physical instances of resources having particular concrete types to the physical topology of the service. Recall that a physical topology is one in which a binding is first created between the abstract type and the concrete type and then between the concrete type and the actual instance in the infrastructure. The former activity is the one that happens during the resource selection phase. If the selection is done manually the console through the actual resource instance manual selection engine provides a means perhaps visual for example using the physical resource type binding view of discussed below to bind an abstract type to a concrete type. Thus the console actual resource instance manual selection engine may present available and relevant resources types in an infrastructure environment such as a data center either in a tabular manner or graphically for example by showing the concrete types .

The deployment option selection engine provides navigation capabilities to a user to select an appropriate topology to be deployed. Once a deploy command is issued the console may send the relevant topology information to a dispatcher such as the dispatcher see which then dispatches the deploy command to an appropriate service manager such as one of service managers . With reference again to the appropriate one of service managers first performs a dry run. The results of this operation are sent back to the console which through the deployment option selection engine interprets the results and visually marks the resources or services in the topology that are going to be affected. The console provides a capability to the user to inspect each resource or service to see the specific task s that will be performed on a particular entity. The submission of a deploy command may result in a job creation the console through the deployment option selection engine depicts this information as well. Once the user gives the final approval for deployment the console sends this information to the management system which may then proceed with the actual deployment.

The control option selection engine provides capabilities to issue control actions such as start stop activate deactivate and so on with reference to an individual resource or service or groups thereof. Each action results in the creation of a job execution of a dry run by default and then subsequent execution of the constituent tasks in the job. As such the presentation aspects are similar to those in a deployment view described below.

The alerting engine presents color coded markers in context with a displayed topology. For example a green marker associated with a service or resource indicates all relevant KPIs are within an expected range an amber marker indicates there is cause for some concern and red indicates failure or an SLO violation. Using visual indicators in context provides a mechanism for operations personnel to quickly determine if there is a problem and if so the chance of propagation of problem to other resources or services.

The perspective displaying engine provides at least three perspectives a service b resource and c business. The perspectives may be crosscutting across the various views. Further there may be navigability between the perspectives especially between services and resources. The navigability is feasible because of a single underlying model each view is a projection of the single model. In an example embodiment in a monitoring view a user can just look at a service s performance and then can choose to observe the performance of resources used by that service. Similarly the user can view the various deployed resources choose one of them query the system to present all services using that resource and then navigate to one of the services. In a similar fashion service to resource navigability can be performed. Thus this capability may provide an insight into an infrastructure.

The profile editing engine provides views to perform create save retrieve update and deletion operations on profiles. Profiles may be scoped under an architecture domain such as a messaging or search architecture domain to provide a visual grouping mechanism for profiles. Furthermore each profile version may be linked with topologies associated with that version. The profile editing engine may include a safety state in which a profile cannot be deleted if a corresponding service is deployed.

The service deployment progression displaying engine displays for each deployment task status and progress updates that the Console may receive perhaps in a periodic or asynchronous manner from a server such as management system . The service deployment progression displaying engine may superpose this information on the corresponding resource in the displayed topology. The superposed information may provide a simple yet very useful indication of the overall progress and impact of the deployment operation. A user can of course log out and log in much later to check on the deployment status. The console through the service deployment progression displaying engine may provide navigability from the job in the view which provides a summary of the progress and status to a graphical depiction of a topology with superposed progress and status updates. In an example embodiment once the deployment finishes with success or failure the results are displayed and the job status is updated.

The user roles and privileges definition engine controls the presentation of views in the console such that those views correspond to user roles and privileges. As described above with reference to authentication and authorization operations are performed by the dispatcher . When the authentication and authorization operations succeed the dispatcher may return known user roles and associated privileges to the console . The console using the user roles and privileges definition engine can then present those views that correspond to the returned user roles and privileges. For example the console may allow a user with an operations architect role to assemble a topology but not deploy it. That is the console through the user roles and privileges definition engine may present a topology designing view but not a deployment view to a user belonging to the operations architect role.

The interdependence displaying engine provides a view into the structure of the various services and resources that the management system manages. The view may show dependencies between services dependencies between resources and dependencies between services and resources.

The performance displaying engine provides a monitoring dashboard that presents for example charts or graphs of various performance metrics. The user can customize the view to observe the metrics and charts of interest. The console using the performance displaying engine may also present a summary of performance information.

Both the modeled topology editing engine and the physical topology editing engine support editing of configuration information for individual entities as well as for groups of entities. For example most of the configuration information for a server is usually the same and instead of forcing the user to enter it for each server the user can enter it once and the modeled topology editing engine can apply the configuration information to a group of servers as selected by the user. The modeled topology editing engine and the physical topology editing engine are also capable of presenting hierarchical structure. For example a server icon when double clicked shows the constituent parts such as Ethernet ports disks and so on. Similarly a service icon when double clicked shows the constituent resources. Most topologies may be quite large for example they may easily contain hundreds of servers. The modeled topology editing engine and the physical topology editing engine provide abstractions that enable ease of use of creating and modifying such topologies for example by using clustering techniques. Instead of starting from a profile and creating a topology the user can select a particular version of the topology and choose to modify it instead. The modeled topology editing engine and the physical topology editing engine allow the proper abstractions or operations to enable this action.

Certain embodiments described herein may be implemented as logic or a number of engines components or mechanisms. An engine logic component or mechanism collectively referred to as an engine may be a tangible unit capable of performing certain operations and is configured or arranged in a certain manner. In certain example embodiments one or more computer systems e.g. a standalone client or server computer system or one or more components of a computer system e.g. a processor or a group of processors may be configured by software e.g. an application or application portion or firmware note that software and firmware can generally be used interchangeably herein as is known by a skilled artisan as an engine that operates to perform certain operations described herein.

In various embodiments an engine may be implemented mechanically or electronically. For example an engine may comprise dedicated circuitry or logic that is permanently configured e.g. within a special purpose processor to perform certain operations. An engine may also comprise programmable logic or circuitry e.g. as encompassed within a general purpose processor or other programmable processor that is temporarily configured by software or firmware to perform certain operations. It will be appreciated that a decision to implement an engine mechanically in the dedicated and permanently configured circuitry or in temporarily configured circuitry e.g. configured by software may be driven by cost and time considerations.

Accordingly the term engine should be understood to encompass a tangible entity be that an entity that is physically constructed permanently configured e.g. hardwired or temporarily configured e.g. programmed to operate in a certain manner and or to perform certain operations described herein. Considering embodiments in which engines or components are temporarily configured e.g. programmed each of the engines or components need not be configured or instantiated at any one instance in time. For example where the engines or components comprise a general purpose processor configured using software the general purpose processor may be configured as respective different engines at different times. Software may accordingly configure the processor to constitute a particular engine at one instance of time and to constitute a different engine at a different instance of time.

Engines can provide information to and receive information from other engines. Accordingly the described engines may be regarded as being communicatively coupled. Where multiples of such engines exist contemporaneously communications may be achieved through signal transmission e.g. over appropriate circuits and buses that connect the engines. In embodiments in which multiple engines are configured or instantiated at different times communications between such engines may be achieved for example through the storage and retrieval of information in memory structures to which the multiple engines have access. For example one engine may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled. A further engine may then at a later time access the memory device to retrieve and process the stored output. Engines may also initiate communications with input or output devices and can operate on a resource e.g. a collection of information .

Example embodiments may be implemented in analog digital or hybrid electronic circuitry or in computer hardware firmware software or in combinations thereof. Example embodiments may be implemented using a computer program product for example a computer program tangibly embodied in an information carrier e.g. in a machine readable medium for execution by or to control the operation of data processing apparatus for example a programmable processor a computer or multiple computers .

A computer program can be written in any form of programming language including compiled or interpreted languages and it can be deployed in any form including as a stand alone program or as an engine component subroutine or other unit suitable for use in a computing environment. A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.

In certain example embodiments operations may be performed by one or more programmable processors executing a computer program to perform functions by operating on input data and generating output. Method operations can also be performed by and apparatus of example embodiments may be implemented as special purpose logic circuitry e.g. a field programmable gate array FPGA or an application specific integrated circuit ASIC .

The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client server relationship to each other. In embodiments deploying a programmable computing system it will be appreciated that both hardware and software architectures require consideration. Specifically it will be appreciated that the choice of whether to implement certain functionality in permanently configured hardware e.g. an ASIC in temporarily configured hardware e.g. a combination of software and a programmable processor or a combination permanently and temporarily configured hardware may be a design choice. Below are set out hardware e.g. machine and software architectures that may be deployed in various example embodiments.

The example computer system is shown to include a processor e.g. a central processing unit CPU a graphics processing unit GPU or both a main memory and a static memory which communicate with each other via a bus . The computer system may further include a video display unit e.g. a liquid crystal display LCD or a cathode ray tube CRT . The computer system also includes an alphanumeric input device e.g. a keyboard a cursor control device e.g. a mouse a disk drive unit a signal generation device e.g. a speaker and a network interface device .

The disk drive unit includes a machine readable medium on which is stored one or more sets of instructions e.g. software embodying any one or more of the methodologies or functions described herein. The software may also reside completely or at least partially within the main memory or within the processor during execution thereof by the computer system the main memory and the processor also constituting machine readable media.

The software may further be transmitted or received over a network via the network interface device .

While the machine readable medium is shown in an example embodiment to be a single medium the term machine readable medium should be taken to include a single medium or multiple media e.g. a centralized or distributed database or associated caches and servers that store the one or more sets of instructions. The term machine readable medium shall also be taken to include any medium that is capable of storing encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any one or more of the methodologies of the present invention. The term machine readable medium shall accordingly be taken to include but not be limited to solid state memories optical and magnetic media and carrier wave signals.

Thus a method and system to manage services have been described. Although the present invention has been described with reference to specific example embodiments it will be evident that various modifications and changes may be made to these embodiments without departing from the broader spirit and scope of the invention. Accordingly the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.

In the foregoing Detailed Description it can be seen that various features are grouped together in a single embodiment for the purpose of streamlining the disclosure. This method of disclosure is not to be interpreted as reflecting an intention that the claimed embodiments require more features than are expressly recited in each claim. Rather as the following claims reflect inventive subject matter lies in less than all features of a single disclosed embodiment. Thus the following claims are hereby incorporated into the Detailed Description with each claim standing on its own as a separate embodiment.

The problem of allowing a user to manage in a systematic and cohesive manner one or more services or resources in a data center a cloud or another infrastructure environment is solved by various example embodiments such as presenting a user interface to control a management system the user interface presenting elements to allow the user to design a deployment profile of a service assemble a modeled deployment topology of the service based on the deployment profile and assemble a physical topology of the service based on the modeled deployment topology.

