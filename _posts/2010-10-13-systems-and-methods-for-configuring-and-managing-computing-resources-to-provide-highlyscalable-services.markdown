---

title: Systems and methods for configuring and managing computing resources to provide highly-scalable services
abstract: One embodiment of the present invention sets forth a cloud computing environment that includes a service cloud and one or more services accessing the service cloud. The service cloud includes multiple resources of different types that support the execution of the services accessing the service cloud. Each resource and service in the cloud computing environment is configured via a centralized configuration service. In addition, resource allocation and predictive performance monitoring engines allocate resources and monitor the resources allocated to the services accessing the service cloud.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09274849&OS=09274849&RS=09274849
owner: Disney Enterprises, Inc.
number: 09274849
owner_city: Burbank
owner_country: US
publication_date: 20101013
---
This application claims benefit of United States provisional patent application entitled Systems and Methods for Configuring and Managing Computing Resources to Provide High Scalable Services filed on Sep. 8 2010 and having a Ser. No. 61 381 037.

The present invention relates generally to online commerce and more specifically to systems and methods for configuring and managing computing resources to provide highly scalable services.

The advent of cloud based computing architectures has allowed for the rapid and scalable deployment of services such as virtual stores media outlets and other online services. In general a cloud based architecture includes a set of resources such as processors operating systems software and other components that can be combined to form systems on which services can be deployed.

A user can request the instantiation of specific set of resources from a management system to deploy a service. For example a user may wish to set up and instantiate a virtual server from the cloud to create a storefront to market products or services on a temporary basis for instance to sell tickets to an upcoming sports or musical performance. In a typical cloud based architecture the user needs to be aware of the specific resources that are needed to deploy a service from the initial stages of service development. The service is often developed in a manner that is closely dependent or coupled to the specific resources on which the service is to be deployed. In addition to deploy a service the user typically configures the specific resources based on the configuration requirements of the service.

One drawback to such cloud based architectures is that the user spends a considerable amount of time on the infrastructure level details such as configuration when deploying the service in the cloud. In addition since the service is developed according to the specific resources and not in a generic manner the development cycles for developing services that can be deployed on such cloud based architectures is undesirably long.

As the foregoing illustrates what is needed in the art is a mechanism for deploying and managing services in a cloud with minimal effort from service developers.

One embodiment of the present invention sets forth a cloud computing environment. The cloud computing environment includes a plurality of resources configured to support one or more of services accessing the cloud computing environment a configuration engine configured to manage and distribute configuration information related to the plurality of resources within the cloud computing environment and a resource allocation engine configured to allocate a portion of the plurality of resources to a first service accessing the cloud computing environment based on at least a portion of the configuration information.

Advantageously the cloud computing environment described herein allows service developers to simply define the functionality of the service and describe the operating characteristics of the service. The allocation configuration and management of the resources and the services are then autonomously performed by engines within the cloud computing environment.

In the following description numerous specific details are set forth to provide a more thorough understanding of the present invention. However it will be apparent to one of skill in the art that the present invention may be practiced without one or more of these specific details. In other instances well known features have not been described in order to avoid obscuring the present invention.

The service cloud includes a collection of resources such as hardware resources platform resources and infrastructure resources that are configured and managed by different engines within the service cloud . The service cloud also manages the allocation of the resources to services such as service interacting with the service cloud . Further the service cloud monitors the performance of the different resources included in the service cloud as well as the services interacting with the service cloud . In one embodiment the service cloud communicates with other collections of resources such as an external cloud to provide a wider array of resources that can be allocated to the services. Additional details of the service cloud including the resource configuration resource allocation and monitoring functionalities are set forth below in conjunction with .

The service framework is an interface layer between the service and the service cloud . In one embodiment the service framework allows the service interact to the service cloud via an abstracted application program interface API . The service is designed and developed to seamlessly interact with the interface provided by the service framework . Additional details of the service framework are set forth below in conjunction with .

The service is any application that interacts with the cloud service to utilize processing capabilities and or infrastructure and platform resources provided by the cloud service . For example the service may be an entertainment distribution service a news service or a financial management service. In one embodiment the service is distributed to one or more end users via the cloud service . The service can be developed in any technically feasible fashion and in any software programming language as long as the service is able to interact with the service framework . Importantly since the service interacts only with the service framework the service does not need to be developed with any knowledge of the specific functionalities and resources provided by the service cloud . Additional details of the service framework are set forth below in conjunction with .

The management platform allows service and network administrators to monitor the performance of resources within the service cloud as well as the services interacting with the service cloud . The management platform also allows service and network administrators to perform various management tasks when needed. For example a service administrator via the management platform may be able to view any captured log data that indicates performance characteristics services interacting with the service cloud . As another example a service administrator via the management platform may view or modify configuration information associated with a service interacting with the service cloud .

Although in the above description the cloud computing environment is shown with one service and one service framework persons skilled in the art will recognize that the architecture of contemplates only an exemplary embodiment of the invention. Other embodiments may include any number of services and any number of service frameworks . Thus is in no way intended to limit the scope of the present invention in any way.

The identity token within the service is a unique key assigned to the service by the service cloud . Based on the identity token the service is authenticated with the service cloud and is able to receive configuration information from the service cloud via the service framework . In one embodiment two or more instances of the service are interacting with the service cloud . In such an embodiment a different identity token is assigned to each instance of the service .

As previously described herein the service interacts only with the service framework and does not have any knowledge of the specific functionalities and resources provided by the service cloud . Instead the service defines the types of resources that are needed by the service to operate and to be accessed by end users. The system definition included in the service defines the type of system the service needs to operate. More specifically the system definition specifies the hardware requirements as well as the operating system requirements for operating the service . Any other system level requirements or specifications associated with the service are included in the system definition . The virtual IP address specifies a virtual IP address associated with the service . The virtual IP address is a public facing address that can be used by end users to access the service .

The external dependencies included in the service specify the types of external resources that are needed for the service to operate. These external resources are external to the system on which the service operates. External resources specified by the service in the external dependencies may include specific data stores application servers legacy systems federation servers virtualization servers etc.

The performance requirements included in the service specify thresholds related to specific performance metrics that the service should meet when the service operates. The performance requirements may specify different thresholds for data throughput latency and resource utilization. As described below the service cloud allocates resources to the service based on the performance requirements .

The service functionality code is software code that defines the function of the service . The service functionality code can be written in any technically feasible manner and in any programming language. When the service interacting with the service cloud is accessed by end users the service functionality code is executed to provide the end users with the functionality associated with the service .

As previously described herein the service framework is an interface layer between the service and the service cloud . More specifically the service is developed such that any interactions between the service and the service cloud are performed via the different layers included in the service framework . The authorization layer interfaces between the service and the service cloud when the service needs to be authorized. In operation the service via the authorization layer transmits the identity token to the service cloud and receives again via the authorization layer an indication whether the authorization with the service cloud based on the identity token was successful.

The configuration layer interfaces between the service and the service cloud when the service needs to receive configuration information associated with the service . As will be discussed in greater detail below configuration information associated with each service such as the service is stored in the service cloud . Among other information the configuration information associated with the service includes the back end resource configuration associated with the service . When the service is ready to receive the associated configuration information a request is transmitted to the service cloud via the configuration layer . In response the service receives the associated configuration information again via the configuration layer .

The logging and metrics collection layer interfaces between the service and the service cloud to collect operating details associated with the service . The operating details collected by the logging and metrics collection layer include logs and metrics associated with the performance of the service end user and resource usage patterns associated with the service periodic environment snapshots etc. The operating details collected by the logging and metrics collection layer are stored in and analyzed by the service cloud as described in greater detail below.

As previously described herein the service cloud provides an infrastructure of resources to services such as the service that interact with the service cloud . The services interacting with the service cloud utilize the processing capabilities and functionalities of the resources to operate and distribute service functionality to end users. The service cloud includes several management engines the configuration engine the resource allocation engine and the predictive engine that manage and monitor the infrastructure of resources provided by the service cloud to facilitate the efficient and reliable operation of the services interacting with the service cloud . For exemplary purposes only the specific operation of the management engines included in the service cloud are described below with respect to the service .

The infrastructure of resources provided by the service cloud includes the platform resources and the hardware resources . Platform resources include operating system resources virtual machine resources database resources etc. Hardware resources include servers routers storage space etc. Each platform resource and hardware resource is associated with a resource type and a unique identity token such as identity token associated with platform resource and identity token associated with hardware resource . A resource type associated with a resource indicates the specific functionalities or processing capabilities of the resource. For example a resource type associated with a platform resource indicates that the platform resource is a database from a specific vendor. As another example a resource type associated with a hardware resource indicates that the hardware resource is a server having a certain processing power. Resources associated with the same resource type can be co located or sparsely located can belong to different organizations and can be purchased from different vendors.

The platform resources and the hardware resources or portions thereof are allocated to different services that interact with the service cloud by the resource allocation engine . For a particular service the resource allocation engine allocates resources included in the service cloud based on the resource requirements specified by the service. When allocating resources to the service the resource allocation engine first identifies the system definition the external dependencies and the performance requirements specified by the service . As previously described herein the service via the system definition the external dependencies and the performance requirements specifies the types of resources that are needed by the service to operate efficiently. The resource allocation engine then identifies one or more platform resources and or one or more hardware resources referred to herein as the identified resources that match the types of resources specified by the service . For example the system definition may specify that the service requires a particular type of operating system and the resource allocation engine identifies a platform resource that is an operating system of the particular type specified by the system definition .

From the identified resources the resource allocation engine selects the particular resources that meet the performance requirements specified by the performance requirements of the service . When selecting the particular resources the resource allocation engine based on the performance requirements may implement one of several resource allocation algorithms. When the performance requirements specify that resource availability should be maximized the resource allocation engine performs resource allocation according to a location sparsity algorithm. With location sparsity algorithms resources within the identified resources that are sparsely located relative to each other are typically selected. One implementation of the location sparsity algorithm is described below in conjunction with . When the performance requirements specify that latency across the service should be minimized the resource allocation engine performs resource allocation according to a location density algorithm. With location density algorithms resources within the identified resources that are close in proximity relative to each other are typically selected. One implementation of the location density algorithm is described below in conjunction with .

Another resource allocation algorithm that may be implemented by the resource allocation engine is the load balancing algorithm. With load balancing algorithms resources are typically allocated such that resources associated with the same resource type have balanced utilization levels. In addition the performance requirements may specify that only resources having a utilization level below a pre determined threshold are to be allocated to the service . In such a scenario any identified resources that do not have a utilization level below the pre determined threshold are not be allocated to the service . In other embodiments any other technically feasible resource allocation algorithms can be implemented by the resource allocation engine to allocate resources based on the performance requirements specified by the service .

As the service operates on resources allocated by the resources allocation engine the logging and metrics engine collects operating details associated with the allocated resources and the service . The operating details associated with the allocated resources include performance metrics associated with the platform resources and the hardware resources environment snapshots of service cloud resource utilization levels failure logs etc. The logging and metrics engine stores the collected operating details in the logs and metrics store . In addition the logging and metrics engine receives operating details collected by the service via the service framework as previously described herein. The operating details collected by the service are also stored in the logs and metrics store .

The predictive engine is a learning based engine that analyzes the operational details stored in the logs and metrics store . In the analysis the predictive engine identifies inefficiencies and usage patterns across the service cloud and predicts future utilization levels of the resources based on pre determined trends and other historic data. In addition the predictive engine compares current patterns with pre determined trends to determine the likelihood of certain events such as failures across the service cloud . Based on the analysis the predictive engine performs one or more remedial operations to improve the overall performance of the service cloud as well the performance of the services interacting with the service cloud . The specific operation of the predictive engine is described in greater detail below in conjunction with .

To realize the functionality of the service cloud described above each of the resources and the services referred to herein as the entities included in and interacting with the service cloud needs to be configured with the associated configuration information. In addition the resource allocation engine and the predictive engine described above need access to the configuration information associated with each of the entities to facilitate the efficient operation of the service cloud . For a particular entity the associated configuration information specifies the different functionalities properties and architectural dependencies associated with the entity. For example the configuration information associated with a database platform resource specifies the different properties associated with the database such as access control information table sizes etc. The configuration information associated with the database platform resource also specifies the storage hardware resource that stores the data associated with the database.

The configuration engine in conjunction with the token store and the configuration store provide a centralized configuration mechanism for configuring each of the entities. The configuration information associated with each entity is populated and maintained in the configuration store by the configuration engine . Within the configuration store configuration information associated with a particular entity is identified by the unique identity token associated with the particular entity. The configuration engine receives requests for configuration information from different entities and in response retrieves the associated configuration information from the configuration store and transmits the retrieved configuration information to the associated entities.

In operation when an entity becomes a part of the service cloud or interacts with the service cloud the configuration engine identifies and generates configuration information associated with the entity for transmission to the entity. When an entity first requests configuration information the configuration engine identifies the entity type associated with the entity and then generates configuration information for the entity based on default configuration information corresponding to the entity type. The default configuration information is stored in the configuration store . The configuration engine also assigns an identity token to the entity that uniquely identifies the entity in the cloud computing environment . The identity token is stored in the token store .

For subsequent requests for configuration information transmitted by the particular entity the configuration engine first extracts the identity token included in the request. The configuration engine next validates the identity token included in the request. To validate the identity token the configuration engine determines whether a record corresponding to the extracted identity token exists within the token store . The identity token is validated when a record corresponding to the identity token exists within the token store and the entity associated with the identity token is authorized to receive configuration as specified by the record corresponding to the extracted identity. Once the identity token is validated the configuration engine retrieves the configuration information associated with the entity from the configuration store based on the identity token. Again within the configuration store configuration information associated with a particular entity is identified by the unique identity token associated with the particular entity. The configuration information is then transmitted to the requesting entity.

In one embodiment the configuration engine performs a validation operation on an entity that requests to become a part of the service cloud or interact with the service cloud . To perform the validation operation the configuration engine validates the entity against other entities already a part of the service cloud that have the same entity type. The validation operation may also be performed using a set of pre defined validation metrics.

In another embodiment the configuration engine also stores a hierarchy of configuration information. When a request is received for a particular type of resource the configuration engine is configured to resolve the request using the hierarchy of configuration information to identify the particular entity in the service cloud to which the request is to be transmitted.

As previously described herein the predictive engine is a learning based engine that analyzes the operational details stored in the logs and metrics store and based on the analysis performs different remedial operations when necessary. The log metric analyzer included in the predictive engine analyzes the operational details collected by the log metrics engine to identify inefficiencies and usage patterns across the service cloud . More specifically the log metric analyzer identifies the types of transactions occurring across the different resources included in the service cloud . The log metric analyzer also identifies load imbalances across resources of the same resource type and identifies resources that are allocated to services and are under utilized by those services. In addition the log metric analyzer determines whether any of the performance metrics collected by the logging and metrics engine are significantly above or below pre defined thresholds.

The log metric analyzer also performs trend analysis by correlating the operational details gathered by the logging and metrics engine with templates and or patterns specified by the templates patterns . The templates patterns specify different trends that have been identified during the operation of the service cloud . The templates patterns may be associated with specific resources resource types and or services. The templates patterns may specify trends that are associated with capacity spikes and drop offs events that are associated with imminent failures etc. The templates patterns may be identified by the predictive engine over time or may be specified by network service administrators via the management platform described above.

The operations layer processes the analysis performed by the log metric analyzer to determine whether any remedial operations need to be performed by the predictive engine . A remedial operation is any action taken by the predictive engine to correct inefficiencies and or problems identified by the log metric analyzer or to avoid future inefficiencies and or problems identified by log metric analyzer based on the trend analysis. In some embodiments the operations layer determines whether remedial operations are needed based on cost metrics and quality metrics identified for a particular service interacting with the service cloud . In some cases the cost of providing a better performing service may be greater than the cost metric identified for the service. In such cases remedial operations may not be performed. In other cases the quality of a service may not increase proportionally to the cost of increasing the performance of the service. In such cases remedial operations may not be performed.

As another example based on the analysis performed by log metric analyzer the operations layer determines that a certain event indicating a failure has occurred and therefore performs one or more pre emptive actions to avoid that failure. As another example based on the trend analysis performed by log metric analyzer the operations layer predicts that the usage of a certain service will spike and therefore allocates extra resources to the service. As yet another example based on the analysis performed by log metric analyzer the operations layer determines that a particular resource is underperforming and therefore alerts the network system administrator via the management platform to address the issue.

In such a manner the predictive engine monitors the performance of the service cloud and the services interacting with the service cloud . In monitoring the performance the predictive engine also develops knowledge with respect to correlations between certain patterns and trends and certain events. This knowledge is valuable in effectively scaling the resources allocated to services and allowing the service cloud to be elastic and flexible.

The method begins at step where the configuration engine receives a request for configuration information from an entity. Again for a particular entity the associated configuration information specifies the different functionalities properties and architectural dependencies associated with the entity. The configuration information associated with each entity is maintained and populated by the configuration engine in the configuration store . The entity therefore does not need to maintain any configuration information.

At step the configuration engine extracts the identity token from the request. As previously described herein each entity in the cloud computing environment is associated with a unique identity token. When transmitting a request for configuration information the entity includes the identity token associated with the entity in the request so the entity can be identified and authorized.

At step the configuration engine validates the identity token. To validate the identity token the configuration engine determines whether a record corresponding to the extracted identity token exists within the token store . The identity token is validated when a record corresponding to the identity token exists within the token store and the entity associated with the identity token is authorized to receive configuration as specified by the record corresponding to the extracted identity.

At step the configuration engine retrieves the configuration information associated with the entity from the configuration store based on the identity token. At step the configuration engine transmits the retrieved configuration information to the requesting entity.

The method begins at step where the resource allocation engine identifies the resource types needed by the service. In one embodiment the resource types needed by the service are defined in system requirements and external dependencies specified by the service. At step the resource allocation engine identifies the performance requirements associated with the service. Again the performance requirements associated with the service specify thresholds related to specific performance metrics that the service should meet when the service operates. The performance requirements may specify different thresholds for performance metrics such as data throughput latency and resource utilization.

At step the resource allocation engine determines whether based on the performance requirements the availability of the resources should be optimized. If at step the resource allocation engine determines that the availability of the resources should be optimized then the method proceeds to step . At step the resource allocation engine performs resource allocation according to a location sparsity algorithm. The details of one implementation of the location sparsity algorithm are described below in conjunction with .

If however at step the resource allocation engine determines that the availability of the resources need not be optimized then the method proceeds to step . At step the resource allocation engine determines whether based on the performance requirements the latency across the service should be minimized. If at step the resource allocation engine determines that the latency across the service should be minimized then the method proceeds to step . At step the resource allocation engine performs resource allocation according to a location density algorithm. The details of one implementation of the location density algorithm are described below in conjunction with .

If however at step the resource allocation engine determines that the latency across the service need not be minimized then the method proceeds to step . At step the resource allocation engine performs resource allocation according to a load balancing algorithm. For the load balancing algorithm the resource allocation engine allocates resources such that resources associated with the same resource type have a balanced utilization levels.

At step for a particular resource type needed by the service as identified at step the resource allocation engine determines the median utilization level across all resources associated with the particular resource type. At step the resource allocation engine identifies one or more locations of specific resources that are below or at the median utilization level determined at step . At step per location the resource with the lowest utilization is allocated to the service. Steps are then repeated for each resource type needed by the service. In such a fashion the locations of the resources allocated to the service are spread out hence increasing the likelihood of high availability across the resources and thus the service.

At step for a particular resource type needed by the service as identified at step the resource allocation engine determines the median utilization level across all resources associated with the particular resource type. At step the resource allocation engine identifies one or more locations of specific resources that are below or at the median utilization level determined at step . At step the resource allocation engine selects the location with the largest number of resources that are below or at the median utilization level. At step the resource allocation engine allocates the resources in the selected location to the service. Steps are then repeated for each resource type needed by the service. In such a fashion the locations of the resources allocated to the service are co located hence increasing the likelihood of low latency across the resources and thus the service.

The method begins at step where the log metric analyzer included in the predictive engine analyzes the operational details collected by the log metrics engine to identify inefficiencies and usage patterns across the operation of a service. More specifically the log metric analyzer identifies the types of transactions occurring across the different resources allocated to the service. The log metric analyzer also identifies load imbalances across resources of the same resource type and identifies resources that are allocated to the service and are under utilized by those services. At step based on this analysis the operations layer included in the predictive engine identifies modifications to resource allocations to the different services to rectify the inefficiencies identified by the log metric analyzer .

At step the log metric analyzer also performs trend analysis by correlating the operational details gathered by the logging and metrics engine with templates and or patterns specified by the templates patterns . The templates patterns specify different trends that have been identified during the operation of the service as well as the general operation of the service cloud . The templates patterns may specify trends that are associated with capacity spikes and drop offs events that are associated with imminent failures etc. At step the operations layer processes the trend analysis performed by the log metric analyzer to determine whether any remedial operations need to be performed by the predictive engine to avoid future problems failures or inefficiencies.

Advantageously the cloud computing environment described herein allows service developers to simply define the functionality of the service and describe the operating characteristics of the service. The allocation configuration and management of the resources and the services are then autonomously performed by engines within the cloud computing environment.

While the foregoing is directed to embodiments of the present invention other and further embodiments of the present invention may be devised without departing from the basic scope thereof. For example aspects of the present invention may be implemented in hardware or software or in a combination of hardware and software. One embodiment of the present invention may be implemented as a program product for use with a computer system. The program s of the program product define functions of the embodiments including the methods described herein and can be contained on a variety of computer readable storage media. Illustrative computer readable storage media include but are not limited to i non writable storage media e.g. read only memory devices within a computer such as CD ROM disks readable by a CD ROM drive flash memory ROM chips or any type of solid state non volatile semiconductor memory on which information is permanently stored and ii writable storage media e.g. floppy disks within a diskette drive or hard disk drive or any type of solid state random access semiconductor memory on which alterable information is stored. Such computer readable storage media when carrying computer readable instructions that direct the functions of the present invention are embodiments of the present invention.

