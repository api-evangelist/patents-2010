---

title: Programming model for collaborative distributed systems
abstract: Described are methods of providing data sharing between applications. The applications run on different computers, communicate via a network, and share a same distributed object. Each application maintains on its computer an invariant copy of the distributed object and a variant copy of the distributed object. Each application performs update operations to the distributed object, where such an update operation issued by a given one of the applications is performed by: executing the update operation on the variant copy maintained by the given application (i) without the given application waiting for the other applications to perform the operation (each invariant copy is guaranteed to converge to a same state) and (ii) at each of the applications, including the given application, executing the update operation on the corresponding invariant copies.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08799922&OS=08799922&RS=08799922
owner: Microsoft Corporation
number: 08799922
owner_city: Redmond
owner_country: US
publication_date: 20100525
---
Distributed systems or distributed applications function as a whole yet operate in components on different computers. Components may have local states and may communicate and cooperate by network to form a global state. In particular different applications on different hosts may share an object such that either may update the object and the object is treated as a single object. In some cases the distributed components may have some shared global data e.g. state of the shared object that each uses by way of a local copy. Programming such distributed systems is difficult for several reasons. When network latency is high communications between components may be delayed making it difficult to keep any shared state consistent. Furthermore logic to maintain concurrency between components when desired is inherently difficult to program and when executed may affect global and local performance. Various types of expected failures nodes links disks etc. create complications. The desire to maintain consistency between components on various machines may need to be balanced against performance requirements.

There are programming models for collaborative distributed applications such as where a group of users or clients are collaborating to perform a shared task. With a centralized synchronization model shared data may be maintained on a central server but this can affect responsiveness due to latency and contention at the server. On the other hand caching or replicating shared data locally at each machine may involve non trivial techniques to keep such replicated shared data consistent. An inherent tradeoff between responsiveness and consistency can be seen with two extremes of each approach. At one extreme a copy serializability model commits every action on shared data at the same time and once committed such changes are made visible to all machines at the same time. While reliable copy serializability is inherently slow. At the other extreme a replicated execution model provides each machine with its own local copy and only this copy of the shared data and local copies are updated independently. While performance is high there is little or no consistency between the states of the various machines.

Discussed below are techniques related to programming models that balance consistency and performance in distributed systems.

The following summary is included only to introduce some concepts discussed in the Detailed Description below. This summary is not comprehensive and is not intended to delineate the scope of the claimed subject matter which is set forth by the claims presented at the end.

Described are methods of providing data sharing between applications. The applications run on different computers communicate via a network and share a same distributed object. Each application maintains on its computer an invariant copy of the distributed object and a variant copy of the distributed object. Each application performs update operations to the distributed object where such an update operation issued by a given one of the applications is performed by executing the update operation on the variant copy maintained by the given application i without the given application waiting for the other applications to perform the operation each invariant copy is guaranteed to converge to a same state and ii at each of the applications including the given application executing the update operation on the corresponding invariant copies.

Many of the attendant features will be explained below with reference to the following detailed description considered in connection with the accompanying drawings.

Embodiments described herein relate to programming models for collaborative distributed applications. Such models may permit programmers to find reasonable balance between consistency and performance. Specifically embodiments may allow each machine in a distributed system to maintain a guesstimated or variant copy of the shared state of the system hence the term Guesstimate will be used to refer to these embodiments . At any point in time the guesstimated copy in each machine contains the expected outcome of the sequence of operations issued locally by that machine assuming no conflicts with operations executed by other machines. Eventually the sequences of operations on all machines converge to the same value or state. However until this happens each machine that issues an action or operation on shared global data proceeds i.e. does not block wait for remote or local completion independently using its current guesstimated copy of a shared object. This programming model may allow operations to be executed by any given machine on its guesstimated state without waiting for communication with or action by other machines sharing the same object. In practice this may be accomplished by having each operation execute multiple times once at the time of issue when it updates the guesstimated state of the issuing machine once when the operation is committed atomically on all machines and several times in between as the guesstimated state converges toward the committed state. The results of the executions from issue to commit are not guaranteed to be the same and constructs may be provided to allow the programmer to deal with such discrepancy.

Furthermore in embodiments of models described herein operations may transform shared objects from consistent state to consistent state. Consistency is explicitly specified i.e. by the programmer by preconditions on operations. If an issued operation fails its precondition on the local guesstimated state it is not issued globally and is rejected providing helpful response times and rapid feedback to permit special handling such as by resubmission . The same operation is executed again at a later time when the operation is committed atomically on all machines and the execution is on a different copy of the shared state which is distinct from the guesstimated state that is guaranteed to be identical on all machines. For some classes of applications these two executions of an operation may be expected to produce identical results most of the time but this is not guaranteed. In particular an operation could succeed when executed initially on the guesstimated state and could fail later during commitment on the committed state. Thus embodiments may allow the programmer to monitor and deal with this discrepancy.

Embodiments of such a programming model are described next. Operational semantics for the programming model will be described. A language extension to C to use the programming model will then be explained. The programming model will be described in parts with reference to an example multi player collaborative Sudoku puzzle which will be addressed first.

The distributed application implements a collaborative Sudoku game where players are distributed across a network e.g. the Internet . Different players collaboratively work on different portions of the puzzle using an instance of the application running on a respective local computer or machine each using a processor and memory to implement the application . The state of the puzzle is replicated locally in each of the machines . Programming such an application from the ground up with replicated state is non trivial. To manage the programming complexity each machine is provided with a model a synchronizer and a user interface UI each implemented by the corresponding local processor and memory in accordance with machine instructions obtainable by implementing embodiments and pseudo code described herein with known programming tools and techniques. The model maintains the state of the puzzle . The UI manages the user interface. The synchronizer performs synchronization between the states of the various machines .

The synchronizer is responsible for communication between all the replicated instances of the model on the machine there may be more than two . Whenever the game s model changes on any machine the corresponding synchronizer needs to propagate changes to all other machines . However synchronization may lead to various conflicts which the application should be able to handle. For instance there may be a conflict when two updates by two different machines would violate the constraints of Sudoku mentioned above. Or there could be a race condition when both the UI and the synchronizer try to simultaneously update the model in each machine . Programming the application to correctly take these issues into account is difficult. A guesstimate layer or module also referred to as Guesstimate may be provided as infrastructure that implements some of this complexity and that provides a programmer via an application programming interface API with abstract access to sharing synchronization logic thus allowing the programmer to focus on expressing the application logic. In other words the complex logic related to keeping a distributed applications state synchronized across various machines described below is handled by the guesstimate module which is a run time system that may be made available for use by any arbitrary distributed application. The guesstimate module will be referenced in and elsewhere as the Guesstimate namespace for example Guesstimate.CreateOperation . Implementation of the guesstimate framework or module will be described later.

How the guesstimate programming model can be used to program the Sudoku application will be described next. shows a Sudoku class . The Sudoku class implements the model that is to say the local copy of the state shared by multiple computers. The Sudoku class uses the guesstimate module by declaring shared objects and by deriving from the GSharedObject interface as shown in . In the Sudoku example the Sudoku class has a 9 9 array puzzle declared in line . The method Check defined in lines returns true if upon updating puzzle row col with val the constraints of Sudoku are satisfied. Otherwise Check returns false. The method Update defined in lines updates puzzle r c to value v if the Update is a legal update and returns true. If the update is not legal Update returns false. Other than deriving from the interface GSharedObject the programmer s implementation is much like an implementation for a non distributed application.

In lines of an event handler is coded to execute in response to an update event of row r column c with value v. In response to such an update event in the case of a non distributed program this code would simply call s.Update r c v . With use of the guesstimate module the programmer creates a new formal operation using Guesstimate.CreateOperation as seen in line and issues the operation using Guesstimate.IssueOperation as seen in line . When the operation is issued the guesstimate module executes the operation on the guesstimate copy of the Sudoku object. If the execution fails due to violation of any of the constraints of Sudoku that is s.Update r c v returns false the operation is dropped. Otherwise the guesstimate module submits the operation internally to a queue to be committed later simultaneously on all machines see the FORMAL MODEL section for details . The code in lines changes the color of the square to yellow if the update succeeded on the guesstimated copy and calls the Redraw function which repaints the user interface window.

At the time of commitment the operation s.Update r c v may succeed or fail. In particular the operation can fail if between the time when s.Update r c v was issued and the time when the operation is committed some other machines issued other operations that were committed and these other operations violate the precondition for execution of s.Update r c v .

After commitment the guesstimate module calls a completion function which is supplied as a delegate in the last argument of the Guesstimate.IssueOperation line . The completion function takes a boolean value as an argument. The boolean is the result of executing the shared operation in this case Update during commitment. The completion operation is used to change the colors on the UI as follows. If the update operation is successful the completion operation changes the color of the square at row r column c to GREEN and if the update fails the color is set to RED lines . The way colors are manipulated by the UI in this example demonstrates how the guesstimate module allows the programmer to deal with changes to the shared state if some operations succeeded initially but failed later due to conflicting intervening operations from other machines.

The guesstimate module also provides a facility to group a set of operations as being atomic with the semantics that either all of the operations in the group succeed or none of them succeed and all of the operations execute together with no other intervening operation. See the PROGRAMMING WITH GUESSTIMATE section for additional detail. In sum programming with Guesstimate may involve the steps shown in .

In this section the Guesstimate programming model is described formally. A distributed system is a pair where M is a tuple of M machines and S is a set of shared objects a machine mmay represent a computer for example . A shared state is a valuation to the set of shared objects. is the set of all shared states. In addition each machine m M maintains a local object L. A local state is a valuation to the local object. is the set of all local states.

Each participating machine can modify its local state using local operations that can read the shared state and local state but can update only local state. A local operation therefore has signature . As local operations do not change shared state they are not applied on other machines. Local operations may be thought of as ways to maintain information regarding submitted operations or ways to query the shared state before applying an operation to change the shared state. The set of all local operations is L.

A shared operation is the only kind of operation that is allowed to both read and update the shared state. Shared operations can succeed or fail. Each shared operation may be thought of as having a pre condition and the operation fails if it is attempted to be executed from a state a . For example if a shared operation tries to buy tickets on an airplane then it can fail if there are not enough seats available. Thus in addition to updating the shared state a shared operation returns a boolean value as well. A shared operation therefore has signature B . The set of all shared operations is S.

A completion operation reads the local state shared state and a boolean value and updates the local state. A completion operation therefore has signature B . The set of all completion operations is C.

A composite operation o is a pair where s S is a shared operation and c C is a completion operation. Typically a completion operation is associated with a shared operation and the boolean value produced by the shared operation is passed as an argument to the corresponding completion operation. The semantics of a composite operation can thus be understood as in b in 

To change the shared state each machine missues a sequence of composite operations o o . . . . Given a composite operation o we use the notation o or equivalently to denote a function with signature with the following semantics .let b s in 

The notation . is extended to map a sequence of composite operations to function with signature with the semantics 

Every shared operation executed by a machine m recall that each composite operation has a shared operation needs to be communicated and executed in each of the other machines m where j i. To ensure that each of the machines sees the same sequence of shared operations using Guesstimate each machine maintains two copies of the shared state 1 a committed or invariant copy which is guaranteed to be identical across all machines and 2 a guesstimated or variant copy which can diverge across machines but is guaranteed to eventually become identical when the system quiesces. When machine missues a composite operation o the shared operation s executes locally on the guesstimated state without the need for any blocking. If the operation s fails on the guesstimated state operation o is dropped. If the operation s succeeds then it is submitted in a pending queue associated with machine m. In the background a pending composite operation is picked from the front of some machine s pending queue committed and moved to a completed queue. During commitment the operation s is executed on all machines and the committed state in each machine is updated. During the commitment process the completion routine c will be run on the machine mwhich issued the operation and if the operation s fails then the completion routine c will have the opportunity to take remedial action.

Every time a remote operation commits the guesstimated state on each machine is re constructed from the committed state on that machine by copying the committed state and applying all operations in the pending queue of the machine which helps to ensure consistency when the system quiesces. Thus each machine satisfies the invariant that if all pending operations are executed on the committed state then the resulting value of the committed state is the same as the guesstimated state. Thus Guesstimate guarantees that when the system quiesces and the pending queues of all machines are empty the guesstimated state and the committed state of all the machines converge to the same value.

Formally the state of a machine mis a 5 tuple where is the local state at machine m C is a sequence of completed shared operations is the committed state at machine m P is a sequence of pending composite operations at machine m and is the guesstimated state in machine mobtained by applying the operator P to the state . That is every machine state should satisfy the invariant P .

The set of machine states is denoted by . Given a machine state call as Local C as Completed as committed P as Pending and as Guesstimate . The state of the distributed system is a function from machine index to with the invariant that for any pair of machines mand m Completed i Completed j Committed i Committed j .

A programmer can combine multiple shared operations to form hierarchical operations. A shared operation can have a hierarchical structure as defined in the grammar below 

An AtomicOp is treated as an indivisible operation with All or Nothing semantics. That is if any one operation were to fail none of the operations that are part of the atomicOp will update the shared state. If all the operations that are part of the atomic succeed then the shared state is updated by all of them. The OrElseOp is a hierarchical operation that allows the programmer to specify an alternate operation in case the main one fails. That is when the programmer issues opOrElse op then at most one of opor opis allowed to succeed with priority given to op. The programmer is allowed to arbitrarily nest operations of the 2 kinds.

An example implementation of Guesstimate as an extension to C will be described next. It should be noted that the Guesstimate programming model may be implemented in any language or environment. Programming an existing single machine non distributed version of an application for Guesstimate may involve the following changes.

Any class that defines a shared object will derive from the GSharedObject interface. The interface expects the programmer to define a special Copy GsharedObject obj method which when passed a source object should copy the shared state from source to the this object. In addition the programmer adds features to make the application distributed in nature. These features include creating objects that can be shared by other users joining objects created by others and applying operations that modify the state of these objects. Guesstimate exposes the following methods to aid the programmer in doing so.

The GSharedObject CreateInstance Type type out string uniqueID method takes the type of the shared object as input and creates a unique object of that type and returns it to the client. The new object along with its unique identifier is registered with Guesstimate.

The List AvailableObjects method returns information about all objects in the system. A list of unique identifiers of objects and their types is returned.

The GSharedObject JoinInstance string uniqueID method when passed the uniqueID of an available object returns a reference to that object. In addition the machine is registered for future synchronizations on the object.

The sharedOp CreateOperation string id string methodName params object parameterList method takes as input the ID of an object the method name and the parameters of the method and returns a sharedOp.

The sharedOp CreateAtomic List ops method takes a list of shared operations as input and returns a hierarchical shared operation.

The sharedOp CreateOrElse sharedOp op1 sharedOp op2 method takes two shared operations as input and returns a new hierarchical shared operation.

The IssueOperation sharedOp s completionOp c method takes 2 arguments. The first parameter is a sharedOp s created using CreateOperation CreateAtomic or CreateOrElse. The second parameter is of type completionOp which is a delegate type defined inside Guesstimate with the signature delegate void CompletionOp bool v . Regarding parameter c the programmer can use this parameter to pass named functions within the program that match this signature or can pass anonymous lambdas as shown above. The IssueOperation method when passed a composite operation applies s to update the guesstimate state immediately without waiting for the operation to take effect on other machines . Further the operation is added to the pending list for committing on all machines. Guesstimate also remembers the machine m that submitted the operation. At the time of commitment the completion operation c is executed on machine m.

Apart from using this API to create shared objects and issue operations on shared objects the programmer can choose to implement the application as desired. It should be appreciated that the names of the methods described above reserved words in the Guesstimate namespace are only examples to convey the general idea of using a guesstimate copy. In general any variety of API may be crafted to implement concepts described in the FORMAL MODEL section. In other words the operational semantics in that section are sufficient to construct and use a Guesstimate type of model independent of implementation details. With this in mind an example runtime Guesstimate will now be described.

To communicate between machines the synchronizer may use Peer Channel a peer to peer P2P communication technology that is part of .NET 3.5 although any communication means may be used including simple ad hoc connection management. PeerChannel allows multiple machines to be combined together to form a mesh. Any member of the mesh can broadcast messages to all other members via a channel associated with the mesh i.e. application level multicasting . The Guesstimate framework uses two such meshes one for sending control signals and another for passing operations. Both meshes preferably contain all participating machines. Synchronization among the machines is done in a master slave mode. One of the machines is designated to be the master and is responsible for periodically initiating the synchronization process. The synchronization i.e. g the updating of the invariant states happens over 3 stages which are as follows.

1. AddUpdatesToMesh. In the first stage the pending operations at each machine are sent to all the other machines to form a consolidated pending list Pat each machine. Starting with the master each machine i on its turn sends all operations in P i as triples of the form via the Operations channel to all the other machines and then passes the turn to the next machine i 1 via a confirmation message on the Signals channel. The number of operations sent on the Operations channel by machine i is piggy backed on the confirmation message sent by machine i. As all machines see these confirmation messages and know the number of participating machines they know the number of operations to expect in P. Once confirmation messages from all the participants are received the master signals the beginning of the second stage.

2. ApplyUpdatesFromMesh. All the machines in parallel apply the operations in P. The machines wait until they receive all the expected operations and then apply operations from Pan in lexicographic order of the pair . Once an operation has been applied it is moved to the Completed list.

An operation in Pcan update the state of machine i in one of two ways. If the operation o were submitted by machine i then o updates i i to the value of o i i . If the operation were submitted by some machine j i then o updates i to s i . Once a machine has applied all operations in all Pit sends an acknowledgment on the Signals channel. After sending the acknowledgment the guesstimate state on i namely i is updated to P i i to reflect the changes made since the synchronization began. This is done by first copying the committed state to the guesstimated state by calling Copy and then applying each operation in P i .

3. FlagCompletion. Once all acknowledgments have been received by the master the synchronization is complete. During these 3 stages the guesstimate runtime may use reflection to transform operations into messages and then back into operations. Reflection is also used to apply a shared operation by calling the appropriate method on a shared object.

It can be shown that there is a simulation relation between the state transitions in the Guesstimate runtime and the rules for operational semantics discussed above. The proof for local operations corresponding to R and issuing composite operations corresponding to R are straightforward. For commitment of operations corresponding to R the crux of the argument is that even though commitment takes several rounds of messages starting with the master broadcasting operations during AddUpdatesToMesh to the last acknowledgment being received just before FlagCompletion all composite operations submitted during this duration can be thought of as being submitted either before or after the entire commitment operation completes. In particular all composite operations issued at machine i before the machine i broadcasts operations during AddUpdatesToMesh can be thought of as issued before the atomic commit and all composite operations issued at machine i after the broadcast by machine i can be thought of as issued after the atomic commit.

Concurrency control will be described next. Within the Guesstimate runtime concurrent updates to shared state are possible. Two kinds of locks are used internally by the Synchronizer to avoid races. Lock lockis used to protect the pending list and for each shared object s lockis used to protect the guesstimated state of the object. Lock lockis acquired every time an operation is added to the list every time the list is copied to the mesh and every time the list is used to update the guesstimate state after sending the acknowledgment in ApplyUpdatesFromMesh. The lock lockis acquired every time an operation is applied to the guesstimate state of object s and every time the consistent state of object s is copied on to the guesstimate state after synchronization.

Note that all these blocking synchronization operations are used from within the Synchronizer and none involve more than one machine. While the current implementation uses pessimistic concurrency control lock free implementations and optimistic concurrency control can also have been used. For example the pending list could be implemented as a lock free queue. Also if the shared state were large optimistic concurrency control would be effective as it would allow concurrent independent operations to be applied without blocking.

All or nothing semantics can be provided for atomic operations using copy on write. The first time an object is updated within an atomic operation a temporary copy of its state is made and from then on all updates within the atomic operation are made to this copy. If the atomic operation succeeds the temporary state is copied back to the shared state.

The Guesstimate runtime can also be provided with logic for adding and removing machines from the distributed system. When an application built with guesstimate is run on a new machine the runtime adds the machine to the Signals and Operations meshes. When a new machine starts up and intimates its presence to the master the master adds it to the list of devices and sends it both the list of available objects and the list of completed operations. The new device initializes its state based on this information and participates in future synchronizations. A machine that leaves the system intimates the master and the master removes this machine from the next synchronization onward.

Failure and fault tolerance are now discussed. When the master starts a new synchronization phase it assumes all machines are active. However if the synchronization is stalled for more than a threshold duration the master goes through the log of messages sent on the signals channel to detect the faulting machine. The master resends the signal to which the machine failed to respond. If the fault was transient the machine might respond to this resent signal. If the machine still does not respond the master removes the machine from the current synchronization phase and sends it a restart signal. On receiving the restart signal the machine shuts down the current instance of the application and starts it as a new process with the committed state and committed queue from the master and an empty pending queue. Thus pending operations in that machine are lost due to the restart. Master failure tolerance can be supported by designating a new machine as master if no synchronization messages are received for a threshold duration.

While a Sudoku example has been shown other applications such as online real time auctions message boards event planning etc. may also use the Guesstimate model. Groups of atomic operations may be useful in an event planning application where a user may wish to register for 2 events atomically. For example a user could decide to go to a party only if there is a car pool available.

Embodiments and features discussed above can be realized in the form of information stored in volatile or non volatile computer or device readable media. This is deemed to include at least media such as optical storage e.g. CD ROM magnetic media flash ROM or any current or future means of storing digital information. The stored information can be in the form of machine executable instructions e.g. compiled executable binary code source code bytecode or any other information that can be used to enable or configure computing devices to perform the various embodiments discussed above. This is also deemed to include at least volatile memory such as RAM and or virtual memory storing information such as CPU instructions during execution of a program carrying out an embodiment as well as non volatile media storing information that allows a program or executable to be loaded and executed. The embodiments and features can be performed on any type of computing device including portable devices workstations servers mobile wireless devices and so on.

