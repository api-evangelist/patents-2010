---

title: Methods, systems, and computer readable medium for tier-based data storage resource allocation and data relocation in a data storage array
abstract: According to one aspect, the subject matter described herein includes a method for tier-based slice allocation and data relocation in a data storage array. The method is performed at a data storage array including physical data storage capacity being logically divided into one or more logical units (LUs) and each of the one or more LUs being further subdivided into one or more slices, where the data storage array includes a resource pool being divided into a plurality of resource groupings, where each resource grouping includes one or more resources of a common type. The method includes receiving a slice allocation request for allocating a slice from the data storage array, where the slice allocation request includes at least one constraint. In response to receiving the slice allocation request, a slice that honors the at least one constraint is allocated.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08443163&OS=08443163&RS=08443163
owner: EMC Corporation
number: 08443163
owner_city: Hopkinton
owner_country: US
publication_date: 20100628
---
The subject matter described herein relates to the intelligent allocation of finite system resources. More particularly the subject matter described herein relates to methods systems and computer readable medium for tier based data storage resource allocation and data relocation in a data storage array.

A traditional storage array herein also referred to as a disk storage array disk array or simply array is a collection of hard disk drives operating together logically as a unified storage device. Storage arrays are designed to store large quantities of data. Storage arrays typically include one or more storage array processors SPs for processing input output I O requests and management type requests. Data storage resource allocation requests are generally generated from internal requests i.e. are not received externally to the data storage array . An SP is the controller for and primary interface to the storage array.

Storage systems may include one or more disk arrays. Disk arrays may use a variety of storage devices with various characteristics for providing storage. Each storage array may logically operate as a unified storage device. While such organization generally allows for a homogenous view of the storage devices it is sometime useful to organize the various storage devices into tiers or classes of storage. A tier is generally delineated by differences in at least one of the four following attributes price performance capacity and function. For example tier 1 storage devices may be comprised of storage media that is very reliable and very fast such as flash memory. Tier 2 storage devices may be comprised of storage media that are slower than tier 1 media but are very reliable e.g. a hard disk . For example tier 2 storage devices may include high performance disks such as 15 000 RPM Fibre Channel FC Disks. Tier 3 storage devices may be comprised of comparatively slower and cheaper storage media than either tier 1 or tier 2 such as 7200 RPM serial ATA SATA disks.

Performance of a storage array may be characterized by the array s total capacity response time and throughput. The capacity of a storage array is the maximum total amount of data that can be stored on the array. The response time of an array is the amount of time that it takes to read data from or write data to the array. The throughput of an array is a measure of the amount of data that can be transferred into or out of i.e. written to or read from the array over a given period of time.

The administrator of a storage array may desire to operate the array in a manner that maximizes throughput and minimizes response time. In general performance of a storage array may be constrained by both physical and temporal constraints. Examples of physical constraints include bus occupancy and availability excessive disk arm movement and uneven distribution of load across disks. Examples of temporal constraints include bus bandwidth bus speed spindle rotational speed serial versus parallel access to multiple read write heads and the size of data transfer buffers.

One factor that may limit the performance of a storage array is the performance of each individual storage component. For example the read access time of a disk storage array is constrained by the access time of the disk drive from which the data is being read. Read access time may be affected by physical characteristics of the disk drive such as the number of revolutions per minute of the spindle the faster the spin the less time it takes for the sector being read to come around to the read write head. The placement of the data on the platter also affects access time because it takes time for the arm to move to detect and properly orient itself over the proper track or cylinder for multihead multiplatter drives . Reducing the read write arm swing reduces the access time. Finally the type of drive interface may have a significant impact on overall disk array storage. For example a multihead drive that supports reads or writes on all heads in parallel will have a much greater throughput than a multihead drive that allows only one head at a time to read or write data.

Furthermore even if a disk storage array uses the fastest disks available the performance of the array may be unnecessarily limited if only one of those disks may be accessed at a time. In other words performance of a storage array whether it is an array of disks tapes flash drives or other storage entities may also be limited by system constraints such the number of data transfer buses available in the system and the density of traffic on each bus.

Thus to maximize performance of a storage array the operational load should be more or less evenly distributed across all physical resources so that each physical resource may operate at its own maximum capacity. Using a disk storage array as an example bandwidth and thus performance is maximized if all spindles are being accessed at the same time. 

Performance of a storage array may also be characterized by the total power consumption of the array. The administrator of a storage array may prefer to operate the array in a manner that minimizes power consumption green mode rather than maximizes performance brown mode . Operating a large storage array in green mode may not only reduce power consumption of the array itself and its associated costs but also may have indirect benefits associated with the reduction of heat being generated by the array. For example storage arrays typically are housed in an environmentally controlled room or site operating an array in green mode may reduce the heat that the air conditioning system must remove thus lowering the cost to run the site HVAC system. Furthermore semiconductor devices age faster in hot environments than in cold environments a storage device whether it is a hard disk drive flash drive or other will age faster if it is mounted in a rack such that it is surrounded by other heat generating storage devices than if it is in the same rack but surrounded by cool e.g. idle storage devices. Thus operating a storage array in green mode may increase the mean time between failure for the devices in the array.

Separate from but intimately related to performance maximization is the problem of underuse of scarce physical resources. Storage arrays are typically used to provide storage space for one or more computer file systems databases applications and the like. For this and other reasons it is common for storage arrays to be logically partitioned into chunks of storage space called logical units or LUs. This allows a unified storage array to appear as a collection of separate file systems network drives and or volumes.

The problem of underuse arises when for example an amount of storage space is allocated to but not used by an operating system program process or user. In this scenario the scarce and probably expensive resource disk storage space for example is unused by the entity that requested its allocation and thus unavailable for use by any other entity. In many cases the unused space cannot be simply given back. For example a database installation may require many terabytes of storage over the long term even though only a small fraction of that space may be needed when the database is first placed into operation. In short it is often the case that the large storage space will be eventually needed but it is not known exactly when the entire space will be needed. In the meantime the space lies unused and unavailable for any other use as well.

Recognizing that more storage space may be provisioned for operating systems programs and users than they may actually use at first the concept of a sparsely populated or thin logical unit TLU was developed. Unlike the more traditional fat or fully allocated logical unit FLU which is created by provisioning and allocating a certain amount of storage area a TLU is provisioned at creation but is not allocated any physical storage until the storage is actually needed. For example physical storage space may be allocated to the TLU upon receipt of an I O write request from a requesting entity referred to herein as a host . Upon receipt of the write request from the host the SP may then determine whether there is enough space already allocated to the TLU to store the data being written and if not allocate to the TLU additional storage space.

While thin logical units provide distinct advantages over fully allocated logical units i.e. where the entire storage space requested is actually allocated and reserved for the exclusive use of the requesting entity the manner in which the data storage resource e.g. slices are allocated across physical disks can have an enormous impact on the performance of the storage array. A slice is a portion of a logical partition of data stored on a physical disk device.

A na ve approach to allocation of storage for sparsely populated logical units i.e. one that does not take into consideration the underlying physical and temporal constraints of the storage array in general and of the FLU pool in particular may fail to meet the goals of the policy such as green or brown for example chosen by the administrator of the storage array. For example if the administrator desires to maximize performance i.e. a brown policy a storage processor using a na ve allocation method might allocate all of the slices from a single physical disk in which case the performance of the entire array may be needlessly constrained by the single disk and thus fail to meet the performance goals of the brown policy.

Systems that manage large numbers or amounts of resources often must impose organizational structures onto the collection of resources in order to manage the collection in a rational way. Preferably the organization is along natural boundaries that consider real physical characteristics and constraints of the collection and its individual components. The difficulties of managing large and complex collections of resources may be mitigated via the use of high level abstractions to represent in simplified form certain aspects of the system the collections or resources and the organization imposed thereon.

A large data storage array is an illustrative example. A traditional storage array herein also referred to as a disk storage array disk array or simply array is a collection of storage entities such as hard disk drives solid state drives tape drives or other entities used to store information for simplicity hereinafter referred to as disks operating together logically as a unified storage device. A storage array may be thought of as a system for managing a large amount of a resource i.e. a large number of disk sectors.

Management of the resource may include allocation of a portion the resource in response to allocation requests. In the storage array example portions of the storage array may be allocated to i.e. exclusively used by entities that request such allocation. One issue that may be considered during allocation of a resource is the selection process namely how to determine which unallocated portion of the collection of resources is to be allocated to the requesting entity.

Conventionally all resources of the same type are treated the same because it was assumed that the performance of components within the data storage array performed similarly and data would be stored and accessed evenly across the array. Initially this assumption may be valid because any performance differences between resources of the same type and any asymmetries in data usage are unknown. However as the data storage array fills up and the stored data is accessed some resources may be more heavily utilized than other resources of the same type and or resources of the same type may begin to perform differently. For example two identical 7 200 rpm disks may initially be assumed to have identical performance and share data storage and processing loads equally because the client initially stores 10 GB on each disk. However at some later point in time the client may either delete or rarely access the data stored on the second disk while constantly updating the files stored on the first disk. As a result the first disk may operate with slower performance. While the client may have previously been able to observe this inefficiency the client was unable to correct it because the client had no input or control regarding how slices were allocated or re allocated. For example no mechanism e.g. slice allocation policy currently exists for allocating a slice from a particular performance tier or other resource constraint specified by the client in a slice allocation request.

Accordingly there exists a need for methods systems and computer readable medium for tier based slice allocation and data relocation in a data storage array.

According to one aspect the subject matter described herein includes a method for tier based slice allocation and data relocation in a data storage array. The method is performed at a data storage array including physical data storage capacity being logically divided into one or more logical units LUs and each of the one or more LUs being further subdivided into one or more slices where the data storage array includes a resource pool being divided into a plurality of resource groupings where each resource grouping includes one or more resources of a common type. The method includes receiving a slice allocation request for a slice from the data storage array where the slice allocation request includes at least one constraint. In response to receiving the slice allocation request a slice that honors the at least one constraint is allocated.

According to another aspect the subject matter described herein includes a system tier based slice allocation and data relocation in a data storage array. The system includes a data storage array including physical data storage capacity being logically divided into one or more logical units LUs wherein each of the one or more LUs are further subdivided into one or more slices wherein the data storage array includes a resource pool that is divided into a plurality of resource groupings where each resource grouping includes one or more resources of a common type. A slice manager is configured to receive a slice allocation request for allocating a slice from the data storage array where the slice allocation request includes at least one constraint. The slice manager is further configured to allocate a slice that honors the at least one constraint in response to receiving the slice allocation request.

The subject matter described herein for tier based slice allocation and data relocation in a data storage array may be implemented in hardware software firmware or any combination thereof. As such the terms function or module as used herein refer to hardware software and or firmware for implementing the feature being described. In one exemplary implementation the subject matter described herein may be implemented using a non transitory computer readable medium having stored thereon computer executable instructions that when executed by the processor of a computer perform steps.

Exemplary non transitory computer readable media suitable for implementing the subject matter described herein include disk memory devices chip memory devices programmable logic devices and application specific integrated circuits. In addition a computer program product that implements the subject matter described herein may be located on a single device or computing platform or may be distributed across multiple devices or computing platforms.

As used herein a resource is a physical or logical resource of the system to be managed and a resource object is a data construct or other abstraction used to represent a specific instance of a resource. Examples of physical resources include processors buses memories or portions of memories storage entities or portions thereof. Examples of logical resources include accounts users quotas permissions access control lists account balances and timeslots.

As used herein an interface is a predefined mechanism by which certain functions or tasks are performed. For example an interface may be a function subroutine class method and the like. The interface abstraction is used so that the underlying manner of manipulating the objects is hidden from the entity that uses the interface e.g. invokes the function or method. This gives flexibility allowing a change of how the objects are stored e.g. lists hash tables b trees etc. without any external changes.

As used herein a thin logical unit or TLU is a sparsely populated LU provisioned at creation but which is not allocated any storage until the storage is actually needed.

As used herein a flare logical unit or fully allocated logical unit or FLU is a fully allocated LU which is created by provisioning and allocating a certain amount of storage.

As used herein a direct logical unit or DLU is a fully provisioned mapped LU with coarse mapping. Even though a DLU is seen as fully provisioned by a user internally storage space is allocated on as needed basis.

As used herein a mapped logical unit or MLU is a mapped LU i.e. a LU managed by the MLU driver i.e. a TLU or DLU.

As used herein a resource collection is a collection of resources having the same type e.g. Bus RG etc. .

As used herein a tier is a collection of storage of similar performance. Exemplary performance buckets may include high performance medium performance and low performance. 

As used herein a performance bucket is a higher level resource grouping or abstraction which is logically located above all other existing resource groups managed by the slice manager e.g. RG Bus etc. . While a performance bucket may correspond to a coarse tier this need not always be the case. Exemplary performance buckets may include high performance medium performance and low performance. 

As used herein a fine tier is an internal not user visible tier indicative of expected performance of that tier.

As used herein a coarse tier is an external user visible tier that aggregates the expected performance of one or more fine tiers.

As used herein a tier descriptor is value associated with a FLU which identifies a fine tier and embodies two properties the index for the coarse tier to which the fine tier belongs and the performance estimate for the fine tier.

As used herein a fine tier value is an internal performance value very specific to a particular LU which could be derived based on the disk type disk consumption revolutions per minute RPM etc.

As used herein a coarse tier value is a user visible performance value and this will be the aggregation of similarly performing LUs.

As used herein a request is a slice manager policy requesting that a slice manager use its best effort to satisfy one or more constraints but will not fail if there are other available slices.

As used herein a requirement is a slice manger policy requiring that a slice satisfy one or more constraints and will fail even if other slices are available but do not meet the policy.

As used herein an allocation is to set apart for a special purpose designate distribute according to a plan allot a certain amount of storage space in a pool.

The subject matter described herein includes methods systems and computer readable media for tier based slice allocation and data relocation in a data storage array. In contrast to conventional slice allocation algorithms which do not take into account a user specified performance tier preference the subject matter described below provides a mechanism for users to specify one or more constraints such as a tier preference when requesting a slice from a data storage array. For example this may be accomplished by adding a tier descriptor for each LU.

With the addition of a tier descriptor per FLU a client can request the slice manager to allocate a slice from a LU with a specific performance value. For example the end user may be able to specify a tier preference of high none or low for mapped LUs. Conceptually LUs with a tier preference of high may have their slices allocated from the slices in the highest performance bucket s LUs with a tier preference of low may have their slices allocated from slices in the lowest performance bucket s and LUNs with no tier preference may have their slices allocated from the entire pool using an existing splatter algorithm.

To support allocations based on the coarse tier value in addition to tracking slices on a RG and Bus basis a slice manager may track slices on a new layer called Performance Buckets. Performance buckets may form the highest level resource grouping atop of existing groups e.g. RAID Groups Buses etc. . All similarly performing slices may be tracked under one performance bucket and in one embodiment which will be described in greater detail below the number of performance buckets may match the number of coarse tiers.

Referring to storage array may include multiple storage devices which are typically hard disk drives but which may be tape drives flash memory flash drives other solid state drives or some combination of the above. In one embodiment storage devices may be organized into multiple shelves each shelf containing multiple storage devices . In the embodiment illustrated in storage array includes three shelves Shelf A Shelf B and Shelf CB. Shelves A C each contain seven storage devices D D. In one embodiment each storage device may be connected to one or more buses . In the embodiment illustrated in each shelf has two buses at least one of which connects to every device on each shelf . For example Shelf A has two buses Bus A and Bus B where Bus A is connected to devices D D and Bus B is connected to devices D D. Similarly Shelf B has two buses Bus C and Bus where Bus C is connected to devices D D and Bus D is connected to devices D D. Lastly Shelf C has one bus Bus E where Bus E is connected to devices D D. Remaining storage devices D D are not shown connected to any busses or associated with any RAID group for simplicity purposes only. It may be appreciated that the configuration of storage array as illustrated in is for illustrative purposes only and is not to be considered a limitation of the subject matter described herein.

In addition to the physical configuration storage devices may also be logically configured. For example multiple storage devices may be organized into redundant array of inexpensive disks RAID groups or RGs shown in being differently shaded as RG A RG B and RG C. Storage devices D D are shown organized into a first RAID group RG A while storage devices D D are organized into a second RAID group RG B and storage devices D D are organized into a third RAID group RG C. In one embodiment a RAID group may span multiple shelves and or multiple buses. Although RAID groups are composed of multiple storage devices a RAID group may be conceptually treated as if it were a single storage device.

Storage devices may be managed by one or more storage processors . Storage processors SPs may be responsible for allocating storage and maintaining information about how that allocated storage is being used. Storage processors may maintain information about the structure of the file system whose contents are being stored in the allocated slices. For example SP A may be connected to Bus A Bus C and Bus E for managing disks D D D D and D D. Similarly SP B may be connected to Bus B and Bus D for managing disks D D and D D.

In one implementation of storage array each logical unit may be associated with a slice allocation table SAT which is used to record information about each slice such as the TLU that is using the slice and whether the slice is free or allocated. The SAT may be stored in the logical unit or it may be stored outside the logical unit to which it is associated. In order to avoid contention between two or more storage processors attempting to modify a particular SAT each SAT may be controlled by only one storage processor . The storage processor that has been given ownership of a particular SAT is hereinafter referred to as the claiming SP for that SAT. Since the SAT for a logical unit contains information about slices within that logical unit the claiming SP of a SAT may be said to be the claiming SP of the logical unit also. The remaining storage processors that are not the claiming SP for a logical unit may be hereinafter referred to as the peer SP . Thus every logical unit may have one claiming SP and one or more peer SPs. Since the claiming SP may be determined for each logical unit individually logical units within the same RAID group may have different claiming SPs.

In addition to RAID groups each logical unit may be further subdivided into portions of a logical unit referred to as slices . In the embodiment illustrated in RG A LU A is shown divided into ten slices SLICE SLICE RG B LU B is shown divided into twenty slices SLICE SLICE and RG C LU C is shown divided into five slices SLICE SLICE. Slices may be allocated de allocated re allocated reserved or redistributed by slice manger .

Slice manager may be a software application or layer that is executed at least in part by one or more SPs . Slice manager may be responsible for implementing a slice allocation policy and or algorithm. For example slice manager may receive slice allocation requests service slice allocation requests and maintain relevant statistical information regarding slices.

As described above depending upon the physical characteristics of the system the collections of resources may have physical electrical or logical constraints on their use. Using the embodiment illustrated in for example a RAID group may span more than one shelf if the RAID controller is connected to at least one bus on both shelves if the RAID controller is associated with only one bus or with buses of only one shelf a RAID group cannot span multiple shelves . Similarly an LU may or may not span multiple storage entities depending on the configuration of storage array .

In the simplified view shown in FIGS. A B a pool of storage devices may be organized into multiple RAID groups where each RAID group may be further divided into a number of LUs from which slices are allocated Slices that are allocated may be physically located anywhere in storage array . As will be discussed in more detail below these slices may be located more or less contiguously but they may also be distributed more or less evenly across all physical resources depending on the slice selection and allocation policy or algorithm.

System resources can be organized into resource pools. For example array may contain a pool i.e. collection of shelves RAID groups buses storage entities LUs and slices . Thus resources to be allocated by slice manager include slices where slices are portions of logical units logical units are portions of RAID groups and RAID groups occupy one or more buses . In one embodiment each resource object may include a header identifying the data structure as a generic resource manager RM object and slice statistics indicating how many total slices exist within that resource how many of the total slices are allocated etc. In alternative embodiments slice statistics may be data stored within the resource object a pointer to slice statistics stored elsewhere or a function call to a slice statistics manager that provides this information for example.

According to various aspects of the subject matter described herein slice manger may support various policies which are described in greater detail below.

In one embodiment slice manger may support a specific performance range policy. This policy indicates that only a specific range of tier values is acceptable for the new slice. If a slice is not available within that tier range the allocation will fail.

When a required performance range is applied the range may be treated from highest to lowest expected performance when choosing the best performance bucket resource. The highest performance bucket resource with available slices within the range may be used to satisfy the request if any additional constraints cannot be met within that performance bucket the next highest performance bucket with available slices within the range may be used until the performance range is exhausted. While the specific performance range policy described above may most often be used when selecting a destination slice for a relocation operation rather than for initial allocation this is not intended to be limiting.

In another embodiment slice manger may support a highest tier preference policy. This policy indicates that the slice should be allocated from the highest tier that has available slices and meets any other selection criteria. This policy may be used for initial allocations.

In another embodiment slice manger may support a lowest tier preference policy. This policy indicates that a slice should be allocated from the lowest tier that has available slices and meets any other selection criteria. This policy may also be used for initial allocations.

In another embodiment slice manger may support a no tier preference policy. This policy indicates that the requesting LU does not have a preference as to which tier its slices are initially allocated from. In this case a performance bucket with the least allocated space may be selected. Other slice selection algorithms may also be used without departing from the scope of the subject matter described herein and are more fully described in U.S. patent application Ser. No. 12 164 959 entitled Methods Systems and Computer Readable Medium for Dynamic Policy Based Allocation of System Resources. This policy may also be used for initial allocations.

In another embodiment slice manger may support a specific RAID group policy. This policy indicates that the requesting LU requests or requires a slice from a designated RAID group. If no slices are available from that RAID group or if there are no slices available that meet all other constraints the request may fail or not fail depending on whether it was a request or a requirement. It may be appreciated that a specific RAID group can belong to multiple busses and or multiple performance buckets and that therefore the selection algorithm may search across each bus and performance bucket that is allowed due to additional constraints . Typically this specific RAID group policy may be used in selecting a destination slice for a relocation operation and not for initial allocation.

In another embodiment slice manger may support a specific SP policy. This policy indicates that the requesting LU requests or requires a slice from a designated SP. In the case of a requirement if no slices are available from that SP or from that SP that meet all other constraints the request fails. Typically this policy may be used in selecting a destination slice for a relocation operation.

Just as the bus resource may be a collection of raid groups that contain physical disks on that bus the performance bucket resource may be a collection of FLUs that belong to the same performance bucket further grouped by their owning RAID groups and busses. It may be appreciated that a FLU may only be a child of a specific performance bucket but that both RAID groups and busses can exist in multiple performance buckets. This is analogous to RAID groups potentially existing on multiple busses in conventional designs. The difference is that if a single FLU can only exist in a single performance bucket the individual RAID group and bus resources need only track their slice usage within a single performance bucket.

A pool may be conceptually broken down into a set of performance buckets i.e. collections of slices with similar performance . It may be possible that the breakdown of performance buckets matches the breakdown of coarse tiers but it is not required.

In one embodiment the number of performance buckets may be fixed and may be derived from the tier descriptors of FLUs using a bitmask. From the tier descriptor the coarse tier ID can be obtained. The mask will determine how many of the most significant bits of the coarse tier ID will be used to create the performance tiers. For example a mask of 8 bits may indicate that the 8 most significant bits of the tier descriptor should be used for the performance tier breakdown and thus there will be 256 performance tiers i.e. 2 256 .

Referring to current resource hierarchy may include a plurality of LU resources i.e. LU resource group . For example the exemplary hierarchy shown in may include LUs A H. Each LU resource A H may be further divided into a plurality of slice resources slices . For example each of LUs A H are shown divided into two slices e.g. LU A may be divided into slice A and B and so on .

The logical resource grouping above LUs may include RAID group resource group i.e. RAID resources A D . For example multiple LUs such as LU A and B may be grouped together to form RAID group A. Above RAID group resources A D may be one or more bus resources. For example bus resource A may be associated with RAID groups A and B while bus resource B may be associated with RAID groups C and D.

However in contrast to previous storage array logical resource hierarchies a performance bucket resource tier may be implemented above bus resource tier RAID group tier LU resource tier and slice resource tier . All similarly performing slices will be tracked under one performance bucket A or B. Additionally resources may belong to multiple performance buckets . For example performance bucket A may be associated with resources A which includes slice resources J and K. Likewise performance bucket B may be associated with resources B which also includes slice resources J and K. Thus different performance buckets e.g. A and B may be associated with separate but overlapping or separate and non overlapping sets of data storage resources.

When a FLU is added to a pool a tier descriptor value may be assigned to that FLU. This value indicates a FLU s expected performance with higher expected performance indicating better expected performance than a FLU with a lower expected performance value. The FLU resource in addition to its owning RAID group and busses may be added to the appropriate performance bucket based on a SPLATTER MASK indicating how many bits of the coarse tier index derived from the tier descriptor are used to group coarse tiers. Specifically FLUs that are similar in performance as defined by the SPLATTER MASK as provided in their associated tier descriptor value may belong to the same performance bucket.

Just as other resources may track their slices the performance bucket resource may track the number of slices it contains and the number of slices used from that bucket. However unlike other slices the performance bucket resource may have multiple selection methods to choose the best resource. For example if the selection method is high preferred the performance bucket representing the highest performing slices with available slices may be selected. If the selection method is low preferred the performance bucket representing the lowest performing slices with available slices may be selected. This will require the selection logic to be able to order the buckets via a property specifically the tier descriptor prior to applying the selection algorithm.

At block the slice allocation request may be processed by allocating a slice that honors the at least one resource constraint. For example in one implementation a series of comparisons may be performed for each level of the resource hierarchy where multiple constraints may be applied to each resource. As described in U.S. patent application Ser. No. 12 164 959 entitled Methods Systems and Computer Readable Medium for Dynamic Policy Based Allocation of System Resources previous methods of slice allocation simply provided the best available slice in response to a slice allocation request by performing a series of comparisons may be performed for each level of the resource hierarchy that applied a single constraint to each resource. Specifically individual resource types currently contain a comparison routine with a single generic object that can be used as a filter to indicate if a particular resource is acceptable e.g. a specific FLU is only acceptable if it is within the RAID group that was selected earlier in the slice selection process. 

For tiering support multiple constraints may be provided that can be applied to each resource. Each resource may be updated to handle the new constraints and the resource manager may be updated to include new constraints introduced by the client and resources higher on the resource tree. It may be appreciated that that there is no requirement that all constraints be recognized by all resource types. For example a required SP constraint may not have to be recognized by the tier or RAID group resource types but may have to be recognized and utilized by the LU resource type.

While allocating a slice a selection constraint may be passed in by the clients that slice manager may enforce when selecting a slice. For example an exemplary constraint may include that the slice should to be selected should must come from a specific performance tier or a RAID group. The passed in selection constraint may be enforced on top of the pool s resource allocation policy. Any conflicting constraints may be handled by the clients. Poaching as used herein is when a second storage processor e.g. SP is allowed to have a slice allocated to it despite the fact that it has already been affined to a first storage processor e.g. SP . Affining is the association of an LU with a particular SP and may cause orphaning. Orphaning occurs when SP needs slices but the only available slices belong to an LU that has already been affined to SP. Unless poaching is allowed those slices are orphaned with respect to SP meaning that the slices will not be allocated to SP. Poaching may be used to avoid orphaning. For example for a pool with a non poaching policy the selection constraint passed by a client should not have the slice allocation SP as the one that is not the default allocation SP of a file system.

For preference types of high the performance bucket resource selection should start from the highest tier and work toward lower tiers and select the performance bucket of the highest tier with slices available. The first performance bucket found with available slices will be selected even if this means poaching from the other SP.

For preference type of low the performance bucket resource selection should start from the lowest tier and work toward higher tiers and select the performance bucket of the lowest tier with slices available. The first performance bucket found with available slices will be selected even if this means poaching from the other SP.

For preference type of none the performance bucket selection should start from the performance bucket with the least number of allocated slices. If that bucket contains no slices affined to the LUN s allocation owner however the performance bucket selection should continue to the bucket containing the next least number of allocated slices and should only poach from the other SP if no performance buckets contain slices affined to the LUN s allocation owner.

Once a performance bucket has been selected the existing splatter algorithm may be applied potentially with new constraints to choose the bus RAID group and FLU from which the slice is selected.

In order to calculate the number of slices available to a performance bucket the following calculations may be made. First the number of slices in each LU may be determined. For example LU contains 10 slices LU contains 20 slices LU contains 5 slices and LU contains 1 slice. Next the number of slices in each raid group may be determined. In the embodiment shown two assumptions may allow for easier calculation of the number of slices in each raid group. The first assumption may be that there is a 1 1 association between raid groups and LUs. The second assumption may be that each slice is shared equally among all disks in a particular raid group. This latter assumption may improve the performance of each slice because disk accesses may be spread equally among multiple disks. However it is appreciated that other relationships between slices and disks in a raid group may be made without departing from the scope of the subject matter described herein. Returning to the embodiment shown RG may include 10 slices RG may include 20 slices RG may contain 5 slices and RG may contain 1 slice. If multiple FLUs RAID groups or busses meet all requirements the existing splatter algorithm may be used to choose the FLU from the set of resources that meets the requirements. Because in this example PB PB and PB are associated directly with RAID groups rather than busses the number of slices available to each performance bucket is equal to the total number of slices available to each associated RAID group. It is appreciated that in the embodiment shown in a RAID group may only be associated with a single performance bucket. Therefore if RG is associated with PB RG may not also be associated with PB. Referring to the embodiment shown in PB is associated with RG and RG PB is associated with RG and PB is associated with RG. Therefore 30 slices are available to PB i.e. 10 20 5 slices are available to PB and slice is available to PB.

In a similar manner but disassociated from the accounting of the number of slices available to each performance bucket described above the number of slices associated with each bus may be calculated. According to exemplary bus descriptors not shown Bus is connected to the first three disks in RG and Bus is connected to the next two disks of RG and the first two disks of RG for a total of four disks Bus is connected to the next four disks of RG Bus is connected to the last two disks in RG and one disk in RG and Bus is connected to two disks in RG and one i.e. the only disk in RG. Therefore since each disk in RG contains 10 slices the total number of slices accessible by Bus is 30 i.e. 3 disks 10 slices disk . Similar calculations may be performed for the remaining busses. Specifically Bus may access a total of 60 slices i.e. 2 disks 10 slices disk 20 2 disks 20 slices disk 40 Bus may access a total of 80 slices i.e. 4 disks 20 slices disk Bus may access a total of 45 slices i.e. 2 disks 20 slices disk 40 1 disk 5 slices disk 5 Bus may access a total of 11 slices i.e. 2 disks 5 slices disk 1 slice .

Returning to it may be appreciated that slices have been associated with one or more coarse tiers e.g. high performance medium performance and low performance . Further slice manager may associate three performance buckets corresponding to the three tiers labeled PB PB and PB. In the embodiment shown in PB may include a high performance bucket PB may include a medium performance bucket and PB may include a low performance bucket. As discussed above it may be appreciated that in addition to being associated with performance different performance buckets may also be associated with for example high medium and low energy consumption or other metrics. Further it may be appreciated that the number of performance buckets may be greater or lesser than the number of coarse tiers to the without departing from the scope of the subject matter described herein.

A client can request that a slice be allocated from any of the three tiers or without any tier preference. Slice manager may use its default splattering algorithm to allocate a slice when a client does not specify any tier preference. Slice manager may determine which resource to choose at random by working its way through a hierarchy in order from performance bucket to bus to raid group to LU and finally to a slice. For example slice manager may select PB from among PB PB. Because PB is connected to Bus Bus and Bus slice manger may next select Bus at random from Bus Bus. Next as Bus is only associated with raid group slice manger must select from its only available choice and choose raid group . Likewise in the embodiment shown LU is the only LU associated with RG and therefore it must also be selected by slice manager . Finally one of the 20 slices associated with LU may be chosen by slice manager .

Additional details of the default splattering algorithm are described more fully in commonly owned U.S. patent application Ser. No. 12 164 959 entitled Methods Systems and Computer Readable Medium for Dynamic Policy Based Allocation of System Resources. In addition to randomly selected each available resource at each level of the hierarchy a static pre defined policy may also be applied for traversing the hierarchy.

Next an embodiment is described where a client requests that the slice allocated by the slice manager is selected from a particular performance bucket but is not correctly performed. This will be contrasted with an embodiment according to the subject matter described herein which dynamically adjusts the search order for traversing the hierarchy such that the slice allocation request may be properly processed.

According to a na ve approach if a client specifies a tier preference while allocating a slice slice manager may first identify the performance bucket associated with that slice. After that slice manager may proceed with its default search order to find the best resource for allocating a slice. However as mentioned above slice manager default s search mechanism may result in selecting a slice that is from a tier that is not requested by client. The problem occurs because of the fact that a Bus object could be a part of more than one performance bucket. Because of this the slice statistics in a bus object includes all the slices regardless of their performance values. Since slice manager relies on slice statistics for picking the best resource it ends up picking an incorrect slice.

For example in the event that client requests a slice from a high performance tier slice manager may select PB as the high performance bucket associated with the high performance tier. Next Bus may be selected from among Bus and Bus and RG may be selected from among RG and RG. Like above because only LU is associated with RG LU may be selected as the only choice once RG is selected and finally slice may be selected from LU. However slice is not a high performance slice. Thus by following a traditional search order the allocated slice may satisfy all searching rules but violate the performance bucket requirement.

Referring to in contrast to the example scenario shown in where a slice allocation request for a high performance slice resulted in allocating a slice from LU i.e. not a high performance slice by dynamically determining the search order of resource hierarchy the correct high performance slice may be allocated from LU. For example in response to receiving a slice allocation request for a high performance slice PB may bypass bus resources e.g. Bus and Bus and instead determine the appropriate RAID group. In this example PB is associated with only one RAID group RG which is selected. Likewise in this example RG is associated with only one LU LU from which a slice is allocated. However it is appreciated that the dynamic search order process described herein may be applied to performance buckets that are associated with multiple RAID groups and or LUs without departing from the scope of the subject matter described herein.

According to one aspect when a FLU is added to the pool the tier definition value for that FLU may be obtained. The tier definition value may determine which performance bucket contains this FLU. Similar to the other resource types the total slices for the performance bucket for that FLU may also be updated to include the unallocated slices from that FLU. It may be appreciated that this assumes that all slices on a FLU have the same tier definition value but that other embodiments are intended to be within the scope of the subject matter described herein.

In one embodiment it may be preferable to place reservations in specific performance buckets. For example if a high tier preference LUN set a reservation of 20 it may be preferable for the set of 20 reserved slices to be from the highest tier available at that time. It may also be preferable to restrict non high tier preference LUNs from getting those 20 reserved slices the same principle applies to low tier preference LUNs . The subject matter described herein does not attempt to keep the no preference LUNs from getting higher or lower performing slices. Instead no preference LUN slice selection may typically be splattered across all available slices.

According to another aspect slice allocation requests may be associated with a slice relocation. When a new slice is requested as a destination slice for a relocation operation the constraints can be firmer in order to avoid potentially causing different performance than that which is being explicitly requested.

To this end additional constraints for required performance range specific raid group and specific SP may be added. This allows an external component such as the Policy Engine to move a slice to a specific performance range to a specific raid group for load balancing and to require that the slice comes from a specific SP to avoid causing potentially worse performance due to redirection.

Once the best LU is found slice manager may use a bottom up approach rather than top down in updating the statistics in all the objects including Bus objects in the tree. Slice manager may need to track the number of unallocated reserved slices per tier preference. To initialize these numbers individual file systems may register their unallocated reserved counts during init or whenever their reservation needs change including changes in a LUN s tier preference property . Slice manager may need to view the unallocated reservations as unusable for slice selection requests coming from a LUN with a different tier preference.

Statistics about the system resources represented in the collections may be maintained. In one embodiment each resource object may include information pertinent to the selection process. In the embodiment illustrated in performance bucket objects bus objects RAID group objects and logical unit objects each maintain slice statistics. For example bus objects may keep track of how many slices have been allocated from storage entities that are connected to each bus. Similarly RAID group objects may keep track of the number of slices that it contains the number of slices that have been allocated the number of slices that are unallocated and so on. In one embodiment RAID group objects may keep track of the buses to which the RAID group has physically connected or logically associated. Logic unit objects may also include slice statistics.

It will be understood that various details of the subject matter described herein may be changed without departing from the scope of the subject matter described herein. Furthermore the foregoing description is for the purpose of illustration only and not for the purpose of limitation.

